{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import numpy \n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wide Residual Network (WRN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 1)) (1.18.1)\n",
      "Requirement already satisfied: torch>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 2)) (1.4.0)\n",
      "Requirement already satisfied: torchvision in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (0.5.0)\n",
      "Requirement already satisfied: fvcore in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (0.1.1.post20200611)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 5)) (4.44.1)\n",
      "Requirement already satisfied: yacs in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 6)) (0.1.7)\n",
      "Requirement already satisfied: apex from git+https://github.com/NVIDIA/apex.git#egg=apex in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 7)) (0.1)\n",
      "Requirement already satisfied: termcolor in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 8)) (1.1.0)\n",
      "Requirement already satisfied: thop<0.0.31.post2004070130 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 9)) (0.0.31.post2001170342)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (1.14.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (7.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (5.3.1)\n",
      "Requirement already satisfied: portalocker in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (1.7.0)\n",
      "Requirement already satisfied: tabulate in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (0.8.7)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Sagemaker Notebook must be of type, conda_pytorch_p36\n",
    "\n",
    "!pip install -r '/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorboard in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (2.2.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (2.23.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.0.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.29.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (0.9.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.17.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.18.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (0.4.1)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (0.34.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.6.0.post3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (3.2.2)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (3.12.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.14.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (46.1.3.post20200330)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (2020.4.5.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (1.25.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (3.0.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (4.1.0)\n",
      "Requirement already satisfied: rsa<5.0,>=3.1.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (3.4.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard) (1.5.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard) (2.2.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Need to add this to requirements.txt\n",
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-12 01:08:42] __main__ INFO: \u001b[0mdevice: cuda\n",
      "cudnn:\n",
      "  benchmark: True\n",
      "  deterministic: False\n",
      "dataset:\n",
      "  name: CIFAR10\n",
      "  dataset_dir: ~/.torch/datasets/CIFAR10\n",
      "  image_size: 32\n",
      "  n_channels: 3\n",
      "  n_classes: 10\n",
      "model:\n",
      "  type: cifar\n",
      "  name: wrn\n",
      "  init_mode: kaiming_fan_in\n",
      "  vgg:\n",
      "    n_channels: [64, 128, 256, 512, 512]\n",
      "    n_layers: [2, 2, 3, 3, 3]\n",
      "    use_bn: True\n",
      "  resnet:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "  resnet_preact:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "  wrn:\n",
      "    depth: 28\n",
      "    initial_channels: 16\n",
      "    widening_factor: 10\n",
      "    drop_rate: 0.0\n",
      "  densenet:\n",
      "    depth: 100\n",
      "    n_blocks: [6, 12, 24, 16]\n",
      "    block_type: bottleneck\n",
      "    growth_rate: 12\n",
      "    drop_rate: 0.0\n",
      "    compression_rate: 0.5\n",
      "  pyramidnet:\n",
      "    depth: 272\n",
      "    n_blocks: [3, 24, 36, 3]\n",
      "    initial_channels: 16\n",
      "    block_type: bottleneck\n",
      "    alpha: 200\n",
      "  resnext:\n",
      "    depth: 29\n",
      "    n_blocks: [3, 4, 6, 3]\n",
      "    initial_channels: 64\n",
      "    cardinality: 8\n",
      "    base_channels: 4\n",
      "  shake_shake:\n",
      "    depth: 26\n",
      "    initial_channels: 96\n",
      "    shake_forward: True\n",
      "    shake_backward: True\n",
      "    shake_image: True\n",
      "  se_resnet_preact:\n",
      "    depth: 110\n",
      "    initial_channels: 16\n",
      "    se_reduction: 16\n",
      "    block_type: basic\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "train:\n",
      "  checkpoint: ''\n",
      "  resume: False\n",
      "  precision: O0\n",
      "  batch_size: 128\n",
      "  subdivision: 1\n",
      "  optimizer: sgd\n",
      "  base_lr: 0.1\n",
      "  momentum: 0.9\n",
      "  nesterov: True\n",
      "  weight_decay: 0.0005\n",
      "  no_weight_decay_on_bn: False\n",
      "  gradient_clip: 0\n",
      "  start_epoch: 0\n",
      "  seed: 0\n",
      "  val_first: True\n",
      "  val_period: 1\n",
      "  val_ratio: 0.1\n",
      "  use_test_as_val: False\n",
      "  output_dir: /home/ec2-user/SageMaker/experiments/wrn_28_10/exp00\n",
      "  log_period: 100\n",
      "  checkpoint_period: 100\n",
      "  use_tensorboard: True\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: True\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "  distributed: False\n",
      "  dist:\n",
      "    backend: nccl\n",
      "    init_method: env://\n",
      "    world_size: -1\n",
      "    node_rank: -1\n",
      "    local_rank: 0\n",
      "    use_sync_bn: False\n",
      "tensorboard:\n",
      "  train_images: False\n",
      "  val_images: False\n",
      "  model_params: False\n",
      "optim:\n",
      "  adam:\n",
      "    betas: (0.9, 0.999)\n",
      "  lars:\n",
      "    eps: 1e-09\n",
      "    threshold: 0.01\n",
      "  adabound:\n",
      "    betas: (0.9, 0.999)\n",
      "    final_lr: 0.1\n",
      "    gamma: 0.001\n",
      "scheduler:\n",
      "  epochs: 200\n",
      "  warmup:\n",
      "    type: none\n",
      "    epochs: 0\n",
      "    start_factor: 0.001\n",
      "    exponent: 4\n",
      "  type: multistep\n",
      "  milestones: [60, 120, 160]\n",
      "  lr_decay: 0.2\n",
      "  lr_min_factor: 0.001\n",
      "  T0: 10\n",
      "  T_mul: 1.0\n",
      "validation:\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: False\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "augmentation:\n",
      "  use_random_crop: True\n",
      "  use_random_horizontal_flip: True\n",
      "  use_cutout: False\n",
      "  use_random_erasing: False\n",
      "  use_dual_cutout: False\n",
      "  use_mixup: False\n",
      "  use_ricap: False\n",
      "  use_cutmix: False\n",
      "  use_label_smoothing: False\n",
      "  random_crop:\n",
      "    padding: 4\n",
      "    fill: 0\n",
      "    padding_mode: constant\n",
      "  random_horizontal_flip:\n",
      "    prob: 0.5\n",
      "  cutout:\n",
      "    prob: 1.0\n",
      "    mask_size: 16\n",
      "    cut_inside: False\n",
      "    mask_color: 0\n",
      "    dual_cutout_alpha: 0.1\n",
      "  random_erasing:\n",
      "    prob: 0.5\n",
      "    area_ratio_range: [0.02, 0.4]\n",
      "    min_aspect_ratio: 0.3\n",
      "    max_attempt: 20\n",
      "  mixup:\n",
      "    alpha: 1.0\n",
      "  ricap:\n",
      "    beta: 0.3\n",
      "  cutmix:\n",
      "    alpha: 1.0\n",
      "  label_smoothing:\n",
      "    epsilon: 0.1\n",
      "tta:\n",
      "  use_resize: False\n",
      "  use_center_crop: False\n",
      "  resize: 256\n",
      "test:\n",
      "  checkpoint: ''\n",
      "  output_dir: ''\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    pin_memory: False\n",
      "\u001b[32m[2020-06-12 01:08:42] __main__ INFO: \u001b[0menv_info:\n",
      "  pytorch_version: 1.4.0\n",
      "  cuda_version: 10.1\n",
      "  cudnn_version: 7603\n",
      "  num_gpus: 1\n",
      "  gpu_name: Tesla K80\n",
      "  gpu_capability: 3.7\n",
      "Files already downloaded and verified\n",
      "\u001b[32m[2020-06-12 01:08:46] __main__ INFO: \u001b[0mMACs  : 5.25G\n",
      "\u001b[32m[2020-06-12 01:08:46] __main__ INFO: \u001b[0m#params: 36.48M\n",
      "Selected optimization level O0:  Pure FP32 training.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\",)\n",
      "\u001b[32m[2020-06-12 01:08:46] __main__ INFO: \u001b[0mVal 0\n",
      "\u001b[32m[2020-06-12 01:09:06] __main__ INFO: \u001b[0mEpoch 0 loss 149.2974 acc@1 0.1016 acc@5 0.4968\n",
      "\u001b[32m[2020-06-12 01:09:06] __main__ INFO: \u001b[0mElapsed 19.51\n",
      "\u001b[32m[2020-06-12 01:09:06] __main__ INFO: \u001b[0mTrain 1 0\n",
      "\u001b[32m[2020-06-12 01:11:03] __main__ INFO: \u001b[0mEpoch 1 Step 100/351 lr 0.100000 loss 1.3510 (1.8388) acc@1 0.5156 (0.3375) acc@5 0.9688 (0.8554)\n",
      "\u001b[32m[2020-06-12 01:12:55] __main__ INFO: \u001b[0mEpoch 1 Step 200/351 lr 0.100000 loss 1.2392 (1.6225) acc@1 0.5859 (0.4142) acc@5 0.9219 (0.8912)\n",
      "\u001b[32m[2020-06-12 01:14:47] __main__ INFO: \u001b[0mEpoch 1 Step 300/351 lr 0.100000 loss 1.0389 (1.4733) acc@1 0.5938 (0.4683) acc@5 0.9453 (0.9117)\n",
      "\u001b[32m[2020-06-12 01:15:44] __main__ INFO: \u001b[0mEpoch 1 Step 351/351 lr 0.100000 loss 0.8749 (1.4157) acc@1 0.7422 (0.4900) acc@5 0.9531 (0.9183)\n",
      "\u001b[32m[2020-06-12 01:15:44] __main__ INFO: \u001b[0mElapsed 398.54\n",
      "\u001b[32m[2020-06-12 01:15:44] __main__ INFO: \u001b[0mVal 1\n",
      "\u001b[32m[2020-06-12 01:15:57] __main__ INFO: \u001b[0mEpoch 1 loss 1.5205 acc@1 0.5512 acc@5 0.9416\n",
      "\u001b[32m[2020-06-12 01:15:57] __main__ INFO: \u001b[0mElapsed 13.18\n",
      "\u001b[32m[2020-06-12 01:15:57] __main__ INFO: \u001b[0mTrain 2 351\n",
      "\u001b[32m[2020-06-12 01:17:49] __main__ INFO: \u001b[0mEpoch 2 Step 100/351 lr 0.100000 loss 0.8952 (0.9664) acc@1 0.6875 (0.6570) acc@5 0.9688 (0.9676)\n",
      "\u001b[32m[2020-06-12 01:19:40] __main__ INFO: \u001b[0mEpoch 2 Step 200/351 lr 0.100000 loss 0.8506 (0.9392) acc@1 0.7031 (0.6668) acc@5 0.9844 (0.9711)\n",
      "\u001b[32m[2020-06-12 01:21:31] __main__ INFO: \u001b[0mEpoch 2 Step 300/351 lr 0.100000 loss 0.7825 (0.8913) acc@1 0.7344 (0.6852) acc@5 0.9609 (0.9742)\n",
      "\u001b[32m[2020-06-12 01:22:28] __main__ INFO: \u001b[0mEpoch 2 Step 351/351 lr 0.100000 loss 0.8020 (0.8749) acc@1 0.7188 (0.6915) acc@5 0.9688 (0.9751)\n",
      "\u001b[32m[2020-06-12 01:22:28] __main__ INFO: \u001b[0mElapsed 390.36\n",
      "\u001b[32m[2020-06-12 01:22:28] __main__ INFO: \u001b[0mVal 2\n",
      "\u001b[32m[2020-06-12 01:22:41] __main__ INFO: \u001b[0mEpoch 2 loss 1.1052 acc@1 0.6380 acc@5 0.9674\n",
      "\u001b[32m[2020-06-12 01:22:41] __main__ INFO: \u001b[0mElapsed 13.15\n",
      "\u001b[32m[2020-06-12 01:22:41] __main__ INFO: \u001b[0mTrain 3 702\n",
      "\u001b[32m[2020-06-12 01:24:33] __main__ INFO: \u001b[0mEpoch 3 Step 100/351 lr 0.100000 loss 0.7831 (0.7023) acc@1 0.7109 (0.7559) acc@5 0.9766 (0.9832)\n",
      "\u001b[32m[2020-06-12 01:26:25] __main__ INFO: \u001b[0mEpoch 3 Step 200/351 lr 0.100000 loss 0.6979 (0.6925) acc@1 0.7656 (0.7617) acc@5 0.9844 (0.9847)\n",
      "\u001b[32m[2020-06-12 01:28:17] __main__ INFO: \u001b[0mEpoch 3 Step 300/351 lr 0.100000 loss 0.7937 (0.6754) acc@1 0.6719 (0.7665) acc@5 0.9766 (0.9855)\n",
      "\u001b[32m[2020-06-12 01:29:14] __main__ INFO: \u001b[0mEpoch 3 Step 351/351 lr 0.100000 loss 0.5718 (0.6700) acc@1 0.7812 (0.7677) acc@5 1.0000 (0.9858)\n",
      "\u001b[32m[2020-06-12 01:29:14] __main__ INFO: \u001b[0mElapsed 393.42\n",
      "\u001b[32m[2020-06-12 01:29:14] __main__ INFO: \u001b[0mVal 3\n",
      "\u001b[32m[2020-06-12 01:29:27] __main__ INFO: \u001b[0mEpoch 3 loss 0.7358 acc@1 0.7444 acc@5 0.9840\n",
      "\u001b[32m[2020-06-12 01:29:27] __main__ INFO: \u001b[0mElapsed 13.14\n",
      "\u001b[32m[2020-06-12 01:29:27] __main__ INFO: \u001b[0mTrain 4 1053\n",
      "\u001b[32m[2020-06-12 01:31:19] __main__ INFO: \u001b[0mEpoch 4 Step 100/351 lr 0.100000 loss 0.7088 (0.5784) acc@1 0.7734 (0.7997) acc@5 0.9766 (0.9891)\n",
      "\u001b[32m[2020-06-12 01:33:10] __main__ INFO: \u001b[0mEpoch 4 Step 200/351 lr 0.100000 loss 0.5448 (0.5710) acc@1 0.8047 (0.8023) acc@5 0.9844 (0.9902)\n",
      "\u001b[32m[2020-06-12 01:35:01] __main__ INFO: \u001b[0mEpoch 4 Step 300/351 lr 0.100000 loss 0.5873 (0.5641) acc@1 0.7891 (0.8053) acc@5 0.9844 (0.9905)\n",
      "\u001b[32m[2020-06-12 01:35:58] __main__ INFO: \u001b[0mEpoch 4 Step 351/351 lr 0.100000 loss 0.5835 (0.5624) acc@1 0.8125 (0.8062) acc@5 0.9922 (0.9904)\n",
      "\u001b[32m[2020-06-12 01:35:58] __main__ INFO: \u001b[0mElapsed 390.33\n",
      "\u001b[32m[2020-06-12 01:35:58] __main__ INFO: \u001b[0mVal 4\n",
      "\u001b[32m[2020-06-12 01:36:11] __main__ INFO: \u001b[0mEpoch 4 loss 0.8027 acc@1 0.7198 acc@5 0.9848\n",
      "\u001b[32m[2020-06-12 01:36:11] __main__ INFO: \u001b[0mElapsed 13.15\n",
      "\u001b[32m[2020-06-12 01:36:11] __main__ INFO: \u001b[0mTrain 5 1404\n",
      "\u001b[32m[2020-06-12 01:38:03] __main__ INFO: \u001b[0mEpoch 5 Step 100/351 lr 0.100000 loss 0.4746 (0.5229) acc@1 0.8438 (0.8198) acc@5 0.9844 (0.9919)\n",
      "\u001b[32m[2020-06-12 01:39:55] __main__ INFO: \u001b[0mEpoch 5 Step 200/351 lr 0.100000 loss 0.5740 (0.5106) acc@1 0.8125 (0.8249) acc@5 0.9844 (0.9920)\n",
      "\u001b[32m[2020-06-12 01:41:47] __main__ INFO: \u001b[0mEpoch 5 Step 300/351 lr 0.100000 loss 0.4620 (0.5079) acc@1 0.8672 (0.8249) acc@5 0.9844 (0.9920)\n",
      "\u001b[32m[2020-06-12 01:42:44] __main__ INFO: \u001b[0mEpoch 5 Step 351/351 lr 0.100000 loss 0.5374 (0.5043) acc@1 0.8047 (0.8260) acc@5 0.9922 (0.9918)\n",
      "\u001b[32m[2020-06-12 01:42:44] __main__ INFO: \u001b[0mElapsed 393.07\n",
      "\u001b[32m[2020-06-12 01:42:44] __main__ INFO: \u001b[0mVal 5\n",
      "\u001b[32m[2020-06-12 01:42:57] __main__ INFO: \u001b[0mEpoch 5 loss 0.5179 acc@1 0.8280 acc@5 0.9890\n",
      "\u001b[32m[2020-06-12 01:42:57] __main__ INFO: \u001b[0mElapsed 13.14\n",
      "\u001b[32m[2020-06-12 01:42:57] __main__ INFO: \u001b[0mTrain 6 1755\n",
      "\u001b[32m[2020-06-12 01:44:48] __main__ INFO: \u001b[0mEpoch 6 Step 100/351 lr 0.100000 loss 0.4280 (0.4533) acc@1 0.8281 (0.8454) acc@5 1.0000 (0.9945)\n",
      "\u001b[32m[2020-06-12 01:46:40] __main__ INFO: \u001b[0mEpoch 6 Step 200/351 lr 0.100000 loss 0.4437 (0.4596) acc@1 0.8594 (0.8432) acc@5 1.0000 (0.9932)\n",
      "\u001b[32m[2020-06-12 01:48:31] __main__ INFO: \u001b[0mEpoch 6 Step 300/351 lr 0.100000 loss 0.4878 (0.4658) acc@1 0.8047 (0.8404) acc@5 1.0000 (0.9931)\n",
      "\u001b[32m[2020-06-12 01:49:27] __main__ INFO: \u001b[0mEpoch 6 Step 351/351 lr 0.100000 loss 0.3557 (0.4643) acc@1 0.8984 (0.8413) acc@5 1.0000 (0.9931)\n",
      "\u001b[32m[2020-06-12 01:49:27] __main__ INFO: \u001b[0mElapsed 390.10\n",
      "\u001b[32m[2020-06-12 01:49:27] __main__ INFO: \u001b[0mVal 6\n",
      "\u001b[32m[2020-06-12 01:49:40] __main__ INFO: \u001b[0mEpoch 6 loss 0.5195 acc@1 0.8282 acc@5 0.9916\n",
      "\u001b[32m[2020-06-12 01:49:40] __main__ INFO: \u001b[0mElapsed 13.11\n",
      "\u001b[32m[2020-06-12 01:49:40] __main__ INFO: \u001b[0mTrain 7 2106\n",
      "\u001b[32m[2020-06-12 01:51:32] __main__ INFO: \u001b[0mEpoch 7 Step 100/351 lr 0.100000 loss 0.3299 (0.4316) acc@1 0.8828 (0.8527) acc@5 0.9922 (0.9937)\n",
      "\u001b[32m[2020-06-12 01:53:24] __main__ INFO: \u001b[0mEpoch 7 Step 200/351 lr 0.100000 loss 0.4876 (0.4308) acc@1 0.8359 (0.8546) acc@5 0.9844 (0.9939)\n",
      "\u001b[32m[2020-06-12 01:55:16] __main__ INFO: \u001b[0mEpoch 7 Step 300/351 lr 0.100000 loss 0.3476 (0.4253) acc@1 0.8672 (0.8550) acc@5 1.0000 (0.9941)\n",
      "\u001b[32m[2020-06-12 01:56:13] __main__ INFO: \u001b[0mEpoch 7 Step 351/351 lr 0.100000 loss 0.3771 (0.4278) acc@1 0.8750 (0.8540) acc@5 0.9922 (0.9940)\n",
      "\u001b[32m[2020-06-12 01:56:13] __main__ INFO: \u001b[0mElapsed 393.18\n",
      "\u001b[32m[2020-06-12 01:56:13] __main__ INFO: \u001b[0mVal 7\n",
      "\u001b[32m[2020-06-12 01:56:26] __main__ INFO: \u001b[0mEpoch 7 loss 0.6019 acc@1 0.8024 acc@5 0.9864\n",
      "\u001b[32m[2020-06-12 01:56:26] __main__ INFO: \u001b[0mElapsed 13.03\n",
      "\u001b[32m[2020-06-12 01:56:26] __main__ INFO: \u001b[0mTrain 8 2457\n",
      "\u001b[32m[2020-06-12 01:58:17] __main__ INFO: \u001b[0mEpoch 8 Step 100/351 lr 0.100000 loss 0.4205 (0.3968) acc@1 0.8516 (0.8641) acc@5 0.9922 (0.9955)\n",
      "\u001b[32m[2020-06-12 02:00:09] __main__ INFO: \u001b[0mEpoch 8 Step 200/351 lr 0.100000 loss 0.3896 (0.4008) acc@1 0.8828 (0.8633) acc@5 1.0000 (0.9958)\n",
      "\u001b[32m[2020-06-12 02:02:00] __main__ INFO: \u001b[0mEpoch 8 Step 300/351 lr 0.100000 loss 0.3574 (0.4071) acc@1 0.8828 (0.8612) acc@5 1.0000 (0.9951)\n",
      "\u001b[32m[2020-06-12 02:02:56] __main__ INFO: \u001b[0mEpoch 8 Step 351/351 lr 0.100000 loss 0.3294 (0.4073) acc@1 0.8984 (0.8609) acc@5 0.9922 (0.9950)\n",
      "\u001b[32m[2020-06-12 02:02:56] __main__ INFO: \u001b[0mElapsed 389.86\n",
      "\u001b[32m[2020-06-12 02:02:56] __main__ INFO: \u001b[0mVal 8\n",
      "\u001b[32m[2020-06-12 02:03:10] __main__ INFO: \u001b[0mEpoch 8 loss 0.6607 acc@1 0.7862 acc@5 0.9832\n",
      "\u001b[32m[2020-06-12 02:03:10] __main__ INFO: \u001b[0mElapsed 13.15\n",
      "\u001b[32m[2020-06-12 02:03:10] __main__ INFO: \u001b[0mTrain 9 2808\n",
      "\u001b[32m[2020-06-12 02:05:01] __main__ INFO: \u001b[0mEpoch 9 Step 100/351 lr 0.100000 loss 0.4545 (0.3811) acc@1 0.8516 (0.8698) acc@5 0.9844 (0.9952)\n",
      "\u001b[32m[2020-06-12 02:06:53] __main__ INFO: \u001b[0mEpoch 9 Step 200/351 lr 0.100000 loss 0.4129 (0.3827) acc@1 0.8672 (0.8689) acc@5 0.9766 (0.9950)\n",
      "\u001b[32m[2020-06-12 02:08:45] __main__ INFO: \u001b[0mEpoch 9 Step 300/351 lr 0.100000 loss 0.4982 (0.3881) acc@1 0.8203 (0.8678) acc@5 0.9922 (0.9950)\n",
      "\u001b[32m[2020-06-12 02:09:42] __main__ INFO: \u001b[0mEpoch 9 Step 351/351 lr 0.100000 loss 0.3990 (0.3889) acc@1 0.8750 (0.8677) acc@5 0.9922 (0.9950)\n",
      "\u001b[32m[2020-06-12 02:09:42] __main__ INFO: \u001b[0mElapsed 392.72\n",
      "\u001b[32m[2020-06-12 02:09:42] __main__ INFO: \u001b[0mVal 9\n",
      "\u001b[32m[2020-06-12 02:09:55] __main__ INFO: \u001b[0mEpoch 9 loss 0.5096 acc@1 0.8272 acc@5 0.9862\n",
      "\u001b[32m[2020-06-12 02:09:55] __main__ INFO: \u001b[0mElapsed 13.10\n",
      "\u001b[32m[2020-06-12 02:09:55] __main__ INFO: \u001b[0mTrain 10 3159\n",
      "\u001b[32m[2020-06-12 02:11:46] __main__ INFO: \u001b[0mEpoch 10 Step 100/351 lr 0.100000 loss 0.4650 (0.3549) acc@1 0.8359 (0.8791) acc@5 1.0000 (0.9956)\n",
      "\u001b[32m[2020-06-12 02:13:37] __main__ INFO: \u001b[0mEpoch 10 Step 200/351 lr 0.100000 loss 0.3444 (0.3632) acc@1 0.8906 (0.8766) acc@5 1.0000 (0.9960)\n",
      "\u001b[32m[2020-06-12 02:15:28] __main__ INFO: \u001b[0mEpoch 10 Step 300/351 lr 0.100000 loss 0.2622 (0.3681) acc@1 0.8984 (0.8743) acc@5 1.0000 (0.9956)\n",
      "\u001b[32m[2020-06-12 02:16:25] __main__ INFO: \u001b[0mEpoch 10 Step 351/351 lr 0.100000 loss 0.3723 (0.3692) acc@1 0.8906 (0.8740) acc@5 0.9844 (0.9955)\n",
      "\u001b[32m[2020-06-12 02:16:25] __main__ INFO: \u001b[0mElapsed 389.42\n",
      "\u001b[32m[2020-06-12 02:16:25] __main__ INFO: \u001b[0mVal 10\n",
      "\u001b[32m[2020-06-12 02:16:38] __main__ INFO: \u001b[0mEpoch 10 loss 0.6570 acc@1 0.7872 acc@5 0.9890\n",
      "\u001b[32m[2020-06-12 02:16:38] __main__ INFO: \u001b[0mElapsed 13.14\n",
      "\u001b[32m[2020-06-12 02:16:38] __main__ INFO: \u001b[0mTrain 11 3510\n",
      "\u001b[32m[2020-06-12 02:18:30] __main__ INFO: \u001b[0mEpoch 11 Step 100/351 lr 0.100000 loss 0.3721 (0.3440) acc@1 0.8672 (0.8849) acc@5 0.9922 (0.9966)\n",
      "\u001b[32m[2020-06-12 02:20:22] __main__ INFO: \u001b[0mEpoch 11 Step 200/351 lr 0.100000 loss 0.2868 (0.3590) acc@1 0.9219 (0.8793) acc@5 1.0000 (0.9960)\n",
      "\u001b[32m[2020-06-12 02:22:13] __main__ INFO: \u001b[0mEpoch 11 Step 300/351 lr 0.100000 loss 0.3955 (0.3614) acc@1 0.8828 (0.8779) acc@5 0.9922 (0.9958)\n",
      "\u001b[32m[2020-06-12 02:23:10] __main__ INFO: \u001b[0mEpoch 11 Step 351/351 lr 0.100000 loss 0.2949 (0.3613) acc@1 0.9141 (0.8778) acc@5 1.0000 (0.9959)\n",
      "\u001b[32m[2020-06-12 02:23:11] __main__ INFO: \u001b[0mElapsed 392.62\n",
      "\u001b[32m[2020-06-12 02:23:11] __main__ INFO: \u001b[0mVal 11\n",
      "\u001b[32m[2020-06-12 02:23:24] __main__ INFO: \u001b[0mEpoch 11 loss 0.6510 acc@1 0.7996 acc@5 0.9848\n",
      "\u001b[32m[2020-06-12 02:23:24] __main__ INFO: \u001b[0mElapsed 13.10\n",
      "\u001b[32m[2020-06-12 02:23:24] __main__ INFO: \u001b[0mTrain 12 3861\n",
      "\u001b[32m[2020-06-12 02:25:15] __main__ INFO: \u001b[0mEpoch 12 Step 100/351 lr 0.100000 loss 0.3215 (0.3385) acc@1 0.8672 (0.8849) acc@5 0.9922 (0.9965)\n",
      "\u001b[32m[2020-06-12 02:27:06] __main__ INFO: \u001b[0mEpoch 12 Step 200/351 lr 0.100000 loss 0.3610 (0.3478) acc@1 0.8984 (0.8818) acc@5 0.9922 (0.9960)\n",
      "\u001b[32m[2020-06-12 02:28:56] __main__ INFO: \u001b[0mEpoch 12 Step 300/351 lr 0.100000 loss 0.3395 (0.3521) acc@1 0.8672 (0.8801) acc@5 0.9922 (0.9961)\n",
      "\u001b[32m[2020-06-12 02:29:53] __main__ INFO: \u001b[0mEpoch 12 Step 351/351 lr 0.100000 loss 0.3409 (0.3541) acc@1 0.8984 (0.8795) acc@5 0.9922 (0.9960)\n",
      "\u001b[32m[2020-06-12 02:29:53] __main__ INFO: \u001b[0mElapsed 389.41\n",
      "\u001b[32m[2020-06-12 02:29:53] __main__ INFO: \u001b[0mVal 12\n",
      "\u001b[32m[2020-06-12 02:30:06] __main__ INFO: \u001b[0mEpoch 12 loss 0.5120 acc@1 0.8330 acc@5 0.9854\n",
      "\u001b[32m[2020-06-12 02:30:06] __main__ INFO: \u001b[0mElapsed 13.13\n",
      "\u001b[32m[2020-06-12 02:30:06] __main__ INFO: \u001b[0mTrain 13 4212\n",
      "\u001b[32m[2020-06-12 02:31:58] __main__ INFO: \u001b[0mEpoch 13 Step 100/351 lr 0.100000 loss 0.4677 (0.3379) acc@1 0.8438 (0.8843) acc@5 0.9844 (0.9966)\n",
      "\u001b[32m[2020-06-12 02:33:50] __main__ INFO: \u001b[0mEpoch 13 Step 200/351 lr 0.100000 loss 0.5226 (0.3454) acc@1 0.8125 (0.8815) acc@5 1.0000 (0.9967)\n",
      "\u001b[32m[2020-06-12 02:35:42] __main__ INFO: \u001b[0mEpoch 13 Step 300/351 lr 0.100000 loss 0.3116 (0.3447) acc@1 0.9141 (0.8831) acc@5 0.9922 (0.9964)\n",
      "\u001b[32m[2020-06-12 02:36:39] __main__ INFO: \u001b[0mEpoch 13 Step 351/351 lr 0.100000 loss 0.3492 (0.3438) acc@1 0.8594 (0.8832) acc@5 1.0000 (0.9963)\n",
      "\u001b[32m[2020-06-12 02:36:39] __main__ INFO: \u001b[0mElapsed 392.59\n",
      "\u001b[32m[2020-06-12 02:36:39] __main__ INFO: \u001b[0mVal 13\n",
      "\u001b[32m[2020-06-12 02:36:52] __main__ INFO: \u001b[0mEpoch 13 loss 0.5826 acc@1 0.8096 acc@5 0.9898\n",
      "\u001b[32m[2020-06-12 02:36:52] __main__ INFO: \u001b[0mElapsed 13.15\n",
      "\u001b[32m[2020-06-12 02:36:52] __main__ INFO: \u001b[0mTrain 14 4563\n",
      "\u001b[32m[2020-06-12 02:38:43] __main__ INFO: \u001b[0mEpoch 14 Step 100/351 lr 0.100000 loss 0.3666 (0.3310) acc@1 0.8438 (0.8852) acc@5 1.0000 (0.9974)\n",
      "\u001b[32m[2020-06-12 02:40:34] __main__ INFO: \u001b[0mEpoch 14 Step 200/351 lr 0.100000 loss 0.2010 (0.3311) acc@1 0.9141 (0.8860) acc@5 1.0000 (0.9970)\n",
      "\u001b[32m[2020-06-12 02:42:25] __main__ INFO: \u001b[0mEpoch 14 Step 300/351 lr 0.100000 loss 0.3326 (0.3359) acc@1 0.8594 (0.8846) acc@5 0.9844 (0.9967)\n",
      "\u001b[32m[2020-06-12 02:43:21] __main__ INFO: \u001b[0mEpoch 14 Step 351/351 lr 0.100000 loss 0.2629 (0.3402) acc@1 0.9141 (0.8839) acc@5 1.0000 (0.9964)\n",
      "\u001b[32m[2020-06-12 02:43:21] __main__ INFO: \u001b[0mElapsed 389.49\n",
      "\u001b[32m[2020-06-12 02:43:21] __main__ INFO: \u001b[0mVal 14\n",
      "\u001b[32m[2020-06-12 02:43:35] __main__ INFO: \u001b[0mEpoch 14 loss 0.5403 acc@1 0.8168 acc@5 0.9916\n",
      "\u001b[32m[2020-06-12 02:43:35] __main__ INFO: \u001b[0mElapsed 13.10\n",
      "\u001b[32m[2020-06-12 02:43:35] __main__ INFO: \u001b[0mTrain 15 4914\n",
      "\u001b[32m[2020-06-12 02:45:27] __main__ INFO: \u001b[0mEpoch 15 Step 100/351 lr 0.100000 loss 0.2936 (0.3125) acc@1 0.9141 (0.8934) acc@5 1.0000 (0.9979)\n",
      "\u001b[32m[2020-06-12 02:47:18] __main__ INFO: \u001b[0mEpoch 15 Step 200/351 lr 0.100000 loss 0.3103 (0.3195) acc@1 0.9141 (0.8910) acc@5 0.9922 (0.9969)\n",
      "\u001b[32m[2020-06-12 02:49:10] __main__ INFO: \u001b[0mEpoch 15 Step 300/351 lr 0.100000 loss 0.2340 (0.3168) acc@1 0.9219 (0.8910) acc@5 1.0000 (0.9971)\n",
      "\u001b[32m[2020-06-12 02:50:07] __main__ INFO: \u001b[0mEpoch 15 Step 351/351 lr 0.100000 loss 0.2825 (0.3242) acc@1 0.9062 (0.8886) acc@5 0.9922 (0.9968)\n",
      "\u001b[32m[2020-06-12 02:50:07] __main__ INFO: \u001b[0mElapsed 392.50\n",
      "\u001b[32m[2020-06-12 02:50:07] __main__ INFO: \u001b[0mVal 15\n",
      "\u001b[32m[2020-06-12 02:50:20] __main__ INFO: \u001b[0mEpoch 15 loss 0.5730 acc@1 0.8112 acc@5 0.9908\n",
      "\u001b[32m[2020-06-12 02:50:20] __main__ INFO: \u001b[0mElapsed 13.16\n",
      "\u001b[32m[2020-06-12 02:50:20] __main__ INFO: \u001b[0mTrain 16 5265\n",
      "\u001b[32m[2020-06-12 02:52:11] __main__ INFO: \u001b[0mEpoch 16 Step 100/351 lr 0.100000 loss 0.3234 (0.3062) acc@1 0.8594 (0.8944) acc@5 1.0000 (0.9977)\n",
      "\u001b[32m[2020-06-12 02:54:02] __main__ INFO: \u001b[0mEpoch 16 Step 200/351 lr 0.100000 loss 0.2810 (0.3135) acc@1 0.9062 (0.8932) acc@5 0.9844 (0.9967)\n",
      "\u001b[32m[2020-06-12 02:55:53] __main__ INFO: \u001b[0mEpoch 16 Step 300/351 lr 0.100000 loss 0.2798 (0.3210) acc@1 0.9062 (0.8906) acc@5 1.0000 (0.9966)\n",
      "\u001b[32m[2020-06-12 02:56:50] __main__ INFO: \u001b[0mEpoch 16 Step 351/351 lr 0.100000 loss 0.3094 (0.3212) acc@1 0.8828 (0.8902) acc@5 0.9922 (0.9965)\n",
      "\u001b[32m[2020-06-12 02:56:50] __main__ INFO: \u001b[0mElapsed 389.53\n",
      "\u001b[32m[2020-06-12 02:56:50] __main__ INFO: \u001b[0mVal 16\n",
      "\u001b[32m[2020-06-12 02:57:03] __main__ INFO: \u001b[0mEpoch 16 loss 0.4700 acc@1 0.8480 acc@5 0.9924\n",
      "\u001b[32m[2020-06-12 02:57:03] __main__ INFO: \u001b[0mElapsed 13.10\n",
      "\u001b[32m[2020-06-12 02:57:03] __main__ INFO: \u001b[0mTrain 17 5616\n",
      "\u001b[32m[2020-06-12 02:58:54] __main__ INFO: \u001b[0mEpoch 17 Step 100/351 lr 0.100000 loss 0.5177 (0.3020) acc@1 0.8125 (0.8938) acc@5 0.9922 (0.9969)\n",
      "\u001b[32m[2020-06-12 03:00:46] __main__ INFO: \u001b[0mEpoch 17 Step 200/351 lr 0.100000 loss 0.3355 (0.3095) acc@1 0.8906 (0.8943) acc@5 1.0000 (0.9963)\n",
      "\u001b[32m[2020-06-12 03:02:38] __main__ INFO: \u001b[0mEpoch 17 Step 300/351 lr 0.100000 loss 0.3280 (0.3104) acc@1 0.9062 (0.8938) acc@5 1.0000 (0.9967)\n",
      "\u001b[32m[2020-06-12 03:03:35] __main__ INFO: \u001b[0mEpoch 17 Step 351/351 lr 0.100000 loss 0.2879 (0.3135) acc@1 0.9062 (0.8922) acc@5 0.9922 (0.9967)\n",
      "\u001b[32m[2020-06-12 03:03:35] __main__ INFO: \u001b[0mElapsed 392.21\n",
      "\u001b[32m[2020-06-12 03:03:35] __main__ INFO: \u001b[0mVal 17\n",
      "\u001b[32m[2020-06-12 03:03:48] __main__ INFO: \u001b[0mEpoch 17 loss 0.5264 acc@1 0.8284 acc@5 0.9918\n",
      "\u001b[32m[2020-06-12 03:03:48] __main__ INFO: \u001b[0mElapsed 13.13\n",
      "\u001b[32m[2020-06-12 03:03:48] __main__ INFO: \u001b[0mTrain 18 5967\n",
      "\u001b[32m[2020-06-12 03:05:39] __main__ INFO: \u001b[0mEpoch 18 Step 100/351 lr 0.100000 loss 0.2303 (0.2839) acc@1 0.9375 (0.9046) acc@5 1.0000 (0.9971)\n",
      "\u001b[32m[2020-06-12 03:07:30] __main__ INFO: \u001b[0mEpoch 18 Step 200/351 lr 0.100000 loss 0.3031 (0.3054) acc@1 0.8672 (0.8957) acc@5 0.9922 (0.9970)\n",
      "\u001b[32m[2020-06-12 03:09:21] __main__ INFO: \u001b[0mEpoch 18 Step 300/351 lr 0.100000 loss 0.3462 (0.3081) acc@1 0.8594 (0.8958) acc@5 0.9922 (0.9969)\n",
      "\u001b[32m[2020-06-12 03:10:17] __main__ INFO: \u001b[0mEpoch 18 Step 351/351 lr 0.100000 loss 0.4298 (0.3103) acc@1 0.8359 (0.8952) acc@5 0.9922 (0.9969)\n",
      "\u001b[32m[2020-06-12 03:10:17] __main__ INFO: \u001b[0mElapsed 389.16\n",
      "\u001b[32m[2020-06-12 03:10:17] __main__ INFO: \u001b[0mVal 18\n",
      "\u001b[32m[2020-06-12 03:10:30] __main__ INFO: \u001b[0mEpoch 18 loss 0.4502 acc@1 0.8494 acc@5 0.9946\n",
      "\u001b[32m[2020-06-12 03:10:30] __main__ INFO: \u001b[0mElapsed 13.12\n",
      "\u001b[32m[2020-06-12 03:10:30] __main__ INFO: \u001b[0mTrain 19 6318\n",
      "\u001b[32m[2020-06-12 03:12:22] __main__ INFO: \u001b[0mEpoch 19 Step 100/351 lr 0.100000 loss 0.4348 (0.2972) acc@1 0.8438 (0.8969) acc@5 0.9922 (0.9974)\n",
      "\u001b[32m[2020-06-12 03:14:14] __main__ INFO: \u001b[0mEpoch 19 Step 200/351 lr 0.100000 loss 0.3725 (0.3018) acc@1 0.8828 (0.8964) acc@5 0.9922 (0.9968)\n",
      "\u001b[32m[2020-06-12 03:16:05] __main__ INFO: \u001b[0mEpoch 19 Step 300/351 lr 0.100000 loss 0.3469 (0.3030) acc@1 0.8516 (0.8962) acc@5 1.0000 (0.9967)\n",
      "\u001b[32m[2020-06-12 03:17:02] __main__ INFO: \u001b[0mEpoch 19 Step 351/351 lr 0.100000 loss 0.2459 (0.3039) acc@1 0.9062 (0.8959) acc@5 0.9922 (0.9965)\n",
      "\u001b[32m[2020-06-12 03:17:02] __main__ INFO: \u001b[0mElapsed 392.03\n",
      "\u001b[32m[2020-06-12 03:17:02] __main__ INFO: \u001b[0mVal 19\n",
      "\u001b[32m[2020-06-12 03:17:16] __main__ INFO: \u001b[0mEpoch 19 loss 0.4406 acc@1 0.8498 acc@5 0.9938\n",
      "\u001b[32m[2020-06-12 03:17:16] __main__ INFO: \u001b[0mElapsed 13.12\n",
      "\u001b[32m[2020-06-12 03:17:16] __main__ INFO: \u001b[0mTrain 20 6669\n",
      "\u001b[32m[2020-06-12 03:19:06] __main__ INFO: \u001b[0mEpoch 20 Step 100/351 lr 0.100000 loss 0.3891 (0.2933) acc@1 0.8516 (0.9012) acc@5 1.0000 (0.9972)\n",
      "\u001b[32m[2020-06-12 03:20:57] __main__ INFO: \u001b[0mEpoch 20 Step 200/351 lr 0.100000 loss 0.2078 (0.2949) acc@1 0.9141 (0.8996) acc@5 0.9922 (0.9975)\n",
      "\u001b[32m[2020-06-12 03:22:48] __main__ INFO: \u001b[0mEpoch 20 Step 300/351 lr 0.100000 loss 0.2876 (0.2968) acc@1 0.9141 (0.8986) acc@5 1.0000 (0.9974)\n",
      "\u001b[32m[2020-06-12 03:23:44] __main__ INFO: \u001b[0mEpoch 20 Step 351/351 lr 0.100000 loss 0.3533 (0.2981) acc@1 0.8672 (0.8980) acc@5 0.9922 (0.9973)\n",
      "\u001b[32m[2020-06-12 03:23:44] __main__ INFO: \u001b[0mElapsed 388.52\n",
      "\u001b[32m[2020-06-12 03:23:44] __main__ INFO: \u001b[0mVal 20\n",
      "\u001b[32m[2020-06-12 03:23:57] __main__ INFO: \u001b[0mEpoch 20 loss 0.4218 acc@1 0.8612 acc@5 0.9916\n",
      "\u001b[32m[2020-06-12 03:23:57] __main__ INFO: \u001b[0mElapsed 13.09\n",
      "\u001b[32m[2020-06-12 03:23:57] __main__ INFO: \u001b[0mTrain 21 7020\n",
      "\u001b[32m[2020-06-12 03:25:49] __main__ INFO: \u001b[0mEpoch 21 Step 100/351 lr 0.100000 loss 0.3568 (0.2672) acc@1 0.8750 (0.9075) acc@5 0.9922 (0.9980)\n",
      "\u001b[32m[2020-06-12 03:27:40] __main__ INFO: \u001b[0mEpoch 21 Step 200/351 lr 0.100000 loss 0.3668 (0.2880) acc@1 0.8984 (0.9030) acc@5 1.0000 (0.9978)\n",
      "\u001b[32m[2020-06-12 03:29:32] __main__ INFO: \u001b[0mEpoch 21 Step 300/351 lr 0.100000 loss 0.2352 (0.2930) acc@1 0.9141 (0.9004) acc@5 0.9922 (0.9974)\n",
      "\u001b[32m[2020-06-12 03:30:29] __main__ INFO: \u001b[0mEpoch 21 Step 351/351 lr 0.100000 loss 0.3611 (0.2954) acc@1 0.8750 (0.8994) acc@5 0.9922 (0.9975)\n",
      "\u001b[32m[2020-06-12 03:30:29] __main__ INFO: \u001b[0mElapsed 391.75\n",
      "\u001b[32m[2020-06-12 03:30:29] __main__ INFO: \u001b[0mVal 21\n",
      "\u001b[32m[2020-06-12 03:30:42] __main__ INFO: \u001b[0mEpoch 21 loss 0.5005 acc@1 0.8356 acc@5 0.9912\n",
      "\u001b[32m[2020-06-12 03:30:42] __main__ INFO: \u001b[0mElapsed 13.09\n",
      "\u001b[32m[2020-06-12 03:30:42] __main__ INFO: \u001b[0mTrain 22 7371\n",
      "\u001b[32m[2020-06-12 03:32:33] __main__ INFO: \u001b[0mEpoch 22 Step 100/351 lr 0.100000 loss 0.2613 (0.2752) acc@1 0.9219 (0.9062) acc@5 1.0000 (0.9984)\n",
      "\u001b[32m[2020-06-12 03:34:24] __main__ INFO: \u001b[0mEpoch 22 Step 200/351 lr 0.100000 loss 0.3814 (0.2845) acc@1 0.8594 (0.9027) acc@5 0.9922 (0.9979)\n",
      "\u001b[32m[2020-06-12 03:36:14] __main__ INFO: \u001b[0mEpoch 22 Step 300/351 lr 0.100000 loss 0.5473 (0.2914) acc@1 0.8125 (0.9006) acc@5 0.9922 (0.9975)\n",
      "\u001b[32m[2020-06-12 03:37:11] __main__ INFO: \u001b[0mEpoch 22 Step 351/351 lr 0.100000 loss 0.2072 (0.2914) acc@1 0.9375 (0.9006) acc@5 1.0000 (0.9975)\n",
      "\u001b[32m[2020-06-12 03:37:11] __main__ INFO: \u001b[0mElapsed 388.84\n",
      "\u001b[32m[2020-06-12 03:37:11] __main__ INFO: \u001b[0mVal 22\n",
      "\u001b[32m[2020-06-12 03:37:24] __main__ INFO: \u001b[0mEpoch 22 loss 0.4426 acc@1 0.8578 acc@5 0.9930\n",
      "\u001b[32m[2020-06-12 03:37:24] __main__ INFO: \u001b[0mElapsed 13.13\n",
      "\u001b[32m[2020-06-12 03:37:24] __main__ INFO: \u001b[0mTrain 23 7722\n",
      "\u001b[32m[2020-06-12 03:39:16] __main__ INFO: \u001b[0mEpoch 23 Step 100/351 lr 0.100000 loss 0.3926 (0.2702) acc@1 0.8516 (0.9089) acc@5 1.0000 (0.9983)\n",
      "\u001b[32m[2020-06-12 03:41:07] __main__ INFO: \u001b[0mEpoch 23 Step 200/351 lr 0.100000 loss 0.3076 (0.2849) acc@1 0.9219 (0.9033) acc@5 1.0000 (0.9979)\n",
      "\u001b[32m[2020-06-12 03:42:59] __main__ INFO: \u001b[0mEpoch 23 Step 300/351 lr 0.100000 loss 0.2704 (0.2828) acc@1 0.8828 (0.9034) acc@5 1.0000 (0.9977)\n",
      "\u001b[32m[2020-06-12 03:43:56] __main__ INFO: \u001b[0mEpoch 23 Step 351/351 lr 0.100000 loss 0.2978 (0.2854) acc@1 0.8984 (0.9025) acc@5 1.0000 (0.9977)\n",
      "\u001b[32m[2020-06-12 03:43:56] __main__ INFO: \u001b[0mElapsed 391.95\n",
      "\u001b[32m[2020-06-12 03:43:56] __main__ INFO: \u001b[0mVal 23\n",
      "\u001b[32m[2020-06-12 03:44:09] __main__ INFO: \u001b[0mEpoch 23 loss 0.4956 acc@1 0.8374 acc@5 0.9938\n",
      "\u001b[32m[2020-06-12 03:44:09] __main__ INFO: \u001b[0mElapsed 13.09\n",
      "\u001b[32m[2020-06-12 03:44:09] __main__ INFO: \u001b[0mTrain 24 8073\n",
      "\u001b[32m[2020-06-12 03:46:00] __main__ INFO: \u001b[0mEpoch 24 Step 100/351 lr 0.100000 loss 0.3280 (0.2688) acc@1 0.8672 (0.9077) acc@5 1.0000 (0.9977)\n",
      "\u001b[32m[2020-06-12 03:47:51] __main__ INFO: \u001b[0mEpoch 24 Step 200/351 lr 0.100000 loss 0.2588 (0.2783) acc@1 0.8984 (0.9043) acc@5 0.9922 (0.9974)\n",
      "\u001b[32m[2020-06-12 03:49:42] __main__ INFO: \u001b[0mEpoch 24 Step 300/351 lr 0.100000 loss 0.4109 (0.2822) acc@1 0.8828 (0.9029) acc@5 0.9766 (0.9972)\n",
      "\u001b[32m[2020-06-12 03:50:38] __main__ INFO: \u001b[0mEpoch 24 Step 351/351 lr 0.100000 loss 0.2109 (0.2831) acc@1 0.9062 (0.9025) acc@5 1.0000 (0.9973)\n",
      "\u001b[32m[2020-06-12 03:50:38] __main__ INFO: \u001b[0mElapsed 389.06\n",
      "\u001b[32m[2020-06-12 03:50:38] __main__ INFO: \u001b[0mVal 24\n",
      "\u001b[32m[2020-06-12 03:50:51] __main__ INFO: \u001b[0mEpoch 24 loss 0.4705 acc@1 0.8382 acc@5 0.9950\n",
      "\u001b[32m[2020-06-12 03:50:51] __main__ INFO: \u001b[0mElapsed 13.07\n",
      "\u001b[32m[2020-06-12 03:50:51] __main__ INFO: \u001b[0mTrain 25 8424\n",
      "\u001b[32m[2020-06-12 03:52:43] __main__ INFO: \u001b[0mEpoch 25 Step 100/351 lr 0.100000 loss 0.3004 (0.2633) acc@1 0.8906 (0.9109) acc@5 0.9922 (0.9981)\n",
      "\u001b[32m[2020-06-12 03:54:35] __main__ INFO: \u001b[0mEpoch 25 Step 200/351 lr 0.100000 loss 0.2634 (0.2742) acc@1 0.9062 (0.9075) acc@5 1.0000 (0.9978)\n",
      "\u001b[32m[2020-06-12 03:56:26] __main__ INFO: \u001b[0mEpoch 25 Step 300/351 lr 0.100000 loss 0.3479 (0.2789) acc@1 0.9062 (0.9066) acc@5 0.9922 (0.9975)\n",
      "\u001b[32m[2020-06-12 03:57:23] __main__ INFO: \u001b[0mEpoch 25 Step 351/351 lr 0.100000 loss 0.2719 (0.2788) acc@1 0.9062 (0.9071) acc@5 1.0000 (0.9975)\n",
      "\u001b[32m[2020-06-12 03:57:23] __main__ INFO: \u001b[0mElapsed 392.07\n",
      "\u001b[32m[2020-06-12 03:57:23] __main__ INFO: \u001b[0mVal 25\n",
      "\u001b[32m[2020-06-12 03:57:36] __main__ INFO: \u001b[0mEpoch 25 loss 0.4441 acc@1 0.8624 acc@5 0.9912\n",
      "\u001b[32m[2020-06-12 03:57:36] __main__ INFO: \u001b[0mElapsed 13.10\n",
      "\u001b[32m[2020-06-12 03:57:36] __main__ INFO: \u001b[0mTrain 26 8775\n"
     ]
    }
   ],
   "source": [
    "# Train the model per the settings specified in the original paper\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "!python train.py --config configs/cifar/wrn.yaml \\\n",
    "    model.wrn.depth 28 \\\n",
    "    model.wrn.widening_factor 10 \\\n",
    "    train.batch_size 128 \\\n",
    "    train.base_lr 0.1 \\\n",
    "    train.output_dir /home/ec2-user/SageMaker/experiments/wrn_28_10/exp00 \\\n",
    "    scheduler.epochs 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-12 23:27:03] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/wrn_28_10/exp00/checkpoint_00200.pth\n",
      "Files already downloaded and verified\n",
      "100%|███████████████████████████████████████████| 40/40 [00:50<00:00,  1.27s/it]\n",
      "\u001b[32m[2020-06-12 23:27:55] __main__ INFO: \u001b[0mElapsed 51.00\n",
      "\u001b[32m[2020-06-12 23:27:55] __main__ INFO: \u001b[0mLoss 0.1760 Accuracy 0.9578\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/wrn.yaml \\\n",
    "    model.wrn.depth 28 \\\n",
    "    model.wrn.widening_factor 10 \\\n",
    "    test.checkpoint /home/ec2-user/SageMaker/experiments/wrn_28_10/exp00/checkpoint_00200.pth \\\n",
    "    test.output_dir /home/ec2-user/SageMaker/experiments/wrn_28_10/exp00/test_results_0200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-12 23:25:26] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/wrn_28_10/exp00/checkpoint_00100.pth\n",
      "Files already downloaded and verified\n",
      "100%|███████████████████████████████████████████| 40/40 [00:50<00:00,  1.27s/it]\n",
      "\u001b[32m[2020-06-12 23:26:18] __main__ INFO: \u001b[0mElapsed 50.91\n",
      "\u001b[32m[2020-06-12 23:26:18] __main__ INFO: \u001b[0mLoss 0.2299 Accuracy 0.9311\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/wrn.yaml \\\n",
    "    model.wrn.depth 28 \\\n",
    "    model.wrn.widening_factor 10 \\\n",
    "    test.checkpoint /home/ec2-user/SageMaker/experiments/wrn_28_10/exp00/checkpoint_00100.pth \\\n",
    "    test.output_dir /home/ec2-user/SageMaker/experiments/wrn_28_10/exp00/test_results_0100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Testset</th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Original_Accuracy</th>\n",
       "      <th>Original_CI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wrn_28_10</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>100</td>\n",
       "      <td>0.2299</td>\n",
       "      <td>0.9311</td>\n",
       "      <td>95.9</td>\n",
       "      <td>(95.5, 96.3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wrn_28_10</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>200</td>\n",
       "      <td>0.1760</td>\n",
       "      <td>0.9578</td>\n",
       "      <td>95.9</td>\n",
       "      <td>(95.5, 96.3)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Model  Testset  Epoch    Loss  Accuracy  Original_Accuracy  \\\n",
       "0  wrn_28_10  cifar10    100  0.2299    0.9311               95.9   \n",
       "1  wrn_28_10  cifar10    200  0.1760    0.9578               95.9   \n",
       "\n",
       "    Original_CI  \n",
       "0  (95.5, 96.3)  \n",
       "1  (95.5, 96.3)  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write the results to a CSV file so that we can analyze later.\n",
    "import pandas as pd\n",
    "\n",
    "results = {'Model': ['wrn_28_10', 'wrn_28_10'],\n",
    "           'Testset': ['cifar10', 'cifar10'],\n",
    "           'Epoch': [100, 200],\n",
    "           'Loss': [0.2299, 0.1760],\n",
    "           'Accuracy': [0.9311, 0.9578],\n",
    "           'Original_Accuracy': [95.9, 95.9],\n",
    "           'Original_CI': [(95.5, 96.3), (95.5, 96.3)]\n",
    "           }\n",
    "\n",
    "df = pd.DataFrame(results, columns = ['Model', 'Testset', 'Epoch', 'Loss', 'Accuracy', \n",
    "                                      'Original_Accuracy', 'Original_CI'])\n",
    "\n",
    "df.to_csv('/home/ec2-user/SageMaker/experiments/wrn_28_10/exp00/results.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['preds', 'probs', 'labels', 'loss', 'acc']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-1.3674926 , -1.6152366 , -0.44728577, ..., -1.0346686 ,\n",
       "        -1.4118685 , -1.5809278 ],\n",
       "       [-0.10573413,  0.7069439 , -1.0454859 , ..., -1.8201877 ,\n",
       "         9.020155  , -1.1099735 ],\n",
       "       [-0.573276  ,  3.92051   , -1.4367174 , ..., -1.7010452 ,\n",
       "         6.67665   , -0.6017765 ],\n",
       "       ...,\n",
       "       [-1.6388117 , -1.4345514 , -0.49067357, ..., -0.44160566,\n",
       "        -1.5362647 , -1.6407192 ],\n",
       "       [-0.81282467,  8.434932  , -0.7057322 , ..., -1.4298878 ,\n",
       "        -0.2568018 , -0.4748483 ],\n",
       "       [-1.2681558 , -1.3762753 , -1.0645399 , ...,  8.957951  ,\n",
       "        -1.9260851 , -0.6347342 ]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Peak inside the output file for predictions\n",
    "import numpy as np\n",
    "output = '/home/ec2-user/SageMaker/experiments/wrn_28_10/exp00/test_results_0200/predictions.npz'\n",
    "npzfile = np.load(output)\n",
    "print(npzfile.files)\n",
    "npzfile['preds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the model checkpoints, configs, and results to S3 \n",
    "bucket='sagemaker-may29'\n",
    "prefix = 'sagemaker/results/original-models/wrn_28_10'\n",
    "path = '/home/ec2-user/SageMaker/experiments/wrn_28_10'\n",
    "\n",
    "s3_resource = boto3.resource(\"s3\", region_name=\"us-east-2\")\n",
    "\n",
    "def uploadDirectory(local_path,bucket_name,s3_prefix):\n",
    "\n",
    "    my_bucket = s3_resource.Bucket(bucket_name)\n",
    "    \n",
    "    for path, subdirs, files in os.walk(local_path):\n",
    "        path = path.replace(\"\\\\\",\"/\")\n",
    "        directory_name = path.replace(local_path,\"\")\n",
    "        for file in files:\n",
    "            #print(\"Local File:\", os.path.join(path, file))\n",
    "            #print(\"      Dest:\", s3_prefix+directory_name+'/'+file)\n",
    "            my_bucket.upload_file(os.path.join(path, file), s3_prefix+directory_name+'/'+file)\n",
    "    \n",
    "uploadDirectory(path,bucket,prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10.1 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/wrn.yaml \\\n",
    "    model.wrn.depth 28 \\\n",
    "    model.wrn.widening_factor 10 \\\n",
    "    dataset.name CIFAR101 \\\n",
    "    test.checkpoint /home/ec2-user/SageMaker/experiments/wrn_28_10/exp00/checkpoint_00200.pth \\\n",
    "    test.output_dir /home/ec2-user/SageMaker/experiments/wrn_28_10/exp00/test_results_0200_CIFAR101"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
