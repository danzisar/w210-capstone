{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import numpy \n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESNEXT_29_4x64D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without Containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 1)) (1.18.1)\n",
      "Requirement already satisfied: torch>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 2)) (1.4.0)\n",
      "Requirement already satisfied: torchvision in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (0.5.0)\n",
      "Requirement already satisfied: fvcore in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (0.1.1.post20200608)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 5)) (4.44.1)\n",
      "Requirement already satisfied: yacs in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 6)) (0.1.7)\n",
      "Requirement already satisfied: apex from git+https://github.com/NVIDIA/apex.git#egg=apex in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 7)) (0.1)\n",
      "Requirement already satisfied: termcolor in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 8)) (1.1.0)\n",
      "Requirement already satisfied: thop<0.0.31.post2004070130 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 9)) (0.0.31.post2001170342)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (1.14.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (7.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (5.3.1)\n",
      "Requirement already satisfied: portalocker in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (1.7.0)\n",
      "Requirement already satisfied: tabulate in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (0.8.7)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Sagemaker Notebook must be of type, conda_pytorch_p36\n",
    "\n",
    "!pip install -r '/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorboard in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (2.2.2)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.14.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (2.23.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.18.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (3.2.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.6.0.post3)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (0.34.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.0.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (0.9.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (0.4.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.29.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (46.1.3.post20200330)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (3.12.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.16.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (1.25.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (2020.4.5.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (3.0.4)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard) (1.5.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (4.1.0)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (3.4.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (0.2.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard) (2.2.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.1.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard) (0.4.8)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Need to add this to requirements.txt\n",
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-08 19:33:15] __main__ INFO: \u001b[0mdevice: cuda\n",
      "cudnn:\n",
      "  benchmark: True\n",
      "  deterministic: False\n",
      "dataset:\n",
      "  name: CIFAR10\n",
      "  dataset_dir: ~/.torch/datasets/CIFAR10\n",
      "  image_size: 32\n",
      "  n_channels: 3\n",
      "  n_classes: 10\n",
      "model:\n",
      "  type: cifar\n",
      "  name: resnext\n",
      "  init_mode: kaiming_fan_out\n",
      "  vgg:\n",
      "    n_channels: [64, 128, 256, 512, 512]\n",
      "    n_layers: [2, 2, 3, 3, 3]\n",
      "    use_bn: True\n",
      "  resnet:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "  resnet_preact:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "  wrn:\n",
      "    depth: 28\n",
      "    initial_channels: 16\n",
      "    widening_factor: 10\n",
      "    drop_rate: 0.0\n",
      "  densenet:\n",
      "    depth: 100\n",
      "    n_blocks: [6, 12, 24, 16]\n",
      "    block_type: bottleneck\n",
      "    growth_rate: 12\n",
      "    drop_rate: 0.0\n",
      "    compression_rate: 0.5\n",
      "  pyramidnet:\n",
      "    depth: 272\n",
      "    n_blocks: [3, 24, 36, 3]\n",
      "    initial_channels: 16\n",
      "    block_type: bottleneck\n",
      "    alpha: 200\n",
      "  resnext:\n",
      "    depth: 29\n",
      "    n_blocks: [3, 4, 6, 3]\n",
      "    initial_channels: 64\n",
      "    cardinality: 4\n",
      "    base_channels: 64\n",
      "  shake_shake:\n",
      "    depth: 26\n",
      "    initial_channels: 96\n",
      "    shake_forward: True\n",
      "    shake_backward: True\n",
      "    shake_image: True\n",
      "  se_resnet_preact:\n",
      "    depth: 110\n",
      "    initial_channels: 16\n",
      "    se_reduction: 16\n",
      "    block_type: basic\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "train:\n",
      "  checkpoint: ''\n",
      "  resume: False\n",
      "  precision: O0\n",
      "  batch_size: 128\n",
      "  subdivision: 1\n",
      "  optimizer: sgd\n",
      "  base_lr: 0.1\n",
      "  momentum: 0.9\n",
      "  nesterov: True\n",
      "  weight_decay: 0.0005\n",
      "  no_weight_decay_on_bn: False\n",
      "  gradient_clip: 0\n",
      "  start_epoch: 0\n",
      "  seed: 0\n",
      "  val_first: True\n",
      "  val_period: 1\n",
      "  val_ratio: 0.1\n",
      "  use_test_as_val: False\n",
      "  output_dir: /home/ec2-user/SageMaker/experiments/resnext_29_4x64d/exp00\n",
      "  log_period: 100\n",
      "  checkpoint_period: 100\n",
      "  use_tensorboard: True\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: True\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "  distributed: False\n",
      "  dist:\n",
      "    backend: nccl\n",
      "    init_method: env://\n",
      "    world_size: -1\n",
      "    node_rank: -1\n",
      "    local_rank: 0\n",
      "    use_sync_bn: False\n",
      "tensorboard:\n",
      "  train_images: False\n",
      "  val_images: False\n",
      "  model_params: False\n",
      "optim:\n",
      "  adam:\n",
      "    betas: (0.9, 0.999)\n",
      "  lars:\n",
      "    eps: 1e-09\n",
      "    threshold: 0.01\n",
      "  adabound:\n",
      "    betas: (0.9, 0.999)\n",
      "    final_lr: 0.1\n",
      "    gamma: 0.001\n",
      "scheduler:\n",
      "  epochs: 300\n",
      "  warmup:\n",
      "    type: none\n",
      "    epochs: 0\n",
      "    start_factor: 0.001\n",
      "    exponent: 4\n",
      "  type: multistep\n",
      "  milestones: [150, 225]\n",
      "  lr_decay: 0.1\n",
      "  lr_min_factor: 0.001\n",
      "  T0: 10\n",
      "  T_mul: 1.0\n",
      "validation:\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: False\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "augmentation:\n",
      "  use_random_crop: True\n",
      "  use_random_horizontal_flip: True\n",
      "  use_cutout: False\n",
      "  use_random_erasing: False\n",
      "  use_dual_cutout: False\n",
      "  use_mixup: False\n",
      "  use_ricap: False\n",
      "  use_cutmix: False\n",
      "  use_label_smoothing: False\n",
      "  random_crop:\n",
      "    padding: 4\n",
      "    fill: 0\n",
      "    padding_mode: constant\n",
      "  random_horizontal_flip:\n",
      "    prob: 0.5\n",
      "  cutout:\n",
      "    prob: 1.0\n",
      "    mask_size: 16\n",
      "    cut_inside: False\n",
      "    mask_color: 0\n",
      "    dual_cutout_alpha: 0.1\n",
      "  random_erasing:\n",
      "    prob: 0.5\n",
      "    area_ratio_range: [0.02, 0.4]\n",
      "    min_aspect_ratio: 0.3\n",
      "    max_attempt: 20\n",
      "  mixup:\n",
      "    alpha: 1.0\n",
      "  ricap:\n",
      "    beta: 0.3\n",
      "  cutmix:\n",
      "    alpha: 1.0\n",
      "  label_smoothing:\n",
      "    epsilon: 0.1\n",
      "tta:\n",
      "  use_resize: False\n",
      "  use_center_crop: False\n",
      "  resize: 256\n",
      "test:\n",
      "  checkpoint: ''\n",
      "  output_dir: ''\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    pin_memory: False\n",
      "\u001b[32m[2020-06-08 19:33:15] __main__ INFO: \u001b[0menv_info:\n",
      "  pytorch_version: 1.4.0\n",
      "  cuda_version: 10.1\n",
      "  cudnn_version: 7603\n",
      "  num_gpus: 1\n",
      "  gpu_name: Tesla K80\n",
      "  gpu_capability: 3.7\n",
      "Files already downloaded and verified\n",
      "\u001b[32m[2020-06-08 19:33:19] __main__ INFO: \u001b[0mMACs  : 2.75G\n",
      "\u001b[32m[2020-06-08 19:33:19] __main__ INFO: \u001b[0m#params: 17.56M\n",
      "Selected optimization level O0:  Pure FP32 training.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\",)\n",
      "\u001b[32m[2020-06-08 19:33:19] __main__ INFO: \u001b[0mVal 0\n",
      "\u001b[32m[2020-06-08 19:33:44] __main__ INFO: \u001b[0mEpoch 0 loss 3.5463 acc@1 0.1426 acc@5 0.4996\n",
      "\u001b[32m[2020-06-08 19:33:44] __main__ INFO: \u001b[0mElapsed 24.64\n",
      "\u001b[32m[2020-06-08 19:33:44] __main__ INFO: \u001b[0mTrain 1 0\n",
      "\u001b[32m[2020-06-08 19:36:08] __main__ INFO: \u001b[0mEpoch 1 Step 100/351 lr 0.100000 loss 2.2147 (7.2891) acc@1 0.1328 (0.1463) acc@5 0.7422 (0.5887)\n",
      "\u001b[32m[2020-06-08 19:38:24] __main__ INFO: \u001b[0mEpoch 1 Step 200/351 lr 0.100000 loss 1.9519 (4.7711) acc@1 0.2500 (0.1849) acc@5 0.8047 (0.6703)\n",
      "\u001b[32m[2020-06-08 19:40:41] __main__ INFO: \u001b[0mEpoch 1 Step 300/351 lr 0.100000 loss 1.9793 (3.8825) acc@1 0.2344 (0.2082) acc@5 0.7969 (0.7091)\n",
      "\u001b[32m[2020-06-08 19:41:51] __main__ INFO: \u001b[0mEpoch 1 Step 351/351 lr 0.100000 loss 1.9685 (3.6073) acc@1 0.2969 (0.2202) acc@5 0.8750 (0.7259)\n",
      "\u001b[32m[2020-06-08 19:41:51] __main__ INFO: \u001b[0mElapsed 486.57\n",
      "\u001b[32m[2020-06-08 19:41:51] __main__ INFO: \u001b[0mVal 1\n",
      "\u001b[32m[2020-06-08 19:42:08] __main__ INFO: \u001b[0mEpoch 1 loss 1.9309 acc@1 0.2942 acc@5 0.8088\n",
      "\u001b[32m[2020-06-08 19:42:08] __main__ INFO: \u001b[0mElapsed 17.05\n",
      "\u001b[32m[2020-06-08 19:42:08] __main__ INFO: \u001b[0mTrain 2 351\n",
      "\u001b[32m[2020-06-08 19:44:24] __main__ INFO: \u001b[0mEpoch 2 Step 100/351 lr 0.100000 loss 1.7875 (1.8641) acc@1 0.3594 (0.3301) acc@5 0.8906 (0.8573)\n",
      "\u001b[32m[2020-06-08 19:46:41] __main__ INFO: \u001b[0mEpoch 2 Step 200/351 lr 0.100000 loss 1.7158 (1.8141) acc@1 0.3516 (0.3443) acc@5 0.8750 (0.8660)\n",
      "\u001b[32m[2020-06-08 19:48:58] __main__ INFO: \u001b[0mEpoch 2 Step 300/351 lr 0.100000 loss 1.3859 (1.7688) acc@1 0.5156 (0.3589) acc@5 0.9453 (0.8737)\n",
      "\u001b[32m[2020-06-08 19:50:07] __main__ INFO: \u001b[0mEpoch 2 Step 351/351 lr 0.100000 loss 1.7792 (1.7492) acc@1 0.3047 (0.3666) acc@5 0.8750 (0.8774)\n",
      "\u001b[32m[2020-06-08 19:50:07] __main__ INFO: \u001b[0mElapsed 479.69\n",
      "\u001b[32m[2020-06-08 19:50:07] __main__ INFO: \u001b[0mVal 2\n",
      "\u001b[32m[2020-06-08 19:50:24] __main__ INFO: \u001b[0mEpoch 2 loss 1.5725 acc@1 0.4182 acc@5 0.8990\n",
      "\u001b[32m[2020-06-08 19:50:24] __main__ INFO: \u001b[0mElapsed 17.02\n",
      "\u001b[32m[2020-06-08 19:50:24] __main__ INFO: \u001b[0mTrain 3 702\n",
      "\u001b[32m[2020-06-08 19:52:41] __main__ INFO: \u001b[0mEpoch 3 Step 100/351 lr 0.100000 loss 1.5389 (1.5490) acc@1 0.4141 (0.4370) acc@5 0.9141 (0.9137)\n",
      "\u001b[32m[2020-06-08 19:54:58] __main__ INFO: \u001b[0mEpoch 3 Step 200/351 lr 0.100000 loss 1.2701 (1.5083) acc@1 0.5938 (0.4529) acc@5 0.9375 (0.9196)\n",
      "\u001b[32m[2020-06-08 19:57:14] __main__ INFO: \u001b[0mEpoch 3 Step 300/351 lr 0.100000 loss 1.3600 (1.4676) acc@1 0.5078 (0.4688) acc@5 0.9453 (0.9237)\n",
      "\u001b[32m[2020-06-08 19:58:24] __main__ INFO: \u001b[0mEpoch 3 Step 351/351 lr 0.100000 loss 1.2874 (1.4483) acc@1 0.5469 (0.4754) acc@5 0.9375 (0.9252)\n",
      "\u001b[32m[2020-06-08 19:58:24] __main__ INFO: \u001b[0mElapsed 479.74\n",
      "\u001b[32m[2020-06-08 19:58:24] __main__ INFO: \u001b[0mVal 3\n",
      "\u001b[32m[2020-06-08 19:58:41] __main__ INFO: \u001b[0mEpoch 3 loss 1.4432 acc@1 0.4928 acc@5 0.9246\n",
      "\u001b[32m[2020-06-08 19:58:41] __main__ INFO: \u001b[0mElapsed 17.04\n",
      "\u001b[32m[2020-06-08 19:58:41] __main__ INFO: \u001b[0mTrain 4 1053\n",
      "\u001b[32m[2020-06-08 20:00:58] __main__ INFO: \u001b[0mEpoch 4 Step 100/351 lr 0.100000 loss 1.2895 (1.2859) acc@1 0.4844 (0.5393) acc@5 0.9453 (0.9417)\n",
      "\u001b[32m[2020-06-08 20:03:15] __main__ INFO: \u001b[0mEpoch 4 Step 200/351 lr 0.100000 loss 1.2694 (1.2486) acc@1 0.6172 (0.5500) acc@5 0.9375 (0.9452)\n",
      "\u001b[32m[2020-06-08 20:05:31] __main__ INFO: \u001b[0mEpoch 4 Step 300/351 lr 0.100000 loss 1.0177 (1.2185) acc@1 0.6484 (0.5609) acc@5 0.9609 (0.9486)\n",
      "\u001b[32m[2020-06-08 20:06:41] __main__ INFO: \u001b[0mEpoch 4 Step 351/351 lr 0.100000 loss 0.8118 (1.2006) acc@1 0.7344 (0.5683) acc@5 0.9766 (0.9501)\n",
      "\u001b[32m[2020-06-08 20:06:41] __main__ INFO: \u001b[0mElapsed 479.98\n",
      "\u001b[32m[2020-06-08 20:06:41] __main__ INFO: \u001b[0mVal 4\n",
      "\u001b[32m[2020-06-08 20:06:58] __main__ INFO: \u001b[0mEpoch 4 loss 1.1614 acc@1 0.5998 acc@5 0.9496\n",
      "\u001b[32m[2020-06-08 20:06:58] __main__ INFO: \u001b[0mElapsed 17.05\n",
      "\u001b[32m[2020-06-08 20:06:58] __main__ INFO: \u001b[0mTrain 5 1404\n",
      "\u001b[32m[2020-06-08 20:09:15] __main__ INFO: \u001b[0mEpoch 5 Step 100/351 lr 0.100000 loss 0.9214 (1.0549) acc@1 0.6641 (0.6280) acc@5 0.9766 (0.9615)\n",
      "\u001b[32m[2020-06-08 20:11:32] __main__ INFO: \u001b[0mEpoch 5 Step 200/351 lr 0.100000 loss 1.1739 (1.0448) acc@1 0.5703 (0.6293) acc@5 0.9609 (0.9629)\n",
      "\u001b[32m[2020-06-08 20:13:48] __main__ INFO: \u001b[0mEpoch 5 Step 300/351 lr 0.100000 loss 0.8313 (1.0300) acc@1 0.6641 (0.6345) acc@5 0.9844 (0.9647)\n",
      "\u001b[32m[2020-06-08 20:14:58] __main__ INFO: \u001b[0mEpoch 5 Step 351/351 lr 0.100000 loss 1.0074 (1.0229) acc@1 0.6406 (0.6370) acc@5 0.9531 (0.9651)\n",
      "\u001b[32m[2020-06-08 20:14:58] __main__ INFO: \u001b[0mElapsed 479.86\n",
      "\u001b[32m[2020-06-08 20:14:58] __main__ INFO: \u001b[0mVal 5\n",
      "\u001b[32m[2020-06-08 20:15:15] __main__ INFO: \u001b[0mEpoch 5 loss 0.9489 acc@1 0.6704 acc@5 0.9650\n",
      "\u001b[32m[2020-06-08 20:15:15] __main__ INFO: \u001b[0mElapsed 17.04\n",
      "\u001b[32m[2020-06-08 20:15:15] __main__ INFO: \u001b[0mTrain 6 1755\n",
      "\u001b[32m[2020-06-08 20:17:32] __main__ INFO: \u001b[0mEpoch 6 Step 100/351 lr 0.100000 loss 0.8948 (0.9392) acc@1 0.6875 (0.6665) acc@5 0.9766 (0.9724)\n",
      "\u001b[32m[2020-06-08 20:19:49] __main__ INFO: \u001b[0mEpoch 6 Step 200/351 lr 0.100000 loss 1.0065 (0.9224) acc@1 0.6875 (0.6729) acc@5 0.9688 (0.9717)\n",
      "\u001b[32m[2020-06-08 20:22:05] __main__ INFO: \u001b[0mEpoch 6 Step 300/351 lr 0.100000 loss 0.9216 (0.9117) acc@1 0.6719 (0.6755) acc@5 0.9766 (0.9732)\n",
      "\u001b[32m[2020-06-08 20:23:15] __main__ INFO: \u001b[0mEpoch 6 Step 351/351 lr 0.100000 loss 0.9839 (0.9108) acc@1 0.6641 (0.6753) acc@5 0.9766 (0.9733)\n",
      "\u001b[32m[2020-06-08 20:23:15] __main__ INFO: \u001b[0mElapsed 479.81\n",
      "\u001b[32m[2020-06-08 20:23:15] __main__ INFO: \u001b[0mVal 6\n",
      "\u001b[32m[2020-06-08 20:23:32] __main__ INFO: \u001b[0mEpoch 6 loss 1.0766 acc@1 0.6306 acc@5 0.9658\n",
      "\u001b[32m[2020-06-08 20:23:32] __main__ INFO: \u001b[0mElapsed 17.01\n",
      "\u001b[32m[2020-06-08 20:23:32] __main__ INFO: \u001b[0mTrain 7 2106\n",
      "\u001b[32m[2020-06-08 20:25:49] __main__ INFO: \u001b[0mEpoch 7 Step 100/351 lr 0.100000 loss 0.8567 (0.8587) acc@1 0.7031 (0.6926) acc@5 0.9844 (0.9789)\n",
      "\u001b[32m[2020-06-08 20:28:05] __main__ INFO: \u001b[0mEpoch 7 Step 200/351 lr 0.100000 loss 0.9020 (0.8350) acc@1 0.7266 (0.7027) acc@5 0.9688 (0.9789)\n",
      "\u001b[32m[2020-06-08 20:30:22] __main__ INFO: \u001b[0mEpoch 7 Step 300/351 lr 0.100000 loss 0.7331 (0.8243) acc@1 0.7578 (0.7073) acc@5 0.9688 (0.9789)\n",
      "\u001b[32m[2020-06-08 20:31:32] __main__ INFO: \u001b[0mEpoch 7 Step 351/351 lr 0.100000 loss 0.7594 (0.8186) acc@1 0.7344 (0.7086) acc@5 0.9922 (0.9795)\n",
      "\u001b[32m[2020-06-08 20:31:32] __main__ INFO: \u001b[0mElapsed 479.79\n",
      "\u001b[32m[2020-06-08 20:31:32] __main__ INFO: \u001b[0mVal 7\n",
      "\u001b[32m[2020-06-08 20:31:49] __main__ INFO: \u001b[0mEpoch 7 loss 0.8673 acc@1 0.7130 acc@5 0.9734\n",
      "\u001b[32m[2020-06-08 20:31:49] __main__ INFO: \u001b[0mElapsed 17.00\n",
      "\u001b[32m[2020-06-08 20:31:49] __main__ INFO: \u001b[0mTrain 8 2457\n",
      "\u001b[32m[2020-06-08 20:34:05] __main__ INFO: \u001b[0mEpoch 8 Step 100/351 lr 0.100000 loss 0.8017 (0.7527) acc@1 0.7578 (0.7358) acc@5 0.9766 (0.9828)\n",
      "\u001b[32m[2020-06-08 20:36:22] __main__ INFO: \u001b[0mEpoch 8 Step 200/351 lr 0.100000 loss 0.5940 (0.7398) acc@1 0.8125 (0.7427) acc@5 0.9766 (0.9834)\n",
      "\u001b[32m[2020-06-08 20:38:39] __main__ INFO: \u001b[0mEpoch 8 Step 300/351 lr 0.100000 loss 0.7239 (0.7241) acc@1 0.7344 (0.7469) acc@5 0.9844 (0.9842)\n",
      "\u001b[32m[2020-06-08 20:39:48] __main__ INFO: \u001b[0mEpoch 8 Step 351/351 lr 0.100000 loss 0.5878 (0.7227) acc@1 0.7891 (0.7470) acc@5 1.0000 (0.9842)\n",
      "\u001b[32m[2020-06-08 20:39:48] __main__ INFO: \u001b[0mElapsed 479.67\n",
      "\u001b[32m[2020-06-08 20:39:48] __main__ INFO: \u001b[0mVal 8\n",
      "\u001b[32m[2020-06-08 20:40:05] __main__ INFO: \u001b[0mEpoch 8 loss 0.8549 acc@1 0.7116 acc@5 0.9796\n",
      "\u001b[32m[2020-06-08 20:40:05] __main__ INFO: \u001b[0mElapsed 17.01\n",
      "\u001b[32m[2020-06-08 20:40:05] __main__ INFO: \u001b[0mTrain 9 2808\n",
      "\u001b[32m[2020-06-08 20:42:22] __main__ INFO: \u001b[0mEpoch 9 Step 100/351 lr 0.100000 loss 0.6551 (0.6560) acc@1 0.7734 (0.7696) acc@5 0.9922 (0.9888)\n",
      "\u001b[32m[2020-06-08 20:44:39] __main__ INFO: \u001b[0mEpoch 9 Step 200/351 lr 0.100000 loss 0.6038 (0.6618) acc@1 0.7969 (0.7684) acc@5 0.9844 (0.9882)\n",
      "\u001b[32m[2020-06-08 20:46:55] __main__ INFO: \u001b[0mEpoch 9 Step 300/351 lr 0.100000 loss 0.6046 (0.6497) acc@1 0.7578 (0.7734) acc@5 0.9844 (0.9883)\n",
      "\u001b[32m[2020-06-08 20:48:05] __main__ INFO: \u001b[0mEpoch 9 Step 351/351 lr 0.100000 loss 0.6576 (0.6481) acc@1 0.8125 (0.7746) acc@5 0.9844 (0.9884)\n",
      "\u001b[32m[2020-06-08 20:48:05] __main__ INFO: \u001b[0mElapsed 479.72\n",
      "\u001b[32m[2020-06-08 20:48:05] __main__ INFO: \u001b[0mVal 9\n",
      "\u001b[32m[2020-06-08 20:48:22] __main__ INFO: \u001b[0mEpoch 9 loss 1.0537 acc@1 0.6758 acc@5 0.9796\n",
      "\u001b[32m[2020-06-08 20:48:22] __main__ INFO: \u001b[0mElapsed 17.02\n",
      "\u001b[32m[2020-06-08 20:48:22] __main__ INFO: \u001b[0mTrain 10 3159\n",
      "\u001b[32m[2020-06-08 20:50:39] __main__ INFO: \u001b[0mEpoch 10 Step 100/351 lr 0.100000 loss 0.7176 (0.6144) acc@1 0.7422 (0.7905) acc@5 0.9766 (0.9893)\n",
      "\u001b[32m[2020-06-08 20:52:55] __main__ INFO: \u001b[0mEpoch 10 Step 200/351 lr 0.100000 loss 0.5246 (0.6110) acc@1 0.8203 (0.7920) acc@5 0.9922 (0.9890)\n",
      "\u001b[32m[2020-06-08 20:55:12] __main__ INFO: \u001b[0mEpoch 10 Step 300/351 lr 0.100000 loss 0.5439 (0.6083) acc@1 0.8125 (0.7917) acc@5 0.9922 (0.9890)\n",
      "\u001b[32m[2020-06-08 20:56:22] __main__ INFO: \u001b[0mEpoch 10 Step 351/351 lr 0.100000 loss 0.5723 (0.6061) acc@1 0.8125 (0.7920) acc@5 1.0000 (0.9889)\n",
      "\u001b[32m[2020-06-08 20:56:22] __main__ INFO: \u001b[0mElapsed 479.42\n",
      "\u001b[32m[2020-06-08 20:56:22] __main__ INFO: \u001b[0mVal 10\n",
      "\u001b[32m[2020-06-08 20:56:39] __main__ INFO: \u001b[0mEpoch 10 loss 0.8943 acc@1 0.7110 acc@5 0.9786\n",
      "\u001b[32m[2020-06-08 20:56:39] __main__ INFO: \u001b[0mElapsed 17.00\n",
      "\u001b[32m[2020-06-08 20:56:39] __main__ INFO: \u001b[0mTrain 11 3510\n",
      "\u001b[32m[2020-06-08 20:58:55] __main__ INFO: \u001b[0mEpoch 11 Step 100/351 lr 0.100000 loss 0.5313 (0.5723) acc@1 0.7891 (0.8034) acc@5 1.0000 (0.9906)\n",
      "\u001b[32m[2020-06-08 21:01:12] __main__ INFO: \u001b[0mEpoch 11 Step 200/351 lr 0.100000 loss 0.6646 (0.5758) acc@1 0.7656 (0.8020) acc@5 0.9844 (0.9902)\n",
      "\u001b[32m[2020-06-08 21:03:28] __main__ INFO: \u001b[0mEpoch 11 Step 300/351 lr 0.100000 loss 0.6846 (0.5688) acc@1 0.7422 (0.8043) acc@5 0.9922 (0.9905)\n",
      "\u001b[32m[2020-06-08 21:04:38] __main__ INFO: \u001b[0mEpoch 11 Step 351/351 lr 0.100000 loss 0.4869 (0.5690) acc@1 0.8281 (0.8041) acc@5 1.0000 (0.9903)\n",
      "\u001b[32m[2020-06-08 21:04:38] __main__ INFO: \u001b[0mElapsed 479.49\n",
      "\u001b[32m[2020-06-08 21:04:38] __main__ INFO: \u001b[0mVal 11\n",
      "\u001b[32m[2020-06-08 21:04:55] __main__ INFO: \u001b[0mEpoch 11 loss 0.7211 acc@1 0.7630 acc@5 0.9804\n",
      "\u001b[32m[2020-06-08 21:04:55] __main__ INFO: \u001b[0mElapsed 17.02\n",
      "\u001b[32m[2020-06-08 21:04:55] __main__ INFO: \u001b[0mTrain 12 3861\n",
      "\u001b[32m[2020-06-08 21:07:12] __main__ INFO: \u001b[0mEpoch 12 Step 100/351 lr 0.100000 loss 0.6387 (0.5543) acc@1 0.7891 (0.8091) acc@5 0.9922 (0.9904)\n",
      "\u001b[32m[2020-06-08 21:09:28] __main__ INFO: \u001b[0mEpoch 12 Step 200/351 lr 0.100000 loss 0.6144 (0.5555) acc@1 0.7969 (0.8084) acc@5 1.0000 (0.9903)\n",
      "\u001b[32m[2020-06-08 21:11:45] __main__ INFO: \u001b[0mEpoch 12 Step 300/351 lr 0.100000 loss 0.6238 (0.5533) acc@1 0.7812 (0.8086) acc@5 0.9922 (0.9907)\n",
      "\u001b[32m[2020-06-08 21:12:54] __main__ INFO: \u001b[0mEpoch 12 Step 351/351 lr 0.100000 loss 0.4170 (0.5526) acc@1 0.8516 (0.8091) acc@5 1.0000 (0.9905)\n",
      "\u001b[32m[2020-06-08 21:12:54] __main__ INFO: \u001b[0mElapsed 479.18\n",
      "\u001b[32m[2020-06-08 21:12:54] __main__ INFO: \u001b[0mVal 12\n",
      "\u001b[32m[2020-06-08 21:13:11] __main__ INFO: \u001b[0mEpoch 12 loss 0.7051 acc@1 0.7716 acc@5 0.9860\n",
      "\u001b[32m[2020-06-08 21:13:11] __main__ INFO: \u001b[0mElapsed 17.02\n",
      "\u001b[32m[2020-06-08 21:13:11] __main__ INFO: \u001b[0mTrain 13 4212\n",
      "\u001b[32m[2020-06-08 21:15:28] __main__ INFO: \u001b[0mEpoch 13 Step 100/351 lr 0.100000 loss 0.6377 (0.5178) acc@1 0.7891 (0.8245) acc@5 0.9766 (0.9917)\n",
      "\u001b[32m[2020-06-08 21:17:45] __main__ INFO: \u001b[0mEpoch 13 Step 200/351 lr 0.100000 loss 0.5661 (0.5245) acc@1 0.7969 (0.8193) acc@5 1.0000 (0.9919)\n",
      "\u001b[32m[2020-06-08 21:20:01] __main__ INFO: \u001b[0mEpoch 13 Step 300/351 lr 0.100000 loss 0.5308 (0.5313) acc@1 0.7734 (0.8170) acc@5 0.9844 (0.9912)\n",
      "\u001b[32m[2020-06-08 21:21:11] __main__ INFO: \u001b[0mEpoch 13 Step 351/351 lr 0.100000 loss 0.5081 (0.5308) acc@1 0.8047 (0.8171) acc@5 1.0000 (0.9915)\n",
      "\u001b[32m[2020-06-08 21:21:11] __main__ INFO: \u001b[0mElapsed 479.39\n",
      "\u001b[32m[2020-06-08 21:21:11] __main__ INFO: \u001b[0mVal 13\n",
      "\u001b[32m[2020-06-08 21:21:28] __main__ INFO: \u001b[0mEpoch 13 loss 0.6043 acc@1 0.8060 acc@5 0.9856\n",
      "\u001b[32m[2020-06-08 21:21:28] __main__ INFO: \u001b[0mElapsed 17.01\n",
      "\u001b[32m[2020-06-08 21:21:28] __main__ INFO: \u001b[0mTrain 14 4563\n",
      "\u001b[32m[2020-06-08 21:23:44] __main__ INFO: \u001b[0mEpoch 14 Step 100/351 lr 0.100000 loss 0.5495 (0.5048) acc@1 0.8203 (0.8230) acc@5 0.9922 (0.9930)\n",
      "\u001b[32m[2020-06-08 21:26:01] __main__ INFO: \u001b[0mEpoch 14 Step 200/351 lr 0.100000 loss 0.6175 (0.5003) acc@1 0.7656 (0.8259) acc@5 1.0000 (0.9928)\n",
      "\u001b[32m[2020-06-08 21:28:18] __main__ INFO: \u001b[0mEpoch 14 Step 300/351 lr 0.100000 loss 0.5077 (0.5051) acc@1 0.8594 (0.8249) acc@5 0.9844 (0.9925)\n",
      "\u001b[32m[2020-06-08 21:29:27] __main__ INFO: \u001b[0mEpoch 14 Step 351/351 lr 0.100000 loss 0.4296 (0.5053) acc@1 0.8516 (0.8242) acc@5 0.9844 (0.9926)\n",
      "\u001b[32m[2020-06-08 21:29:27] __main__ INFO: \u001b[0mElapsed 479.46\n",
      "\u001b[32m[2020-06-08 21:29:27] __main__ INFO: \u001b[0mVal 14\n",
      "\u001b[32m[2020-06-08 21:29:44] __main__ INFO: \u001b[0mEpoch 14 loss 0.8377 acc@1 0.7350 acc@5 0.9776\n",
      "\u001b[32m[2020-06-08 21:29:44] __main__ INFO: \u001b[0mElapsed 17.01\n",
      "\u001b[32m[2020-06-08 21:29:44] __main__ INFO: \u001b[0mTrain 15 4914\n",
      "\u001b[32m[2020-06-08 21:32:01] __main__ INFO: \u001b[0mEpoch 15 Step 100/351 lr 0.100000 loss 0.6495 (0.4906) acc@1 0.7734 (0.8303) acc@5 0.9766 (0.9926)\n",
      "\u001b[32m[2020-06-08 21:34:18] __main__ INFO: \u001b[0mEpoch 15 Step 200/351 lr 0.100000 loss 0.5412 (0.4981) acc@1 0.8281 (0.8297) acc@5 1.0000 (0.9926)\n",
      "\u001b[32m[2020-06-08 21:36:34] __main__ INFO: \u001b[0mEpoch 15 Step 300/351 lr 0.100000 loss 0.5282 (0.4985) acc@1 0.8203 (0.8286) acc@5 0.9922 (0.9927)\n",
      "\u001b[32m[2020-06-08 21:37:44] __main__ INFO: \u001b[0mEpoch 15 Step 351/351 lr 0.100000 loss 0.5487 (0.4963) acc@1 0.8281 (0.8293) acc@5 0.9766 (0.9928)\n",
      "\u001b[32m[2020-06-08 21:37:44] __main__ INFO: \u001b[0mElapsed 479.48\n",
      "\u001b[32m[2020-06-08 21:37:44] __main__ INFO: \u001b[0mVal 15\n",
      "\u001b[32m[2020-06-08 21:38:01] __main__ INFO: \u001b[0mEpoch 15 loss 0.8611 acc@1 0.7306 acc@5 0.9892\n",
      "\u001b[32m[2020-06-08 21:38:01] __main__ INFO: \u001b[0mElapsed 17.02\n",
      "\u001b[32m[2020-06-08 21:38:01] __main__ INFO: \u001b[0mTrain 16 5265\n",
      "\u001b[32m[2020-06-08 21:40:17] __main__ INFO: \u001b[0mEpoch 16 Step 100/351 lr 0.100000 loss 0.4436 (0.4741) acc@1 0.8594 (0.8366) acc@5 0.9844 (0.9934)\n",
      "\u001b[32m[2020-06-08 21:42:34] __main__ INFO: \u001b[0mEpoch 16 Step 200/351 lr 0.100000 loss 0.3507 (0.4841) acc@1 0.8828 (0.8332) acc@5 1.0000 (0.9928)\n",
      "\u001b[32m[2020-06-08 21:44:51] __main__ INFO: \u001b[0mEpoch 16 Step 300/351 lr 0.100000 loss 0.5427 (0.4830) acc@1 0.8047 (0.8334) acc@5 0.9766 (0.9925)\n",
      "\u001b[32m[2020-06-08 21:46:00] __main__ INFO: \u001b[0mEpoch 16 Step 351/351 lr 0.100000 loss 0.4024 (0.4817) acc@1 0.8359 (0.8348) acc@5 0.9922 (0.9926)\n",
      "\u001b[32m[2020-06-08 21:46:00] __main__ INFO: \u001b[0mElapsed 479.63\n",
      "\u001b[32m[2020-06-08 21:46:00] __main__ INFO: \u001b[0mVal 16\n",
      "\u001b[32m[2020-06-08 21:46:17] __main__ INFO: \u001b[0mEpoch 16 loss 0.7274 acc@1 0.7618 acc@5 0.9900\n",
      "\u001b[32m[2020-06-08 21:46:17] __main__ INFO: \u001b[0mElapsed 17.02\n",
      "\u001b[32m[2020-06-08 21:46:17] __main__ INFO: \u001b[0mTrain 17 5616\n",
      "\u001b[32m[2020-06-08 21:48:34] __main__ INFO: \u001b[0mEpoch 17 Step 100/351 lr 0.100000 loss 0.4052 (0.4655) acc@1 0.8594 (0.8384) acc@5 0.9922 (0.9945)\n",
      "\u001b[32m[2020-06-08 21:50:51] __main__ INFO: \u001b[0mEpoch 17 Step 200/351 lr 0.100000 loss 0.3735 (0.4676) acc@1 0.8828 (0.8380) acc@5 0.9922 (0.9940)\n",
      "\u001b[32m[2020-06-08 21:53:07] __main__ INFO: \u001b[0mEpoch 17 Step 300/351 lr 0.100000 loss 0.5546 (0.4680) acc@1 0.7812 (0.8370) acc@5 0.9922 (0.9939)\n",
      "\u001b[32m[2020-06-08 21:54:17] __main__ INFO: \u001b[0mEpoch 17 Step 351/351 lr 0.100000 loss 0.4103 (0.4673) acc@1 0.8672 (0.8368) acc@5 0.9844 (0.9939)\n",
      "\u001b[32m[2020-06-08 21:54:17] __main__ INFO: \u001b[0mElapsed 479.32\n",
      "\u001b[32m[2020-06-08 21:54:17] __main__ INFO: \u001b[0mVal 17\n",
      "\u001b[32m[2020-06-08 21:54:34] __main__ INFO: \u001b[0mEpoch 17 loss 0.7401 acc@1 0.7550 acc@5 0.9844\n",
      "\u001b[32m[2020-06-08 21:54:34] __main__ INFO: \u001b[0mElapsed 17.02\n",
      "\u001b[32m[2020-06-08 21:54:34] __main__ INFO: \u001b[0mTrain 18 5967\n",
      "\u001b[32m[2020-06-08 21:56:50] __main__ INFO: \u001b[0mEpoch 18 Step 100/351 lr 0.100000 loss 0.5365 (0.4537) acc@1 0.8281 (0.8434) acc@5 1.0000 (0.9934)\n",
      "\u001b[32m[2020-06-08 21:59:07] __main__ INFO: \u001b[0mEpoch 18 Step 200/351 lr 0.100000 loss 0.4494 (0.4542) acc@1 0.8281 (0.8432) acc@5 0.9922 (0.9934)\n",
      "\u001b[32m[2020-06-08 22:01:23] __main__ INFO: \u001b[0mEpoch 18 Step 300/351 lr 0.100000 loss 0.4221 (0.4544) acc@1 0.8672 (0.8429) acc@5 0.9844 (0.9937)\n",
      "\u001b[32m[2020-06-08 22:02:33] __main__ INFO: \u001b[0mEpoch 18 Step 351/351 lr 0.100000 loss 0.2350 (0.4562) acc@1 0.9375 (0.8423) acc@5 1.0000 (0.9936)\n",
      "\u001b[32m[2020-06-08 22:02:33] __main__ INFO: \u001b[0mElapsed 479.28\n",
      "\u001b[32m[2020-06-08 22:02:33] __main__ INFO: \u001b[0mVal 18\n",
      "\u001b[32m[2020-06-08 22:02:50] __main__ INFO: \u001b[0mEpoch 18 loss 0.7316 acc@1 0.7728 acc@5 0.9816\n",
      "\u001b[32m[2020-06-08 22:02:50] __main__ INFO: \u001b[0mElapsed 17.05\n",
      "\u001b[32m[2020-06-08 22:02:50] __main__ INFO: \u001b[0mTrain 19 6318\n",
      "\u001b[32m[2020-06-08 22:05:07] __main__ INFO: \u001b[0mEpoch 19 Step 100/351 lr 0.100000 loss 0.4381 (0.4407) acc@1 0.8516 (0.8484) acc@5 1.0000 (0.9947)\n",
      "\u001b[32m[2020-06-08 22:07:23] __main__ INFO: \u001b[0mEpoch 19 Step 200/351 lr 0.100000 loss 0.4602 (0.4419) acc@1 0.8281 (0.8459) acc@5 0.9766 (0.9941)\n",
      "\u001b[32m[2020-06-08 22:09:40] __main__ INFO: \u001b[0mEpoch 19 Step 300/351 lr 0.100000 loss 0.6429 (0.4492) acc@1 0.8203 (0.8441) acc@5 0.9922 (0.9937)\n",
      "\u001b[32m[2020-06-08 22:10:49] __main__ INFO: \u001b[0mEpoch 19 Step 351/351 lr 0.100000 loss 0.5504 (0.4504) acc@1 0.8281 (0.8435) acc@5 0.9844 (0.9937)\n",
      "\u001b[32m[2020-06-08 22:10:49] __main__ INFO: \u001b[0mElapsed 479.40\n",
      "\u001b[32m[2020-06-08 22:10:49] __main__ INFO: \u001b[0mVal 19\n",
      "\u001b[32m[2020-06-08 22:11:06] __main__ INFO: \u001b[0mEpoch 19 loss 0.6495 acc@1 0.7934 acc@5 0.9862\n",
      "\u001b[32m[2020-06-08 22:11:06] __main__ INFO: \u001b[0mElapsed 17.01\n",
      "\u001b[32m[2020-06-08 22:11:06] __main__ INFO: \u001b[0mTrain 20 6669\n",
      "\u001b[32m[2020-06-08 22:13:23] __main__ INFO: \u001b[0mEpoch 20 Step 100/351 lr 0.100000 loss 0.5039 (0.4372) acc@1 0.8203 (0.8504) acc@5 1.0000 (0.9952)\n",
      "\u001b[32m[2020-06-08 22:15:40] __main__ INFO: \u001b[0mEpoch 20 Step 200/351 lr 0.100000 loss 0.4428 (0.4324) acc@1 0.8281 (0.8510) acc@5 1.0000 (0.9951)\n",
      "\u001b[32m[2020-06-08 22:17:56] __main__ INFO: \u001b[0mEpoch 20 Step 300/351 lr 0.100000 loss 0.5526 (0.4369) acc@1 0.8125 (0.8499) acc@5 0.9766 (0.9946)\n",
      "\u001b[32m[2020-06-08 22:19:06] __main__ INFO: \u001b[0mEpoch 20 Step 351/351 lr 0.100000 loss 0.4306 (0.4374) acc@1 0.8672 (0.8500) acc@5 0.9922 (0.9946)\n",
      "\u001b[32m[2020-06-08 22:19:06] __main__ INFO: \u001b[0mElapsed 479.68\n",
      "\u001b[32m[2020-06-08 22:19:06] __main__ INFO: \u001b[0mVal 20\n",
      "\u001b[32m[2020-06-08 22:19:23] __main__ INFO: \u001b[0mEpoch 20 loss 0.8001 acc@1 0.7568 acc@5 0.9876\n",
      "\u001b[32m[2020-06-08 22:19:23] __main__ INFO: \u001b[0mElapsed 17.05\n",
      "\u001b[32m[2020-06-08 22:19:23] __main__ INFO: \u001b[0mTrain 21 7020\n",
      "\u001b[32m[2020-06-08 22:21:40] __main__ INFO: \u001b[0mEpoch 21 Step 100/351 lr 0.100000 loss 0.3611 (0.4082) acc@1 0.8672 (0.8598) acc@5 0.9922 (0.9948)\n",
      "\u001b[32m[2020-06-08 22:23:57] __main__ INFO: \u001b[0mEpoch 21 Step 200/351 lr 0.100000 loss 0.4795 (0.4199) acc@1 0.8594 (0.8552) acc@5 0.9844 (0.9947)\n",
      "\u001b[32m[2020-06-08 22:26:13] __main__ INFO: \u001b[0mEpoch 21 Step 300/351 lr 0.100000 loss 0.5480 (0.4287) acc@1 0.8203 (0.8527) acc@5 0.9922 (0.9946)\n",
      "\u001b[32m[2020-06-08 22:27:23] __main__ INFO: \u001b[0mEpoch 21 Step 351/351 lr 0.100000 loss 0.4523 (0.4304) acc@1 0.8516 (0.8512) acc@5 0.9922 (0.9944)\n",
      "\u001b[32m[2020-06-08 22:27:23] __main__ INFO: \u001b[0mElapsed 479.91\n",
      "\u001b[32m[2020-06-08 22:27:23] __main__ INFO: \u001b[0mVal 21\n",
      "\u001b[32m[2020-06-08 22:27:40] __main__ INFO: \u001b[0mEpoch 21 loss 0.5972 acc@1 0.8072 acc@5 0.9826\n",
      "\u001b[32m[2020-06-08 22:27:40] __main__ INFO: \u001b[0mElapsed 17.03\n",
      "\u001b[32m[2020-06-08 22:27:40] __main__ INFO: \u001b[0mTrain 22 7371\n",
      "\u001b[32m[2020-06-08 22:29:57] __main__ INFO: \u001b[0mEpoch 22 Step 100/351 lr 0.100000 loss 0.5916 (0.4115) acc@1 0.7891 (0.8593) acc@5 1.0000 (0.9948)\n",
      "\u001b[32m[2020-06-08 22:32:14] __main__ INFO: \u001b[0mEpoch 22 Step 200/351 lr 0.100000 loss 0.5752 (0.4169) acc@1 0.7969 (0.8557) acc@5 1.0000 (0.9950)\n",
      "\u001b[32m[2020-06-08 22:34:30] __main__ INFO: \u001b[0mEpoch 22 Step 300/351 lr 0.100000 loss 0.3522 (0.4243) acc@1 0.8906 (0.8523) acc@5 0.9922 (0.9952)\n",
      "\u001b[32m[2020-06-08 22:35:40] __main__ INFO: \u001b[0mEpoch 22 Step 351/351 lr 0.100000 loss 0.4640 (0.4250) acc@1 0.8672 (0.8530) acc@5 0.9922 (0.9951)\n",
      "\u001b[32m[2020-06-08 22:35:40] __main__ INFO: \u001b[0mElapsed 479.84\n",
      "\u001b[32m[2020-06-08 22:35:40] __main__ INFO: \u001b[0mVal 22\n",
      "\u001b[32m[2020-06-08 22:35:57] __main__ INFO: \u001b[0mEpoch 22 loss 0.5740 acc@1 0.8122 acc@5 0.9882\n",
      "\u001b[32m[2020-06-08 22:35:57] __main__ INFO: \u001b[0mElapsed 17.03\n",
      "\u001b[32m[2020-06-08 22:35:57] __main__ INFO: \u001b[0mTrain 23 7722\n",
      "\u001b[32m[2020-06-08 22:38:14] __main__ INFO: \u001b[0mEpoch 23 Step 100/351 lr 0.100000 loss 0.3743 (0.4067) acc@1 0.8594 (0.8610) acc@5 0.9922 (0.9953)\n",
      "\u001b[32m[2020-06-08 22:40:30] __main__ INFO: \u001b[0mEpoch 23 Step 200/351 lr 0.100000 loss 0.4467 (0.4109) acc@1 0.8594 (0.8602) acc@5 0.9922 (0.9954)\n",
      "\u001b[32m[2020-06-08 22:42:47] __main__ INFO: \u001b[0mEpoch 23 Step 300/351 lr 0.100000 loss 0.4921 (0.4150) acc@1 0.8047 (0.8572) acc@5 1.0000 (0.9950)\n",
      "\u001b[32m[2020-06-08 22:43:57] __main__ INFO: \u001b[0mEpoch 23 Step 351/351 lr 0.100000 loss 0.4381 (0.4181) acc@1 0.8516 (0.8558) acc@5 1.0000 (0.9950)\n",
      "\u001b[32m[2020-06-08 22:43:57] __main__ INFO: \u001b[0mElapsed 479.59\n",
      "\u001b[32m[2020-06-08 22:43:57] __main__ INFO: \u001b[0mVal 23\n",
      "\u001b[32m[2020-06-08 22:44:14] __main__ INFO: \u001b[0mEpoch 23 loss 0.6027 acc@1 0.8052 acc@5 0.9924\n",
      "\u001b[32m[2020-06-08 22:44:14] __main__ INFO: \u001b[0mElapsed 17.03\n",
      "\u001b[32m[2020-06-08 22:44:14] __main__ INFO: \u001b[0mTrain 24 8073\n",
      "\u001b[32m[2020-06-08 22:46:30] __main__ INFO: \u001b[0mEpoch 24 Step 100/351 lr 0.100000 loss 0.4577 (0.3874) acc@1 0.8672 (0.8677) acc@5 0.9844 (0.9949)\n",
      "\u001b[32m[2020-06-08 22:48:47] __main__ INFO: \u001b[0mEpoch 24 Step 200/351 lr 0.100000 loss 0.6011 (0.4008) acc@1 0.7812 (0.8619) acc@5 0.9844 (0.9944)\n",
      "\u001b[32m[2020-06-08 22:51:04] __main__ INFO: \u001b[0mEpoch 24 Step 300/351 lr 0.100000 loss 0.3878 (0.4110) acc@1 0.8750 (0.8586) acc@5 0.9922 (0.9943)\n",
      "\u001b[32m[2020-06-08 22:52:13] __main__ INFO: \u001b[0mEpoch 24 Step 351/351 lr 0.100000 loss 0.4846 (0.4114) acc@1 0.8359 (0.8588) acc@5 1.0000 (0.9942)\n",
      "\u001b[32m[2020-06-08 22:52:13] __main__ INFO: \u001b[0mElapsed 479.58\n",
      "\u001b[32m[2020-06-08 22:52:13] __main__ INFO: \u001b[0mVal 24\n",
      "\u001b[32m[2020-06-08 22:52:30] __main__ INFO: \u001b[0mEpoch 24 loss 0.6858 acc@1 0.7718 acc@5 0.9840\n",
      "\u001b[32m[2020-06-08 22:52:30] __main__ INFO: \u001b[0mElapsed 17.04\n",
      "\u001b[32m[2020-06-08 22:52:30] __main__ INFO: \u001b[0mTrain 25 8424\n",
      "\u001b[32m[2020-06-08 22:54:47] __main__ INFO: \u001b[0mEpoch 25 Step 100/351 lr 0.100000 loss 0.5407 (0.3897) acc@1 0.8203 (0.8671) acc@5 0.9922 (0.9958)\n",
      "\u001b[32m[2020-06-08 22:57:04] __main__ INFO: \u001b[0mEpoch 25 Step 200/351 lr 0.100000 loss 0.3163 (0.4021) acc@1 0.8984 (0.8621) acc@5 0.9922 (0.9955)\n",
      "\u001b[32m[2020-06-08 22:59:20] __main__ INFO: \u001b[0mEpoch 25 Step 300/351 lr 0.100000 loss 0.3067 (0.4032) acc@1 0.8672 (0.8618) acc@5 1.0000 (0.9953)\n",
      "\u001b[32m[2020-06-08 23:00:30] __main__ INFO: \u001b[0mEpoch 25 Step 351/351 lr 0.100000 loss 0.4252 (0.4028) acc@1 0.8672 (0.8620) acc@5 1.0000 (0.9953)\n",
      "\u001b[32m[2020-06-08 23:00:30] __main__ INFO: \u001b[0mElapsed 479.49\n",
      "\u001b[32m[2020-06-08 23:00:30] __main__ INFO: \u001b[0mVal 25\n",
      "\u001b[32m[2020-06-08 23:00:47] __main__ INFO: \u001b[0mEpoch 25 loss 0.5102 acc@1 0.8280 acc@5 0.9926\n",
      "\u001b[32m[2020-06-08 23:00:47] __main__ INFO: \u001b[0mElapsed 17.02\n",
      "\u001b[32m[2020-06-08 23:00:47] __main__ INFO: \u001b[0mTrain 26 8775\n",
      "\u001b[32m[2020-06-08 23:03:04] __main__ INFO: \u001b[0mEpoch 26 Step 100/351 lr 0.100000 loss 0.4103 (0.3893) acc@1 0.8594 (0.8673) acc@5 1.0000 (0.9953)\n",
      "\u001b[32m[2020-06-08 23:05:20] __main__ INFO: \u001b[0mEpoch 26 Step 200/351 lr 0.100000 loss 0.3397 (0.3919) acc@1 0.8828 (0.8640) acc@5 1.0000 (0.9947)\n",
      "\u001b[32m[2020-06-08 23:07:37] __main__ INFO: \u001b[0mEpoch 26 Step 300/351 lr 0.100000 loss 0.4735 (0.3952) acc@1 0.8281 (0.8629) acc@5 1.0000 (0.9950)\n",
      "\u001b[32m[2020-06-08 23:08:47] __main__ INFO: \u001b[0mEpoch 26 Step 351/351 lr 0.100000 loss 0.3653 (0.3945) acc@1 0.8594 (0.8630) acc@5 0.9922 (0.9949)\n",
      "\u001b[32m[2020-06-08 23:08:47] __main__ INFO: \u001b[0mElapsed 479.77\n",
      "\u001b[32m[2020-06-08 23:08:47] __main__ INFO: \u001b[0mVal 26\n",
      "\u001b[32m[2020-06-08 23:09:04] __main__ INFO: \u001b[0mEpoch 26 loss 0.5024 acc@1 0.8300 acc@5 0.9924\n",
      "\u001b[32m[2020-06-08 23:09:04] __main__ INFO: \u001b[0mElapsed 17.03\n",
      "\u001b[32m[2020-06-08 23:09:04] __main__ INFO: \u001b[0mTrain 27 9126\n",
      "\u001b[32m[2020-06-08 23:11:20] __main__ INFO: \u001b[0mEpoch 27 Step 100/351 lr 0.100000 loss 0.3681 (0.3855) acc@1 0.8516 (0.8679) acc@5 1.0000 (0.9954)\n",
      "\u001b[32m[2020-06-08 23:13:37] __main__ INFO: \u001b[0mEpoch 27 Step 200/351 lr 0.100000 loss 0.4134 (0.3874) acc@1 0.8516 (0.8677) acc@5 0.9844 (0.9953)\n",
      "\u001b[32m[2020-06-08 23:15:54] __main__ INFO: \u001b[0mEpoch 27 Step 300/351 lr 0.100000 loss 0.3372 (0.3896) acc@1 0.8984 (0.8663) acc@5 1.0000 (0.9954)\n",
      "\u001b[32m[2020-06-08 23:17:03] __main__ INFO: \u001b[0mEpoch 27 Step 351/351 lr 0.100000 loss 0.4705 (0.3918) acc@1 0.8359 (0.8650) acc@5 1.0000 (0.9953)\n",
      "\u001b[32m[2020-06-08 23:17:03] __main__ INFO: \u001b[0mElapsed 479.62\n",
      "\u001b[32m[2020-06-08 23:17:03] __main__ INFO: \u001b[0mVal 27\n",
      "\u001b[32m[2020-06-08 23:17:20] __main__ INFO: \u001b[0mEpoch 27 loss 0.6018 acc@1 0.8060 acc@5 0.9894\n",
      "\u001b[32m[2020-06-08 23:17:20] __main__ INFO: \u001b[0mElapsed 17.02\n",
      "\u001b[32m[2020-06-08 23:17:20] __main__ INFO: \u001b[0mTrain 28 9477\n",
      "\u001b[32m[2020-06-08 23:19:37] __main__ INFO: \u001b[0mEpoch 28 Step 100/351 lr 0.100000 loss 0.4749 (0.3798) acc@1 0.8281 (0.8684) acc@5 0.9766 (0.9961)\n",
      "\u001b[32m[2020-06-08 23:21:54] __main__ INFO: \u001b[0mEpoch 28 Step 200/351 lr 0.100000 loss 0.3686 (0.3933) acc@1 0.8594 (0.8644) acc@5 1.0000 (0.9952)\n",
      "\u001b[32m[2020-06-08 23:24:10] __main__ INFO: \u001b[0mEpoch 28 Step 300/351 lr 0.100000 loss 0.4489 (0.3910) acc@1 0.8359 (0.8641) acc@5 0.9922 (0.9953)\n",
      "\u001b[32m[2020-06-08 23:25:20] __main__ INFO: \u001b[0mEpoch 28 Step 351/351 lr 0.100000 loss 0.2369 (0.3917) acc@1 0.9297 (0.8637) acc@5 1.0000 (0.9954)\n",
      "\u001b[32m[2020-06-08 23:25:20] __main__ INFO: \u001b[0mElapsed 479.52\n",
      "\u001b[32m[2020-06-08 23:25:20] __main__ INFO: \u001b[0mVal 28\n",
      "\u001b[32m[2020-06-08 23:25:37] __main__ INFO: \u001b[0mEpoch 28 loss 0.4935 acc@1 0.8362 acc@5 0.9904\n",
      "\u001b[32m[2020-06-08 23:25:37] __main__ INFO: \u001b[0mElapsed 17.05\n",
      "\u001b[32m[2020-06-08 23:25:37] __main__ INFO: \u001b[0mTrain 29 9828\n",
      "\u001b[32m[2020-06-08 23:27:54] __main__ INFO: \u001b[0mEpoch 29 Step 100/351 lr 0.100000 loss 0.3586 (0.3641) acc@1 0.8906 (0.8745) acc@5 0.9922 (0.9959)\n",
      "\u001b[32m[2020-06-08 23:30:10] __main__ INFO: \u001b[0mEpoch 29 Step 200/351 lr 0.100000 loss 0.3812 (0.3793) acc@1 0.8750 (0.8682) acc@5 1.0000 (0.9957)\n",
      "\u001b[32m[2020-06-08 23:32:27] __main__ INFO: \u001b[0mEpoch 29 Step 300/351 lr 0.100000 loss 0.4055 (0.3810) acc@1 0.8516 (0.8673) acc@5 1.0000 (0.9959)\n",
      "\u001b[32m[2020-06-08 23:33:37] __main__ INFO: \u001b[0mEpoch 29 Step 351/351 lr 0.100000 loss 0.3166 (0.3825) acc@1 0.8750 (0.8671) acc@5 0.9922 (0.9957)\n",
      "\u001b[32m[2020-06-08 23:33:37] __main__ INFO: \u001b[0mElapsed 479.72\n",
      "\u001b[32m[2020-06-08 23:33:37] __main__ INFO: \u001b[0mVal 29\n",
      "\u001b[32m[2020-06-08 23:33:54] __main__ INFO: \u001b[0mEpoch 29 loss 0.7183 acc@1 0.7850 acc@5 0.9862\n",
      "\u001b[32m[2020-06-08 23:33:54] __main__ INFO: \u001b[0mElapsed 17.06\n",
      "\u001b[32m[2020-06-08 23:33:54] __main__ INFO: \u001b[0mTrain 30 10179\n",
      "\u001b[32m[2020-06-08 23:36:10] __main__ INFO: \u001b[0mEpoch 30 Step 100/351 lr 0.100000 loss 0.3544 (0.3700) acc@1 0.8828 (0.8716) acc@5 0.9922 (0.9969)\n",
      "\u001b[32m[2020-06-08 23:38:27] __main__ INFO: \u001b[0mEpoch 30 Step 200/351 lr 0.100000 loss 0.4459 (0.3783) acc@1 0.8594 (0.8686) acc@5 1.0000 (0.9961)\n",
      "\u001b[32m[2020-06-08 23:40:43] __main__ INFO: \u001b[0mEpoch 30 Step 300/351 lr 0.100000 loss 0.3365 (0.3784) acc@1 0.9062 (0.8694) acc@5 1.0000 (0.9958)\n",
      "\u001b[32m[2020-06-08 23:41:53] __main__ INFO: \u001b[0mEpoch 30 Step 351/351 lr 0.100000 loss 0.4465 (0.3796) acc@1 0.8594 (0.8694) acc@5 0.9922 (0.9956)\n",
      "\u001b[32m[2020-06-08 23:41:53] __main__ INFO: \u001b[0mElapsed 479.43\n",
      "\u001b[32m[2020-06-08 23:41:53] __main__ INFO: \u001b[0mVal 30\n",
      "\u001b[32m[2020-06-08 23:42:10] __main__ INFO: \u001b[0mEpoch 30 loss 0.4530 acc@1 0.8442 acc@5 0.9926\n",
      "\u001b[32m[2020-06-08 23:42:10] __main__ INFO: \u001b[0mElapsed 17.02\n",
      "\u001b[32m[2020-06-08 23:42:10] __main__ INFO: \u001b[0mTrain 31 10530\n",
      "\u001b[32m[2020-06-08 23:44:27] __main__ INFO: \u001b[0mEpoch 31 Step 100/351 lr 0.100000 loss 0.3353 (0.3579) acc@1 0.8906 (0.8742) acc@5 1.0000 (0.9963)\n",
      "\u001b[32m[2020-06-08 23:46:43] __main__ INFO: \u001b[0mEpoch 31 Step 200/351 lr 0.100000 loss 0.3719 (0.3644) acc@1 0.8906 (0.8732) acc@5 1.0000 (0.9961)\n",
      "\u001b[32m[2020-06-08 23:49:00] __main__ INFO: \u001b[0mEpoch 31 Step 300/351 lr 0.100000 loss 0.4493 (0.3710) acc@1 0.8359 (0.8712) acc@5 1.0000 (0.9961)\n",
      "\u001b[32m[2020-06-08 23:50:10] __main__ INFO: \u001b[0mEpoch 31 Step 351/351 lr 0.100000 loss 0.4738 (0.3755) acc@1 0.8281 (0.8707) acc@5 1.0000 (0.9959)\n",
      "\u001b[32m[2020-06-08 23:50:10] __main__ INFO: \u001b[0mElapsed 479.59\n",
      "\u001b[32m[2020-06-08 23:50:10] __main__ INFO: \u001b[0mVal 31\n",
      "\u001b[32m[2020-06-08 23:50:27] __main__ INFO: \u001b[0mEpoch 31 loss 0.6134 acc@1 0.8012 acc@5 0.9868\n",
      "\u001b[32m[2020-06-08 23:50:27] __main__ INFO: \u001b[0mElapsed 17.05\n",
      "\u001b[32m[2020-06-08 23:50:27] __main__ INFO: \u001b[0mTrain 32 10881\n",
      "\u001b[32m[2020-06-08 23:52:43] __main__ INFO: \u001b[0mEpoch 32 Step 100/351 lr 0.100000 loss 0.3219 (0.3568) acc@1 0.9219 (0.8765) acc@5 1.0000 (0.9964)\n",
      "\u001b[32m[2020-06-08 23:55:00] __main__ INFO: \u001b[0mEpoch 32 Step 200/351 lr 0.100000 loss 0.3817 (0.3628) acc@1 0.8750 (0.8748) acc@5 0.9844 (0.9958)\n",
      "\u001b[32m[2020-06-08 23:57:16] __main__ INFO: \u001b[0mEpoch 32 Step 300/351 lr 0.100000 loss 0.3957 (0.3691) acc@1 0.8359 (0.8718) acc@5 1.0000 (0.9955)\n",
      "\u001b[32m[2020-06-08 23:58:26] __main__ INFO: \u001b[0mEpoch 32 Step 351/351 lr 0.100000 loss 0.2990 (0.3699) acc@1 0.9141 (0.8722) acc@5 1.0000 (0.9957)\n",
      "\u001b[32m[2020-06-08 23:58:26] __main__ INFO: \u001b[0mElapsed 479.32\n",
      "\u001b[32m[2020-06-08 23:58:26] __main__ INFO: \u001b[0mVal 32\n",
      "\u001b[32m[2020-06-08 23:58:43] __main__ INFO: \u001b[0mEpoch 32 loss 0.4968 acc@1 0.8332 acc@5 0.9940\n",
      "\u001b[32m[2020-06-08 23:58:43] __main__ INFO: \u001b[0mElapsed 17.06\n",
      "\u001b[32m[2020-06-08 23:58:43] __main__ INFO: \u001b[0mTrain 33 11232\n",
      "\u001b[32m[2020-06-09 00:01:00] __main__ INFO: \u001b[0mEpoch 33 Step 100/351 lr 0.100000 loss 0.1984 (0.3405) acc@1 0.9375 (0.8800) acc@5 1.0000 (0.9973)\n",
      "\u001b[32m[2020-06-09 00:03:16] __main__ INFO: \u001b[0mEpoch 33 Step 200/351 lr 0.100000 loss 0.4422 (0.3554) acc@1 0.8750 (0.8775) acc@5 0.9922 (0.9963)\n",
      "\u001b[32m[2020-06-09 00:05:33] __main__ INFO: \u001b[0mEpoch 33 Step 300/351 lr 0.100000 loss 0.2787 (0.3609) acc@1 0.8828 (0.8749) acc@5 1.0000 (0.9961)\n",
      "\u001b[32m[2020-06-09 00:06:43] __main__ INFO: \u001b[0mEpoch 33 Step 351/351 lr 0.100000 loss 0.2860 (0.3666) acc@1 0.8984 (0.8728) acc@5 1.0000 (0.9959)\n",
      "\u001b[32m[2020-06-09 00:06:43] __main__ INFO: \u001b[0mElapsed 479.50\n",
      "\u001b[32m[2020-06-09 00:06:43] __main__ INFO: \u001b[0mVal 33\n",
      "\u001b[32m[2020-06-09 00:07:00] __main__ INFO: \u001b[0mEpoch 33 loss 0.8855 acc@1 0.7418 acc@5 0.9696\n",
      "\u001b[32m[2020-06-09 00:07:00] __main__ INFO: \u001b[0mElapsed 17.05\n",
      "\u001b[32m[2020-06-09 00:07:00] __main__ INFO: \u001b[0mTrain 34 11583\n",
      "\u001b[32m[2020-06-09 00:09:16] __main__ INFO: \u001b[0mEpoch 34 Step 100/351 lr 0.100000 loss 0.3193 (0.3333) acc@1 0.8828 (0.8869) acc@5 1.0000 (0.9973)\n",
      "\u001b[32m[2020-06-09 00:11:33] __main__ INFO: \u001b[0mEpoch 34 Step 200/351 lr 0.100000 loss 0.3729 (0.3530) acc@1 0.8750 (0.8785) acc@5 1.0000 (0.9966)\n",
      "\u001b[32m[2020-06-09 00:13:50] __main__ INFO: \u001b[0mEpoch 34 Step 300/351 lr 0.100000 loss 0.5406 (0.3615) acc@1 0.8359 (0.8749) acc@5 1.0000 (0.9962)\n",
      "\u001b[32m[2020-06-09 00:14:59] __main__ INFO: \u001b[0mEpoch 34 Step 351/351 lr 0.100000 loss 0.4005 (0.3624) acc@1 0.8594 (0.8740) acc@5 0.9922 (0.9962)\n",
      "\u001b[32m[2020-06-09 00:14:59] __main__ INFO: \u001b[0mElapsed 479.80\n",
      "\u001b[32m[2020-06-09 00:14:59] __main__ INFO: \u001b[0mVal 34\n",
      "\u001b[32m[2020-06-09 00:15:16] __main__ INFO: \u001b[0mEpoch 34 loss 0.5342 acc@1 0.8262 acc@5 0.9898\n",
      "\u001b[32m[2020-06-09 00:15:16] __main__ INFO: \u001b[0mElapsed 17.04\n",
      "\u001b[32m[2020-06-09 00:15:16] __main__ INFO: \u001b[0mTrain 35 11934\n",
      "\u001b[32m[2020-06-09 00:17:33] __main__ INFO: \u001b[0mEpoch 35 Step 100/351 lr 0.100000 loss 0.4524 (0.3431) acc@1 0.8516 (0.8840) acc@5 0.9922 (0.9962)\n",
      "\u001b[32m[2020-06-09 00:19:50] __main__ INFO: \u001b[0mEpoch 35 Step 200/351 lr 0.100000 loss 0.2483 (0.3547) acc@1 0.9141 (0.8788) acc@5 1.0000 (0.9961)\n",
      "\u001b[32m[2020-06-09 00:22:06] __main__ INFO: \u001b[0mEpoch 35 Step 300/351 lr 0.100000 loss 0.3006 (0.3576) acc@1 0.8828 (0.8787) acc@5 0.9922 (0.9962)\n",
      "\u001b[32m[2020-06-09 00:23:16] __main__ INFO: \u001b[0mEpoch 35 Step 351/351 lr 0.100000 loss 0.3668 (0.3585) acc@1 0.8438 (0.8784) acc@5 1.0000 (0.9960)\n",
      "\u001b[32m[2020-06-09 00:23:16] __main__ INFO: \u001b[0mElapsed 479.63\n",
      "\u001b[32m[2020-06-09 00:23:16] __main__ INFO: \u001b[0mVal 35\n",
      "\u001b[32m[2020-06-09 00:23:33] __main__ INFO: \u001b[0mEpoch 35 loss 0.5732 acc@1 0.8216 acc@5 0.9902\n",
      "\u001b[32m[2020-06-09 00:23:33] __main__ INFO: \u001b[0mElapsed 17.04\n",
      "\u001b[32m[2020-06-09 00:23:33] __main__ INFO: \u001b[0mTrain 36 12285\n",
      "\u001b[32m[2020-06-09 00:25:50] __main__ INFO: \u001b[0mEpoch 36 Step 100/351 lr 0.100000 loss 0.3693 (0.3277) acc@1 0.8750 (0.8883) acc@5 0.9922 (0.9969)\n",
      "\u001b[32m[2020-06-09 00:28:07] __main__ INFO: \u001b[0mEpoch 36 Step 200/351 lr 0.100000 loss 0.3714 (0.3406) acc@1 0.8594 (0.8830) acc@5 0.9766 (0.9966)\n",
      "\u001b[32m[2020-06-09 00:30:23] __main__ INFO: \u001b[0mEpoch 36 Step 300/351 lr 0.100000 loss 0.3974 (0.3539) acc@1 0.8672 (0.8781) acc@5 0.9844 (0.9961)\n",
      "\u001b[32m[2020-06-09 00:31:33] __main__ INFO: \u001b[0mEpoch 36 Step 351/351 lr 0.100000 loss 0.4353 (0.3577) acc@1 0.8438 (0.8767) acc@5 0.9922 (0.9958)\n",
      "\u001b[32m[2020-06-09 00:31:33] __main__ INFO: \u001b[0mElapsed 479.64\n",
      "\u001b[32m[2020-06-09 00:31:33] __main__ INFO: \u001b[0mVal 36\n",
      "\u001b[32m[2020-06-09 00:31:50] __main__ INFO: \u001b[0mEpoch 36 loss 0.6412 acc@1 0.7990 acc@5 0.9860\n",
      "\u001b[32m[2020-06-09 00:31:50] __main__ INFO: \u001b[0mElapsed 17.04\n",
      "\u001b[32m[2020-06-09 00:31:50] __main__ INFO: \u001b[0mTrain 37 12636\n",
      "\u001b[32m[2020-06-09 00:34:06] __main__ INFO: \u001b[0mEpoch 37 Step 100/351 lr 0.100000 loss 0.2924 (0.3482) acc@1 0.8828 (0.8808) acc@5 1.0000 (0.9966)\n",
      "\u001b[32m[2020-06-09 00:36:23] __main__ INFO: \u001b[0mEpoch 37 Step 200/351 lr 0.100000 loss 0.3207 (0.3437) acc@1 0.9062 (0.8817) acc@5 0.9922 (0.9963)\n",
      "\u001b[32m[2020-06-09 00:38:40] __main__ INFO: \u001b[0mEpoch 37 Step 300/351 lr 0.100000 loss 0.3653 (0.3540) acc@1 0.8594 (0.8775) acc@5 1.0000 (0.9960)\n",
      "\u001b[32m[2020-06-09 00:39:49] __main__ INFO: \u001b[0mEpoch 37 Step 351/351 lr 0.100000 loss 0.2886 (0.3567) acc@1 0.9219 (0.8766) acc@5 1.0000 (0.9959)\n",
      "\u001b[32m[2020-06-09 00:39:49] __main__ INFO: \u001b[0mElapsed 479.37\n",
      "\u001b[32m[2020-06-09 00:39:49] __main__ INFO: \u001b[0mVal 37\n",
      "\u001b[32m[2020-06-09 00:40:06] __main__ INFO: \u001b[0mEpoch 37 loss 0.5304 acc@1 0.8200 acc@5 0.9912\n",
      "\u001b[32m[2020-06-09 00:40:06] __main__ INFO: \u001b[0mElapsed 17.04\n",
      "\u001b[32m[2020-06-09 00:40:06] __main__ INFO: \u001b[0mTrain 38 12987\n",
      "\u001b[32m[2020-06-09 00:42:23] __main__ INFO: \u001b[0mEpoch 38 Step 100/351 lr 0.100000 loss 0.2886 (0.3331) acc@1 0.8828 (0.8845) acc@5 1.0000 (0.9967)\n",
      "\u001b[32m[2020-06-09 00:44:39] __main__ INFO: \u001b[0mEpoch 38 Step 200/351 lr 0.100000 loss 0.3298 (0.3385) acc@1 0.9141 (0.8816) acc@5 1.0000 (0.9967)\n",
      "\u001b[32m[2020-06-09 00:46:56] __main__ INFO: \u001b[0mEpoch 38 Step 300/351 lr 0.100000 loss 0.3454 (0.3481) acc@1 0.8828 (0.8788) acc@5 1.0000 (0.9966)\n",
      "\u001b[32m[2020-06-09 00:48:06] __main__ INFO: \u001b[0mEpoch 38 Step 351/351 lr 0.100000 loss 0.4636 (0.3462) acc@1 0.8516 (0.8796) acc@5 0.9844 (0.9967)\n",
      "\u001b[32m[2020-06-09 00:48:06] __main__ INFO: \u001b[0mElapsed 479.41\n",
      "\u001b[32m[2020-06-09 00:48:06] __main__ INFO: \u001b[0mVal 38\n",
      "\u001b[32m[2020-06-09 00:48:23] __main__ INFO: \u001b[0mEpoch 38 loss 0.5565 acc@1 0.8202 acc@5 0.9850\n",
      "\u001b[32m[2020-06-09 00:48:23] __main__ INFO: \u001b[0mElapsed 17.02\n",
      "\u001b[32m[2020-06-09 00:48:23] __main__ INFO: \u001b[0mTrain 39 13338\n",
      "\u001b[32m[2020-06-09 00:50:39] __main__ INFO: \u001b[0mEpoch 39 Step 100/351 lr 0.100000 loss 0.2857 (0.3443) acc@1 0.8906 (0.8796) acc@5 1.0000 (0.9975)\n",
      "\u001b[32m[2020-06-09 00:52:56] __main__ INFO: \u001b[0mEpoch 39 Step 200/351 lr 0.100000 loss 0.3799 (0.3466) acc@1 0.8672 (0.8779) acc@5 0.9922 (0.9972)\n",
      "\u001b[32m[2020-06-09 00:55:12] __main__ INFO: \u001b[0mEpoch 39 Step 300/351 lr 0.100000 loss 0.3287 (0.3512) acc@1 0.8750 (0.8767) acc@5 1.0000 (0.9968)\n",
      "\u001b[32m[2020-06-09 00:56:22] __main__ INFO: \u001b[0mEpoch 39 Step 351/351 lr 0.100000 loss 0.3247 (0.3515) acc@1 0.8672 (0.8774) acc@5 0.9844 (0.9967)\n",
      "\u001b[32m[2020-06-09 00:56:22] __main__ INFO: \u001b[0mElapsed 479.31\n",
      "\u001b[32m[2020-06-09 00:56:22] __main__ INFO: \u001b[0mVal 39\n",
      "\u001b[32m[2020-06-09 00:56:39] __main__ INFO: \u001b[0mEpoch 39 loss 0.6169 acc@1 0.8134 acc@5 0.9900\n",
      "\u001b[32m[2020-06-09 00:56:39] __main__ INFO: \u001b[0mElapsed 17.08\n",
      "\u001b[32m[2020-06-09 00:56:39] __main__ INFO: \u001b[0mTrain 40 13689\n",
      "\u001b[32m[2020-06-09 00:58:56] __main__ INFO: \u001b[0mEpoch 40 Step 100/351 lr 0.100000 loss 0.4179 (0.3242) acc@1 0.8359 (0.8886) acc@5 0.9844 (0.9973)\n",
      "\u001b[32m[2020-06-09 01:01:12] __main__ INFO: \u001b[0mEpoch 40 Step 200/351 lr 0.100000 loss 0.3589 (0.3342) acc@1 0.8906 (0.8859) acc@5 1.0000 (0.9964)\n",
      "\u001b[32m[2020-06-09 01:03:29] __main__ INFO: \u001b[0mEpoch 40 Step 300/351 lr 0.100000 loss 0.4426 (0.3419) acc@1 0.8672 (0.8838) acc@5 0.9922 (0.9961)\n",
      "\u001b[32m[2020-06-09 01:04:39] __main__ INFO: \u001b[0mEpoch 40 Step 351/351 lr 0.100000 loss 0.2996 (0.3430) acc@1 0.8750 (0.8832) acc@5 1.0000 (0.9961)\n",
      "\u001b[32m[2020-06-09 01:04:39] __main__ INFO: \u001b[0mElapsed 479.68\n",
      "\u001b[32m[2020-06-09 01:04:39] __main__ INFO: \u001b[0mVal 40\n",
      "\u001b[32m[2020-06-09 01:04:56] __main__ INFO: \u001b[0mEpoch 40 loss 0.4921 acc@1 0.8398 acc@5 0.9902\n",
      "\u001b[32m[2020-06-09 01:04:56] __main__ INFO: \u001b[0mElapsed 17.03\n",
      "\u001b[32m[2020-06-09 01:04:56] __main__ INFO: \u001b[0mTrain 41 14040\n",
      "\u001b[32m[2020-06-09 01:07:12] __main__ INFO: \u001b[0mEpoch 41 Step 100/351 lr 0.100000 loss 0.3339 (0.3219) acc@1 0.8906 (0.8898) acc@5 1.0000 (0.9970)\n",
      "\u001b[32m[2020-06-09 01:09:29] __main__ INFO: \u001b[0mEpoch 41 Step 200/351 lr 0.100000 loss 0.3806 (0.3365) acc@1 0.8750 (0.8855) acc@5 1.0000 (0.9961)\n",
      "\u001b[32m[2020-06-09 01:11:46] __main__ INFO: \u001b[0mEpoch 41 Step 300/351 lr 0.100000 loss 0.2538 (0.3421) acc@1 0.9062 (0.8838) acc@5 1.0000 (0.9959)\n",
      "\u001b[32m[2020-06-09 01:12:55] __main__ INFO: \u001b[0mEpoch 41 Step 351/351 lr 0.100000 loss 0.3618 (0.3413) acc@1 0.8906 (0.8839) acc@5 1.0000 (0.9959)\n",
      "\u001b[32m[2020-06-09 01:12:55] __main__ INFO: \u001b[0mElapsed 479.52\n",
      "\u001b[32m[2020-06-09 01:12:55] __main__ INFO: \u001b[0mVal 41\n",
      "\u001b[32m[2020-06-09 01:13:12] __main__ INFO: \u001b[0mEpoch 41 loss 0.8309 acc@1 0.7562 acc@5 0.9738\n",
      "\u001b[32m[2020-06-09 01:13:12] __main__ INFO: \u001b[0mElapsed 17.04\n",
      "\u001b[32m[2020-06-09 01:13:12] __main__ INFO: \u001b[0mTrain 42 14391\n",
      "\u001b[32m[2020-06-09 01:15:29] __main__ INFO: \u001b[0mEpoch 42 Step 100/351 lr 0.100000 loss 0.3569 (0.3352) acc@1 0.9062 (0.8846) acc@5 1.0000 (0.9969)\n",
      "\u001b[32m[2020-06-09 01:17:46] __main__ INFO: \u001b[0mEpoch 42 Step 200/351 lr 0.100000 loss 0.2733 (0.3400) acc@1 0.8984 (0.8830) acc@5 1.0000 (0.9970)\n",
      "\u001b[32m[2020-06-09 01:20:02] __main__ INFO: \u001b[0mEpoch 42 Step 300/351 lr 0.100000 loss 0.5434 (0.3419) acc@1 0.8125 (0.8822) acc@5 0.9844 (0.9969)\n",
      "\u001b[32m[2020-06-09 01:21:12] __main__ INFO: \u001b[0mEpoch 42 Step 351/351 lr 0.100000 loss 0.4647 (0.3406) acc@1 0.8438 (0.8827) acc@5 1.0000 (0.9969)\n",
      "\u001b[32m[2020-06-09 01:21:12] __main__ INFO: \u001b[0mElapsed 479.34\n",
      "\u001b[32m[2020-06-09 01:21:12] __main__ INFO: \u001b[0mVal 42\n",
      "\u001b[32m[2020-06-09 01:21:29] __main__ INFO: \u001b[0mEpoch 42 loss 0.5068 acc@1 0.8328 acc@5 0.9912\n",
      "\u001b[32m[2020-06-09 01:21:29] __main__ INFO: \u001b[0mElapsed 17.03\n",
      "\u001b[32m[2020-06-09 01:21:29] __main__ INFO: \u001b[0mTrain 43 14742\n",
      "\u001b[32m[2020-06-09 01:23:45] __main__ INFO: \u001b[0mEpoch 43 Step 100/351 lr 0.100000 loss 0.3897 (0.3266) acc@1 0.8438 (0.8869) acc@5 1.0000 (0.9967)\n"
     ]
    }
   ],
   "source": [
    "# Train the model per the settings specified for ResNext 29_4x64d in the original paper\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "!python train.py --config configs/cifar/resnext.yaml \\\n",
    "    model.resnext.cardinality 4 \\\n",
    "    train.batch_size 128 \\\n",
    "    train.base_lr 0.1 \\\n",
    "    train.output_dir /home/ec2-user/SageMaker/experiments/resnext_29_4x64d/exp00 \\\n",
    "    scheduler.epochs 300\n",
    "\n",
    "# Number of epochs should be 300!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-11 01:08:55] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/resnext_29_4x64d/exp00/checkpoint_00300.pth\n",
      "Files already downloaded and verified\n",
      "100%|███████████████████████████████████████████| 79/79 [00:34<00:00,  2.29it/s]\n",
      "\u001b[32m[2020-06-11 01:09:30] __main__ INFO: \u001b[0mElapsed 34.44\n",
      "\u001b[32m[2020-06-11 01:09:30] __main__ INFO: \u001b[0mLoss 0.1517 Accuracy 0.9535\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/resnext.yaml \\\n",
    "   model.resnext.cardinality 4 \\\n",
    "   test.batch_size 128 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/resnext_29_4x64d/exp00/checkpoint_00300.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/resnext_29_4x64d/exp00/test_results_0300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-11 01:50:14] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/resnext_29_4x64d/exp00/checkpoint_00200.pth\n",
      "Files already downloaded and verified\n",
      "100%|███████████████████████████████████████████| 79/79 [00:34<00:00,  2.30it/s]\n",
      "\u001b[32m[2020-06-11 01:50:49] __main__ INFO: \u001b[0mElapsed 34.36\n",
      "\u001b[32m[2020-06-11 01:50:49] __main__ INFO: \u001b[0mLoss 0.2311 Accuracy 0.9321\n"
     ]
    }
   ],
   "source": [
    "!python evaluate.py --config configs/cifar/resnext.yaml \\\n",
    "   model.resnext.cardinality 4 \\\n",
    "   test.batch_size 128 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/resnext_29_4x64d/exp00/checkpoint_00200.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/resnext_29_4x64d/exp00/test_results_0200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-11 01:51:05] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/resnext_29_4x64d/exp00/checkpoint_00100.pth\n",
      "Files already downloaded and verified\n",
      "100%|███████████████████████████████████████████| 79/79 [00:34<00:00,  2.28it/s]\n",
      "\u001b[32m[2020-06-11 01:51:41] __main__ INFO: \u001b[0mElapsed 34.69\n",
      "\u001b[32m[2020-06-11 01:51:41] __main__ INFO: \u001b[0mLoss 0.6746 Accuracy 0.8019\n"
     ]
    }
   ],
   "source": [
    "!python evaluate.py --config configs/cifar/resnext.yaml \\\n",
    "   model.resnext.cardinality 4 \\\n",
    "   test.batch_size 128 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/resnext_29_4x64d/exp00/checkpoint_00100.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/resnext_29_4x64d/exp00/test_results_0100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Testset</th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Original_Accuracy</th>\n",
       "      <th>Original_CI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>resnext_29_4x64d</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>100</td>\n",
       "      <td>0.6746</td>\n",
       "      <td>0.8019</td>\n",
       "      <td>96.4</td>\n",
       "      <td>(96.0, 96.7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>resnext_29_4x64d</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>200</td>\n",
       "      <td>0.2311</td>\n",
       "      <td>0.9321</td>\n",
       "      <td>96.4</td>\n",
       "      <td>(96.0, 96.7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>resnext_29_4x64d</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>300</td>\n",
       "      <td>0.1517</td>\n",
       "      <td>0.9535</td>\n",
       "      <td>96.4</td>\n",
       "      <td>(96.0, 96.7)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Model  Testset  Epoch    Loss  Accuracy  Original_Accuracy  \\\n",
       "0  resnext_29_4x64d  cifar10    100  0.6746    0.8019               96.4   \n",
       "1  resnext_29_4x64d  cifar10    200  0.2311    0.9321               96.4   \n",
       "2  resnext_29_4x64d  cifar10    300  0.1517    0.9535               96.4   \n",
       "\n",
       "    Original_CI  \n",
       "0  (96.0, 96.7)  \n",
       "1  (96.0, 96.7)  \n",
       "2  (96.0, 96.7)  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write the results to a CSV file so that we can analyze later.\n",
    "import pandas as pd\n",
    "\n",
    "results = {'Model': ['resnext_29_4x64d', 'resnext_29_4x64d', 'resnext_29_4x64d'],\n",
    "           'Testset': ['cifar10', 'cifar10', 'cifar10'],\n",
    "           'Epoch': [100, 200, 300],\n",
    "           'Loss': [0.6746, 0.2311, 0.1517],\n",
    "           'Accuracy': [0.8019, 0.9321, 0.9535],\n",
    "           'Original_Accuracy': [96.4, 96.4, 96.4],\n",
    "           'Original_CI': [(96.0, 96.7), (96.0, 96.7), (96.0, 96.7)]\n",
    "           }\n",
    "\n",
    "df = pd.DataFrame(results, columns = ['Model', 'Testset', 'Epoch', 'Loss', 'Accuracy', \n",
    "                                      'Original_Accuracy', 'Original_CI'])\n",
    "\n",
    "\n",
    "df.to_csv('/home/ec2-user/SageMaker/experiments/resnext_29_4x64d/exp00/results.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['preds', 'probs', 'labels', 'loss', 'acc']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-1.2041907 , -1.8999833 , -0.24285015, ..., -1.5008752 ,\n",
       "        -1.8426697 , -2.8560946 ],\n",
       "       [ 0.5460079 ,  2.220384  , -1.9393705 , ..., -2.6070693 ,\n",
       "        11.327686  , -1.2085156 ],\n",
       "       [-1.3446747 ,  2.1730833 , -1.1615647 , ..., -2.2299995 ,\n",
       "        10.984515  , -0.75660706],\n",
       "       ...,\n",
       "       [-2.4790986 , -1.3337001 ,  0.61669415, ..., -0.83421385,\n",
       "        -1.8529658 , -1.7280097 ],\n",
       "       [-0.90489024,  9.350766  ,  1.0618937 , ..., -2.3210623 ,\n",
       "        -0.9061641 , -1.8115013 ],\n",
       "       [-1.4560711 , -1.0518838 , -1.4613396 , ..., 12.668192  ,\n",
       "        -2.1191459 , -0.8881919 ]], dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Peak inside the output file for predictions\n",
    "import numpy as np\n",
    "output = '/home/ec2-user/SageMaker/experiments/resnext_29_4x64d/exp00/test_results_0300/predictions.npz'\n",
    "npzfile = np.load(output)\n",
    "print(npzfile.files)\n",
    "npzfile['preds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the model checkpoints, configs, and results to S3 \n",
    "bucket='sagemaker-may29'\n",
    "prefix = 'sagemaker/results/original-models/'\n",
    "path = '/home/ec2-user/SageMaker/experiments/'\n",
    "\n",
    "s3_resource = boto3.resource(\"s3\", region_name=\"us-east-2\")\n",
    "\n",
    "def uploadDirectory(local_path,bucket_name,s3_prefix):\n",
    "\n",
    "    my_bucket = s3_resource.Bucket(bucket_name)\n",
    "    \n",
    "    for path, subdirs, files in os.walk(local_path):\n",
    "        path = path.replace(\"\\\\\",\"/\")\n",
    "        directory_name = path.replace(local_path,\"\")\n",
    "        for file in files:\n",
    "            #print(\"Local File:\", os.path.join(path, file))\n",
    "            #print(\"      Dest:\", s3_prefix+directory_name+'/'+file)\n",
    "            my_bucket.upload_file(os.path.join(path, file), s3_prefix+directory_name+'/'+file)\n",
    "    \n",
    "uploadDirectory(path,bucket,prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Everything Below Is In-Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SageMaker Way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 Variable Definitions \n",
    "bucket='sagemaker-may29'\n",
    "prefix = '/sagemaker-may29/sagemaker/cifar102/data/cifar10_10k'\n",
    "model_path = '/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/'\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "#bucket = sagemaker.Session().default_bucket(\n",
    "#sagemaker_session = sagemaker.Session()\n",
    "bucket='sagemaker-may29'\n",
    "sagemaker_session = sagemaker.Session(default_bucket=bucket)\n",
    "\n",
    "\n",
    "# Set S3 dataset path \n",
    "#inputs = 's3://' + bucket + '/sagemaker/{}'.format(args.train_data)\n",
    "inputs = 's3://' + bucket + '/sagemaker-may29/sagemaker/cifar102/data/cifar10_10k/cifar10_10k_test'\n",
    "inputs = 's3://' + bucket + '/sagemaker-may29/sagemaker/cifar102/data/cifar10_10k/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/'\n",
    "container_data_dir = '/opt/ml/input/data/training'\n",
    "container_model_dir = '/opt/ml/model'\n",
    "\n",
    "parameters = {\n",
    "    'config': 'resnext.yaml',\n",
    "    'resnext.depth': 29,\n",
    "    'train.batch_size': 128,\n",
    "    'train.base_lr': 0.1,\n",
    "    #'data_dir': container_data_dir,\n",
    "    #'dataset.dataset_dir': container_data_dir\n",
    "    #'output_dir': container_model_dir,\n",
    "    #'num_train_epochs': 3,\n",
    "    #'per_gpu_train_batch_size': 64,\n",
    "    #'per_gpu_eval_batch_size': 64,\n",
    "    #'save_steps': 150,\n",
    "    #'logging_steps': 150\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the PyTorch class that enables the model script to run as a \n",
    "# training job on the SageMaker distributed, managed training infrastructure\n",
    "estimator = PyTorch(entry_point= model_path + 'train.py',\n",
    "                    role=role,\n",
    "                    framework_version='1.4.0',\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type='ml.p2.xlarge',\n",
    "                    hyperparameters=parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n",
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-08 14:49:34 Starting - Starting the training job...\n",
      "2020-06-08 14:49:36 Starting - Launching requested ML instances.........\n",
      "2020-06-08 14:51:06 Starting - Preparing the instances for training......\n",
      "2020-06-08 14:52:21 Downloading - Downloading input data...\n",
      "2020-06-08 14:53:01 Training - Downloading the training image...........\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-06-08 14:54:49,375 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-06-08 14:54:49,397 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-06-08 14:54:49,403 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-06-08 14:54:49,684 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-06-08 14:54:49,684 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-06-08 14:54:49,684 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-06-08 14:54:49,685 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m pip install . \u001b[0m\n",
      "\u001b[34mProcessing /tmp/tmprotlgjc8/module_dir\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: default-user-module-name\n",
      "  Building wheel for default-user-module-name (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for default-user-module-name (setup.py): finished with status 'done'\n",
      "  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=10386 sha256=4f15af3dee30a7116f4f4bdfc068a488b47ab4d7d874d41ddf7a13c58d82ad7a\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-hdo2srdp/wheels/c9/42/19/42704d4efcc75760cfda35a2b5e353ab766536836839b671fb\u001b[0m\n",
      "\u001b[34mSuccessfully built default-user-module-name\u001b[0m\n",
      "\u001b[34mInstalling collected packages: default-user-module-name\u001b[0m\n",
      "\u001b[34mSuccessfully installed default-user-module-name-1.0.0\u001b[0m\n",
      "\u001b[34m2020-06-08 14:54:51,994 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"resnext.depth\": 29,\n",
      "        \"train.base_lr\": 0.1,\n",
      "        \"train.batch_size\": 128,\n",
      "        \"config\": \"resnext.yaml\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2020-06-08-14-49-34-014\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-2-681306256852/pytorch-training-2020-06-08-14-49-34-014/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"config\":\"resnext.yaml\",\"resnext.depth\":29,\"train.base_lr\":0.1,\"train.batch_size\":128}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-2-681306256852/pytorch-training-2020-06-08-14-49-34-014/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"config\":\"resnext.yaml\",\"resnext.depth\":29,\"train.base_lr\":0.1,\"train.batch_size\":128},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2020-06-08-14-49-34-014\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-681306256852/pytorch-training-2020-06-08-14-49-34-014/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--config\",\"resnext.yaml\",\"--resnext.depth\",\"29\",\"--train.base_lr\",\"0.1\",\"--train.batch_size\",\"128\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_RESNEXT.DEPTH=29\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN.BASE_LR=0.1\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN.BATCH_SIZE=128\u001b[0m\n",
      "\u001b[34mSM_HP_CONFIG=resnext.yaml\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python train.py --config resnext.yaml --resnext.depth 29 --train.base_lr 0.1 --train.batch_size 128\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m2020-06-08 14:54:52,052 sagemaker-containers ERROR    ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[34mCommand \"/opt/conda/bin/python train.py --config resnext.yaml --resnext.depth 29 --train.base_lr 0.1 --train.batch_size 128\"\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"train.py\", line 7, in <module>\n",
      "    import apex\u001b[0m\n",
      "\u001b[34mModuleNotFoundError: No module named 'apex'\u001b[0m\n",
      "\n",
      "2020-06-08 14:55:00 Uploading - Uploading generated training model\n",
      "2020-06-08 14:55:00 Failed - Training job failed\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job pytorch-training-2020-06-08-14-49-34-014: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"/opt/conda/bin/python train.py --config resnext.yaml --resnext.depth 29 --train.base_lr 0.1 --train.batch_size 128\"\nTraceback (most recent call last):\n  File \"train.py\", line 7, in <module>\n    import apex\nModuleNotFoundError: No module named 'apex'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b5cd4c918894>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m's3://'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbucket\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/sagemaker-may29/sagemaker/cifar102/data/cifar10_10k/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#estimator.fit({'training': inputs})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compilation_job_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0;31m# If logs are requested, call logs_for_jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"None\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1109\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1110\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type)\u001b[0m\n\u001b[1;32m   3066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3067\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3068\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TrainingJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3069\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3070\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   2660\u001b[0m                 ),\n\u001b[1;32m   2661\u001b[0m                 \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Stopped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2662\u001b[0;31m                 \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2663\u001b[0m             )\n\u001b[1;32m   2664\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job pytorch-training-2020-06-08-14-49-34-014: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"/opt/conda/bin/python train.py --config resnext.yaml --resnext.depth 29 --train.base_lr 0.1 --train.batch_size 128\"\nTraceback (most recent call last):\n  File \"train.py\", line 7, in <module>\n    import apex\nModuleNotFoundError: No module named 'apex'"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "#inputs = 's3://' + bucket + '/sagemaker-may29/sagemaker/cifar102/data/cifar10_10k/cifar10_10k_test'\n",
    "inputs = 's3://' + bucket + '/sagemaker-may29/sagemaker/cifar102/data/cifar10_10k/'\n",
    "#estimator.fit({'training': inputs})\n",
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ec2-user/SageMaker'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Augmentation \n",
    "https://imgaug.readthedocs.io/en/latest/source/overview/collections.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imgaug in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (0.4.0)\n",
      "Requirement already satisfied: imageio in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from imgaug) (2.3.0)\n",
      "Requirement already satisfied: opencv-python in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from imgaug) (4.1.1.26)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from imgaug) (1.11.0)\n",
      "Requirement already satisfied: matplotlib in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from imgaug) (3.0.3)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from imgaug) (1.1.0)\n",
      "Requirement already satisfied: Pillow in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from imgaug) (5.4.1)\n",
      "Requirement already satisfied: Shapely in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from imgaug) (1.7.0)\n",
      "Requirement already satisfied: scikit-image>=0.14.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from imgaug) (0.17.2)\n",
      "Requirement already satisfied: numpy>=1.15 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from imgaug) (1.15.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from matplotlib->imgaug) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from matplotlib->imgaug) (1.0.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from matplotlib->imgaug) (2.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from matplotlib->imgaug) (2.7.3)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from scikit-image>=0.14.2->imgaug) (1.1.1)\n",
      "Requirement already satisfied: networkx>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from scikit-image>=0.14.2->imgaug) (2.1)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from scikit-image>=0.14.2->imgaug) (2020.6.3)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib->imgaug) (47.1.1)\n",
      "Requirement already satisfied: decorator>=4.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from networkx>=2.0->scikit-image>=0.14.2->imgaug) (4.3.0)\n",
      "\u001b[31mfastai 1.0.60 requires nvidia-ml-py3, which is not installed.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.2b1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install imgaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imgaug.augmenters as iaa\n",
    "aug = iaa.RandAugment(n=2, m=(0, 9))  \n",
    "  # n is the number of transformations to apply per image\n",
    "  # m is magnitude -> specifying a tuple will randomly select values between the min and max (max is 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3 \n",
    "from PIL import Image\n",
    "\n",
    "cifar10 = \"sagemaker/cifar-10/cifar/\"\n",
    "#cifar10_test_rec = \"s3://sagemaker-may29/sagemaker/cifar-10/cifar/test.rec\"\n",
    "\n",
    "def download_all_objects_in_folder(b, p):\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    my_bucket = s3_resource.Bucket(b)\n",
    "    objects = my_bucket.objects.filter(Prefix=p)\n",
    "    for obj in objects:\n",
    "        path, filename = os.path.split(obj.key)\n",
    "        my_bucket.download_file(obj.key, filename)    \n",
    "\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "# Download the unzipped data from the cifar10 folder\n",
    "download_all_objects_in_folder(bucket, cifar10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.rec: data\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandAugment \n",
    "https://arxiv.org/abs/1909.13719\n",
    "https://pypi.org/project/randaugment/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting randaugment\n",
      "  Using cached https://files.pythonhosted.org/packages/fb/ea/e24549f459800dc3bed21cd4e9c0d49d5b8deed65214b2444bd3e5a49f30/randaugment-1.0.2-py3-none-any.whl\n",
      "\u001b[31mfastai 1.0.60 requires nvidia-ml-py3, which is not installed.\u001b[0m\n",
      "Installing collected packages: randaugment\n",
      "Successfully installed randaugment-1.0.2\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.2b1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install randaugment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ImageFolder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-114a2729ff3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrandaugment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandAugment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageNetPolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m data = ImageFolder('/home/ec2-user/SageMaker/w210-capstone/data/cifar10/subset300', transform=transforms.Compose(\n\u001b[0m\u001b[1;32m      3\u001b[0m                         [\n\u001b[1;32m      4\u001b[0m                             \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomCrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# fill parameter needs torchvision installed from source\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                             \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomHorizontalFlip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ImageFolder' is not defined"
     ]
    }
   ],
   "source": [
    "from randaugment import RandAugment, ImageNetPolicy\n",
    "data = ImageFolder('/home/ec2-user/SageMaker/w210-capstone/data/cifar10/subset300', transform=transforms.Compose(\n",
    "                        [\n",
    "                            transforms.RandomCrop(32, padding=4, fill=128), # fill parameter needs torchvision installed from source\n",
    "                            transforms.RandomHorizontalFlip(), \n",
    "                            RandAugment(),\n",
    "                            #ImageNetPolicy(),\n",
    "                            transforms.ToTensor(), \n",
    "                            Cutout(size=16), # (https://github.com/uoguelph-mlrg/Cutout/blob/master/util/cutout.py)\n",
    "                            transforms.Normalize(...)\n",
    "                        ])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Augment\n",
    "https://blog.insightdatascience.com/automl-for-data-augmentation-e87cf692c366\n",
    "https://colab.research.google.com/drive/1KCAv2i_F3E3m_PKh56nbbZY8WnaASvgl#scrollTo=SuhR6Q3AMFy3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deepaugment\n",
      "  Downloading https://files.pythonhosted.org/packages/99/f9/40211d827039df475091639c6aded9a1786849f898b9c619e24c15efc82a/deepaugment-1.1.2-py2.py3-none-any.whl\n",
      "Collecting keras-applications==1.0.6 (from deepaugment)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/c4/2ff40221029f7098d58f8d7fb99b97e8100f3293f9856f0fb5834bef100b/Keras_Applications-1.0.6-py2.py3-none-any.whl (44kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 4.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pandas==0.23.4 (from deepaugment)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/d8/feeb346d41f181e83fba45224ab14a8d8af019b48af742e047f3845d8cff/pandas-0.23.4-cp36-cp36m-manylinux1_x86_64.whl (8.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 8.9MB 5.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scikit-optimize==0.5.2 (from deepaugment)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/44/60f82c97d1caa98752c7da2c1681cab5c7a390a0fdd3a55fac672b321cac/scikit_optimize-0.5.2-py2.py3-none-any.whl (74kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 34.7MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting click==7.0 (from deepaugment)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fa/37/45185cb5abbc30d7257104c434fe0b07e5a195a6847506c074527aa599ec/Click-7.0-py2.py3-none-any.whl (81kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 33.4MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting numpy==1.15.4 (from deepaugment)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/7f/9d804d2348471c67a7d8b5f84f9bc59fd1cefa148986f2b74552f8573555/numpy-1.15.4-cp36-cp36m-manylinux1_x86_64.whl (13.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 13.9MB 3.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opencv-contrib-python (from deepaugment)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c6/db/790dbc6bcfea87fc6f790c6306509c2691ce31c96d82e5b826545d90ea52/opencv_contrib_python-4.2.0.34-cp36-cp36m-manylinux1_x86_64.whl (34.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 34.2MB 1.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting imgaug==0.2.7 (from deepaugment)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/fc/c56a7da8c23122b7c5325b941850013880a7a93c21dc95e2b1ecd4750108/imgaug-0.2.7-py3-none-any.whl (644kB)\n",
      "\u001b[K    100% |████████████████████████████████| 645kB 34.7MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow==1.12.0 (from deepaugment)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/cc/ca70b78087015d21c5f3f93694107f34ebccb3be9624385a911d4b52ecef/tensorflow-1.12.0-cp36-cp36m-manylinux1_x86_64.whl (83.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 83.1MB 574kB/s eta 0:00:01    62% |███████████████████▉            | 51.6MB 52.9MB/s eta 0:00:01    76% |████████████████████████▋       | 63.8MB 62.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting matplotlib==3.0.2 (from deepaugment)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/07/16d781df15be30df4acfd536c479268f1208b2dfbc91e9ca5d92c9caf673/matplotlib-3.0.2-cp36-cp36m-manylinux1_x86_64.whl (12.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 12.9MB 4.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting setuptools==40.6.3 (from deepaugment)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/37/06/754589caf971b0d2d48f151c2586f62902d93dc908e2fd9b9b9f6aa3c9dd/setuptools-40.6.3-py2.py3-none-any.whl (573kB)\n",
      "\u001b[K    100% |████████████████████████████████| 573kB 31.6MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting keras==2.2.4 (from deepaugment)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl (312kB)\n",
      "\u001b[K    100% |████████████████████████████████| 317kB 37.5MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: h5py in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from keras-applications==1.0.6->deepaugment) (2.8.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from pandas==0.23.4->deepaugment) (2.7.3)\n",
      "Requirement already satisfied: pytz>=2011k in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from pandas==0.23.4->deepaugment) (2018.4)\n",
      "Requirement already satisfied: scipy>=0.14.0 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from scikit-optimize==0.5.2->deepaugment) (1.1.0)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from scikit-optimize==0.5.2->deepaugment) (0.20.3)\n",
      "Requirement already satisfied: scikit-image>=0.11.0 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from imgaug==0.2.7->deepaugment) (0.13.1)\n",
      "Collecting Shapely (from imgaug==0.2.7->deepaugment)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/fa/c96d3461fda99ed8e82ff0b219ac2c8384694b4e640a611a1a8390ecd415/Shapely-1.7.0-cp36-cp36m-manylinux1_x86_64.whl (1.8MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.8MB 18.5MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: Pillow in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from imgaug==0.2.7->deepaugment) (5.1.0)\n",
      "Requirement already satisfied: imageio in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from imgaug==0.2.7->deepaugment) (2.3.0)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from imgaug==0.2.7->deepaugment) (1.11.0)\n",
      "Collecting absl-py>=0.1.6 (from tensorflow==1.12.0->deepaugment)\n",
      "Collecting keras-preprocessing>=1.0.5 (from tensorflow==1.12.0->deepaugment)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/4c/7c3275a01e12ef9368a892926ab932b33bb13d55794881e3573482b378a7/Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 16.3MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting protobuf>=3.6.1 (from tensorflow==1.12.0->deepaugment)\n",
      "  Using cached https://files.pythonhosted.org/packages/28/05/9867ef8eafd12265267bee138fa2c46ebf34a276ea4cbe184cba4c606e8b/protobuf-3.12.2-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting grpcio>=1.8.6 (from tensorflow==1.12.0->deepaugment)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow==1.12.0->deepaugment)\n",
      "Collecting astor>=0.6.0 (from tensorflow==1.12.0->deepaugment)\n",
      "  Downloading https://files.pythonhosted.org/packages/c3/88/97eef84f48fa04fbd6750e62dcceafba6c63c81b7ac1420856c8dcc0a3f9/astor-0.8.1-py2.py3-none-any.whl\n",
      "Collecting tensorboard<1.13.0,>=1.12.0 (from tensorflow==1.12.0->deepaugment)\n",
      "\u001b[33m  Cache entry deserialization failed, entry ignored\u001b[0m\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/53/8d32ce9471c18f8d99028b7cef2e5b39ea8765bd7ef250ca05b490880971/tensorboard-1.12.2-py3-none-any.whl (3.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.1MB 20.0MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from tensorflow==1.12.0->deepaugment) (0.31.1)\n",
      "Collecting gast>=0.2.0 (from tensorflow==1.12.0->deepaugment)\n",
      "  Downloading https://files.pythonhosted.org/packages/d6/84/759f5dd23fec8ba71952d97bcc7e2c9d7d63bdc582421f3cd4be845f0c98/gast-0.3.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from matplotlib==3.0.2->deepaugment) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from matplotlib==3.0.2->deepaugment) (2.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from matplotlib==3.0.2->deepaugment) (1.0.1)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from keras==2.2.4->deepaugment) (5.3.1)\n",
      "Requirement already satisfied: networkx>=1.8 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from scikit-image>=0.11.0->imgaug==0.2.7->deepaugment) (2.1)\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from scikit-image>=0.11.0->imgaug==0.2.7->deepaugment) (0.5.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.10 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.12.0->deepaugment) (0.14.1)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.12.0->deepaugment)\n",
      "  Using cached https://files.pythonhosted.org/packages/a4/63/eaec2bd025ab48c754b55e8819af0f6a69e2b1e187611dd40cbbe101ee7f/Markdown-3.2.2-py3-none-any.whl\n",
      "Requirement already satisfied: decorator>=4.1.0 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from networkx>=1.8->scikit-image>=0.11.0->imgaug==0.2.7->deepaugment) (4.3.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard<1.13.0,>=1.12.0->tensorflow==1.12.0->deepaugment) (1.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.13.0,>=1.12.0->tensorflow==1.12.0->deepaugment) (3.0.0)\n",
      "\u001b[31mamazonei-mxnet 1.5.1 has requirement numpy<1.15.0,>=1.8.2, but you'll have numpy 1.15.4 which is incompatible.\u001b[0m\n",
      "Installing collected packages: numpy, keras-applications, pandas, scikit-optimize, click, opencv-contrib-python, Shapely, matplotlib, imgaug, absl-py, keras-preprocessing, setuptools, protobuf, grpcio, termcolor, astor, markdown, tensorboard, gast, tensorflow, keras, deepaugment\n",
      "  Found existing installation: numpy 1.14.6\n",
      "    Uninstalling numpy-1.14.6:\n",
      "      Successfully uninstalled numpy-1.14.6\n",
      "  Found existing installation: pandas 0.24.2\n",
      "    Uninstalling pandas-0.24.2:\n",
      "      Successfully uninstalled pandas-0.24.2\n",
      "  Found existing installation: click 6.7\n",
      "    Uninstalling click-6.7:\n",
      "      Successfully uninstalled click-6.7\n",
      "  Found existing installation: matplotlib 3.0.3\n",
      "    Uninstalling matplotlib-3.0.3:\n",
      "      Successfully uninstalled matplotlib-3.0.3\n",
      "  Found existing installation: setuptools 39.1.0\n",
      "    Uninstalling setuptools-39.1.0:\n",
      "      Successfully uninstalled setuptools-39.1.0\n",
      "  Found existing installation: protobuf 3.5.2\n",
      "    Uninstalling protobuf-3.5.2:\n",
      "      Successfully uninstalled protobuf-3.5.2\n",
      "Successfully installed Shapely-1.7.0 absl-py-0.9.0 astor-0.8.1 click-7.0 deepaugment-1.1.2 gast-0.3.3 grpcio-1.29.0 imgaug-0.2.7 keras-2.2.4 keras-applications-1.0.6 keras-preprocessing-1.1.2 markdown-3.2.2 matplotlib-3.0.2 numpy-1.15.4 opencv-contrib-python-4.2.0.34 pandas-0.23.4 protobuf-3.12.2 scikit-optimize-0.5.2 setuptools-40.6.3 tensorboard-1.12.2 tensorflow-1.12.0 termcolor-1.1.0\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.2b1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install deepaugment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__new__() got an unexpected keyword argument 'serialized_options'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-952c53067624>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdeepaugment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepaugment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDeepAugment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m deepaug = DeepAugment('/home/ec2-user/SageMaker/w210-capstone/data/cifar10/subset300', \n\u001b[1;32m      4\u001b[0m                       '/home/ec2-user/SageMaker/w210-capstone/data/cifar10/trainLabels.csv')\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/deepaugment/deepaugment.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# (C) 2019 Baris Ozmen <hbaristr@gmail.com>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# pylint: disable=g-bad-import-order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m  \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;31m# Protocol buffers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_pb2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_def_pb2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary_pb2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/tensorflow/core/framework/graph_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnode_def_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_node__def__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunction_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_function__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mversions_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_versions__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/tensorflow/core/framework/node_def_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mattr_value_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_attr__value__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/tensorflow/core/framework/attr_value_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_tensor__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_shape_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypes_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_types__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/tensorflow/core/framework/tensor_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mresource_handle_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_resource__handle__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_shape_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypes_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_types__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/tensorflow/core/framework/resource_handle_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0msyntax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'proto3'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0mserialized_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\030org.tensorflow.frameworkB\\016ResourceHandleP\\001Z=github.com/tensorflow/tensorflow/tensorflow/go/core/framework\\370\\001\\001'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m   \u001b[0mserialized_pb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n/tensorflow/core/framework/resource_handle.proto\\x12\\ntensorflow\\\"r\\n\\x13ResourceHandleProto\\x12\\x0e\\n\\x06\\x64\\x65vice\\x18\\x01 \\x01(\\t\\x12\\x11\\n\\tcontainer\\x18\\x02 \\x01(\\t\\x12\\x0c\\n\\x04name\\x18\\x03 \\x01(\\t\\x12\\x11\\n\\thash_code\\x18\\x04 \\x01(\\x04\\x12\\x17\\n\\x0fmaybe_type_name\\x18\\x05 \\x01(\\tBn\\n\\x18org.tensorflow.frameworkB\\x0eResourceHandleP\\x01Z=github.com/tensorflow/tensorflow/tensorflow/go/core/framework\\xf8\\x01\\x01\\x62\\x06proto3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __new__() got an unexpected keyword argument 'serialized_options'"
     ]
    }
   ],
   "source": [
    "from deepaugment.deepaugment import DeepAugment\n",
    "\n",
    "deepaug = DeepAugment('/home/ec2-user/SageMaker/w210-capstone/data/cifar10/subset300', \n",
    "                      '/home/ec2-user/SageMaker/w210-capstone/data/cifar10/trainLabels.csv')\n",
    "\n",
    "best_policies = deepaug.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker\n"
     ]
    }
   ],
   "source": [
    "!ls /home/ec2-user/SageMaker/w210-capstone/data/cifar10/subset300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must be conda_pytroch_p36 notebook\n",
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import sagemaker\n",
    "import torch\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.pytorch import PyTorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r ./w210-capstone/models/pytorch_imageclass/requirements.txt (line 1)) (1.15.4)\n",
      "Requirement already satisfied: torch>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r ./w210-capstone/models/pytorch_imageclass/requirements.txt (line 2)) (1.4.0)\n",
      "Requirement already satisfied: torchvision in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r ./w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (0.5.0)\n",
      "Requirement already satisfied: fvcore in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r ./w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (0.1.1.post20200603)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r ./w210-capstone/models/pytorch_imageclass/requirements.txt (line 5)) (4.42.1)\n",
      "Requirement already satisfied: yacs in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r ./w210-capstone/models/pytorch_imageclass/requirements.txt (line 6)) (0.1.7)\n",
      "Requirement already satisfied: apex from git+https://github.com/NVIDIA/apex.git#egg=apex in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r ./w210-capstone/models/pytorch_imageclass/requirements.txt (line 7)) (0.1)\n",
      "Requirement already satisfied: termcolor in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r ./w210-capstone/models/pytorch_imageclass/requirements.txt (line 8)) (1.1.0)\n",
      "Requirement already satisfied: thop<0.0.31.post2004070130 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r ./w210-capstone/models/pytorch_imageclass/requirements.txt (line 9)) (0.0.31.post2001170342)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision->-r ./w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (1.11.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision->-r ./w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (5.4.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r ./w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (5.3.1)\n",
      "Requirement already satisfied: tabulate in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r ./w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (0.8.7)\n",
      "Requirement already satisfied: portalocker in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r ./w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (1.7.0)\n",
      "\u001b[31mfastai 1.0.60 requires nvidia-ml-py3, which is not installed.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.2b1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r ./w210-capstone/models/pytorch_imageclass/requirements.txt\n",
    "\n",
    "# Do we need NVIDIA installed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-04 00:41:31] __main__ INFO: \u001b[0mdevice: cpu\n",
      "cudnn:\n",
      "  benchmark: True\n",
      "  deterministic: False\n",
      "dataset:\n",
      "  name: CIFAR10\n",
      "  dataset_dir: ~/.torch/datasets/CIFAR10\n",
      "  image_size: 32\n",
      "  n_channels: 3\n",
      "  n_classes: 10\n",
      "model:\n",
      "  type: cifar\n",
      "  name: resnext\n",
      "  init_mode: kaiming_fan_out\n",
      "  vgg:\n",
      "    n_channels: [64, 128, 256, 512, 512]\n",
      "    n_layers: [2, 2, 3, 3, 3]\n",
      "    use_bn: True\n",
      "  resnet:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "  resnet_preact:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "  wrn:\n",
      "    depth: 28\n",
      "    initial_channels: 16\n",
      "    widening_factor: 10\n",
      "    drop_rate: 0.0\n",
      "  densenet:\n",
      "    depth: 100\n",
      "    n_blocks: [6, 12, 24, 16]\n",
      "    block_type: bottleneck\n",
      "    growth_rate: 12\n",
      "    drop_rate: 0.0\n",
      "    compression_rate: 0.5\n",
      "  pyramidnet:\n",
      "    depth: 272\n",
      "    n_blocks: [3, 24, 36, 3]\n",
      "    initial_channels: 16\n",
      "    block_type: bottleneck\n",
      "    alpha: 200\n",
      "  resnext:\n",
      "    depth: 29\n",
      "    n_blocks: [3, 4, 6, 3]\n",
      "    initial_channels: 64\n",
      "    cardinality: 4\n",
      "    base_channels: 64\n",
      "  shake_shake:\n",
      "    depth: 26\n",
      "    initial_channels: 96\n",
      "    shake_forward: True\n",
      "    shake_backward: True\n",
      "    shake_image: True\n",
      "  se_resnet_preact:\n",
      "    depth: 110\n",
      "    initial_channels: 16\n",
      "    se_reduction: 16\n",
      "    block_type: basic\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "train:\n",
      "  checkpoint: ''\n",
      "  resume: False\n",
      "  precision: O0\n",
      "  batch_size: 128\n",
      "  subdivision: 1\n",
      "  optimizer: sgd\n",
      "  base_lr: 0.1\n",
      "  momentum: 0.9\n",
      "  nesterov: True\n",
      "  weight_decay: 0.0005\n",
      "  no_weight_decay_on_bn: False\n",
      "  gradient_clip: 0\n",
      "  start_epoch: 0\n",
      "  seed: 0\n",
      "  val_first: True\n",
      "  val_period: 1\n",
      "  val_ratio: 0.1\n",
      "  use_test_as_val: False\n",
      "  output_dir: experiments/resnext_29_4x64d/exp00\n",
      "  log_period: 100\n",
      "  checkpoint_period: 100\n",
      "  use_tensorboard: True\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: True\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "  distributed: False\n",
      "  dist:\n",
      "    backend: nccl\n",
      "    init_method: env://\n",
      "    world_size: -1\n",
      "    node_rank: -1\n",
      "    local_rank: 0\n",
      "    use_sync_bn: False\n",
      "tensorboard:\n",
      "  train_images: False\n",
      "  val_images: False\n",
      "  model_params: False\n",
      "optim:\n",
      "  adam:\n",
      "    betas: (0.9, 0.999)\n",
      "  lars:\n",
      "    eps: 1e-09\n",
      "    threshold: 0.01\n",
      "  adabound:\n",
      "    betas: (0.9, 0.999)\n",
      "    final_lr: 0.1\n",
      "    gamma: 0.001\n",
      "scheduler:\n",
      "  epochs: 300\n",
      "  warmup:\n",
      "    type: none\n",
      "    epochs: 0\n",
      "    start_factor: 0.001\n",
      "    exponent: 4\n",
      "  type: multistep\n",
      "  milestones: [150, 225]\n",
      "  lr_decay: 0.1\n",
      "  lr_min_factor: 0.001\n",
      "  T0: 10\n",
      "  T_mul: 1.0\n",
      "validation:\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: False\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "augmentation:\n",
      "  use_random_crop: True\n",
      "  use_random_horizontal_flip: True\n",
      "  use_cutout: False\n",
      "  use_random_erasing: False\n",
      "  use_dual_cutout: False\n",
      "  use_mixup: False\n",
      "  use_ricap: False\n",
      "  use_cutmix: False\n",
      "  use_label_smoothing: False\n",
      "  random_crop:\n",
      "    padding: 4\n",
      "    fill: 0\n",
      "    padding_mode: constant\n",
      "  random_horizontal_flip:\n",
      "    prob: 0.5\n",
      "  cutout:\n",
      "    prob: 1.0\n",
      "    mask_size: 16\n",
      "    cut_inside: False\n",
      "    mask_color: 0\n",
      "    dual_cutout_alpha: 0.1\n",
      "  random_erasing:\n",
      "    prob: 0.5\n",
      "    area_ratio_range: [0.02, 0.4]\n",
      "    min_aspect_ratio: 0.3\n",
      "    max_attempt: 20\n",
      "  mixup:\n",
      "    alpha: 1.0\n",
      "  ricap:\n",
      "    beta: 0.3\n",
      "  cutmix:\n",
      "    alpha: 1.0\n",
      "  label_smoothing:\n",
      "    epsilon: 0.1\n",
      "tta:\n",
      "  use_resize: False\n",
      "  use_center_crop: False\n",
      "  resize: 256\n",
      "test:\n",
      "  checkpoint: ''\n",
      "  output_dir: ''\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    pin_memory: False\n",
      "\u001b[32m[2020-06-04 00:41:31] __main__ INFO: \u001b[0menv_info:\n",
      "  pytorch_version: 1.4.0\n",
      "  cuda_version: 10.1\n",
      "  cudnn_version: 7603\n",
      "Files already downloaded and verified\n",
      "\u001b[32m[2020-06-04 00:41:33] __main__ INFO: \u001b[0mMACs  : 2.75G\n",
      "\u001b[32m[2020-06-04 00:41:33] __main__ INFO: \u001b[0m#params: 17.56M\n",
      "Selected optimization level O0:  Pure FP32 training.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Traceback (most recent call last):\n",
      "  File \"w210-capstone/models/pytorch_imageclass/train.py\", line 436, in <module>\n",
      "    main()\n",
      "  File \"w210-capstone/models/pytorch_imageclass/train.py\", line 362, in main\n",
      "    opt_level=config.train.precision)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/apex/amp/frontend.py\", line 358, in initialize\n",
      "    return _initialize(models, optimizers, _amp_state.opt_properties, num_losses, cast_model_outputs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/apex/amp/_initialize.py\", line 171, in _initialize\n",
      "    check_params_fp32(models)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/apex/amp/_initialize.py\", line 93, in check_params_fp32\n",
      "    name, param.type()))\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/apex/amp/_amp_state.py\", line 32, in warn_or_err\n",
      "    raise RuntimeError(msg)\n",
      "RuntimeError: Found param conv.weight with type torch.FloatTensor, expected torch.cuda.FloatTensor.\n",
      "When using amp.initialize, you need to provide a model with parameters\n",
      "located on a CUDA device before passing it no matter what optimization level\n",
      "you chose. Use model.to('cuda') to use the default device.\n"
     ]
    }
   ],
   "source": [
    "# Must be conda_pytroch_p36 notebook\n",
    "!python w210-capstone/models/pytorch_imageclass/train.py --config w210-capstone/models/pytorch_imageclass/configs/cifar/resnext.yaml \\\n",
    "   model.resnext.depth 29 \\\n",
    "   model.resnext.cardinality 4 \\\n",
    "   train.batch_size 128 \\\n",
    "   train.base_lr 0.1 \\\n",
    "   train.output_dir experiments/resnext_29_4x64d/exp00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCRATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Data and model checkpoints directories\n",
    "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                        help='input batch size for training (default: 64)')\n",
    "    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                        help='input batch size for testing (default: 1000)')\n",
    "    parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
    "                        help='number of epochs to train (default: 10)')\n",
    "    parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n",
    "                        help='learning rate (default: 0.01)')\n",
    "    parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n",
    "                        help='SGD momentum (default: 0.5)')\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    parser.add_argument('--log-interval', type=int, default=100, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "    parser.add_argument('--backend', type=str, default=None,\n",
    "                        help='backend for distributed training (tcp, gloo on cpu and gloo, nccl on gpu)')\n",
    "\n",
    "    \n",
    "    parser.add_argument('--epochs', type=int, default=10)\n",
    "    parser.add_argument('--batch-size', type=int, default=128)\n",
    "    parser.add_argument('--learning-rate', type=float, default=0.1)\n",
    "    \n",
    "    parser.add_argument('--model_name', dest='model_name', type=str, help='model to train')\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'],\n",
    "                        help='directory to save trained model to')\n",
    "    parser.add_argument('--train_data', dest='train_data', type=str, help='dataset for model training') \n",
    "    parser.add_argument('--workers', dest='workers', type=int, help='number of V100 worker instances; 1 indicates non-distributed training')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Container environment\n",
    "    env = sagemaker_containers.training_env()\n",
    "    parser.add_argument('--hosts', type=list, default=env.hosts)\n",
    "    parser.add_argument('--current-host', type=str, default=env.current_host)\n",
    "    parser.add_argument('--model-dir', type=str, default=env.model_dir)\n",
    "    parser.add_argument('--data-dir', type=str,\n",
    "                        default=env.channel_input_dirs['training'])\n",
    "    parser.add_argument('--num-gpus', type=int, default=env.num_gpus)\n",
    "\n",
    "    train(parser.parse_args())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-3319f1f978a5>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-3319f1f978a5>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    echo $CUDA_PATH\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#torch.cuda.get_device_name(0)\n",
    "echo $CUDA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mCould not open requirements file: [Errno 2] No such file or directory: './w210-capstone/models/pytorch/requirements.txt'\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.2b1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r ./w210-capstone/models/pytorch_/requirements.txt\n",
    "\n",
    "# Do we need NVIDIA installed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance type = local\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "instance_type = 'local'\n",
    "\n",
    "if subprocess.call('nvidia-smi') == 0:\n",
    "    ## Set type to GPU if one is present\n",
    "    instance_type = 'local_gpu'\n",
    "    \n",
    "print(\"Instance type = \" + instance_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-03 20:54:56] __main__ INFO: \u001b[0mdevice: cpu\n",
      "cudnn:\n",
      "  benchmark: True\n",
      "  deterministic: False\n",
      "dataset:\n",
      "  name: CIFAR10\n",
      "  dataset_dir: ~/.torch/datasets/CIFAR10\n",
      "  image_size: 32\n",
      "  n_channels: 3\n",
      "  n_classes: 10\n",
      "model:\n",
      "  type: cifar\n",
      "  name: resnext\n",
      "  init_mode: kaiming_fan_out\n",
      "  vgg:\n",
      "    n_channels: [64, 128, 256, 512, 512]\n",
      "    n_layers: [2, 2, 3, 3, 3]\n",
      "    use_bn: True\n",
      "  resnet:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "  resnet_preact:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "  wrn:\n",
      "    depth: 28\n",
      "    initial_channels: 16\n",
      "    widening_factor: 10\n",
      "    drop_rate: 0.0\n",
      "  densenet:\n",
      "    depth: 100\n",
      "    n_blocks: [6, 12, 24, 16]\n",
      "    block_type: bottleneck\n",
      "    growth_rate: 12\n",
      "    drop_rate: 0.0\n",
      "    compression_rate: 0.5\n",
      "  pyramidnet:\n",
      "    depth: 272\n",
      "    n_blocks: [3, 24, 36, 3]\n",
      "    initial_channels: 16\n",
      "    block_type: bottleneck\n",
      "    alpha: 200\n",
      "  resnext:\n",
      "    depth: 29\n",
      "    n_blocks: [3, 4, 6, 3]\n",
      "    initial_channels: 64\n",
      "    cardinality: 4\n",
      "    base_channels: 64\n",
      "  shake_shake:\n",
      "    depth: 26\n",
      "    initial_channels: 96\n",
      "    shake_forward: True\n",
      "    shake_backward: True\n",
      "    shake_image: True\n",
      "  se_resnet_preact:\n",
      "    depth: 110\n",
      "    initial_channels: 16\n",
      "    se_reduction: 16\n",
      "    block_type: basic\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "train:\n",
      "  checkpoint: ''\n",
      "  resume: False\n",
      "  precision: O0\n",
      "  batch_size: 128\n",
      "  subdivision: 1\n",
      "  optimizer: sgd\n",
      "  base_lr: 0.1\n",
      "  momentum: 0.9\n",
      "  nesterov: True\n",
      "  weight_decay: 0.0005\n",
      "  no_weight_decay_on_bn: False\n",
      "  gradient_clip: 0\n",
      "  start_epoch: 0\n",
      "  seed: 0\n",
      "  val_first: True\n",
      "  val_period: 1\n",
      "  val_ratio: 0.1\n",
      "  use_test_as_val: False\n",
      "  output_dir: experiments/resnext_29_4x64d/exp00\n",
      "  log_period: 100\n",
      "  checkpoint_period: 100\n",
      "  use_tensorboard: True\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: True\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "  distributed: False\n",
      "  dist:\n",
      "    backend: nccl\n",
      "    init_method: env://\n",
      "    world_size: -1\n",
      "    node_rank: -1\n",
      "    local_rank: 0\n",
      "    use_sync_bn: False\n",
      "tensorboard:\n",
      "  train_images: False\n",
      "  val_images: False\n",
      "  model_params: False\n",
      "optim:\n",
      "  adam:\n",
      "    betas: (0.9, 0.999)\n",
      "  lars:\n",
      "    eps: 1e-09\n",
      "    threshold: 0.01\n",
      "  adabound:\n",
      "    betas: (0.9, 0.999)\n",
      "    final_lr: 0.1\n",
      "    gamma: 0.001\n",
      "scheduler:\n",
      "  epochs: 300\n",
      "  warmup:\n",
      "    type: none\n",
      "    epochs: 0\n",
      "    start_factor: 0.001\n",
      "    exponent: 4\n",
      "  type: multistep\n",
      "  milestones: [150, 225]\n",
      "  lr_decay: 0.1\n",
      "  lr_min_factor: 0.001\n",
      "  T0: 10\n",
      "  T_mul: 1.0\n",
      "validation:\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: False\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "augmentation:\n",
      "  use_random_crop: True\n",
      "  use_random_horizontal_flip: True\n",
      "  use_cutout: False\n",
      "  use_random_erasing: False\n",
      "  use_dual_cutout: False\n",
      "  use_mixup: False\n",
      "  use_ricap: False\n",
      "  use_cutmix: False\n",
      "  use_label_smoothing: False\n",
      "  random_crop:\n",
      "    padding: 4\n",
      "    fill: 0\n",
      "    padding_mode: constant\n",
      "  random_horizontal_flip:\n",
      "    prob: 0.5\n",
      "  cutout:\n",
      "    prob: 1.0\n",
      "    mask_size: 16\n",
      "    cut_inside: False\n",
      "    mask_color: 0\n",
      "    dual_cutout_alpha: 0.1\n",
      "  random_erasing:\n",
      "    prob: 0.5\n",
      "    area_ratio_range: [0.02, 0.4]\n",
      "    min_aspect_ratio: 0.3\n",
      "    max_attempt: 20\n",
      "  mixup:\n",
      "    alpha: 1.0\n",
      "  ricap:\n",
      "    beta: 0.3\n",
      "  cutmix:\n",
      "    alpha: 1.0\n",
      "  label_smoothing:\n",
      "    epsilon: 0.1\n",
      "tta:\n",
      "  use_resize: False\n",
      "  use_center_crop: False\n",
      "  resize: 256\n",
      "test:\n",
      "  checkpoint: ''\n",
      "  output_dir: ''\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    pin_memory: False\n",
      "\u001b[32m[2020-06-03 20:54:56] __main__ INFO: \u001b[0menv_info:\n",
      "  pytorch_version: 1.4.0\n",
      "  cuda_version: 10.1\n",
      "  cudnn_version: 7603\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /home/ec2-user/.torch/datasets/CIFAR10/cifar-10-python.tar.gz\n",
      "170500096it [00:02, 67641451.46it/s]                                            \n",
      "Extracting /home/ec2-user/.torch/datasets/CIFAR10/cifar-10-python.tar.gz to /home/ec2-user/.torch/datasets/CIFAR10\n",
      "\u001b[32m[2020-06-03 20:55:02] __main__ INFO: \u001b[0mMACs  : 2.75G\n",
      "\u001b[32m[2020-06-03 20:55:02] __main__ INFO: \u001b[0m#params: 17.56M\n",
      "Selected optimization level O0:  Pure FP32 training.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Traceback (most recent call last):\n",
      "  File \"w210-capstone/models/pytorch/train.py\", line 436, in <module>\n",
      "    main()\n",
      "  File \"w210-capstone/models/pytorch/train.py\", line 362, in main\n",
      "    opt_level=config.train.precision)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/apex/amp/frontend.py\", line 358, in initialize\n",
      "    return _initialize(models, optimizers, _amp_state.opt_properties, num_losses, cast_model_outputs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/apex/amp/_initialize.py\", line 171, in _initialize\n",
      "    check_params_fp32(models)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/apex/amp/_initialize.py\", line 93, in check_params_fp32\n",
      "    name, param.type()))\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/apex/amp/_amp_state.py\", line 32, in warn_or_err\n",
      "    raise RuntimeError(msg)\n",
      "RuntimeError: Found param conv.weight with type torch.FloatTensor, expected torch.cuda.FloatTensor.\n",
      "When using amp.initialize, you need to provide a model with parameters\n",
      "located on a CUDA device before passing it no matter what optimization level\n",
      "you chose. Use model.to('cuda') to use the default device.\n"
     ]
    }
   ],
   "source": [
    "# ResNext-29_4x64\n",
    "!python w210-capstone/models/pytorch/train.py --config w210-capstone/models/pytorch/configs/cifar/resnext.yaml \\\n",
    "   model.resnext.depth 29 \\\n",
    "   model.resnext.cardinality 4 \\\n",
    "   train.batch_size 128 \\\n",
    "   train.base_lr 0.1 \\\n",
    "   train.output_dir experiments/resnext_29_4x64d/exp00\n",
    "\n",
    "# ResNeXt-29 4x64d with a single GPU, batch size 32 and initial learning rate 0.025 \n",
    "# (8 GPUs, batch size 128 and initial learning rate 0.1 in paper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# initialize command line argument parsing\n",
    "if __name__=='__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--epochs', type=int, default=10)\n",
    "    parser.add_argument('--batch-size', type=int, default=128)\n",
    "    parser.add_argument('--learning-rate', type=float, default=0.1)\n",
    "    \n",
    "    parser.add_argument('--model_name', dest='model_name', type=str, help='model to train')\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'],\n",
    "                        help='directory to save trained model to')\n",
    "    parser.add_argument('--train_data', dest='train_data', type=str, help='dataset for model training') \n",
    "    parser.add_argument('--workers', dest='workers', type=int, help='number of V100 worker instances; 1 indicates non-distributed training')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Check quality of arguments\n",
    "    valid_args = {'datasets': ['cifar10', 'cifar10_10k', 'cifar10_30k', 'cifar102', 'cifar102_30k'],\n",
    "                  'model_names': ['wrn', 'shake_shake_32', 'shake_shake_96', 'shake_shake_112', 'pyramid_net']}\n",
    "\n",
    "    if args.train_data not in valid_args['datasets']:\n",
    "        parser.error('Invalid train_data parameter')\n",
    "\n",
    "    if args.model_name not in valid_args['model_names']:\n",
    "        parser.error('Invalid model_name parameter')\n",
    "    \n",
    "    if args.workers < 1:\n",
    "        parser.error('Invalid number of workers')\n",
    "\n",
    "    if not args.model_name:\n",
    "        parser.error('--model_name parameter is required')\n",
    "    elif not args.train_data:\n",
    "        parser.error('--train_data parameter is required')\n",
    "    elif not args.workers:\n",
    "        parser.error('--workers parameter is required')\n",
    "\n",
    "    # Set SageMaker session & execution role\n",
    "    bucket='sagemaker-may29'\n",
    "    sagemaker_session = sagemaker.Session(default_bucket=bucket)\n",
    "    role = get_execution_role()\n",
    "\n",
    "\n",
    "    # Set S3 path for data batches\n",
    "    inputs = 's3://' + bucket + '/sagemaker/{}'.format(args.train_data)\n",
    "\n",
    "    # Create the sagemaker estimator\n",
    "    pytorch_estimator = PyTorch('./w210-capstone/models/pytorch/train.py',\n",
    "                                train_instance_type='ml.p3.2xlarge',\n",
    "                                train_instance_count=1,\n",
    "                                framework_version='1.0.0',\n",
    "                                hyperparameters = {'epochs': 20, 'batch-size': 64, 'learning-rate': 0.1})\n",
    "\n",
    "    --config w210-capstone/models/pytorch/configs/cifar/resnext.yaml \\\n",
    "   model.resnext.depth 29 \\\n",
    "   model.resnext.cardinality 4 \\\n",
    "   train.batch_size 128 \\\n",
    "   train.base_lr 0.1 \\\n",
    "   train.output_dir experiments/resnext_29_4x64d/exp00\n",
    "    \n",
    "    \n",
    "    # Train the Model\n",
    "    pytorch_estimator.fit({'train': 's3://my-data-bucket/path/to/my/training/data',\n",
    "                           'test': 's3://my-data-bucket/path/to/my/test/data'})\n",
    "\n",
    "\n",
    "    # After training, save the model to `model_dir`\n",
    "    with open(os.path.join(args.model_dir, 'model.pth'), 'wb') as f:\n",
    "        torch.save(model.state_dict(), f)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
