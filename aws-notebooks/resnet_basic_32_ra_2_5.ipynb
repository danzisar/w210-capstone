{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import numpy \n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESNET 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 1)) (1.18.1)\n",
      "Requirement already satisfied: torch>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 2)) (1.4.0)\n",
      "Requirement already satisfied: torchvision in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (0.5.0)\n",
      "Requirement already satisfied: fvcore in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (0.1.1.post20200619)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 5)) (4.44.1)\n",
      "Requirement already satisfied: yacs in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 6)) (0.1.7)\n",
      "Requirement already satisfied: apex from git+https://github.com/NVIDIA/apex.git#egg=apex in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 7)) (0.1)\n",
      "Requirement already satisfied: termcolor in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 8)) (1.1.0)\n",
      "Requirement already satisfied: thop<0.0.31.post2004070130 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 9)) (0.0.31.post2001170342)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (1.14.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (7.0.0)\n",
      "Requirement already satisfied: portalocker in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (1.7.0)\n",
      "Requirement already satisfied: tabulate in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (0.8.7)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (5.3.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Sagemaker Notebook must be of type, conda_pytorch_p36\n",
    "\n",
    "!pip install -r '/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorboard in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.18.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (46.1.3.post20200330)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.29.0)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (3.12.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (3.2.2)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (0.34.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.6.0.post3)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.0.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (0.9.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (0.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (2.23.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.14.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.18.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard) (1.5.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (1.25.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (2020.4.5.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (2.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (4.1.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (3.4.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard) (2.2.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.1.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard) (0.4.8)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Need to add this to requirements.txt\n",
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model per the settings specified in the original RESNET paper\n",
    "# os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "# !python train.py --config configs/cifar/resnet.yaml \\\n",
    "#     model.resnet.depth 32 \\\n",
    "#     train.batch_size 128 \\\n",
    "#     dataset.name CIFAR10 \\\n",
    "#     train.base_lr 0.1 \\\n",
    "#     train.output_dir /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_5/exp00 \\\n",
    "#     scheduler.epochs 1\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-20 03:47:33] __main__ INFO: \u001b[0mdevice: cuda\n",
      "cudnn:\n",
      "  benchmark: True\n",
      "  deterministic: False\n",
      "dataset:\n",
      "  name: CIFAR10_RA_2_5\n",
      "  dataset_dir: ''\n",
      "  image_size: 32\n",
      "  n_channels: 3\n",
      "  n_classes: 10\n",
      "model:\n",
      "  type: cifar\n",
      "  name: resnet\n",
      "  init_mode: kaiming_fan_out\n",
      "  vgg:\n",
      "    n_channels: [64, 128, 256, 512, 512]\n",
      "    n_layers: [2, 2, 3, 3, 3]\n",
      "    use_bn: True\n",
      "  resnet:\n",
      "    depth: 32\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "  resnet_preact:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "  wrn:\n",
      "    depth: 28\n",
      "    initial_channels: 16\n",
      "    widening_factor: 10\n",
      "    drop_rate: 0.0\n",
      "  densenet:\n",
      "    depth: 100\n",
      "    n_blocks: [6, 12, 24, 16]\n",
      "    block_type: bottleneck\n",
      "    growth_rate: 12\n",
      "    drop_rate: 0.0\n",
      "    compression_rate: 0.5\n",
      "  pyramidnet:\n",
      "    depth: 272\n",
      "    n_blocks: [3, 24, 36, 3]\n",
      "    initial_channels: 16\n",
      "    block_type: bottleneck\n",
      "    alpha: 200\n",
      "  resnext:\n",
      "    depth: 29\n",
      "    n_blocks: [3, 4, 6, 3]\n",
      "    initial_channels: 64\n",
      "    cardinality: 8\n",
      "    base_channels: 4\n",
      "  shake_shake:\n",
      "    depth: 26\n",
      "    initial_channels: 96\n",
      "    shake_forward: True\n",
      "    shake_backward: True\n",
      "    shake_image: True\n",
      "  se_resnet_preact:\n",
      "    depth: 110\n",
      "    initial_channels: 16\n",
      "    se_reduction: 16\n",
      "    block_type: basic\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "train:\n",
      "  checkpoint: ''\n",
      "  resume: False\n",
      "  precision: O0\n",
      "  batch_size: 128\n",
      "  subdivision: 1\n",
      "  optimizer: sgd\n",
      "  base_lr: 0.1\n",
      "  momentum: 0.9\n",
      "  nesterov: True\n",
      "  weight_decay: 0.0001\n",
      "  no_weight_decay_on_bn: False\n",
      "  gradient_clip: 0\n",
      "  start_epoch: 0\n",
      "  seed: 0\n",
      "  val_first: True\n",
      "  val_period: 1\n",
      "  val_ratio: 0.1\n",
      "  use_test_as_val: False\n",
      "  output_dir: /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_5/exp00\n",
      "  log_period: 100\n",
      "  checkpoint_period: 100\n",
      "  use_tensorboard: True\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: True\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "  distributed: False\n",
      "  dist:\n",
      "    backend: nccl\n",
      "    init_method: env://\n",
      "    world_size: -1\n",
      "    node_rank: -1\n",
      "    local_rank: 0\n",
      "    use_sync_bn: False\n",
      "tensorboard:\n",
      "  train_images: False\n",
      "  val_images: False\n",
      "  model_params: False\n",
      "optim:\n",
      "  adam:\n",
      "    betas: (0.9, 0.999)\n",
      "  lars:\n",
      "    eps: 1e-09\n",
      "    threshold: 0.01\n",
      "  adabound:\n",
      "    betas: (0.9, 0.999)\n",
      "    final_lr: 0.1\n",
      "    gamma: 0.001\n",
      "scheduler:\n",
      "  epochs: 400\n",
      "  warmup:\n",
      "    type: none\n",
      "    epochs: 0\n",
      "    start_factor: 0.001\n",
      "    exponent: 4\n",
      "  type: multistep\n",
      "  milestones: [80, 120]\n",
      "  lr_decay: 0.1\n",
      "  lr_min_factor: 0.001\n",
      "  T0: 10\n",
      "  T_mul: 1.0\n",
      "validation:\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: False\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "augmentation:\n",
      "  use_random_crop: True\n",
      "  use_random_horizontal_flip: True\n",
      "  use_cutout: False\n",
      "  use_random_erasing: False\n",
      "  use_dual_cutout: False\n",
      "  use_mixup: False\n",
      "  use_ricap: False\n",
      "  use_cutmix: False\n",
      "  use_label_smoothing: False\n",
      "  random_crop:\n",
      "    padding: 4\n",
      "    fill: 0\n",
      "    padding_mode: constant\n",
      "  random_horizontal_flip:\n",
      "    prob: 0.5\n",
      "  cutout:\n",
      "    prob: 1.0\n",
      "    mask_size: 16\n",
      "    cut_inside: False\n",
      "    mask_color: 0\n",
      "    dual_cutout_alpha: 0.1\n",
      "  random_erasing:\n",
      "    prob: 0.5\n",
      "    area_ratio_range: [0.02, 0.4]\n",
      "    min_aspect_ratio: 0.3\n",
      "    max_attempt: 20\n",
      "  mixup:\n",
      "    alpha: 1.0\n",
      "  ricap:\n",
      "    beta: 0.3\n",
      "  cutmix:\n",
      "    alpha: 1.0\n",
      "  label_smoothing:\n",
      "    epsilon: 0.1\n",
      "tta:\n",
      "  use_resize: False\n",
      "  use_center_crop: False\n",
      "  resize: 256\n",
      "test:\n",
      "  checkpoint: ''\n",
      "  output_dir: ''\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    pin_memory: False\n",
      "\u001b[32m[2020-06-20 03:47:33] __main__ INFO: \u001b[0menv_info:\n",
      "  pytorch_version: 1.4.0\n",
      "  cuda_version: 10.1\n",
      "  cudnn_version: 7603\n",
      "  num_gpus: 1\n",
      "  gpu_name: Tesla K80\n",
      "  gpu_capability: 3.7\n",
      "(50000, 32, 32, 3)\n",
      "\u001b[32m[2020-06-20 03:47:37] __main__ INFO: \u001b[0mMACs  : 69.76M\n",
      "\u001b[32m[2020-06-20 03:47:37] __main__ INFO: \u001b[0m#params: 466.91K\n",
      "Selected optimization level O0:  Pure FP32 training.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\",)\n",
      "\u001b[32m[2020-06-20 03:47:37] __main__ INFO: \u001b[0mVal 0\n",
      "\u001b[32m[2020-06-20 03:47:39] __main__ INFO: \u001b[0mEpoch 0 loss 607.3919 acc@1 0.1008 acc@5 0.5044\n",
      "\u001b[32m[2020-06-20 03:47:39] __main__ INFO: \u001b[0mElapsed 1.43\n",
      "\u001b[32m[2020-06-20 03:47:39] __main__ INFO: \u001b[0mTrain 1 0\n",
      "\u001b[32m[2020-06-20 03:47:48] __main__ INFO: \u001b[0mEpoch 1 Step 100/351 lr 0.100000 loss 2.2775 (2.7635) acc@1 0.1484 (0.1091) acc@5 0.5938 (0.5177)\n",
      "\u001b[32m[2020-06-20 03:47:57] __main__ INFO: \u001b[0mEpoch 1 Step 200/351 lr 0.100000 loss 2.2538 (2.5216) acc@1 0.1094 (0.1199) acc@5 0.5703 (0.5464)\n",
      "\u001b[32m[2020-06-20 03:48:06] __main__ INFO: \u001b[0mEpoch 1 Step 300/351 lr 0.100000 loss 2.2610 (2.4351) acc@1 0.1328 (0.1240) acc@5 0.5781 (0.5613)\n",
      "\u001b[32m[2020-06-20 03:48:11] __main__ INFO: \u001b[0mEpoch 1 Step 351/351 lr 0.100000 loss 2.2529 (2.4078) acc@1 0.1172 (0.1260) acc@5 0.5625 (0.5685)\n",
      "\u001b[32m[2020-06-20 03:48:11] __main__ INFO: \u001b[0mElapsed 32.22\n",
      "\u001b[32m[2020-06-20 03:48:11] __main__ INFO: \u001b[0mVal 1\n",
      "\u001b[32m[2020-06-20 03:48:12] __main__ INFO: \u001b[0mEpoch 1 loss 2.2504 acc@1 0.1296 acc@5 0.6016\n",
      "\u001b[32m[2020-06-20 03:48:12] __main__ INFO: \u001b[0mElapsed 1.06\n",
      "\u001b[32m[2020-06-20 03:48:12] __main__ INFO: \u001b[0mTrain 2 351\n",
      "\u001b[32m[2020-06-20 03:48:21] __main__ INFO: \u001b[0mEpoch 2 Step 100/351 lr 0.100000 loss 2.1848 (2.2267) acc@1 0.1641 (0.1434) acc@5 0.7109 (0.6339)\n",
      "\u001b[32m[2020-06-20 03:48:30] __main__ INFO: \u001b[0mEpoch 2 Step 200/351 lr 0.100000 loss 2.2606 (2.2001) acc@1 0.1250 (0.1565) acc@5 0.6484 (0.6477)\n",
      "\u001b[32m[2020-06-20 03:48:39] __main__ INFO: \u001b[0mEpoch 2 Step 300/351 lr 0.100000 loss 2.0754 (2.1770) acc@1 0.2266 (0.1703) acc@5 0.7188 (0.6585)\n",
      "\u001b[32m[2020-06-20 03:48:44] __main__ INFO: \u001b[0mEpoch 2 Step 351/351 lr 0.100000 loss 2.1286 (2.1663) acc@1 0.1328 (0.1752) acc@5 0.6875 (0.6654)\n",
      "\u001b[32m[2020-06-20 03:48:44] __main__ INFO: \u001b[0mElapsed 32.06\n",
      "\u001b[32m[2020-06-20 03:48:44] __main__ INFO: \u001b[0mVal 2\n",
      "\u001b[32m[2020-06-20 03:48:45] __main__ INFO: \u001b[0mEpoch 2 loss 2.0885 acc@1 0.2210 acc@5 0.7000\n",
      "\u001b[32m[2020-06-20 03:48:45] __main__ INFO: \u001b[0mElapsed 1.20\n",
      "\u001b[32m[2020-06-20 03:48:45] __main__ INFO: \u001b[0mTrain 3 702\n",
      "\u001b[32m[2020-06-20 03:48:54] __main__ INFO: \u001b[0mEpoch 3 Step 100/351 lr 0.100000 loss 2.1640 (2.0756) acc@1 0.1562 (0.2158) acc@5 0.6641 (0.7017)\n",
      "\u001b[32m[2020-06-20 03:49:04] __main__ INFO: \u001b[0mEpoch 3 Step 200/351 lr 0.100000 loss 2.0141 (2.0580) acc@1 0.2500 (0.2236) acc@5 0.7344 (0.7087)\n",
      "\u001b[32m[2020-06-20 03:49:13] __main__ INFO: \u001b[0mEpoch 3 Step 300/351 lr 0.100000 loss 2.0496 (2.0527) acc@1 0.2344 (0.2261) acc@5 0.7266 (0.7136)\n",
      "\u001b[32m[2020-06-20 03:49:17] __main__ INFO: \u001b[0mEpoch 3 Step 351/351 lr 0.100000 loss 2.0042 (2.0478) acc@1 0.1797 (0.2285) acc@5 0.7578 (0.7145)\n",
      "\u001b[32m[2020-06-20 03:49:17] __main__ INFO: \u001b[0mElapsed 32.14\n",
      "\u001b[32m[2020-06-20 03:49:17] __main__ INFO: \u001b[0mVal 3\n",
      "\u001b[32m[2020-06-20 03:49:18] __main__ INFO: \u001b[0mEpoch 3 loss 2.0020 acc@1 0.2532 acc@5 0.7336\n",
      "\u001b[32m[2020-06-20 03:49:18] __main__ INFO: \u001b[0mElapsed 1.05\n",
      "\u001b[32m[2020-06-20 03:49:18] __main__ INFO: \u001b[0mTrain 4 1053\n",
      "\u001b[32m[2020-06-20 03:49:28] __main__ INFO: \u001b[0mEpoch 4 Step 100/351 lr 0.100000 loss 1.9766 (1.9976) acc@1 0.2266 (0.2505) acc@5 0.7969 (0.7330)\n",
      "\u001b[32m[2020-06-20 03:49:37] __main__ INFO: \u001b[0mEpoch 4 Step 200/351 lr 0.100000 loss 1.9360 (1.9924) acc@1 0.2812 (0.2542) acc@5 0.6875 (0.7338)\n",
      "\u001b[32m[2020-06-20 03:49:46] __main__ INFO: \u001b[0mEpoch 4 Step 300/351 lr 0.100000 loss 1.9710 (1.9778) acc@1 0.2812 (0.2592) acc@5 0.7578 (0.7368)\n",
      "\u001b[32m[2020-06-20 03:49:51] __main__ INFO: \u001b[0mEpoch 4 Step 351/351 lr 0.100000 loss 1.9898 (1.9742) acc@1 0.2578 (0.2597) acc@5 0.7109 (0.7389)\n",
      "\u001b[32m[2020-06-20 03:49:51] __main__ INFO: \u001b[0mElapsed 32.33\n",
      "\u001b[32m[2020-06-20 03:49:51] __main__ INFO: \u001b[0mVal 4\n",
      "\u001b[32m[2020-06-20 03:49:52] __main__ INFO: \u001b[0mEpoch 4 loss 2.0498 acc@1 0.2382 acc@5 0.7476\n",
      "\u001b[32m[2020-06-20 03:49:52] __main__ INFO: \u001b[0mElapsed 1.05\n",
      "\u001b[32m[2020-06-20 03:49:52] __main__ INFO: \u001b[0mTrain 5 1404\n",
      "\u001b[32m[2020-06-20 03:50:01] __main__ INFO: \u001b[0mEpoch 5 Step 100/351 lr 0.100000 loss 1.8134 (1.9297) acc@1 0.3281 (0.2762) acc@5 0.7969 (0.7513)\n",
      "\u001b[32m[2020-06-20 03:50:10] __main__ INFO: \u001b[0mEpoch 5 Step 200/351 lr 0.100000 loss 1.9298 (1.9151) acc@1 0.3125 (0.2832) acc@5 0.7578 (0.7560)\n",
      "\u001b[32m[2020-06-20 03:50:19] __main__ INFO: \u001b[0mEpoch 5 Step 300/351 lr 0.100000 loss 1.7945 (1.8976) acc@1 0.3359 (0.2902) acc@5 0.7969 (0.7598)\n",
      "\u001b[32m[2020-06-20 03:50:24] __main__ INFO: \u001b[0mEpoch 5 Step 351/351 lr 0.100000 loss 1.8172 (1.8908) acc@1 0.3438 (0.2924) acc@5 0.7812 (0.7610)\n",
      "\u001b[32m[2020-06-20 03:50:24] __main__ INFO: \u001b[0mElapsed 32.30\n",
      "\u001b[32m[2020-06-20 03:50:24] __main__ INFO: \u001b[0mVal 5\n",
      "\u001b[32m[2020-06-20 03:50:25] __main__ INFO: \u001b[0mEpoch 5 loss 2.0749 acc@1 0.2520 acc@5 0.7274\n",
      "\u001b[32m[2020-06-20 03:50:25] __main__ INFO: \u001b[0mElapsed 1.08\n",
      "\u001b[32m[2020-06-20 03:50:25] __main__ INFO: \u001b[0mTrain 6 1755\n",
      "\u001b[32m[2020-06-20 03:50:34] __main__ INFO: \u001b[0mEpoch 6 Step 100/351 lr 0.100000 loss 1.6568 (1.8258) acc@1 0.4219 (0.3212) acc@5 0.8438 (0.7772)\n",
      "\u001b[32m[2020-06-20 03:50:44] __main__ INFO: \u001b[0mEpoch 6 Step 200/351 lr 0.100000 loss 1.9301 (1.8192) acc@1 0.3203 (0.3241) acc@5 0.7109 (0.7776)\n",
      "\u001b[32m[2020-06-20 03:50:53] __main__ INFO: \u001b[0mEpoch 6 Step 300/351 lr 0.100000 loss 1.7715 (1.8050) acc@1 0.3125 (0.3284) acc@5 0.7734 (0.7790)\n",
      "\u001b[32m[2020-06-20 03:50:57] __main__ INFO: \u001b[0mEpoch 6 Step 351/351 lr 0.100000 loss 1.8196 (1.7985) acc@1 0.3047 (0.3320) acc@5 0.7422 (0.7812)\n",
      "\u001b[32m[2020-06-20 03:50:58] __main__ INFO: \u001b[0mElapsed 32.36\n",
      "\u001b[32m[2020-06-20 03:50:58] __main__ INFO: \u001b[0mVal 6\n",
      "\u001b[32m[2020-06-20 03:50:59] __main__ INFO: \u001b[0mEpoch 6 loss 1.8222 acc@1 0.3246 acc@5 0.7836\n",
      "\u001b[32m[2020-06-20 03:50:59] __main__ INFO: \u001b[0mElapsed 1.05\n",
      "\u001b[32m[2020-06-20 03:50:59] __main__ INFO: \u001b[0mTrain 7 2106\n",
      "\u001b[32m[2020-06-20 03:51:08] __main__ INFO: \u001b[0mEpoch 7 Step 100/351 lr 0.100000 loss 1.8078 (1.7395) acc@1 0.3750 (0.3492) acc@5 0.7422 (0.7987)\n",
      "\u001b[32m[2020-06-20 03:51:17] __main__ INFO: \u001b[0mEpoch 7 Step 200/351 lr 0.100000 loss 1.6706 (1.7283) acc@1 0.4297 (0.3546) acc@5 0.8359 (0.7965)\n",
      "\u001b[32m[2020-06-20 03:51:26] __main__ INFO: \u001b[0mEpoch 7 Step 300/351 lr 0.100000 loss 1.7203 (1.7245) acc@1 0.3594 (0.3583) acc@5 0.8203 (0.7960)\n",
      "\u001b[32m[2020-06-20 03:51:31] __main__ INFO: \u001b[0mEpoch 7 Step 351/351 lr 0.100000 loss 1.6499 (1.7220) acc@1 0.3984 (0.3586) acc@5 0.7812 (0.7953)\n",
      "\u001b[32m[2020-06-20 03:51:31] __main__ INFO: \u001b[0mElapsed 32.48\n",
      "\u001b[32m[2020-06-20 03:51:31] __main__ INFO: \u001b[0mVal 7\n",
      "\u001b[32m[2020-06-20 03:51:32] __main__ INFO: \u001b[0mEpoch 7 loss 1.7126 acc@1 0.3742 acc@5 0.8074\n",
      "\u001b[32m[2020-06-20 03:51:32] __main__ INFO: \u001b[0mElapsed 1.05\n",
      "\u001b[32m[2020-06-20 03:51:32] __main__ INFO: \u001b[0mTrain 8 2457\n",
      "\u001b[32m[2020-06-20 03:51:41] __main__ INFO: \u001b[0mEpoch 8 Step 100/351 lr 0.100000 loss 1.6800 (1.6610) acc@1 0.3672 (0.3848) acc@5 0.8125 (0.8018)\n",
      "\u001b[32m[2020-06-20 03:51:51] __main__ INFO: \u001b[0mEpoch 8 Step 200/351 lr 0.100000 loss 1.7421 (1.6572) acc@1 0.3828 (0.3838) acc@5 0.7812 (0.8020)\n",
      "\u001b[32m[2020-06-20 03:52:00] __main__ INFO: \u001b[0mEpoch 8 Step 300/351 lr 0.100000 loss 1.6553 (1.6519) acc@1 0.3359 (0.3866) acc@5 0.7656 (0.8043)\n",
      "\u001b[32m[2020-06-20 03:52:05] __main__ INFO: \u001b[0mEpoch 8 Step 351/351 lr 0.100000 loss 1.7689 (1.6544) acc@1 0.3516 (0.3854) acc@5 0.7891 (0.8036)\n",
      "\u001b[32m[2020-06-20 03:52:05] __main__ INFO: \u001b[0mElapsed 32.44\n",
      "\u001b[32m[2020-06-20 03:52:05] __main__ INFO: \u001b[0mVal 8\n",
      "\u001b[32m[2020-06-20 03:52:06] __main__ INFO: \u001b[0mEpoch 8 loss 1.7222 acc@1 0.3668 acc@5 0.8012\n",
      "\u001b[32m[2020-06-20 03:52:06] __main__ INFO: \u001b[0mElapsed 1.06\n",
      "\u001b[32m[2020-06-20 03:52:06] __main__ INFO: \u001b[0mTrain 9 2808\n",
      "\u001b[32m[2020-06-20 03:52:15] __main__ INFO: \u001b[0mEpoch 9 Step 100/351 lr 0.100000 loss 1.6577 (1.6009) acc@1 0.4141 (0.4066) acc@5 0.7422 (0.8097)\n",
      "\u001b[32m[2020-06-20 03:52:24] __main__ INFO: \u001b[0mEpoch 9 Step 200/351 lr 0.100000 loss 1.6513 (1.6030) acc@1 0.3906 (0.4061) acc@5 0.8125 (0.8095)\n",
      "\u001b[32m[2020-06-20 03:52:33] __main__ INFO: \u001b[0mEpoch 9 Step 300/351 lr 0.100000 loss 1.6821 (1.6001) acc@1 0.3984 (0.4073) acc@5 0.7969 (0.8103)\n",
      "\u001b[32m[2020-06-20 03:52:38] __main__ INFO: \u001b[0mEpoch 9 Step 351/351 lr 0.100000 loss 1.5314 (1.5971) acc@1 0.4297 (0.4088) acc@5 0.8438 (0.8115)\n",
      "\u001b[32m[2020-06-20 03:52:38] __main__ INFO: \u001b[0mElapsed 32.37\n",
      "\u001b[32m[2020-06-20 03:52:38] __main__ INFO: \u001b[0mVal 9\n",
      "\u001b[32m[2020-06-20 03:52:39] __main__ INFO: \u001b[0mEpoch 9 loss 1.7260 acc@1 0.3778 acc@5 0.7960\n",
      "\u001b[32m[2020-06-20 03:52:39] __main__ INFO: \u001b[0mElapsed 1.04\n",
      "\u001b[32m[2020-06-20 03:52:39] __main__ INFO: \u001b[0mTrain 10 3159\n",
      "\u001b[32m[2020-06-20 03:52:48] __main__ INFO: \u001b[0mEpoch 10 Step 100/351 lr 0.100000 loss 1.5507 (1.5704) acc@1 0.4375 (0.4208) acc@5 0.7969 (0.8155)\n",
      "\u001b[32m[2020-06-20 03:52:58] __main__ INFO: \u001b[0mEpoch 10 Step 200/351 lr 0.100000 loss 1.5611 (1.5649) acc@1 0.4531 (0.4204) acc@5 0.7891 (0.8166)\n",
      "\u001b[32m[2020-06-20 03:53:07] __main__ INFO: \u001b[0mEpoch 10 Step 300/351 lr 0.100000 loss 1.5148 (1.5592) acc@1 0.4453 (0.4244) acc@5 0.8203 (0.8161)\n",
      "\u001b[32m[2020-06-20 03:53:11] __main__ INFO: \u001b[0mEpoch 10 Step 351/351 lr 0.100000 loss 1.3966 (1.5575) acc@1 0.4766 (0.4249) acc@5 0.8828 (0.8162)\n",
      "\u001b[32m[2020-06-20 03:53:11] __main__ INFO: \u001b[0mElapsed 32.43\n",
      "\u001b[32m[2020-06-20 03:53:11] __main__ INFO: \u001b[0mVal 10\n",
      "\u001b[32m[2020-06-20 03:53:13] __main__ INFO: \u001b[0mEpoch 10 loss 1.5871 acc@1 0.4230 acc@5 0.8242\n",
      "\u001b[32m[2020-06-20 03:53:13] __main__ INFO: \u001b[0mElapsed 1.07\n",
      "\u001b[32m[2020-06-20 03:53:13] __main__ INFO: \u001b[0mTrain 11 3510\n",
      "\u001b[32m[2020-06-20 03:53:22] __main__ INFO: \u001b[0mEpoch 11 Step 100/351 lr 0.100000 loss 1.5047 (1.5247) acc@1 0.4453 (0.4360) acc@5 0.8359 (0.8180)\n",
      "\u001b[32m[2020-06-20 03:53:31] __main__ INFO: \u001b[0mEpoch 11 Step 200/351 lr 0.100000 loss 1.6506 (1.5193) acc@1 0.3984 (0.4418) acc@5 0.8516 (0.8189)\n",
      "\u001b[32m[2020-06-20 03:53:40] __main__ INFO: \u001b[0mEpoch 11 Step 300/351 lr 0.100000 loss 1.4986 (1.5184) acc@1 0.4688 (0.4409) acc@5 0.8438 (0.8184)\n",
      "\u001b[32m[2020-06-20 03:53:45] __main__ INFO: \u001b[0mEpoch 11 Step 351/351 lr 0.100000 loss 1.3804 (1.5174) acc@1 0.4609 (0.4404) acc@5 0.8359 (0.8191)\n",
      "\u001b[32m[2020-06-20 03:53:45] __main__ INFO: \u001b[0mElapsed 32.38\n",
      "\u001b[32m[2020-06-20 03:53:45] __main__ INFO: \u001b[0mVal 11\n",
      "\u001b[32m[2020-06-20 03:53:46] __main__ INFO: \u001b[0mEpoch 11 loss 1.5190 acc@1 0.4450 acc@5 0.8238\n",
      "\u001b[32m[2020-06-20 03:53:46] __main__ INFO: \u001b[0mElapsed 1.06\n",
      "\u001b[32m[2020-06-20 03:53:46] __main__ INFO: \u001b[0mTrain 12 3861\n",
      "\u001b[32m[2020-06-20 03:53:55] __main__ INFO: \u001b[0mEpoch 12 Step 100/351 lr 0.100000 loss 1.6069 (1.4794) acc@1 0.3984 (0.4560) acc@5 0.7266 (0.8248)\n",
      "\u001b[32m[2020-06-20 03:54:04] __main__ INFO: \u001b[0mEpoch 12 Step 200/351 lr 0.100000 loss 1.4857 (1.4832) acc@1 0.4297 (0.4512) acc@5 0.8203 (0.8242)\n",
      "\u001b[32m[2020-06-20 03:54:14] __main__ INFO: \u001b[0mEpoch 12 Step 300/351 lr 0.100000 loss 1.4412 (1.4777) acc@1 0.4766 (0.4540) acc@5 0.8125 (0.8235)\n",
      "\u001b[32m[2020-06-20 03:54:18] __main__ INFO: \u001b[0mEpoch 12 Step 351/351 lr 0.100000 loss 1.4512 (1.4764) acc@1 0.4375 (0.4551) acc@5 0.8203 (0.8233)\n",
      "\u001b[32m[2020-06-20 03:54:18] __main__ INFO: \u001b[0mElapsed 32.42\n",
      "\u001b[32m[2020-06-20 03:54:18] __main__ INFO: \u001b[0mVal 12\n",
      "\u001b[32m[2020-06-20 03:54:19] __main__ INFO: \u001b[0mEpoch 12 loss 1.4895 acc@1 0.4562 acc@5 0.8290\n",
      "\u001b[32m[2020-06-20 03:54:19] __main__ INFO: \u001b[0mElapsed 1.05\n",
      "\u001b[32m[2020-06-20 03:54:19] __main__ INFO: \u001b[0mTrain 13 4212\n",
      "\u001b[32m[2020-06-20 03:54:29] __main__ INFO: \u001b[0mEpoch 13 Step 100/351 lr 0.100000 loss 1.5017 (1.4463) acc@1 0.4141 (0.4620) acc@5 0.7891 (0.8331)\n",
      "\u001b[32m[2020-06-20 03:54:38] __main__ INFO: \u001b[0mEpoch 13 Step 200/351 lr 0.100000 loss 1.4150 (1.4399) acc@1 0.4609 (0.4683) acc@5 0.8125 (0.8294)\n",
      "\u001b[32m[2020-06-20 03:54:47] __main__ INFO: \u001b[0mEpoch 13 Step 300/351 lr 0.100000 loss 1.3454 (1.4396) acc@1 0.5312 (0.4690) acc@5 0.8750 (0.8282)\n",
      "\u001b[32m[2020-06-20 03:54:52] __main__ INFO: \u001b[0mEpoch 13 Step 351/351 lr 0.100000 loss 1.3785 (1.4387) acc@1 0.4844 (0.4698) acc@5 0.8672 (0.8284)\n",
      "\u001b[32m[2020-06-20 03:54:52] __main__ INFO: \u001b[0mElapsed 32.43\n",
      "\u001b[32m[2020-06-20 03:54:52] __main__ INFO: \u001b[0mVal 13\n",
      "\u001b[32m[2020-06-20 03:54:53] __main__ INFO: \u001b[0mEpoch 13 loss 1.5044 acc@1 0.4470 acc@5 0.8218\n",
      "\u001b[32m[2020-06-20 03:54:53] __main__ INFO: \u001b[0mElapsed 1.07\n",
      "\u001b[32m[2020-06-20 03:54:53] __main__ INFO: \u001b[0mTrain 14 4563\n",
      "\u001b[32m[2020-06-20 03:55:02] __main__ INFO: \u001b[0mEpoch 14 Step 100/351 lr 0.100000 loss 1.6578 (1.4195) acc@1 0.4141 (0.4764) acc@5 0.8203 (0.8295)\n",
      "\u001b[32m[2020-06-20 03:55:11] __main__ INFO: \u001b[0mEpoch 14 Step 200/351 lr 0.100000 loss 1.3428 (1.4074) acc@1 0.4766 (0.4809) acc@5 0.8359 (0.8314)\n",
      "\u001b[32m[2020-06-20 03:55:21] __main__ INFO: \u001b[0mEpoch 14 Step 300/351 lr 0.100000 loss 1.5037 (1.4155) acc@1 0.4688 (0.4784) acc@5 0.8203 (0.8292)\n",
      "\u001b[32m[2020-06-20 03:55:25] __main__ INFO: \u001b[0mEpoch 14 Step 351/351 lr 0.100000 loss 1.5379 (1.4108) acc@1 0.4688 (0.4807) acc@5 0.8125 (0.8299)\n",
      "\u001b[32m[2020-06-20 03:55:25] __main__ INFO: \u001b[0mElapsed 32.37\n",
      "\u001b[32m[2020-06-20 03:55:25] __main__ INFO: \u001b[0mVal 14\n",
      "\u001b[32m[2020-06-20 03:55:26] __main__ INFO: \u001b[0mEpoch 14 loss 1.5126 acc@1 0.4540 acc@5 0.8252\n",
      "\u001b[32m[2020-06-20 03:55:26] __main__ INFO: \u001b[0mElapsed 1.05\n",
      "\u001b[32m[2020-06-20 03:55:26] __main__ INFO: \u001b[0mTrain 15 4914\n",
      "\u001b[32m[2020-06-20 03:55:36] __main__ INFO: \u001b[0mEpoch 15 Step 100/351 lr 0.100000 loss 1.3481 (1.3853) acc@1 0.4688 (0.4924) acc@5 0.8203 (0.8341)\n",
      "\u001b[32m[2020-06-20 03:55:45] __main__ INFO: \u001b[0mEpoch 15 Step 200/351 lr 0.100000 loss 1.3778 (1.3847) acc@1 0.5156 (0.4921) acc@5 0.8672 (0.8353)\n",
      "\u001b[32m[2020-06-20 03:55:54] __main__ INFO: \u001b[0mEpoch 15 Step 300/351 lr 0.100000 loss 1.4059 (1.3875) acc@1 0.4453 (0.4908) acc@5 0.8047 (0.8337)\n",
      "\u001b[32m[2020-06-20 03:55:59] __main__ INFO: \u001b[0mEpoch 15 Step 351/351 lr 0.100000 loss 1.4428 (1.3865) acc@1 0.4297 (0.4910) acc@5 0.8203 (0.8345)\n",
      "\u001b[32m[2020-06-20 03:55:59] __main__ INFO: \u001b[0mElapsed 32.29\n",
      "\u001b[32m[2020-06-20 03:55:59] __main__ INFO: \u001b[0mVal 15\n",
      "\u001b[32m[2020-06-20 03:56:00] __main__ INFO: \u001b[0mEpoch 15 loss 1.4660 acc@1 0.4622 acc@5 0.8272\n",
      "\u001b[32m[2020-06-20 03:56:00] __main__ INFO: \u001b[0mElapsed 1.04\n",
      "\u001b[32m[2020-06-20 03:56:00] __main__ INFO: \u001b[0mTrain 16 5265\n",
      "\u001b[32m[2020-06-20 03:56:09] __main__ INFO: \u001b[0mEpoch 16 Step 100/351 lr 0.100000 loss 1.3198 (1.3443) acc@1 0.5000 (0.5028) acc@5 0.8281 (0.8377)\n",
      "\u001b[32m[2020-06-20 03:56:18] __main__ INFO: \u001b[0mEpoch 16 Step 200/351 lr 0.100000 loss 1.3806 (1.3523) acc@1 0.4609 (0.5010) acc@5 0.8125 (0.8359)\n",
      "\u001b[32m[2020-06-20 03:56:27] __main__ INFO: \u001b[0mEpoch 16 Step 300/351 lr 0.100000 loss 1.4116 (1.3608) acc@1 0.5312 (0.4973) acc@5 0.8281 (0.8342)\n",
      "\u001b[32m[2020-06-20 03:56:32] __main__ INFO: \u001b[0mEpoch 16 Step 351/351 lr 0.100000 loss 1.2560 (1.3619) acc@1 0.5234 (0.4971) acc@5 0.8125 (0.8346)\n",
      "\u001b[32m[2020-06-20 03:56:32] __main__ INFO: \u001b[0mElapsed 32.41\n",
      "\u001b[32m[2020-06-20 03:56:32] __main__ INFO: \u001b[0mVal 16\n",
      "\u001b[32m[2020-06-20 03:56:33] __main__ INFO: \u001b[0mEpoch 16 loss 1.5711 acc@1 0.4556 acc@5 0.8052\n",
      "\u001b[32m[2020-06-20 03:56:33] __main__ INFO: \u001b[0mElapsed 1.04\n",
      "\u001b[32m[2020-06-20 03:56:33] __main__ INFO: \u001b[0mTrain 17 5616\n",
      "\u001b[32m[2020-06-20 03:56:42] __main__ INFO: \u001b[0mEpoch 17 Step 100/351 lr 0.100000 loss 1.4021 (1.3462) acc@1 0.4844 (0.4991) acc@5 0.7969 (0.8356)\n",
      "\u001b[32m[2020-06-20 03:56:52] __main__ INFO: \u001b[0mEpoch 17 Step 200/351 lr 0.100000 loss 1.3696 (1.3457) acc@1 0.5000 (0.5022) acc@5 0.8359 (0.8344)\n",
      "\u001b[32m[2020-06-20 03:57:01] __main__ INFO: \u001b[0mEpoch 17 Step 300/351 lr 0.100000 loss 1.2931 (1.3446) acc@1 0.4922 (0.5046) acc@5 0.8672 (0.8359)\n",
      "\u001b[32m[2020-06-20 03:57:05] __main__ INFO: \u001b[0mEpoch 17 Step 351/351 lr 0.100000 loss 1.2567 (1.3428) acc@1 0.4766 (0.5050) acc@5 0.7891 (0.8359)\n",
      "\u001b[32m[2020-06-20 03:57:06] __main__ INFO: \u001b[0mElapsed 32.35\n",
      "\u001b[32m[2020-06-20 03:57:06] __main__ INFO: \u001b[0mVal 17\n",
      "\u001b[32m[2020-06-20 03:57:07] __main__ INFO: \u001b[0mEpoch 17 loss 1.4083 acc@1 0.4840 acc@5 0.8322\n",
      "\u001b[32m[2020-06-20 03:57:07] __main__ INFO: \u001b[0mElapsed 1.06\n",
      "\u001b[32m[2020-06-20 03:57:07] __main__ INFO: \u001b[0mTrain 18 5967\n",
      "\u001b[32m[2020-06-20 03:57:16] __main__ INFO: \u001b[0mEpoch 18 Step 100/351 lr 0.100000 loss 1.3040 (1.3392) acc@1 0.5156 (0.5046) acc@5 0.8828 (0.8380)\n",
      "\u001b[32m[2020-06-20 03:57:25] __main__ INFO: \u001b[0mEpoch 18 Step 200/351 lr 0.100000 loss 1.4316 (1.3262) acc@1 0.4688 (0.5126) acc@5 0.8516 (0.8375)\n",
      "\u001b[32m[2020-06-20 03:57:34] __main__ INFO: \u001b[0mEpoch 18 Step 300/351 lr 0.100000 loss 1.4686 (1.3243) acc@1 0.4844 (0.5133) acc@5 0.7969 (0.8383)\n",
      "\u001b[32m[2020-06-20 03:57:39] __main__ INFO: \u001b[0mEpoch 18 Step 351/351 lr 0.100000 loss 1.3234 (1.3245) acc@1 0.5078 (0.5139) acc@5 0.8359 (0.8378)\n",
      "\u001b[32m[2020-06-20 03:57:39] __main__ INFO: \u001b[0mElapsed 32.37\n",
      "\u001b[32m[2020-06-20 03:57:39] __main__ INFO: \u001b[0mVal 18\n",
      "\u001b[32m[2020-06-20 03:57:40] __main__ INFO: \u001b[0mEpoch 18 loss 1.4643 acc@1 0.4714 acc@5 0.8308\n",
      "\u001b[32m[2020-06-20 03:57:40] __main__ INFO: \u001b[0mElapsed 1.05\n",
      "\u001b[32m[2020-06-20 03:57:40] __main__ INFO: \u001b[0mTrain 19 6318\n",
      "\u001b[32m[2020-06-20 03:57:49] __main__ INFO: \u001b[0mEpoch 19 Step 100/351 lr 0.100000 loss 1.2958 (1.3133) acc@1 0.5547 (0.5163) acc@5 0.8359 (0.8391)\n",
      "\u001b[32m[2020-06-20 03:57:58] __main__ INFO: \u001b[0mEpoch 19 Step 200/351 lr 0.100000 loss 1.3755 (1.3061) acc@1 0.5312 (0.5175) acc@5 0.8828 (0.8370)\n",
      "\u001b[32m[2020-06-20 03:58:08] __main__ INFO: \u001b[0mEpoch 19 Step 300/351 lr 0.100000 loss 1.5350 (1.3065) acc@1 0.4766 (0.5160) acc@5 0.7891 (0.8379)\n",
      "\u001b[32m[2020-06-20 03:58:12] __main__ INFO: \u001b[0mEpoch 19 Step 351/351 lr 0.100000 loss 1.2097 (1.3033) acc@1 0.6016 (0.5170) acc@5 0.8438 (0.8387)\n",
      "\u001b[32m[2020-06-20 03:58:12] __main__ INFO: \u001b[0mElapsed 32.36\n",
      "\u001b[32m[2020-06-20 03:58:12] __main__ INFO: \u001b[0mVal 19\n",
      "\u001b[32m[2020-06-20 03:58:13] __main__ INFO: \u001b[0mEpoch 19 loss 1.4451 acc@1 0.4832 acc@5 0.8320\n",
      "\u001b[32m[2020-06-20 03:58:13] __main__ INFO: \u001b[0mElapsed 1.05\n",
      "\u001b[32m[2020-06-20 03:58:13] __main__ INFO: \u001b[0mTrain 20 6669\n",
      "\u001b[32m[2020-06-20 03:58:23] __main__ INFO: \u001b[0mEpoch 20 Step 100/351 lr 0.100000 loss 1.2307 (1.2924) acc@1 0.5703 (0.5209) acc@5 0.8359 (0.8384)\n",
      "\u001b[32m[2020-06-20 03:58:32] __main__ INFO: \u001b[0mEpoch 20 Step 200/351 lr 0.100000 loss 1.4645 (1.2944) acc@1 0.5078 (0.5236) acc@5 0.8203 (0.8396)\n",
      "\u001b[32m[2020-06-20 03:58:41] __main__ INFO: \u001b[0mEpoch 20 Step 300/351 lr 0.100000 loss 1.3448 (1.2958) acc@1 0.5312 (0.5235) acc@5 0.8516 (0.8409)\n",
      "\u001b[32m[2020-06-20 03:58:46] __main__ INFO: \u001b[0mEpoch 20 Step 351/351 lr 0.100000 loss 1.2330 (1.2945) acc@1 0.5234 (0.5231) acc@5 0.8516 (0.8408)\n",
      "\u001b[32m[2020-06-20 03:58:46] __main__ INFO: \u001b[0mElapsed 32.38\n",
      "\u001b[32m[2020-06-20 03:58:46] __main__ INFO: \u001b[0mVal 20\n",
      "\u001b[32m[2020-06-20 03:58:47] __main__ INFO: \u001b[0mEpoch 20 loss 1.3679 acc@1 0.5062 acc@5 0.8424\n",
      "\u001b[32m[2020-06-20 03:58:47] __main__ INFO: \u001b[0mElapsed 1.06\n",
      "\u001b[32m[2020-06-20 03:58:47] __main__ INFO: \u001b[0mTrain 21 7020\n",
      "\u001b[32m[2020-06-20 03:58:56] __main__ INFO: \u001b[0mEpoch 21 Step 100/351 lr 0.100000 loss 1.3220 (1.2803) acc@1 0.4922 (0.5263) acc@5 0.8203 (0.8391)\n",
      "\u001b[32m[2020-06-20 03:59:05] __main__ INFO: \u001b[0mEpoch 21 Step 200/351 lr 0.100000 loss 1.3148 (1.2777) acc@1 0.5234 (0.5262) acc@5 0.8203 (0.8395)\n",
      "\u001b[32m[2020-06-20 03:59:15] __main__ INFO: \u001b[0mEpoch 21 Step 300/351 lr 0.100000 loss 1.2454 (1.2805) acc@1 0.5469 (0.5265) acc@5 0.8672 (0.8396)\n",
      "\u001b[32m[2020-06-20 03:59:19] __main__ INFO: \u001b[0mEpoch 21 Step 351/351 lr 0.100000 loss 1.2303 (1.2788) acc@1 0.5703 (0.5273) acc@5 0.8906 (0.8412)\n",
      "\u001b[32m[2020-06-20 03:59:19] __main__ INFO: \u001b[0mElapsed 32.32\n",
      "\u001b[32m[2020-06-20 03:59:19] __main__ INFO: \u001b[0mVal 21\n",
      "\u001b[32m[2020-06-20 03:59:20] __main__ INFO: \u001b[0mEpoch 21 loss 1.4227 acc@1 0.4844 acc@5 0.8332\n",
      "\u001b[32m[2020-06-20 03:59:20] __main__ INFO: \u001b[0mElapsed 1.05\n",
      "\u001b[32m[2020-06-20 03:59:20] __main__ INFO: \u001b[0mTrain 22 7371\n",
      "\u001b[32m[2020-06-20 03:59:30] __main__ INFO: \u001b[0mEpoch 22 Step 100/351 lr 0.100000 loss 1.4375 (1.2593) acc@1 0.4844 (0.5342) acc@5 0.8281 (0.8432)\n",
      "\u001b[32m[2020-06-20 03:59:39] __main__ INFO: \u001b[0mEpoch 22 Step 200/351 lr 0.100000 loss 1.1387 (1.2573) acc@1 0.5547 (0.5348) acc@5 0.9453 (0.8416)\n",
      "\u001b[32m[2020-06-20 03:59:48] __main__ INFO: \u001b[0mEpoch 22 Step 300/351 lr 0.100000 loss 1.1495 (1.2663) acc@1 0.5938 (0.5319) acc@5 0.8906 (0.8402)\n",
      "\u001b[32m[2020-06-20 03:59:53] __main__ INFO: \u001b[0mEpoch 22 Step 351/351 lr 0.100000 loss 1.1459 (1.2675) acc@1 0.5781 (0.5316) acc@5 0.8984 (0.8403)\n",
      "\u001b[32m[2020-06-20 03:59:53] __main__ INFO: \u001b[0mElapsed 32.34\n",
      "\u001b[32m[2020-06-20 03:59:53] __main__ INFO: \u001b[0mVal 22\n",
      "\u001b[32m[2020-06-20 03:59:54] __main__ INFO: \u001b[0mEpoch 22 loss 1.4554 acc@1 0.4846 acc@5 0.8320\n",
      "\u001b[32m[2020-06-20 03:59:54] __main__ INFO: \u001b[0mElapsed 1.07\n",
      "\u001b[32m[2020-06-20 03:59:54] __main__ INFO: \u001b[0mTrain 23 7722\n",
      "\u001b[32m[2020-06-20 04:00:03] __main__ INFO: \u001b[0mEpoch 23 Step 100/351 lr 0.100000 loss 1.1026 (1.2497) acc@1 0.6328 (0.5345) acc@5 0.8672 (0.8438)\n",
      "\u001b[32m[2020-06-20 04:00:12] __main__ INFO: \u001b[0mEpoch 23 Step 200/351 lr 0.100000 loss 1.1222 (1.2451) acc@1 0.5859 (0.5375) acc@5 0.8906 (0.8450)\n",
      "\u001b[32m[2020-06-20 04:00:21] __main__ INFO: \u001b[0mEpoch 23 Step 300/351 lr 0.100000 loss 1.3086 (1.2511) acc@1 0.5234 (0.5366) acc@5 0.8203 (0.8446)\n",
      "\u001b[32m[2020-06-20 04:00:26] __main__ INFO: \u001b[0mEpoch 23 Step 351/351 lr 0.100000 loss 1.3886 (1.2531) acc@1 0.4844 (0.5354) acc@5 0.8281 (0.8444)\n",
      "\u001b[32m[2020-06-20 04:00:26] __main__ INFO: \u001b[0mElapsed 32.39\n",
      "\u001b[32m[2020-06-20 04:00:26] __main__ INFO: \u001b[0mVal 23\n",
      "\u001b[32m[2020-06-20 04:00:27] __main__ INFO: \u001b[0mEpoch 23 loss 1.5075 acc@1 0.4676 acc@5 0.8314\n",
      "\u001b[32m[2020-06-20 04:00:27] __main__ INFO: \u001b[0mElapsed 1.05\n",
      "\u001b[32m[2020-06-20 04:00:27] __main__ INFO: \u001b[0mTrain 24 8073\n",
      "\u001b[32m[2020-06-20 04:00:36] __main__ INFO: \u001b[0mEpoch 24 Step 100/351 lr 0.100000 loss 1.0709 (1.2438) acc@1 0.5859 (0.5392) acc@5 0.8984 (0.8478)\n",
      "\u001b[32m[2020-06-20 04:00:46] __main__ INFO: \u001b[0mEpoch 24 Step 200/351 lr 0.100000 loss 1.2785 (1.2442) acc@1 0.5312 (0.5397) acc@5 0.8516 (0.8463)\n",
      "\u001b[32m[2020-06-20 04:00:55] __main__ INFO: \u001b[0mEpoch 24 Step 300/351 lr 0.100000 loss 1.2696 (1.2433) acc@1 0.5625 (0.5401) acc@5 0.8984 (0.8462)\n",
      "\u001b[32m[2020-06-20 04:00:59] __main__ INFO: \u001b[0mEpoch 24 Step 351/351 lr 0.100000 loss 1.2465 (1.2446) acc@1 0.5078 (0.5388) acc@5 0.8438 (0.8460)\n",
      "\u001b[32m[2020-06-20 04:00:59] __main__ INFO: \u001b[0mElapsed 32.38\n",
      "\u001b[32m[2020-06-20 04:00:59] __main__ INFO: \u001b[0mVal 24\n",
      "\u001b[32m[2020-06-20 04:01:01] __main__ INFO: \u001b[0mEpoch 24 loss 1.3426 acc@1 0.5076 acc@5 0.8378\n",
      "\u001b[32m[2020-06-20 04:01:01] __main__ INFO: \u001b[0mElapsed 1.05\n",
      "\u001b[32m[2020-06-20 04:01:01] __main__ INFO: \u001b[0mTrain 25 8424\n",
      "\u001b[32m[2020-06-20 04:01:10] __main__ INFO: \u001b[0mEpoch 25 Step 100/351 lr 0.100000 loss 1.1468 (1.2261) acc@1 0.6172 (0.5487) acc@5 0.8516 (0.8472)\n",
      "\u001b[32m[2020-06-20 04:01:19] __main__ INFO: \u001b[0mEpoch 25 Step 200/351 lr 0.100000 loss 1.2866 (1.2317) acc@1 0.5312 (0.5441) acc@5 0.8672 (0.8448)\n",
      "\u001b[32m[2020-06-20 04:01:28] __main__ INFO: \u001b[0mEpoch 25 Step 300/351 lr 0.100000 loss 1.4140 (1.2358) acc@1 0.4375 (0.5422) acc@5 0.7969 (0.8459)\n",
      "\u001b[32m[2020-06-20 04:01:33] __main__ INFO: \u001b[0mEpoch 25 Step 351/351 lr 0.100000 loss 1.1991 (1.2358) acc@1 0.5625 (0.5431) acc@5 0.8281 (0.8459)\n",
      "\u001b[32m[2020-06-20 04:01:33] __main__ INFO: \u001b[0mElapsed 32.33\n",
      "\u001b[32m[2020-06-20 04:01:33] __main__ INFO: \u001b[0mVal 25\n",
      "\u001b[32m[2020-06-20 04:01:34] __main__ INFO: \u001b[0mEpoch 25 loss 1.3938 acc@1 0.4912 acc@5 0.8384\n",
      "\u001b[32m[2020-06-20 04:01:34] __main__ INFO: \u001b[0mElapsed 1.05\n",
      "\u001b[32m[2020-06-20 04:01:34] __main__ INFO: \u001b[0mTrain 26 8775\n",
      "\u001b[32m[2020-06-20 04:01:43] __main__ INFO: \u001b[0mEpoch 26 Step 100/351 lr 0.100000 loss 1.2277 (1.2137) acc@1 0.5078 (0.5517) acc@5 0.8203 (0.8465)\n",
      "\u001b[32m[2020-06-20 04:01:52] __main__ INFO: \u001b[0mEpoch 26 Step 200/351 lr 0.100000 loss 1.2410 (1.2213) acc@1 0.5312 (0.5466) acc@5 0.7891 (0.8452)\n",
      "\u001b[32m[2020-06-20 04:02:02] __main__ INFO: \u001b[0mEpoch 26 Step 300/351 lr 0.100000 loss 1.2431 (1.2239) acc@1 0.5469 (0.5479) acc@5 0.8594 (0.8445)\n",
      "\u001b[32m[2020-06-20 04:02:06] __main__ INFO: \u001b[0mEpoch 26 Step 351/351 lr 0.100000 loss 1.1258 (1.2261) acc@1 0.5703 (0.5468) acc@5 0.8516 (0.8447)\n",
      "\u001b[32m[2020-06-20 04:02:06] __main__ INFO: \u001b[0mElapsed 32.40\n",
      "\u001b[32m[2020-06-20 04:02:06] __main__ INFO: \u001b[0mVal 26\n",
      "\u001b[32m[2020-06-20 04:02:07] __main__ INFO: \u001b[0mEpoch 26 loss 1.3650 acc@1 0.5034 acc@5 0.8376\n",
      "\u001b[32m[2020-06-20 04:02:07] __main__ INFO: \u001b[0mElapsed 1.07\n",
      "\u001b[32m[2020-06-20 04:02:07] __main__ INFO: \u001b[0mTrain 27 9126\n",
      "\u001b[32m[2020-06-20 04:02:17] __main__ INFO: \u001b[0mEpoch 27 Step 100/351 lr 0.100000 loss 1.2557 (1.1980) acc@1 0.5547 (0.5541) acc@5 0.8203 (0.8451)\n",
      "\u001b[32m[2020-06-20 04:02:26] __main__ INFO: \u001b[0mEpoch 27 Step 200/351 lr 0.100000 loss 1.2318 (1.2100) acc@1 0.5312 (0.5519) acc@5 0.8359 (0.8450)\n",
      "\u001b[32m[2020-06-20 04:02:35] __main__ INFO: \u001b[0mEpoch 27 Step 300/351 lr 0.100000 loss 1.1366 (1.2125) acc@1 0.5625 (0.5507) acc@5 0.8281 (0.8452)\n",
      "\u001b[32m[2020-06-20 04:02:40] __main__ INFO: \u001b[0mEpoch 27 Step 351/351 lr 0.100000 loss 1.4240 (1.2122) acc@1 0.4688 (0.5504) acc@5 0.8438 (0.8444)\n",
      "\u001b[32m[2020-06-20 04:02:40] __main__ INFO: \u001b[0mElapsed 32.30\n",
      "\u001b[32m[2020-06-20 04:02:40] __main__ INFO: \u001b[0mVal 27\n",
      "\u001b[32m[2020-06-20 04:02:41] __main__ INFO: \u001b[0mEpoch 27 loss 1.3658 acc@1 0.5154 acc@5 0.8340\n",
      "\u001b[32m[2020-06-20 04:02:41] __main__ INFO: \u001b[0mElapsed 1.05\n",
      "\u001b[32m[2020-06-20 04:02:41] __main__ INFO: \u001b[0mTrain 28 9477\n",
      "\u001b[32m[2020-06-20 04:02:50] __main__ INFO: \u001b[0mEpoch 28 Step 100/351 lr 0.100000 loss 1.0198 (1.2177) acc@1 0.6484 (0.5471) acc@5 0.8438 (0.8443)\n",
      "\u001b[32m[2020-06-20 04:02:59] __main__ INFO: \u001b[0mEpoch 28 Step 200/351 lr 0.100000 loss 1.2141 (1.2052) acc@1 0.5234 (0.5540) acc@5 0.8047 (0.8452)\n",
      "\u001b[32m[2020-06-20 04:03:08] __main__ INFO: \u001b[0mEpoch 28 Step 300/351 lr 0.100000 loss 1.2890 (1.2095) acc@1 0.4922 (0.5529) acc@5 0.8047 (0.8461)\n",
      "\u001b[32m[2020-06-20 04:03:13] __main__ INFO: \u001b[0mEpoch 28 Step 351/351 lr 0.100000 loss 1.0572 (1.2107) acc@1 0.6250 (0.5526) acc@5 0.8594 (0.8472)\n",
      "\u001b[32m[2020-06-20 04:03:13] __main__ INFO: \u001b[0mElapsed 32.34\n",
      "\u001b[32m[2020-06-20 04:03:13] __main__ INFO: \u001b[0mVal 28\n",
      "\u001b[32m[2020-06-20 04:03:14] __main__ INFO: \u001b[0mEpoch 28 loss 1.3639 acc@1 0.5026 acc@5 0.8372\n",
      "\u001b[32m[2020-06-20 04:03:14] __main__ INFO: \u001b[0mElapsed 1.05\n",
      "\u001b[32m[2020-06-20 04:03:14] __main__ INFO: \u001b[0mTrain 29 9828\n",
      "\u001b[32m[2020-06-20 04:03:23] __main__ INFO: \u001b[0mEpoch 29 Step 100/351 lr 0.100000 loss 1.1884 (1.1744) acc@1 0.6016 (0.5682) acc@5 0.8203 (0.8515)\n",
      "\u001b[32m[2020-06-20 04:03:33] __main__ INFO: \u001b[0mEpoch 29 Step 200/351 lr 0.100000 loss 1.1423 (1.1920) acc@1 0.6172 (0.5595) acc@5 0.8828 (0.8481)\n",
      "\u001b[32m[2020-06-20 04:03:42] __main__ INFO: \u001b[0mEpoch 29 Step 300/351 lr 0.100000 loss 1.2013 (1.1971) acc@1 0.5625 (0.5577) acc@5 0.8125 (0.8471)\n",
      "\u001b[32m[2020-06-20 04:03:47] __main__ INFO: \u001b[0mEpoch 29 Step 351/351 lr 0.100000 loss 1.1369 (1.1976) acc@1 0.5312 (0.5571) acc@5 0.8594 (0.8477)\n",
      "\u001b[32m[2020-06-20 04:03:47] __main__ INFO: \u001b[0mElapsed 32.39\n",
      "\u001b[32m[2020-06-20 04:03:47] __main__ INFO: \u001b[0mVal 29\n",
      "\u001b[32m[2020-06-20 04:03:48] __main__ INFO: \u001b[0mEpoch 29 loss 1.3157 acc@1 0.5306 acc@5 0.8364\n",
      "\u001b[32m[2020-06-20 04:03:48] __main__ INFO: \u001b[0mElapsed 1.06\n",
      "\u001b[32m[2020-06-20 04:03:48] __main__ INFO: \u001b[0mTrain 30 10179\n",
      "\u001b[32m[2020-06-20 04:03:57] __main__ INFO: \u001b[0mEpoch 30 Step 100/351 lr 0.100000 loss 1.0491 (1.1877) acc@1 0.6172 (0.5603) acc@5 0.8125 (0.8477)\n",
      "\u001b[32m[2020-06-20 04:04:06] __main__ INFO: \u001b[0mEpoch 30 Step 200/351 lr 0.100000 loss 1.0897 (1.1896) acc@1 0.5938 (0.5594) acc@5 0.9062 (0.8469)\n",
      "\u001b[32m[2020-06-20 04:04:15] __main__ INFO: \u001b[0mEpoch 30 Step 300/351 lr 0.100000 loss 1.2898 (1.1841) acc@1 0.5391 (0.5603) acc@5 0.8203 (0.8484)\n",
      "\u001b[32m[2020-06-20 04:04:20] __main__ INFO: \u001b[0mEpoch 30 Step 351/351 lr 0.100000 loss 1.1311 (1.1907) acc@1 0.5703 (0.5577) acc@5 0.9141 (0.8476)\n",
      "\u001b[32m[2020-06-20 04:04:20] __main__ INFO: \u001b[0mElapsed 32.34\n",
      "\u001b[32m[2020-06-20 04:04:20] __main__ INFO: \u001b[0mVal 30\n",
      "\u001b[32m[2020-06-20 04:04:21] __main__ INFO: \u001b[0mEpoch 30 loss 1.3488 acc@1 0.5134 acc@5 0.8412\n",
      "\u001b[32m[2020-06-20 04:04:21] __main__ INFO: \u001b[0mElapsed 1.06\n",
      "\u001b[32m[2020-06-20 04:04:21] __main__ INFO: \u001b[0mTrain 31 10530\n",
      "\u001b[32m[2020-06-20 04:04:30] __main__ INFO: \u001b[0mEpoch 31 Step 100/351 lr 0.100000 loss 1.1429 (1.1740) acc@1 0.5391 (0.5620) acc@5 0.8750 (0.8491)\n",
      "\u001b[32m[2020-06-20 04:04:39] __main__ INFO: \u001b[0mEpoch 31 Step 200/351 lr 0.100000 loss 1.2230 (1.1788) acc@1 0.5625 (0.5619) acc@5 0.8984 (0.8477)\n",
      "\u001b[32m[2020-06-20 04:04:49] __main__ INFO: \u001b[0mEpoch 31 Step 300/351 lr 0.100000 loss 1.1721 (1.1800) acc@1 0.5469 (0.5621) acc@5 0.8516 (0.8459)\n",
      "\u001b[32m[2020-06-20 04:04:53] __main__ INFO: \u001b[0mEpoch 31 Step 351/351 lr 0.100000 loss 1.2474 (1.1851) acc@1 0.5312 (0.5607) acc@5 0.8594 (0.8462)\n",
      "\u001b[32m[2020-06-20 04:04:53] __main__ INFO: \u001b[0mElapsed 32.34\n",
      "\u001b[32m[2020-06-20 04:04:53] __main__ INFO: \u001b[0mVal 31\n",
      "\u001b[32m[2020-06-20 04:04:54] __main__ INFO: \u001b[0mEpoch 31 loss 1.3793 acc@1 0.5138 acc@5 0.8464\n",
      "\u001b[32m[2020-06-20 04:04:54] __main__ INFO: \u001b[0mElapsed 1.04\n",
      "\u001b[32m[2020-06-20 04:04:54] __main__ INFO: \u001b[0mTrain 32 10881\n",
      "\u001b[32m[2020-06-20 04:05:04] __main__ INFO: \u001b[0mEpoch 32 Step 100/351 lr 0.100000 loss 1.0348 (1.1815) acc@1 0.6562 (0.5659) acc@5 0.8906 (0.8459)\n",
      "\u001b[32m[2020-06-20 04:05:13] __main__ INFO: \u001b[0mEpoch 32 Step 200/351 lr 0.100000 loss 1.1025 (1.1751) acc@1 0.6016 (0.5651) acc@5 0.8359 (0.8478)\n",
      "\u001b[32m[2020-06-20 04:05:22] __main__ INFO: \u001b[0mEpoch 32 Step 300/351 lr 0.100000 loss 1.1443 (1.1778) acc@1 0.5859 (0.5630) acc@5 0.8672 (0.8494)\n",
      "\u001b[32m[2020-06-20 04:05:27] __main__ INFO: \u001b[0mEpoch 32 Step 351/351 lr 0.100000 loss 1.3187 (1.1783) acc@1 0.5078 (0.5627) acc@5 0.7891 (0.8497)\n",
      "\u001b[32m[2020-06-20 04:05:27] __main__ INFO: \u001b[0mElapsed 32.37\n",
      "\u001b[32m[2020-06-20 04:05:27] __main__ INFO: \u001b[0mVal 32\n",
      "\u001b[32m[2020-06-20 04:05:28] __main__ INFO: \u001b[0mEpoch 32 loss 1.4231 acc@1 0.5028 acc@5 0.8400\n",
      "\u001b[32m[2020-06-20 04:05:28] __main__ INFO: \u001b[0mElapsed 1.07\n",
      "\u001b[32m[2020-06-20 04:05:28] __main__ INFO: \u001b[0mTrain 33 11232\n",
      "\u001b[32m[2020-06-20 04:05:37] __main__ INFO: \u001b[0mEpoch 33 Step 100/351 lr 0.100000 loss 1.1875 (1.1727) acc@1 0.5469 (0.5656) acc@5 0.8203 (0.8459)\n",
      "\u001b[32m[2020-06-20 04:05:46] __main__ INFO: \u001b[0mEpoch 33 Step 200/351 lr 0.100000 loss 1.1576 (1.1705) acc@1 0.5625 (0.5663) acc@5 0.8594 (0.8490)\n",
      "\u001b[32m[2020-06-20 04:05:55] __main__ INFO: \u001b[0mEpoch 33 Step 300/351 lr 0.100000 loss 1.2461 (1.1685) acc@1 0.5391 (0.5664) acc@5 0.7891 (0.8493)\n",
      "\u001b[32m[2020-06-20 04:06:00] __main__ INFO: \u001b[0mEpoch 33 Step 351/351 lr 0.100000 loss 1.1868 (1.1712) acc@1 0.5547 (0.5650) acc@5 0.8750 (0.8500)\n",
      "\u001b[32m[2020-06-20 04:06:00] __main__ INFO: \u001b[0mElapsed 32.32\n",
      "\u001b[32m[2020-06-20 04:06:00] __main__ INFO: \u001b[0mVal 33\n",
      "\u001b[32m[2020-06-20 04:06:01] __main__ INFO: \u001b[0mEpoch 33 loss 1.4042 acc@1 0.5012 acc@5 0.8414\n",
      "\u001b[32m[2020-06-20 04:06:01] __main__ INFO: \u001b[0mElapsed 1.06\n",
      "\u001b[32m[2020-06-20 04:06:01] __main__ INFO: \u001b[0mTrain 34 11583\n",
      "\u001b[32m[2020-06-20 04:06:11] __main__ INFO: \u001b[0mEpoch 34 Step 100/351 lr 0.100000 loss 1.0960 (1.1590) acc@1 0.5859 (0.5685) acc@5 0.8203 (0.8499)\n",
      "\u001b[32m[2020-06-20 04:06:20] __main__ INFO: \u001b[0mEpoch 34 Step 200/351 lr 0.100000 loss 1.3902 (1.1628) acc@1 0.5000 (0.5688) acc@5 0.8438 (0.8497)\n",
      "\u001b[32m[2020-06-20 04:06:29] __main__ INFO: \u001b[0mEpoch 34 Step 300/351 lr 0.100000 loss 1.2060 (1.1686) acc@1 0.5156 (0.5673) acc@5 0.7969 (0.8497)\n",
      "\u001b[32m[2020-06-20 04:06:34] __main__ INFO: \u001b[0mEpoch 34 Step 351/351 lr 0.100000 loss 0.9739 (1.1662) acc@1 0.6719 (0.5673) acc@5 0.8672 (0.8503)\n",
      "\u001b[32m[2020-06-20 04:06:34] __main__ INFO: \u001b[0mElapsed 32.36\n",
      "\u001b[32m[2020-06-20 04:06:34] __main__ INFO: \u001b[0mVal 34\n",
      "\u001b[32m[2020-06-20 04:06:35] __main__ INFO: \u001b[0mEpoch 34 loss 1.3305 acc@1 0.5162 acc@5 0.8430\n",
      "\u001b[32m[2020-06-20 04:06:35] __main__ INFO: \u001b[0mElapsed 1.07\n",
      "\u001b[32m[2020-06-20 04:06:35] __main__ INFO: \u001b[0mTrain 35 11934\n",
      "\u001b[32m[2020-06-20 04:06:44] __main__ INFO: \u001b[0mEpoch 35 Step 100/351 lr 0.100000 loss 1.0012 (1.1382) acc@1 0.6406 (0.5787) acc@5 0.8828 (0.8512)\n",
      "\u001b[32m[2020-06-20 04:06:53] __main__ INFO: \u001b[0mEpoch 35 Step 200/351 lr 0.100000 loss 1.2264 (1.1557) acc@1 0.5625 (0.5728) acc@5 0.8203 (0.8500)\n",
      "\u001b[32m[2020-06-20 04:07:02] __main__ INFO: \u001b[0mEpoch 35 Step 300/351 lr 0.100000 loss 1.1476 (1.1600) acc@1 0.6016 (0.5699) acc@5 0.8750 (0.8499)\n",
      "\u001b[32m[2020-06-20 04:07:07] __main__ INFO: \u001b[0mEpoch 35 Step 351/351 lr 0.100000 loss 1.3855 (1.1604) acc@1 0.5156 (0.5700) acc@5 0.8281 (0.8510)\n",
      "\u001b[32m[2020-06-20 04:07:07] __main__ INFO: \u001b[0mElapsed 32.37\n",
      "\u001b[32m[2020-06-20 04:07:07] __main__ INFO: \u001b[0mVal 35\n",
      "\u001b[32m[2020-06-20 04:07:08] __main__ INFO: \u001b[0mEpoch 35 loss 1.3166 acc@1 0.5244 acc@5 0.8430\n",
      "\u001b[32m[2020-06-20 04:07:08] __main__ INFO: \u001b[0mElapsed 1.06\n",
      "\u001b[32m[2020-06-20 04:07:08] __main__ INFO: \u001b[0mTrain 36 12285\n",
      "\u001b[32m[2020-06-20 04:07:17] __main__ INFO: \u001b[0mEpoch 36 Step 100/351 lr 0.100000 loss 1.1327 (1.1556) acc@1 0.6094 (0.5694) acc@5 0.8438 (0.8441)\n",
      "\u001b[32m[2020-06-20 04:07:27] __main__ INFO: \u001b[0mEpoch 36 Step 200/351 lr 0.100000 loss 1.1553 (1.1532) acc@1 0.5547 (0.5723) acc@5 0.7969 (0.8477)\n",
      "\u001b[32m[2020-06-20 04:07:36] __main__ INFO: \u001b[0mEpoch 36 Step 300/351 lr 0.100000 loss 1.1825 (1.1559) acc@1 0.5547 (0.5711) acc@5 0.8203 (0.8482)\n",
      "\u001b[32m[2020-06-20 04:07:40] __main__ INFO: \u001b[0mEpoch 36 Step 351/351 lr 0.100000 loss 0.9144 (1.1550) acc@1 0.6797 (0.5720) acc@5 0.9062 (0.8491)\n",
      "\u001b[32m[2020-06-20 04:07:40] __main__ INFO: \u001b[0mElapsed 32.34\n",
      "\u001b[32m[2020-06-20 04:07:40] __main__ INFO: \u001b[0mVal 36\n",
      "\u001b[32m[2020-06-20 04:07:41] __main__ INFO: \u001b[0mEpoch 36 loss 1.3911 acc@1 0.5148 acc@5 0.8312\n",
      "\u001b[32m[2020-06-20 04:07:41] __main__ INFO: \u001b[0mElapsed 1.07\n",
      "\u001b[32m[2020-06-20 04:07:41] __main__ INFO: \u001b[0mTrain 37 12636\n",
      "\u001b[32m[2020-06-20 04:07:51] __main__ INFO: \u001b[0mEpoch 37 Step 100/351 lr 0.100000 loss 1.2252 (1.1296) acc@1 0.5469 (0.5784) acc@5 0.8594 (0.8526)\n",
      "\u001b[32m[2020-06-20 04:08:00] __main__ INFO: \u001b[0mEpoch 37 Step 200/351 lr 0.100000 loss 1.0391 (1.1409) acc@1 0.6328 (0.5745) acc@5 0.8359 (0.8494)\n",
      "\u001b[32m[2020-06-20 04:08:09] __main__ INFO: \u001b[0mEpoch 37 Step 300/351 lr 0.100000 loss 1.3317 (1.1461) acc@1 0.5547 (0.5735) acc@5 0.8125 (0.8491)\n",
      "\u001b[32m[2020-06-20 04:08:14] __main__ INFO: \u001b[0mEpoch 37 Step 351/351 lr 0.100000 loss 1.0017 (1.1493) acc@1 0.6406 (0.5728) acc@5 0.8828 (0.8488)\n",
      "\u001b[32m[2020-06-20 04:08:14] __main__ INFO: \u001b[0mElapsed 32.35\n",
      "\u001b[32m[2020-06-20 04:08:14] __main__ INFO: \u001b[0mVal 37\n",
      "\u001b[32m[2020-06-20 04:08:15] __main__ INFO: \u001b[0mEpoch 37 loss 1.3302 acc@1 0.5214 acc@5 0.8486\n",
      "\u001b[32m[2020-06-20 04:08:15] __main__ INFO: \u001b[0mElapsed 1.05\n",
      "\u001b[32m[2020-06-20 04:08:15] __main__ INFO: \u001b[0mTrain 38 12987\n",
      "\u001b[32m[2020-06-20 04:08:24] __main__ INFO: \u001b[0mEpoch 38 Step 100/351 lr 0.100000 loss 0.9424 (1.1343) acc@1 0.6641 (0.5777) acc@5 0.9062 (0.8529)\n",
      "\u001b[32m[2020-06-20 04:08:33] __main__ INFO: \u001b[0mEpoch 38 Step 200/351 lr 0.100000 loss 1.0963 (1.1342) acc@1 0.6172 (0.5751) acc@5 0.8359 (0.8508)\n",
      "\u001b[32m[2020-06-20 04:08:42] __main__ INFO: \u001b[0mEpoch 38 Step 300/351 lr 0.100000 loss 1.1338 (1.1383) acc@1 0.6016 (0.5752) acc@5 0.8750 (0.8511)\n",
      "\u001b[32m[2020-06-20 04:08:47] __main__ INFO: \u001b[0mEpoch 38 Step 351/351 lr 0.100000 loss 1.2473 (1.1421) acc@1 0.5078 (0.5747) acc@5 0.8281 (0.8503)\n",
      "\u001b[32m[2020-06-20 04:08:47] __main__ INFO: \u001b[0mElapsed 32.26\n",
      "\u001b[32m[2020-06-20 04:08:47] __main__ INFO: \u001b[0mVal 38\n",
      "\u001b[32m[2020-06-20 04:08:48] __main__ INFO: \u001b[0mEpoch 38 loss 1.3180 acc@1 0.5330 acc@5 0.8440\n",
      "\u001b[32m[2020-06-20 04:08:48] __main__ INFO: \u001b[0mElapsed 1.08\n",
      "\u001b[32m[2020-06-20 04:08:48] __main__ INFO: \u001b[0mTrain 39 13338\n",
      "\u001b[32m[2020-06-20 04:08:58] __main__ INFO: \u001b[0mEpoch 39 Step 100/351 lr 0.100000 loss 1.1237 (1.1360) acc@1 0.5469 (0.5779) acc@5 0.8594 (0.8522)\n",
      "\u001b[32m[2020-06-20 04:09:07] __main__ INFO: \u001b[0mEpoch 39 Step 200/351 lr 0.100000 loss 1.0504 (1.1392) acc@1 0.6094 (0.5757) acc@5 0.8594 (0.8520)\n",
      "\u001b[32m[2020-06-20 04:09:16] __main__ INFO: \u001b[0mEpoch 39 Step 300/351 lr 0.100000 loss 0.9027 (1.1372) acc@1 0.6797 (0.5780) acc@5 0.8828 (0.8521)\n",
      "\u001b[32m[2020-06-20 04:09:21] __main__ INFO: \u001b[0mEpoch 39 Step 351/351 lr 0.100000 loss 1.2220 (1.1361) acc@1 0.5391 (0.5789) acc@5 0.7891 (0.8515)\n",
      "\u001b[32m[2020-06-20 04:09:21] __main__ INFO: \u001b[0mElapsed 32.35\n",
      "\u001b[32m[2020-06-20 04:09:21] __main__ INFO: \u001b[0mVal 39\n",
      "\u001b[32m[2020-06-20 04:09:22] __main__ INFO: \u001b[0mEpoch 39 loss 1.3002 acc@1 0.5242 acc@5 0.8458\n",
      "\u001b[32m[2020-06-20 04:09:22] __main__ INFO: \u001b[0mElapsed 1.04\n",
      "\u001b[32m[2020-06-20 04:09:22] __main__ INFO: \u001b[0mTrain 40 13689\n",
      "\u001b[32m[2020-06-20 04:09:31] __main__ INFO: \u001b[0mEpoch 40 Step 100/351 lr 0.100000 loss 1.1663 (1.1296) acc@1 0.5547 (0.5808) acc@5 0.7969 (0.8479)\n",
      "\u001b[32m[2020-06-20 04:09:40] __main__ INFO: \u001b[0mEpoch 40 Step 200/351 lr 0.100000 loss 1.1339 (1.1241) acc@1 0.5938 (0.5837) acc@5 0.8047 (0.8512)\n",
      "\u001b[32m[2020-06-20 04:09:49] __main__ INFO: \u001b[0mEpoch 40 Step 300/351 lr 0.100000 loss 0.9347 (1.1287) acc@1 0.6719 (0.5822) acc@5 0.9141 (0.8516)\n",
      "\u001b[32m[2020-06-20 04:09:54] __main__ INFO: \u001b[0mEpoch 40 Step 351/351 lr 0.100000 loss 1.3695 (1.1330) acc@1 0.5078 (0.5796) acc@5 0.8125 (0.8514)\n",
      "\u001b[32m[2020-06-20 04:09:54] __main__ INFO: \u001b[0mElapsed 32.43\n",
      "\u001b[32m[2020-06-20 04:09:54] __main__ INFO: \u001b[0mVal 40\n",
      "\u001b[32m[2020-06-20 04:09:55] __main__ INFO: \u001b[0mEpoch 40 loss 1.3578 acc@1 0.5176 acc@5 0.8412\n",
      "\u001b[32m[2020-06-20 04:09:55] __main__ INFO: \u001b[0mElapsed 1.06\n",
      "\u001b[32m[2020-06-20 04:09:55] __main__ INFO: \u001b[0mTrain 41 14040\n",
      "\u001b[32m[2020-06-20 04:10:04] __main__ INFO: \u001b[0mEpoch 41 Step 100/351 lr 0.100000 loss 0.9880 (1.1107) acc@1 0.6250 (0.5873) acc@5 0.8906 (0.8518)\n",
      "\u001b[32m[2020-06-20 04:10:14] __main__ INFO: \u001b[0mEpoch 41 Step 200/351 lr 0.100000 loss 1.1882 (1.1178) acc@1 0.5703 (0.5832) acc@5 0.8359 (0.8514)\n",
      "\u001b[32m[2020-06-20 04:10:23] __main__ INFO: \u001b[0mEpoch 41 Step 300/351 lr 0.100000 loss 1.0498 (1.1214) acc@1 0.6172 (0.5832) acc@5 0.8672 (0.8512)\n",
      "\u001b[32m[2020-06-20 04:10:27] __main__ INFO: \u001b[0mEpoch 41 Step 351/351 lr 0.100000 loss 1.0897 (1.1251) acc@1 0.5703 (0.5809) acc@5 0.8594 (0.8517)\n",
      "\u001b[32m[2020-06-20 04:10:27] __main__ INFO: \u001b[0mElapsed 32.36\n",
      "\u001b[32m[2020-06-20 04:10:27] __main__ INFO: \u001b[0mVal 41\n",
      "\u001b[32m[2020-06-20 04:10:29] __main__ INFO: \u001b[0mEpoch 41 loss 1.3128 acc@1 0.5250 acc@5 0.8410\n",
      "\u001b[32m[2020-06-20 04:10:29] __main__ INFO: \u001b[0mElapsed 1.05\n",
      "\u001b[32m[2020-06-20 04:10:29] __main__ INFO: \u001b[0mTrain 42 14391\n",
      "\u001b[32m[2020-06-20 04:10:38] __main__ INFO: \u001b[0mEpoch 42 Step 100/351 lr 0.100000 loss 1.1230 (1.1179) acc@1 0.5859 (0.5836) acc@5 0.8203 (0.8523)\n",
      "\u001b[32m[2020-06-20 04:10:47] __main__ INFO: \u001b[0mEpoch 42 Step 200/351 lr 0.100000 loss 1.1790 (1.1107) acc@1 0.5938 (0.5888) acc@5 0.8750 (0.8553)\n",
      "\u001b[32m[2020-06-20 04:10:56] __main__ INFO: \u001b[0mEpoch 42 Step 300/351 lr 0.100000 loss 1.1638 (1.1151) acc@1 0.5234 (0.5864) acc@5 0.8125 (0.8533)\n",
      "\u001b[32m[2020-06-20 04:11:01] __main__ INFO: \u001b[0mEpoch 42 Step 351/351 lr 0.100000 loss 0.9931 (1.1182) acc@1 0.6094 (0.5851) acc@5 0.8281 (0.8530)\n",
      "\u001b[32m[2020-06-20 04:11:01] __main__ INFO: \u001b[0mElapsed 32.32\n",
      "\u001b[32m[2020-06-20 04:11:01] __main__ INFO: \u001b[0mVal 42\n",
      "\u001b[32m[2020-06-20 04:11:02] __main__ INFO: \u001b[0mEpoch 42 loss 1.3415 acc@1 0.5326 acc@5 0.8476\n",
      "\u001b[32m[2020-06-20 04:11:02] __main__ INFO: \u001b[0mElapsed 1.17\n",
      "\u001b[32m[2020-06-20 04:11:02] __main__ INFO: \u001b[0mTrain 43 14742\n",
      "\u001b[32m[2020-06-20 04:11:11] __main__ INFO: \u001b[0mEpoch 43 Step 100/351 lr 0.100000 loss 1.2380 (1.1144) acc@1 0.5156 (0.5870) acc@5 0.7812 (0.8486)\n",
      "\u001b[32m[2020-06-20 04:11:20] __main__ INFO: \u001b[0mEpoch 43 Step 200/351 lr 0.100000 loss 1.1632 (1.1146) acc@1 0.5781 (0.5854) acc@5 0.8672 (0.8496)\n",
      "\u001b[32m[2020-06-20 04:11:30] __main__ INFO: \u001b[0mEpoch 43 Step 300/351 lr 0.100000 loss 1.0750 (1.1090) acc@1 0.5938 (0.5878) acc@5 0.8516 (0.8513)\n",
      "\u001b[32m[2020-06-20 04:11:34] __main__ INFO: \u001b[0mEpoch 43 Step 351/351 lr 0.100000 loss 0.9866 (1.1140) acc@1 0.6406 (0.5861) acc@5 0.8750 (0.8511)\n",
      "\u001b[32m[2020-06-20 04:11:34] __main__ INFO: \u001b[0mElapsed 32.35\n",
      "\u001b[32m[2020-06-20 04:11:34] __main__ INFO: \u001b[0mVal 43\n",
      "\u001b[32m[2020-06-20 04:11:35] __main__ INFO: \u001b[0mEpoch 43 loss 1.3680 acc@1 0.5218 acc@5 0.8412\n",
      "\u001b[32m[2020-06-20 04:11:35] __main__ INFO: \u001b[0mElapsed 1.04\n",
      "\u001b[32m[2020-06-20 04:11:35] __main__ INFO: \u001b[0mTrain 44 15093\n",
      "\u001b[32m[2020-06-20 04:11:45] __main__ INFO: \u001b[0mEpoch 44 Step 100/351 lr 0.100000 loss 1.1985 (1.1010) acc@1 0.5703 (0.5878) acc@5 0.8125 (0.8507)\n",
      "\u001b[32m[2020-06-20 04:11:54] __main__ INFO: \u001b[0mEpoch 44 Step 200/351 lr 0.100000 loss 0.9452 (1.1117) acc@1 0.6562 (0.5841) acc@5 0.9062 (0.8492)\n",
      "\u001b[32m[2020-06-20 04:12:03] __main__ INFO: \u001b[0mEpoch 44 Step 300/351 lr 0.100000 loss 1.0860 (1.1169) acc@1 0.6172 (0.5833) acc@5 0.8516 (0.8502)\n",
      "\u001b[32m[2020-06-20 04:12:08] __main__ INFO: \u001b[0mEpoch 44 Step 351/351 lr 0.100000 loss 1.3120 (1.1186) acc@1 0.5703 (0.5835) acc@5 0.8516 (0.8516)\n",
      "\u001b[32m[2020-06-20 04:12:08] __main__ INFO: \u001b[0mElapsed 32.37\n",
      "\u001b[32m[2020-06-20 04:12:08] __main__ INFO: \u001b[0mVal 44\n",
      "\u001b[32m[2020-06-20 04:12:09] __main__ INFO: \u001b[0mEpoch 44 loss 1.4367 acc@1 0.5106 acc@5 0.8322\n",
      "\u001b[32m[2020-06-20 04:12:09] __main__ INFO: \u001b[0mElapsed 1.07\n",
      "\u001b[32m[2020-06-20 04:12:09] __main__ INFO: \u001b[0mTrain 45 15444\n",
      "\u001b[32m[2020-06-20 04:12:18] __main__ INFO: \u001b[0mEpoch 45 Step 100/351 lr 0.100000 loss 0.9612 (1.0878) acc@1 0.6406 (0.5952) acc@5 0.8984 (0.8536)\n",
      "\u001b[32m[2020-06-20 04:12:27] __main__ INFO: \u001b[0mEpoch 45 Step 200/351 lr 0.100000 loss 1.1559 (1.1051) acc@1 0.5781 (0.5876) acc@5 0.8281 (0.8518)\n",
      "\u001b[32m[2020-06-20 04:12:37] __main__ INFO: \u001b[0mEpoch 45 Step 300/351 lr 0.100000 loss 1.1984 (1.1033) acc@1 0.5078 (0.5885) acc@5 0.7891 (0.8522)\n",
      "\u001b[32m[2020-06-20 04:12:41] __main__ INFO: \u001b[0mEpoch 45 Step 351/351 lr 0.100000 loss 1.2203 (1.1076) acc@1 0.5781 (0.5876) acc@5 0.8828 (0.8523)\n",
      "\u001b[32m[2020-06-20 04:12:41] __main__ INFO: \u001b[0mElapsed 32.38\n",
      "\u001b[32m[2020-06-20 04:12:41] __main__ INFO: \u001b[0mVal 45\n",
      "\u001b[32m[2020-06-20 04:12:42] __main__ INFO: \u001b[0mEpoch 45 loss 1.3586 acc@1 0.5196 acc@5 0.8450\n",
      "\u001b[32m[2020-06-20 04:12:42] __main__ INFO: \u001b[0mElapsed 1.04\n",
      "\u001b[32m[2020-06-20 04:12:42] __main__ INFO: \u001b[0mTrain 46 15795\n",
      "\u001b[32m[2020-06-20 04:12:52] __main__ INFO: \u001b[0mEpoch 46 Step 100/351 lr 0.100000 loss 1.1424 (1.0952) acc@1 0.5781 (0.5910) acc@5 0.8281 (0.8529)\n",
      "\u001b[32m[2020-06-20 04:13:01] __main__ INFO: \u001b[0mEpoch 46 Step 200/351 lr 0.100000 loss 1.2577 (1.0932) acc@1 0.5391 (0.5925) acc@5 0.8438 (0.8569)\n",
      "\u001b[32m[2020-06-20 04:13:10] __main__ INFO: \u001b[0mEpoch 46 Step 300/351 lr 0.100000 loss 1.2315 (1.1031) acc@1 0.5312 (0.5887) acc@5 0.8359 (0.8545)\n",
      "\u001b[32m[2020-06-20 04:13:15] __main__ INFO: \u001b[0mEpoch 46 Step 351/351 lr 0.100000 loss 1.1401 (1.1056) acc@1 0.5781 (0.5875) acc@5 0.8516 (0.8548)\n",
      "\u001b[32m[2020-06-20 04:13:15] __main__ INFO: \u001b[0mElapsed 32.34\n",
      "\u001b[32m[2020-06-20 04:13:15] __main__ INFO: \u001b[0mVal 46\n",
      "\u001b[32m[2020-06-20 04:13:16] __main__ INFO: \u001b[0mEpoch 46 loss 1.3221 acc@1 0.5330 acc@5 0.8384\n",
      "\u001b[32m[2020-06-20 04:13:16] __main__ INFO: \u001b[0mElapsed 1.05\n",
      "\u001b[32m[2020-06-20 04:13:16] __main__ INFO: \u001b[0mTrain 47 16146\n",
      "\u001b[32m[2020-06-20 04:13:25] __main__ INFO: \u001b[0mEpoch 47 Step 100/351 lr 0.100000 loss 1.0957 (1.0860) acc@1 0.6094 (0.5930) acc@5 0.8594 (0.8548)\n",
      "\u001b[32m[2020-06-20 04:13:34] __main__ INFO: \u001b[0mEpoch 47 Step 200/351 lr 0.100000 loss 1.0164 (1.1014) acc@1 0.6562 (0.5878) acc@5 0.9062 (0.8531)\n",
      "\u001b[32m[2020-06-20 04:13:43] __main__ INFO: \u001b[0mEpoch 47 Step 300/351 lr 0.100000 loss 1.1241 (1.0994) acc@1 0.5859 (0.5893) acc@5 0.8438 (0.8535)\n",
      "\u001b[32m[2020-06-20 04:13:48] __main__ INFO: \u001b[0mEpoch 47 Step 351/351 lr 0.100000 loss 1.0293 (1.0995) acc@1 0.6172 (0.5892) acc@5 0.8047 (0.8538)\n",
      "\u001b[32m[2020-06-20 04:13:48] __main__ INFO: \u001b[0mElapsed 32.35\n",
      "\u001b[32m[2020-06-20 04:13:48] __main__ INFO: \u001b[0mVal 47\n",
      "\u001b[32m[2020-06-20 04:13:49] __main__ INFO: \u001b[0mEpoch 47 loss 1.4507 acc@1 0.5050 acc@5 0.8380\n",
      "\u001b[32m[2020-06-20 04:13:49] __main__ INFO: \u001b[0mElapsed 1.06\n",
      "\u001b[32m[2020-06-20 04:13:49] __main__ INFO: \u001b[0mTrain 48 16497\n",
      "\u001b[32m[2020-06-20 04:13:58] __main__ INFO: \u001b[0mEpoch 48 Step 100/351 lr 0.100000 loss 1.2466 (1.0807) acc@1 0.5781 (0.5973) acc@5 0.8750 (0.8520)\n",
      "\u001b[32m[2020-06-20 04:14:08] __main__ INFO: \u001b[0mEpoch 48 Step 200/351 lr 0.100000 loss 1.1747 (1.0866) acc@1 0.5703 (0.5930) acc@5 0.8359 (0.8514)\n",
      "\u001b[32m[2020-06-20 04:14:17] __main__ INFO: \u001b[0mEpoch 48 Step 300/351 lr 0.100000 loss 0.8613 (1.0934) acc@1 0.6797 (0.5908) acc@5 0.9297 (0.8532)\n",
      "\u001b[32m[2020-06-20 04:14:21] __main__ INFO: \u001b[0mEpoch 48 Step 351/351 lr 0.100000 loss 1.0894 (1.0976) acc@1 0.6094 (0.5904) acc@5 0.8828 (0.8533)\n",
      "\u001b[32m[2020-06-20 04:14:21] __main__ INFO: \u001b[0mElapsed 32.41\n",
      "\u001b[32m[2020-06-20 04:14:21] __main__ INFO: \u001b[0mVal 48\n",
      "\u001b[32m[2020-06-20 04:14:23] __main__ INFO: \u001b[0mEpoch 48 loss 1.3577 acc@1 0.5224 acc@5 0.8392\n",
      "\u001b[32m[2020-06-20 04:14:23] __main__ INFO: \u001b[0mElapsed 1.04\n",
      "\u001b[32m[2020-06-20 04:14:23] __main__ INFO: \u001b[0mTrain 49 16848\n",
      "\u001b[32m[2020-06-20 04:14:32] __main__ INFO: \u001b[0mEpoch 49 Step 100/351 lr 0.100000 loss 0.9773 (1.0781) acc@1 0.6250 (0.5959) acc@5 0.8906 (0.8508)\n",
      "\u001b[32m[2020-06-20 04:14:41] __main__ INFO: \u001b[0mEpoch 49 Step 200/351 lr 0.100000 loss 1.3179 (1.0895) acc@1 0.4531 (0.5952) acc@5 0.8359 (0.8506)\n",
      "\u001b[32m[2020-06-20 04:14:50] __main__ INFO: \u001b[0mEpoch 49 Step 300/351 lr 0.100000 loss 1.0064 (1.0911) acc@1 0.5938 (0.5945) acc@5 0.8047 (0.8532)\n",
      "\u001b[32m[2020-06-20 04:14:55] __main__ INFO: \u001b[0mEpoch 49 Step 351/351 lr 0.100000 loss 1.0393 (1.0935) acc@1 0.6250 (0.5946) acc@5 0.8750 (0.8540)\n",
      "\u001b[32m[2020-06-20 04:14:55] __main__ INFO: \u001b[0mElapsed 32.35\n",
      "\u001b[32m[2020-06-20 04:14:55] __main__ INFO: \u001b[0mVal 49\n",
      "\u001b[32m[2020-06-20 04:14:56] __main__ INFO: \u001b[0mEpoch 49 loss 1.2575 acc@1 0.5498 acc@5 0.8506\n",
      "\u001b[32m[2020-06-20 04:14:56] __main__ INFO: \u001b[0mElapsed 1.03\n",
      "\u001b[32m[2020-06-20 04:14:56] __main__ INFO: \u001b[0mTrain 50 17199\n",
      "\u001b[32m[2020-06-20 04:15:05] __main__ INFO: \u001b[0mEpoch 50 Step 100/351 lr 0.100000 loss 1.1486 (1.0791) acc@1 0.5625 (0.5977) acc@5 0.8359 (0.8532)\n",
      "\u001b[32m[2020-06-20 04:15:14] __main__ INFO: \u001b[0mEpoch 50 Step 200/351 lr 0.100000 loss 1.3176 (1.0854) acc@1 0.5234 (0.5950) acc@5 0.8594 (0.8530)\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 436, in <module>\n",
      "    main()\n",
      "  File \"train.py\", line 412, in main\n",
      "    train_loader, logger, tensorboard_writer, tensorboard_writer2)\n",
      "  File \"train.py\", line 187, in train\n",
      "    loss = loss.item()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# Train the model per the settings specified in the original paper\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "!python train.py --config configs/cifar/resnet.yaml \\\n",
    "    model.resnet.depth 32 \\\n",
    "    train.batch_size 128 \\\n",
    "    dataset.name CIFAR10_RA_2_5 \\\n",
    "    train.base_lr 0.1 \\\n",
    "    train.output_dir /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_5/exp00 \\\n",
    "    scheduler.epochs 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: train.py [-h] [--config CONFIG] [--resume RESUME]\n",
      "                [--local_rank LOCAL_RANK]\n",
      "                ...\n",
      "\n",
      "positional arguments:\n",
      "  options\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --config CONFIG\n",
      "  --resume RESUME\n",
      "  --local_rank LOCAL_RANK\n"
     ]
    }
   ],
   "source": [
    "# Resume training with the un-augmented data\n",
    "!python train.py --config configs/cifar/resnet.yaml \\\n",
    "    model.resnet.depth 32 \\\n",
    "    train.batch_size 128 \\\n",
    "    config.train.checkpoint = /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_5/exp00/checkpoint_00400.pth \\\n",
    "    config.train.resume = True \\ \n",
    "    dataset.name CIFAR10_RA_2_5 \\\n",
    "    train.base_lr 0.1 \\       #### SET BASED ON ENDING LR\n",
    "    train.output_dir /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_5/exp00_resume \\\n",
    "    scheduler.epochs 50\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-12 01:00:00] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/resnet_basic_32/exp00/checkpoint_00160.pth\n",
      "Files already downloaded and verified\n",
      "100%|| 79/79 [00:03<00:00, 25.66it/s]\n",
      "\u001b[32m[2020-06-12 01:00:04] __main__ INFO: \u001b[0mElapsed 3.08\n",
      "\u001b[32m[2020-06-12 01:00:04] __main__ INFO: \u001b[0mLoss 0.4011 Accuracy 0.9232\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/resnet.yaml \\\n",
    "   model.resnet.depth 32 \\\n",
    "   test.batch_size 128 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/resnet_basic_32/exp00/checkpoint_00160.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/resnet_basic_32/exp00/test_results_0160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-12 01:00:11] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/resnet_basic_32/exp00/checkpoint_00100.pth\n",
      "Files already downloaded and verified\n",
      "100%|| 79/79 [00:03<00:00, 26.19it/s]\n",
      "\u001b[32m[2020-06-12 01:00:15] __main__ INFO: \u001b[0mElapsed 3.02\n",
      "\u001b[32m[2020-06-12 01:00:15] __main__ INFO: \u001b[0mLoss 0.3604 Accuracy 0.9170\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/resnet.yaml \\\n",
    "   model.resnet.depth 32 \\\n",
    "   test.batch_size 128 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/resnet_basic_32/exp00/checkpoint_00100.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/resnet_basic_32/exp00/test_results_0100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-14 03:30:35] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/resnet_basic_32/exp00/checkpoint_00160.pth\n",
      "CIFAR 10.1\n",
      "100%|| 16/16 [00:00<00:00, 17.70it/s]\n",
      "\u001b[32m[2020-06-14 03:30:36] __main__ INFO: \u001b[0mElapsed 0.91\n",
      "\u001b[32m[2020-06-14 03:30:36] __main__ INFO: \u001b[0mLoss 0.8051 Accuracy 0.8320\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10.1 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "!python evaluate.py --config configs/cifar/resnet.yaml \\\n",
    "    model.resnet.depth 32 \\\n",
    "    test.batch_size 128 \\\n",
    "    dataset.name CIFAR101 \\\n",
    "    test.checkpoint /home/ec2-user/SageMaker/experiments/resnet_basic_32/exp00/checkpoint_00160.pth \\\n",
    "    test.output_dir /home/ec2-user/SageMaker/experiments/resnet_basic_32/exp00/test_results_0160_CIFAR101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-16 00:42:33] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/resnet_basic_32/exp00/checkpoint_00160.pth\n",
      "CIFAR 10 Random Augmentation N=3 M=5\n",
      "100%|| 1/1 [00:00<00:00,  3.59it/s]\n",
      "\u001b[32m[2020-06-16 00:42:33] __main__ INFO: \u001b[0mElapsed 0.28\n",
      "\u001b[32m[2020-06-16 00:42:33] __main__ INFO: \u001b[0mLoss 6.1237 Accuracy 0.3500\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10.1 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "!python evaluate.py --config configs/cifar/resnet.yaml \\\n",
    "    model.resnet.depth 32 \\\n",
    "    dataset.name CIFAR10_RA_3_5 \\\n",
    "    test.checkpoint /home/ec2-user/SageMaker/experiments/resnet_basic_32/exp00/checkpoint_00160.pth \\\n",
    "    test.output_dir /home/ec2-user/SageMaker/experiments/resnet_basic_32/exp00/test_results_0160_RANDAUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Testset</th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Original_Accuracy</th>\n",
       "      <th>Original_CI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>resnet_basic_32</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>100</td>\n",
       "      <td>0.3604</td>\n",
       "      <td>0.9170</td>\n",
       "      <td>92.5</td>\n",
       "      <td>(92.0, 93.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>resnet_basic_32</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>160</td>\n",
       "      <td>0.4011</td>\n",
       "      <td>0.9232</td>\n",
       "      <td>92.5</td>\n",
       "      <td>(92.0, 93.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>resnet_basic_32</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>160</td>\n",
       "      <td>0.8051</td>\n",
       "      <td>0.8320</td>\n",
       "      <td>84.9</td>\n",
       "      <td>(83.2, 86.4)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Model    Testset  Epoch    Loss  Accuracy  Original_Accuracy  \\\n",
       "0  resnet_basic_32    cifar10    100  0.3604    0.9170               92.5   \n",
       "1  resnet_basic_32    cifar10    160  0.4011    0.9232               92.5   \n",
       "2  resnet_basic_32  cifar10.1    160  0.8051    0.8320               84.9   \n",
       "\n",
       "    Original_CI  \n",
       "0  (92.0, 93.0)  \n",
       "1  (92.0, 93.0)  \n",
       "2  (83.2, 86.4)  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write the results to a CSV file so that we can analyze later.\n",
    "import pandas as pd\n",
    "\n",
    "results = {'Model': ['resnet_basic_32', 'resnet_basic_32', 'resnet_basic_32'],\n",
    "           'Testset': ['cifar10', 'cifar10', 'cifar10.1'],\n",
    "           'Epoch': [100, 160, 160],\n",
    "           'Loss': [0.3604, 0.4011, 0.8051],\n",
    "           'Accuracy': [0.9170, 0.9232, 0.8320],\n",
    "           'Original_Accuracy': [92.5, 92.5, 84.9],\n",
    "           'Original_CI': [(92.0, 93.0), (92.0, 93.0), (83.2, 86.4)]\n",
    "           }\n",
    "\n",
    "df = pd.DataFrame(results, columns = ['Model', 'Testset', 'Epoch', 'Loss', 'Accuracy', \n",
    "                                      'Original_Accuracy', 'Original_CI'])\n",
    "\n",
    "\n",
    "df.to_csv('/home/ec2-user/SageMaker/experiments/resnet_basic_32/exp00/results.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['preds', 'probs', 'labels', 'loss', 'acc']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ -7.153804  ,  -0.1832159 ,  -0.69570637, ...,  -0.50926757,\n",
       "         -5.526208  , -12.987257  ],\n",
       "       [  2.862379  ,   7.963458  ,  -6.603018  , ...,  -4.740323  ,\n",
       "         25.90399   ,  -0.52988565],\n",
       "       [  4.25749   ,   8.408992  ,  -4.3299227 , ...,  -2.3715498 ,\n",
       "         13.468082  ,   4.5792727 ],\n",
       "       ...,\n",
       "       [ -4.7270765 ,  -1.2400844 ,   1.3852903 , ...,  -0.51062894,\n",
       "         -3.399443  ,  -2.4969094 ],\n",
       "       [ -2.7640457 ,  14.635863  ,   6.7449965 , ...,  -1.3011913 ,\n",
       "         -3.036379  ,  -7.061736  ],\n",
       "       [ -2.6933427 ,   1.8961854 ,  -3.6396854 , ...,  18.63456   ,\n",
       "         -2.7524152 ,  -3.2204888 ]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Peak inside the output file for predictions\n",
    "import numpy as np\n",
    "output = '/home/ec2-user/SageMaker/experiments/resnet_basic_32/exp00/test_results_0160/predictions.npz'\n",
    "npzfile = np.load(output)\n",
    "print(npzfile.files)\n",
    "npzfile['preds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the model checkpoints, configs, and results to S3 \n",
    "bucket='sagemaker-may29'\n",
    "prefix = 'sagemaker/results/original-models/resnet_basic_32'\n",
    "path = '/home/ec2-user/SageMaker/experiments/resnet_basic_32'\n",
    "\n",
    "s3_resource = boto3.resource(\"s3\", region_name=\"us-east-2\")\n",
    "\n",
    "def uploadDirectory(local_path,bucket_name,s3_prefix):\n",
    "\n",
    "    my_bucket = s3_resource.Bucket(bucket_name)\n",
    "    \n",
    "    for path, subdirs, files in os.walk(local_path):\n",
    "        path = path.replace(\"\\\\\",\"/\")\n",
    "        directory_name = path.replace(local_path,\"\")\n",
    "        for file in files:\n",
    "            #print(\"Local File:\", os.path.join(path, file))\n",
    "            #print(\"      Dest:\", s3_prefix+directory_name+'/'+file)\n",
    "            my_bucket.upload_file(os.path.join(path, file), s3_prefix+directory_name+'/'+file)\n",
    "    \n",
    "uploadDirectory(path,bucket,prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
