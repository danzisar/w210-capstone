{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import numpy \n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wide Residual Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 1)) (1.18.1)\n",
      "Requirement already satisfied: torch>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 2)) (1.4.0)\n",
      "Requirement already satisfied: torchvision in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (0.5.0)\n",
      "Requirement already satisfied: fvcore in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (0.1.1.post20200704)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 5)) (4.44.1)\n",
      "Requirement already satisfied: yacs in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 6)) (0.1.7)\n",
      "Requirement already satisfied: apex from git+https://github.com/NVIDIA/apex.git#egg=apex in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 7)) (0.1)\n",
      "Requirement already satisfied: termcolor in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 8)) (1.1.0)\n",
      "Requirement already satisfied: thop<0.0.31.post2004070130 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 9)) (0.0.31.post2001170342)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (1.14.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (7.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (5.3.1)\n",
      "Requirement already satisfied: tabulate in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (0.8.7)\n",
      "Requirement already satisfied: portalocker in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (1.7.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Sagemaker Notebook must be of type, conda_pytorch_p36\n",
    "\n",
    "!pip install -r '/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorboard in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (2.2.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.0.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.18.0)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (3.12.2)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.30.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (0.4.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.7.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.18.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (46.1.3.post20200330)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (3.2.2)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.14.0)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (0.34.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (0.9.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (2.23.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (3.4.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (4.1.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard) (1.5.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (2020.4.5.2)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (1.25.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (2.9)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard) (2.2.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Need to add this to requirements.txt\n",
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model per the settings specified in the original RESNET paper\n",
    "# os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "# !python train.py --config configs/cifar/wrn.yaml \\\n",
    "#     model.wrn.depth 28 \\\n",
    "#     model.wrn.widening_factor 10 \\\n",
    "#     train.batch_size 128 \\\n",
    "#     train.base_lr 0.1 \\\n",
    "#     train.output_dir /home/ec2-user/SageMaker/experiments/wrn_28_10/exp00 \\\n",
    "#     scheduler.epochs 200\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-11 15:53:31] __main__ INFO: \u001b[0mdevice: cuda\n",
      "cudnn:\n",
      "  benchmark: True\n",
      "  deterministic: False\n",
      "dataset:\n",
      "  name: CIFAR10_RA_1_20\n",
      "  dataset_dir: ''\n",
      "  image_size: 32\n",
      "  n_channels: 3\n",
      "  n_classes: 10\n",
      "model:\n",
      "  type: cifar\n",
      "  name: wrn\n",
      "  init_mode: kaiming_fan_in\n",
      "  vgg:\n",
      "    n_channels: [64, 128, 256, 512, 512]\n",
      "    n_layers: [2, 2, 3, 3, 3]\n",
      "    use_bn: True\n",
      "  resnet:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "  resnet_preact:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "  wrn:\n",
      "    depth: 28\n",
      "    initial_channels: 16\n",
      "    widening_factor: 10\n",
      "    drop_rate: 0.0\n",
      "  densenet:\n",
      "    depth: 100\n",
      "    n_blocks: [6, 12, 24, 16]\n",
      "    block_type: bottleneck\n",
      "    growth_rate: 12\n",
      "    drop_rate: 0.0\n",
      "    compression_rate: 0.5\n",
      "  pyramidnet:\n",
      "    depth: 272\n",
      "    n_blocks: [3, 24, 36, 3]\n",
      "    initial_channels: 16\n",
      "    block_type: bottleneck\n",
      "    alpha: 200\n",
      "  resnext:\n",
      "    depth: 29\n",
      "    n_blocks: [3, 4, 6, 3]\n",
      "    initial_channels: 64\n",
      "    cardinality: 8\n",
      "    base_channels: 4\n",
      "  shake_shake:\n",
      "    depth: 26\n",
      "    initial_channels: 96\n",
      "    shake_forward: True\n",
      "    shake_backward: True\n",
      "    shake_image: True\n",
      "  se_resnet_preact:\n",
      "    depth: 110\n",
      "    initial_channels: 16\n",
      "    se_reduction: 16\n",
      "    block_type: basic\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "train:\n",
      "  checkpoint: ''\n",
      "  resume: False\n",
      "  precision: O0\n",
      "  batch_size: 128\n",
      "  subdivision: 1\n",
      "  optimizer: sgd\n",
      "  base_lr: 0.1\n",
      "  momentum: 0.9\n",
      "  nesterov: True\n",
      "  weight_decay: 0.0005\n",
      "  no_weight_decay_on_bn: False\n",
      "  gradient_clip: 0\n",
      "  start_epoch: 0\n",
      "  seed: 0\n",
      "  val_first: True\n",
      "  val_period: 1\n",
      "  val_ratio: 0.1\n",
      "  use_test_as_val: False\n",
      "  output_dir: /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_1_20/exp00\n",
      "  log_period: 100\n",
      "  checkpoint_period: 100\n",
      "  use_tensorboard: True\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: True\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "  distributed: False\n",
      "  dist:\n",
      "    backend: nccl\n",
      "    init_method: env://\n",
      "    world_size: -1\n",
      "    node_rank: -1\n",
      "    local_rank: 0\n",
      "    use_sync_bn: False\n",
      "tensorboard:\n",
      "  train_images: False\n",
      "  val_images: False\n",
      "  model_params: False\n",
      "optim:\n",
      "  adam:\n",
      "    betas: (0.9, 0.999)\n",
      "  lars:\n",
      "    eps: 1e-09\n",
      "    threshold: 0.01\n",
      "  adabound:\n",
      "    betas: (0.9, 0.999)\n",
      "    final_lr: 0.1\n",
      "    gamma: 0.001\n",
      "scheduler:\n",
      "  epochs: 400\n",
      "  warmup:\n",
      "    type: none\n",
      "    epochs: 0\n",
      "    start_factor: 0.001\n",
      "    exponent: 4\n",
      "  type: multistep\n",
      "  milestones: [60, 120, 160]\n",
      "  lr_decay: 0.2\n",
      "  lr_min_factor: 0.001\n",
      "  T0: 10\n",
      "  T_mul: 1.0\n",
      "validation:\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: False\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "augmentation:\n",
      "  use_random_crop: True\n",
      "  use_random_horizontal_flip: True\n",
      "  use_cutout: False\n",
      "  use_random_erasing: False\n",
      "  use_dual_cutout: False\n",
      "  use_mixup: False\n",
      "  use_ricap: False\n",
      "  use_cutmix: False\n",
      "  use_label_smoothing: False\n",
      "  random_crop:\n",
      "    padding: 4\n",
      "    fill: 0\n",
      "    padding_mode: constant\n",
      "  random_horizontal_flip:\n",
      "    prob: 0.5\n",
      "  cutout:\n",
      "    prob: 1.0\n",
      "    mask_size: 16\n",
      "    cut_inside: False\n",
      "    mask_color: 0\n",
      "    dual_cutout_alpha: 0.1\n",
      "  random_erasing:\n",
      "    prob: 0.5\n",
      "    area_ratio_range: [0.02, 0.4]\n",
      "    min_aspect_ratio: 0.3\n",
      "    max_attempt: 20\n",
      "  mixup:\n",
      "    alpha: 1.0\n",
      "  ricap:\n",
      "    beta: 0.3\n",
      "  cutmix:\n",
      "    alpha: 1.0\n",
      "  label_smoothing:\n",
      "    epsilon: 0.1\n",
      "tta:\n",
      "  use_resize: False\n",
      "  use_center_crop: False\n",
      "  resize: 256\n",
      "test:\n",
      "  checkpoint: ''\n",
      "  output_dir: ''\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    pin_memory: False\n",
      "\u001b[32m[2020-07-11 15:53:31] __main__ INFO: \u001b[0menv_info:\n",
      "  pytorch_version: 1.4.0\n",
      "  cuda_version: 10.1\n",
      "  cudnn_version: 7603\n",
      "  num_gpus: 1\n",
      "  gpu_name: Tesla K80\n",
      "  gpu_capability: 3.7\n",
      "\u001b[32m[2020-07-11 15:53:36] __main__ INFO: \u001b[0mMACs  : 5.25G\n",
      "\u001b[32m[2020-07-11 15:53:36] __main__ INFO: \u001b[0m#params: 36.48M\n",
      "Selected optimization level O0:  Pure FP32 training.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\",)\n",
      "\u001b[32m[2020-07-11 15:53:36] __main__ INFO: \u001b[0mVal 0\n",
      "\u001b[32m[2020-07-11 15:53:56] __main__ INFO: \u001b[0mEpoch 0 loss 239.3287 acc@1 0.1016 acc@5 0.4966\n",
      "\u001b[32m[2020-07-11 15:53:56] __main__ INFO: \u001b[0mElapsed 19.56\n",
      "\u001b[32m[2020-07-11 15:53:56] __main__ INFO: \u001b[0mTrain 1 0\n",
      "\u001b[32m[2020-07-11 15:55:52] __main__ INFO: \u001b[0mEpoch 1 Step 100/351 lr 0.100000 loss 2.0489 (2.3085) acc@1 0.2734 (0.1668) acc@5 0.6875 (0.6350)\n",
      "\u001b[32m[2020-07-11 15:57:43] __main__ INFO: \u001b[0mEpoch 1 Step 200/351 lr 0.100000 loss 1.9828 (2.1810) acc@1 0.2188 (0.1968) acc@5 0.7891 (0.6847)\n",
      "\u001b[32m[2020-07-11 15:59:35] __main__ INFO: \u001b[0mEpoch 1 Step 300/351 lr 0.100000 loss 1.8365 (2.1097) acc@1 0.2734 (0.2210) acc@5 0.8047 (0.7094)\n",
      "\u001b[32m[2020-07-11 16:00:32] __main__ INFO: \u001b[0mEpoch 1 Step 351/351 lr 0.100000 loss 1.8688 (2.0816) acc@1 0.3516 (0.2318) acc@5 0.7891 (0.7172)\n",
      "\u001b[32m[2020-07-11 16:00:32] __main__ INFO: \u001b[0mElapsed 396.28\n",
      "\u001b[32m[2020-07-11 16:00:32] __main__ INFO: \u001b[0mVal 1\n",
      "\u001b[32m[2020-07-11 16:00:45] __main__ INFO: \u001b[0mEpoch 1 loss 1.9493 acc@1 0.2872 acc@5 0.7784\n",
      "\u001b[32m[2020-07-11 16:00:45] __main__ INFO: \u001b[0mElapsed 13.18\n",
      "\u001b[32m[2020-07-11 16:00:45] __main__ INFO: \u001b[0mTrain 2 351\n",
      "\u001b[32m[2020-07-11 16:02:37] __main__ INFO: \u001b[0mEpoch 2 Step 100/351 lr 0.100000 loss 1.8233 (1.8363) acc@1 0.3047 (0.3231) acc@5 0.7656 (0.7852)\n",
      "\u001b[32m[2020-07-11 16:04:29] __main__ INFO: \u001b[0mEpoch 2 Step 200/351 lr 0.100000 loss 1.6066 (1.8029) acc@1 0.3672 (0.3366) acc@5 0.8516 (0.7959)\n"
     ]
    }
   ],
   "source": [
    "# Train the model per the settings specified in the original paper\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "!python train.py --config configs/cifar/wrn.yaml \\\n",
    "    model.wrn.depth 28 \\\n",
    "    model.wrn.widening_factor 10 \\\n",
    "    train.batch_size 128 \\\n",
    "    train.base_lr 0.1 \\\n",
    "    dataset.name CIFAR10_RA_1_20 \\\n",
    "    train.output_dir /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_1_20/exp00 \\\n",
    "    scheduler.epochs 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-23 21:22:10] __main__ INFO: \u001b[0mdevice: cuda\n",
      "cudnn:\n",
      "  benchmark: True\n",
      "  deterministic: False\n",
      "dataset:\n",
      "  name: CIFAR10\n",
      "  dataset_dir: ~/.torch/datasets/CIFAR10\n",
      "  image_size: 32\n",
      "  n_channels: 3\n",
      "  n_classes: 10\n",
      "model:\n",
      "  type: cifar\n",
      "  name: wrn\n",
      "  init_mode: kaiming_fan_in\n",
      "  vgg:\n",
      "    n_channels: [64, 128, 256, 512, 512]\n",
      "    n_layers: [2, 2, 3, 3, 3]\n",
      "    use_bn: True\n",
      "  resnet:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "  resnet_preact:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "  wrn:\n",
      "    depth: 28\n",
      "    initial_channels: 16\n",
      "    widening_factor: 10\n",
      "    drop_rate: 0.0\n",
      "  densenet:\n",
      "    depth: 100\n",
      "    n_blocks: [6, 12, 24, 16]\n",
      "    block_type: bottleneck\n",
      "    growth_rate: 12\n",
      "    drop_rate: 0.0\n",
      "    compression_rate: 0.5\n",
      "  pyramidnet:\n",
      "    depth: 272\n",
      "    n_blocks: [3, 24, 36, 3]\n",
      "    initial_channels: 16\n",
      "    block_type: bottleneck\n",
      "    alpha: 200\n",
      "  resnext:\n",
      "    depth: 29\n",
      "    n_blocks: [3, 4, 6, 3]\n",
      "    initial_channels: 64\n",
      "    cardinality: 8\n",
      "    base_channels: 4\n",
      "  shake_shake:\n",
      "    depth: 26\n",
      "    initial_channels: 96\n",
      "    shake_forward: True\n",
      "    shake_backward: True\n",
      "    shake_image: True\n",
      "  se_resnet_preact:\n",
      "    depth: 110\n",
      "    initial_channels: 16\n",
      "    se_reduction: 16\n",
      "    block_type: basic\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "train:\n",
      "  checkpoint: /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_20/exp00/checkpoint_00400.pth\n",
      "  resume: False\n",
      "  precision: O0\n",
      "  batch_size: 128\n",
      "  subdivision: 1\n",
      "  optimizer: sgd\n",
      "  base_lr: 0.001\n",
      "  momentum: 0.9\n",
      "  nesterov: True\n",
      "  weight_decay: 0.0005\n",
      "  no_weight_decay_on_bn: False\n",
      "  gradient_clip: 0\n",
      "  start_epoch: 0\n",
      "  seed: 0\n",
      "  val_first: True\n",
      "  val_period: 1\n",
      "  val_ratio: 0.1\n",
      "  use_test_as_val: False\n",
      "  output_dir: /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_20/exp00_resume400_50\n",
      "  log_period: 100\n",
      "  checkpoint_period: 100\n",
      "  use_tensorboard: True\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: True\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "  distributed: False\n",
      "  dist:\n",
      "    backend: nccl\n",
      "    init_method: env://\n",
      "    world_size: -1\n",
      "    node_rank: -1\n",
      "    local_rank: 0\n",
      "    use_sync_bn: False\n",
      "tensorboard:\n",
      "  train_images: False\n",
      "  val_images: False\n",
      "  model_params: False\n",
      "optim:\n",
      "  adam:\n",
      "    betas: (0.9, 0.999)\n",
      "  lars:\n",
      "    eps: 1e-09\n",
      "    threshold: 0.01\n",
      "  adabound:\n",
      "    betas: (0.9, 0.999)\n",
      "    final_lr: 0.1\n",
      "    gamma: 0.001\n",
      "scheduler:\n",
      "  epochs: 50\n",
      "  warmup:\n",
      "    type: none\n",
      "    epochs: 0\n",
      "    start_factor: 0.001\n",
      "    exponent: 4\n",
      "  type: multistep\n",
      "  milestones: [60, 120, 160]\n",
      "  lr_decay: 0.2\n",
      "  lr_min_factor: 0.001\n",
      "  T0: 10\n",
      "  T_mul: 1.0\n",
      "validation:\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: False\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "augmentation:\n",
      "  use_random_crop: True\n",
      "  use_random_horizontal_flip: True\n",
      "  use_cutout: False\n",
      "  use_random_erasing: False\n",
      "  use_dual_cutout: False\n",
      "  use_mixup: False\n",
      "  use_ricap: False\n",
      "  use_cutmix: False\n",
      "  use_label_smoothing: False\n",
      "  random_crop:\n",
      "    padding: 4\n",
      "    fill: 0\n",
      "    padding_mode: constant\n",
      "  random_horizontal_flip:\n",
      "    prob: 0.5\n",
      "  cutout:\n",
      "    prob: 1.0\n",
      "    mask_size: 16\n",
      "    cut_inside: False\n",
      "    mask_color: 0\n",
      "    dual_cutout_alpha: 0.1\n",
      "  random_erasing:\n",
      "    prob: 0.5\n",
      "    area_ratio_range: [0.02, 0.4]\n",
      "    min_aspect_ratio: 0.3\n",
      "    max_attempt: 20\n",
      "  mixup:\n",
      "    alpha: 1.0\n",
      "  ricap:\n",
      "    beta: 0.3\n",
      "  cutmix:\n",
      "    alpha: 1.0\n",
      "  label_smoothing:\n",
      "    epsilon: 0.1\n",
      "tta:\n",
      "  use_resize: False\n",
      "  use_center_crop: False\n",
      "  resize: 256\n",
      "test:\n",
      "  checkpoint: ''\n",
      "  output_dir: ''\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    pin_memory: False\n",
      "\u001b[32m[2020-06-23 21:22:10] __main__ INFO: \u001b[0menv_info:\n",
      "  pytorch_version: 1.4.0\n",
      "  cuda_version: 10.1\n",
      "  cudnn_version: 7603\n",
      "  num_gpus: 1\n",
      "  gpu_name: Tesla K80\n",
      "  gpu_capability: 3.7\n",
      "Files already downloaded and verified\n",
      "\u001b[32m[2020-06-23 21:22:15] __main__ INFO: \u001b[0mMACs  : 5.25G\n",
      "\u001b[32m[2020-06-23 21:22:15] __main__ INFO: \u001b[0m#params: 36.48M\n",
      "Selected optimization level O0:  Pure FP32 training.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\",)\n",
      "\u001b[32m[2020-06-23 21:22:15] __main__ INFO: \u001b[0mVal 0\n",
      "\u001b[32m[2020-06-23 21:22:35] __main__ INFO: \u001b[0mEpoch 0 loss 0.6997 acc@1 0.8318 acc@5 0.9882\n",
      "\u001b[32m[2020-06-23 21:22:35] __main__ INFO: \u001b[0mElapsed 20.01\n",
      "\u001b[32m[2020-06-23 21:22:35] __main__ INFO: \u001b[0mTrain 1 0\n",
      "\u001b[32m[2020-06-23 21:24:34] __main__ INFO: \u001b[0mEpoch 1 Step 100/351 lr 0.001000 loss 0.1580 (0.3052) acc@1 0.9375 (0.9086) acc@5 1.0000 (0.9956)\n",
      "\u001b[32m[2020-06-23 21:26:29] __main__ INFO: \u001b[0mEpoch 1 Step 200/351 lr 0.001000 loss 0.3502 (0.2849) acc@1 0.9062 (0.9127) acc@5 1.0000 (0.9965)\n",
      "\u001b[32m[2020-06-23 21:28:24] __main__ INFO: \u001b[0mEpoch 1 Step 300/351 lr 0.001000 loss 0.2202 (0.2677) acc@1 0.9297 (0.9169) acc@5 0.9922 (0.9966)\n",
      "\u001b[32m[2020-06-23 21:29:22] __main__ INFO: \u001b[0mEpoch 1 Step 351/351 lr 0.001000 loss 0.1890 (0.2612) acc@1 0.9297 (0.9185) acc@5 1.0000 (0.9968)\n",
      "\u001b[32m[2020-06-23 21:29:22] __main__ INFO: \u001b[0mElapsed 407.48\n",
      "\u001b[32m[2020-06-23 21:29:22] __main__ INFO: \u001b[0mVal 1\n",
      "\u001b[32m[2020-06-23 21:29:36] __main__ INFO: \u001b[0mEpoch 1 loss 0.2794 acc@1 0.9088 acc@5 0.9972\n",
      "\u001b[32m[2020-06-23 21:29:36] __main__ INFO: \u001b[0mElapsed 13.51\n",
      "\u001b[32m[2020-06-23 21:29:36] __main__ INFO: \u001b[0mTrain 2 351\n",
      "\u001b[32m[2020-06-23 21:31:31] __main__ INFO: \u001b[0mEpoch 2 Step 100/351 lr 0.001000 loss 0.1729 (0.1743) acc@1 0.9531 (0.9460) acc@5 0.9922 (0.9981)\n",
      "\u001b[32m[2020-06-23 21:33:25] __main__ INFO: \u001b[0mEpoch 2 Step 200/351 lr 0.001000 loss 0.1747 (0.1756) acc@1 0.9375 (0.9459) acc@5 1.0000 (0.9985)\n",
      "\u001b[32m[2020-06-23 21:35:20] __main__ INFO: \u001b[0mEpoch 2 Step 300/351 lr 0.001000 loss 0.1577 (0.1733) acc@1 0.9688 (0.9464) acc@5 1.0000 (0.9987)\n",
      "\u001b[32m[2020-06-23 21:36:18] __main__ INFO: \u001b[0mEpoch 2 Step 351/351 lr 0.001000 loss 0.2117 (0.1728) acc@1 0.9297 (0.9464) acc@5 0.9922 (0.9986)\n",
      "\u001b[32m[2020-06-23 21:36:18] __main__ INFO: \u001b[0mElapsed 402.56\n",
      "\u001b[32m[2020-06-23 21:36:18] __main__ INFO: \u001b[0mVal 2\n",
      "\u001b[32m[2020-06-23 21:36:32] __main__ INFO: \u001b[0mEpoch 2 loss 0.2692 acc@1 0.9132 acc@5 0.9968\n",
      "\u001b[32m[2020-06-23 21:36:32] __main__ INFO: \u001b[0mElapsed 13.50\n",
      "\u001b[32m[2020-06-23 21:36:32] __main__ INFO: \u001b[0mTrain 3 702\n",
      "\u001b[32m[2020-06-23 21:38:27] __main__ INFO: \u001b[0mEpoch 3 Step 100/351 lr 0.001000 loss 0.1097 (0.1266) acc@1 0.9688 (0.9599) acc@5 1.0000 (0.9995)\n",
      "\u001b[32m[2020-06-23 21:40:21] __main__ INFO: \u001b[0mEpoch 3 Step 200/351 lr 0.001000 loss 0.1284 (0.1323) acc@1 0.9609 (0.9593) acc@5 1.0000 (0.9993)\n",
      "\u001b[32m[2020-06-23 21:42:16] __main__ INFO: \u001b[0mEpoch 3 Step 300/351 lr 0.001000 loss 0.2124 (0.1337) acc@1 0.9297 (0.9591) acc@5 0.9844 (0.9993)\n",
      "\u001b[32m[2020-06-23 21:43:14] __main__ INFO: \u001b[0mEpoch 3 Step 351/351 lr 0.001000 loss 0.1848 (0.1347) acc@1 0.9375 (0.9592) acc@5 1.0000 (0.9992)\n",
      "\u001b[32m[2020-06-23 21:43:14] __main__ INFO: \u001b[0mElapsed 402.38\n",
      "\u001b[32m[2020-06-23 21:43:14] __main__ INFO: \u001b[0mVal 3\n",
      "\u001b[32m[2020-06-23 21:43:28] __main__ INFO: \u001b[0mEpoch 3 loss 0.2608 acc@1 0.9170 acc@5 0.9966\n",
      "\u001b[32m[2020-06-23 21:43:28] __main__ INFO: \u001b[0mElapsed 13.53\n",
      "\u001b[32m[2020-06-23 21:43:28] __main__ INFO: \u001b[0mTrain 4 1053\n",
      "\u001b[32m[2020-06-23 21:45:22] __main__ INFO: \u001b[0mEpoch 4 Step 100/351 lr 0.001000 loss 0.1520 (0.0977) acc@1 0.9453 (0.9723) acc@5 1.0000 (0.9995)\n",
      "\u001b[32m[2020-06-23 21:47:17] __main__ INFO: \u001b[0mEpoch 4 Step 200/351 lr 0.001000 loss 0.1540 (0.0993) acc@1 0.9609 (0.9710) acc@5 1.0000 (0.9995)\n",
      "\u001b[32m[2020-06-23 21:49:12] __main__ INFO: \u001b[0mEpoch 4 Step 300/351 lr 0.001000 loss 0.1157 (0.1013) acc@1 0.9609 (0.9708) acc@5 1.0000 (0.9993)\n",
      "\u001b[32m[2020-06-23 21:50:10] __main__ INFO: \u001b[0mEpoch 4 Step 351/351 lr 0.001000 loss 0.1954 (0.1038) acc@1 0.9375 (0.9701) acc@5 1.0000 (0.9993)\n",
      "\u001b[32m[2020-06-23 21:50:10] __main__ INFO: \u001b[0mElapsed 402.47\n",
      "\u001b[32m[2020-06-23 21:50:10] __main__ INFO: \u001b[0mVal 4\n",
      "\u001b[32m[2020-06-23 21:50:24] __main__ INFO: \u001b[0mEpoch 4 loss 0.2663 acc@1 0.9192 acc@5 0.9970\n",
      "\u001b[32m[2020-06-23 21:50:24] __main__ INFO: \u001b[0mElapsed 13.51\n",
      "\u001b[32m[2020-06-23 21:50:24] __main__ INFO: \u001b[0mTrain 5 1404\n",
      "\u001b[32m[2020-06-23 21:52:19] __main__ INFO: \u001b[0mEpoch 5 Step 100/351 lr 0.001000 loss 0.0481 (0.0853) acc@1 0.9844 (0.9766) acc@5 1.0000 (0.9995)\n",
      "\u001b[32m[2020-06-23 21:54:13] __main__ INFO: \u001b[0mEpoch 5 Step 200/351 lr 0.001000 loss 0.1013 (0.0835) acc@1 0.9609 (0.9762) acc@5 1.0000 (0.9994)\n",
      "\u001b[32m[2020-06-23 21:56:08] __main__ INFO: \u001b[0mEpoch 5 Step 300/351 lr 0.001000 loss 0.1236 (0.0860) acc@1 0.9453 (0.9752) acc@5 1.0000 (0.9995)\n",
      "\u001b[32m[2020-06-23 21:57:07] __main__ INFO: \u001b[0mEpoch 5 Step 351/351 lr 0.001000 loss 0.1098 (0.0853) acc@1 0.9766 (0.9756) acc@5 1.0000 (0.9995)\n",
      "\u001b[32m[2020-06-23 21:57:07] __main__ INFO: \u001b[0mElapsed 402.90\n",
      "\u001b[32m[2020-06-23 21:57:07] __main__ INFO: \u001b[0mVal 5\n",
      "\u001b[32m[2020-06-23 21:57:20] __main__ INFO: \u001b[0mEpoch 5 loss 0.2605 acc@1 0.9200 acc@5 0.9978\n",
      "\u001b[32m[2020-06-23 21:57:20] __main__ INFO: \u001b[0mElapsed 13.53\n",
      "\u001b[32m[2020-06-23 21:57:20] __main__ INFO: \u001b[0mTrain 6 1755\n",
      "\u001b[32m[2020-06-23 21:59:15] __main__ INFO: \u001b[0mEpoch 6 Step 100/351 lr 0.001000 loss 0.0486 (0.0671) acc@1 0.9922 (0.9818) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-23 22:01:10] __main__ INFO: \u001b[0mEpoch 6 Step 200/351 lr 0.001000 loss 0.0401 (0.0666) acc@1 0.9922 (0.9814) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-06-23 22:03:05] __main__ INFO: \u001b[0mEpoch 6 Step 300/351 lr 0.001000 loss 0.0433 (0.0673) acc@1 0.9922 (0.9813) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-06-23 22:04:03] __main__ INFO: \u001b[0mEpoch 6 Step 351/351 lr 0.001000 loss 0.0469 (0.0670) acc@1 0.9922 (0.9816) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-06-23 22:04:03] __main__ INFO: \u001b[0mElapsed 403.03\n",
      "\u001b[32m[2020-06-23 22:04:03] __main__ INFO: \u001b[0mVal 6\n",
      "\u001b[32m[2020-06-23 22:04:17] __main__ INFO: \u001b[0mEpoch 6 loss 0.2593 acc@1 0.9242 acc@5 0.9978\n",
      "\u001b[32m[2020-06-23 22:04:17] __main__ INFO: \u001b[0mElapsed 13.51\n",
      "\u001b[32m[2020-06-23 22:04:17] __main__ INFO: \u001b[0mTrain 7 2106\n",
      "\u001b[32m[2020-06-23 22:06:12] __main__ INFO: \u001b[0mEpoch 7 Step 100/351 lr 0.001000 loss 0.0641 (0.0608) acc@1 0.9844 (0.9844) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-06-23 22:08:06] __main__ INFO: \u001b[0mEpoch 7 Step 200/351 lr 0.001000 loss 0.0709 (0.0585) acc@1 0.9844 (0.9845) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-23 22:10:01] __main__ INFO: \u001b[0mEpoch 7 Step 300/351 lr 0.001000 loss 0.0349 (0.0572) acc@1 1.0000 (0.9850) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-23 22:11:00] __main__ INFO: \u001b[0mEpoch 7 Step 351/351 lr 0.001000 loss 0.0544 (0.0579) acc@1 0.9766 (0.9847) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-23 22:11:00] __main__ INFO: \u001b[0mElapsed 403.05\n",
      "\u001b[32m[2020-06-23 22:11:00] __main__ INFO: \u001b[0mVal 7\n",
      "\u001b[32m[2020-06-23 22:11:13] __main__ INFO: \u001b[0mEpoch 7 loss 0.2702 acc@1 0.9176 acc@5 0.9980\n",
      "\u001b[32m[2020-06-23 22:11:13] __main__ INFO: \u001b[0mElapsed 13.52\n",
      "\u001b[32m[2020-06-23 22:11:13] __main__ INFO: \u001b[0mTrain 8 2457\n",
      "\u001b[32m[2020-06-23 22:13:08] __main__ INFO: \u001b[0mEpoch 8 Step 100/351 lr 0.001000 loss 0.0183 (0.0474) acc@1 1.0000 (0.9882) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-23 22:15:03] __main__ INFO: \u001b[0mEpoch 8 Step 200/351 lr 0.001000 loss 0.0336 (0.0463) acc@1 1.0000 (0.9881) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-23 22:16:58] __main__ INFO: \u001b[0mEpoch 8 Step 300/351 lr 0.001000 loss 0.0458 (0.0472) acc@1 0.9844 (0.9877) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-23 22:17:56] __main__ INFO: \u001b[0mEpoch 8 Step 351/351 lr 0.001000 loss 0.0531 (0.0473) acc@1 0.9688 (0.9877) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-23 22:17:56] __main__ INFO: \u001b[0mElapsed 402.83\n",
      "\u001b[32m[2020-06-23 22:17:56] __main__ INFO: \u001b[0mVal 8\n",
      "\u001b[32m[2020-06-23 22:18:10] __main__ INFO: \u001b[0mEpoch 8 loss 0.2655 acc@1 0.9216 acc@5 0.9972\n",
      "\u001b[32m[2020-06-23 22:18:10] __main__ INFO: \u001b[0mElapsed 13.51\n",
      "\u001b[32m[2020-06-23 22:18:10] __main__ INFO: \u001b[0mTrain 9 2808\n",
      "\u001b[32m[2020-06-23 22:20:04] __main__ INFO: \u001b[0mEpoch 9 Step 100/351 lr 0.001000 loss 0.0404 (0.0382) acc@1 0.9844 (0.9909) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-23 22:21:59] __main__ INFO: \u001b[0mEpoch 9 Step 200/351 lr 0.001000 loss 0.0232 (0.0384) acc@1 1.0000 (0.9905) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-23 22:23:54] __main__ INFO: \u001b[0mEpoch 9 Step 300/351 lr 0.001000 loss 0.0392 (0.0386) acc@1 1.0000 (0.9905) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-23 22:24:52] __main__ INFO: \u001b[0mEpoch 9 Step 351/351 lr 0.001000 loss 0.0540 (0.0386) acc@1 0.9844 (0.9904) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-23 22:24:52] __main__ INFO: \u001b[0mElapsed 402.85\n",
      "\u001b[32m[2020-06-23 22:24:52] __main__ INFO: \u001b[0mVal 9\n",
      "\u001b[32m[2020-06-23 22:25:06] __main__ INFO: \u001b[0mEpoch 9 loss 0.2657 acc@1 0.9228 acc@5 0.9970\n",
      "\u001b[32m[2020-06-23 22:25:06] __main__ INFO: \u001b[0mElapsed 13.52\n",
      "\u001b[32m[2020-06-23 22:25:06] __main__ INFO: \u001b[0mTrain 10 3159\n",
      "\u001b[32m[2020-06-23 22:27:01] __main__ INFO: \u001b[0mEpoch 10 Step 100/351 lr 0.001000 loss 0.0584 (0.0307) acc@1 0.9922 (0.9937) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 22:28:56] __main__ INFO: \u001b[0mEpoch 10 Step 200/351 lr 0.001000 loss 0.0258 (0.0306) acc@1 1.0000 (0.9935) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 22:30:50] __main__ INFO: \u001b[0mEpoch 10 Step 300/351 lr 0.001000 loss 0.0158 (0.0318) acc@1 1.0000 (0.9926) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 22:31:49] __main__ INFO: \u001b[0mEpoch 10 Step 351/351 lr 0.001000 loss 0.0312 (0.0325) acc@1 1.0000 (0.9925) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 22:31:49] __main__ INFO: \u001b[0mElapsed 402.94\n",
      "\u001b[32m[2020-06-23 22:31:49] __main__ INFO: \u001b[0mVal 10\n",
      "\u001b[32m[2020-06-23 22:32:02] __main__ INFO: \u001b[0mEpoch 10 loss 0.2793 acc@1 0.9228 acc@5 0.9970\n",
      "\u001b[32m[2020-06-23 22:32:02] __main__ INFO: \u001b[0mElapsed 13.53\n",
      "\u001b[32m[2020-06-23 22:32:02] __main__ INFO: \u001b[0mTrain 11 3510\n",
      "\u001b[32m[2020-06-23 22:33:57] __main__ INFO: \u001b[0mEpoch 11 Step 100/351 lr 0.001000 loss 0.0151 (0.0226) acc@1 1.0000 (0.9955) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 22:35:52] __main__ INFO: \u001b[0mEpoch 11 Step 200/351 lr 0.001000 loss 0.0100 (0.0246) acc@1 1.0000 (0.9946) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 22:37:47] __main__ INFO: \u001b[0mEpoch 11 Step 300/351 lr 0.001000 loss 0.0333 (0.0258) acc@1 0.9922 (0.9942) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 22:38:45] __main__ INFO: \u001b[0mEpoch 11 Step 351/351 lr 0.001000 loss 0.0241 (0.0265) acc@1 0.9922 (0.9939) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 22:38:45] __main__ INFO: \u001b[0mElapsed 402.94\n",
      "\u001b[32m[2020-06-23 22:38:45] __main__ INFO: \u001b[0mVal 11\n",
      "\u001b[32m[2020-06-23 22:38:59] __main__ INFO: \u001b[0mEpoch 11 loss 0.2827 acc@1 0.9230 acc@5 0.9970\n",
      "\u001b[32m[2020-06-23 22:38:59] __main__ INFO: \u001b[0mElapsed 13.51\n",
      "\u001b[32m[2020-06-23 22:38:59] __main__ INFO: \u001b[0mTrain 12 3861\n",
      "\u001b[32m[2020-06-23 22:40:54] __main__ INFO: \u001b[0mEpoch 12 Step 100/351 lr 0.001000 loss 0.0311 (0.0244) acc@1 0.9922 (0.9943) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 22:42:49] __main__ INFO: \u001b[0mEpoch 12 Step 200/351 lr 0.001000 loss 0.0292 (0.0252) acc@1 0.9844 (0.9941) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 22:44:43] __main__ INFO: \u001b[0mEpoch 12 Step 300/351 lr 0.001000 loss 0.0077 (0.0251) acc@1 1.0000 (0.9943) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 22:45:42] __main__ INFO: \u001b[0mEpoch 12 Step 351/351 lr 0.001000 loss 0.0140 (0.0256) acc@1 1.0000 (0.9941) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 22:45:42] __main__ INFO: \u001b[0mElapsed 402.94\n",
      "\u001b[32m[2020-06-23 22:45:42] __main__ INFO: \u001b[0mVal 12\n",
      "\u001b[32m[2020-06-23 22:45:55] __main__ INFO: \u001b[0mEpoch 12 loss 0.2865 acc@1 0.9234 acc@5 0.9972\n",
      "\u001b[32m[2020-06-23 22:45:55] __main__ INFO: \u001b[0mElapsed 13.54\n",
      "\u001b[32m[2020-06-23 22:45:55] __main__ INFO: \u001b[0mTrain 13 4212\n",
      "\u001b[32m[2020-06-23 22:47:50] __main__ INFO: \u001b[0mEpoch 13 Step 100/351 lr 0.001000 loss 0.0412 (0.0211) acc@1 0.9922 (0.9955) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 22:49:45] __main__ INFO: \u001b[0mEpoch 13 Step 200/351 lr 0.001000 loss 0.0375 (0.0215) acc@1 0.9844 (0.9952) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 22:51:40] __main__ INFO: \u001b[0mEpoch 13 Step 300/351 lr 0.001000 loss 0.0151 (0.0210) acc@1 0.9922 (0.9955) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 22:52:38] __main__ INFO: \u001b[0mEpoch 13 Step 351/351 lr 0.001000 loss 0.0254 (0.0210) acc@1 0.9922 (0.9954) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 22:52:38] __main__ INFO: \u001b[0mElapsed 402.67\n",
      "\u001b[32m[2020-06-23 22:52:38] __main__ INFO: \u001b[0mVal 13\n",
      "\u001b[32m[2020-06-23 22:52:52] __main__ INFO: \u001b[0mEpoch 13 loss 0.2831 acc@1 0.9234 acc@5 0.9964\n",
      "\u001b[32m[2020-06-23 22:52:52] __main__ INFO: \u001b[0mElapsed 13.52\n",
      "\u001b[32m[2020-06-23 22:52:52] __main__ INFO: \u001b[0mTrain 14 4563\n",
      "\u001b[32m[2020-06-23 22:54:46] __main__ INFO: \u001b[0mEpoch 14 Step 100/351 lr 0.001000 loss 0.0422 (0.0203) acc@1 0.9844 (0.9950) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 22:56:41] __main__ INFO: \u001b[0mEpoch 14 Step 200/351 lr 0.001000 loss 0.0269 (0.0210) acc@1 0.9922 (0.9950) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 22:58:36] __main__ INFO: \u001b[0mEpoch 14 Step 300/351 lr 0.001000 loss 0.0246 (0.0213) acc@1 0.9844 (0.9946) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 22:59:34] __main__ INFO: \u001b[0mEpoch 14 Step 351/351 lr 0.001000 loss 0.0065 (0.0210) acc@1 1.0000 (0.9949) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 22:59:34] __main__ INFO: \u001b[0mElapsed 402.86\n",
      "\u001b[32m[2020-06-23 22:59:34] __main__ INFO: \u001b[0mVal 14\n",
      "\u001b[32m[2020-06-23 22:59:48] __main__ INFO: \u001b[0mEpoch 14 loss 0.2774 acc@1 0.9262 acc@5 0.9974\n",
      "\u001b[32m[2020-06-23 22:59:48] __main__ INFO: \u001b[0mElapsed 13.52\n",
      "\u001b[32m[2020-06-23 22:59:48] __main__ INFO: \u001b[0mTrain 15 4914\n",
      "\u001b[32m[2020-06-23 23:01:43] __main__ INFO: \u001b[0mEpoch 15 Step 100/351 lr 0.001000 loss 0.0071 (0.0156) acc@1 1.0000 (0.9970) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:03:37] __main__ INFO: \u001b[0mEpoch 15 Step 200/351 lr 0.001000 loss 0.0095 (0.0159) acc@1 1.0000 (0.9970) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:05:32] __main__ INFO: \u001b[0mEpoch 15 Step 300/351 lr 0.001000 loss 0.0049 (0.0161) acc@1 1.0000 (0.9971) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:06:31] __main__ INFO: \u001b[0mEpoch 15 Step 351/351 lr 0.001000 loss 0.0119 (0.0165) acc@1 1.0000 (0.9969) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:06:31] __main__ INFO: \u001b[0mElapsed 402.66\n",
      "\u001b[32m[2020-06-23 23:06:31] __main__ INFO: \u001b[0mVal 15\n",
      "\u001b[32m[2020-06-23 23:06:44] __main__ INFO: \u001b[0mEpoch 15 loss 0.2800 acc@1 0.9272 acc@5 0.9966\n",
      "\u001b[32m[2020-06-23 23:06:44] __main__ INFO: \u001b[0mElapsed 13.51\n",
      "\u001b[32m[2020-06-23 23:06:44] __main__ INFO: \u001b[0mTrain 16 5265\n",
      "\u001b[32m[2020-06-23 23:08:39] __main__ INFO: \u001b[0mEpoch 16 Step 100/351 lr 0.001000 loss 0.0086 (0.0140) acc@1 1.0000 (0.9975) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:10:33] __main__ INFO: \u001b[0mEpoch 16 Step 200/351 lr 0.001000 loss 0.0066 (0.0147) acc@1 1.0000 (0.9971) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:12:28] __main__ INFO: \u001b[0mEpoch 16 Step 300/351 lr 0.001000 loss 0.0080 (0.0156) acc@1 1.0000 (0.9967) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:13:26] __main__ INFO: \u001b[0mEpoch 16 Step 351/351 lr 0.001000 loss 0.0077 (0.0157) acc@1 1.0000 (0.9967) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:13:26] __main__ INFO: \u001b[0mElapsed 401.98\n",
      "\u001b[32m[2020-06-23 23:13:26] __main__ INFO: \u001b[0mVal 16\n",
      "\u001b[32m[2020-06-23 23:13:40] __main__ INFO: \u001b[0mEpoch 16 loss 0.2737 acc@1 0.9246 acc@5 0.9968\n",
      "\u001b[32m[2020-06-23 23:13:40] __main__ INFO: \u001b[0mElapsed 13.49\n",
      "\u001b[32m[2020-06-23 23:13:40] __main__ INFO: \u001b[0mTrain 17 5616\n",
      "\u001b[32m[2020-06-23 23:15:34] __main__ INFO: \u001b[0mEpoch 17 Step 100/351 lr 0.001000 loss 0.0118 (0.0124) acc@1 1.0000 (0.9977) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:17:29] __main__ INFO: \u001b[0mEpoch 17 Step 200/351 lr 0.001000 loss 0.0153 (0.0132) acc@1 1.0000 (0.9975) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:19:23] __main__ INFO: \u001b[0mEpoch 17 Step 300/351 lr 0.001000 loss 0.0224 (0.0129) acc@1 0.9922 (0.9977) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:20:22] __main__ INFO: \u001b[0mEpoch 17 Step 351/351 lr 0.001000 loss 0.0196 (0.0132) acc@1 0.9922 (0.9975) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:20:22] __main__ INFO: \u001b[0mElapsed 402.05\n",
      "\u001b[32m[2020-06-23 23:20:22] __main__ INFO: \u001b[0mVal 17\n",
      "\u001b[32m[2020-06-23 23:20:35] __main__ INFO: \u001b[0mEpoch 17 loss 0.2845 acc@1 0.9252 acc@5 0.9964\n",
      "\u001b[32m[2020-06-23 23:20:35] __main__ INFO: \u001b[0mElapsed 13.50\n",
      "\u001b[32m[2020-06-23 23:20:35] __main__ INFO: \u001b[0mTrain 18 5967\n",
      "\u001b[32m[2020-06-23 23:22:30] __main__ INFO: \u001b[0mEpoch 18 Step 100/351 lr 0.001000 loss 0.0077 (0.0151) acc@1 1.0000 (0.9971) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-23 23:24:24] __main__ INFO: \u001b[0mEpoch 18 Step 200/351 lr 0.001000 loss 0.0127 (0.0141) acc@1 0.9922 (0.9973) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:26:19] __main__ INFO: \u001b[0mEpoch 18 Step 300/351 lr 0.001000 loss 0.0130 (0.0142) acc@1 1.0000 (0.9974) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:27:17] __main__ INFO: \u001b[0mEpoch 18 Step 351/351 lr 0.001000 loss 0.0273 (0.0139) acc@1 0.9922 (0.9975) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:27:17] __main__ INFO: \u001b[0mElapsed 401.99\n",
      "\u001b[32m[2020-06-23 23:27:17] __main__ INFO: \u001b[0mVal 18\n",
      "\u001b[32m[2020-06-23 23:27:31] __main__ INFO: \u001b[0mEpoch 18 loss 0.2727 acc@1 0.9284 acc@5 0.9968\n",
      "\u001b[32m[2020-06-23 23:27:31] __main__ INFO: \u001b[0mElapsed 13.52\n",
      "\u001b[32m[2020-06-23 23:27:31] __main__ INFO: \u001b[0mTrain 19 6318\n",
      "\u001b[32m[2020-06-23 23:29:25] __main__ INFO: \u001b[0mEpoch 19 Step 100/351 lr 0.001000 loss 0.0232 (0.0135) acc@1 1.0000 (0.9970) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:31:20] __main__ INFO: \u001b[0mEpoch 19 Step 200/351 lr 0.001000 loss 0.0271 (0.0136) acc@1 0.9844 (0.9970) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:33:14] __main__ INFO: \u001b[0mEpoch 19 Step 300/351 lr 0.001000 loss 0.0118 (0.0140) acc@1 1.0000 (0.9969) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:34:13] __main__ INFO: \u001b[0mEpoch 19 Step 351/351 lr 0.001000 loss 0.0061 (0.0139) acc@1 1.0000 (0.9970) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:34:13] __main__ INFO: \u001b[0mElapsed 402.12\n",
      "\u001b[32m[2020-06-23 23:34:13] __main__ INFO: \u001b[0mVal 19\n",
      "\u001b[32m[2020-06-23 23:34:26] __main__ INFO: \u001b[0mEpoch 19 loss 0.2898 acc@1 0.9210 acc@5 0.9972\n",
      "\u001b[32m[2020-06-23 23:34:26] __main__ INFO: \u001b[0mElapsed 13.51\n",
      "\u001b[32m[2020-06-23 23:34:26] __main__ INFO: \u001b[0mTrain 20 6669\n",
      "\u001b[32m[2020-06-23 23:36:21] __main__ INFO: \u001b[0mEpoch 20 Step 100/351 lr 0.001000 loss 0.0105 (0.0130) acc@1 1.0000 (0.9977) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:38:15] __main__ INFO: \u001b[0mEpoch 20 Step 200/351 lr 0.001000 loss 0.0065 (0.0126) acc@1 1.0000 (0.9977) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:40:10] __main__ INFO: \u001b[0mEpoch 20 Step 300/351 lr 0.001000 loss 0.0054 (0.0123) acc@1 1.0000 (0.9978) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:41:08] __main__ INFO: \u001b[0mEpoch 20 Step 351/351 lr 0.001000 loss 0.0052 (0.0121) acc@1 1.0000 (0.9978) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:41:08] __main__ INFO: \u001b[0mElapsed 401.85\n",
      "\u001b[32m[2020-06-23 23:41:08] __main__ INFO: \u001b[0mVal 20\n",
      "\u001b[32m[2020-06-23 23:41:22] __main__ INFO: \u001b[0mEpoch 20 loss 0.2844 acc@1 0.9264 acc@5 0.9968\n",
      "\u001b[32m[2020-06-23 23:41:22] __main__ INFO: \u001b[0mElapsed 13.50\n",
      "\u001b[32m[2020-06-23 23:41:22] __main__ INFO: \u001b[0mTrain 21 7020\n",
      "\u001b[32m[2020-06-23 23:43:16] __main__ INFO: \u001b[0mEpoch 21 Step 100/351 lr 0.001000 loss 0.0089 (0.0104) acc@1 1.0000 (0.9984) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:45:11] __main__ INFO: \u001b[0mEpoch 21 Step 200/351 lr 0.001000 loss 0.0194 (0.0101) acc@1 0.9922 (0.9984) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:47:05] __main__ INFO: \u001b[0mEpoch 21 Step 300/351 lr 0.001000 loss 0.0038 (0.0098) acc@1 1.0000 (0.9985) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:48:03] __main__ INFO: \u001b[0mEpoch 21 Step 351/351 lr 0.001000 loss 0.0252 (0.0103) acc@1 0.9844 (0.9983) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:48:03] __main__ INFO: \u001b[0mElapsed 401.83\n",
      "\u001b[32m[2020-06-23 23:48:03] __main__ INFO: \u001b[0mVal 21\n",
      "\u001b[32m[2020-06-23 23:48:17] __main__ INFO: \u001b[0mEpoch 21 loss 0.2783 acc@1 0.9272 acc@5 0.9972\n",
      "\u001b[32m[2020-06-23 23:48:17] __main__ INFO: \u001b[0mElapsed 13.51\n",
      "\u001b[32m[2020-06-23 23:48:17] __main__ INFO: \u001b[0mTrain 22 7371\n",
      "\u001b[32m[2020-06-23 23:50:12] __main__ INFO: \u001b[0mEpoch 22 Step 100/351 lr 0.001000 loss 0.0053 (0.0097) acc@1 1.0000 (0.9985) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:52:06] __main__ INFO: \u001b[0mEpoch 22 Step 200/351 lr 0.001000 loss 0.0404 (0.0094) acc@1 0.9922 (0.9985) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:54:00] __main__ INFO: \u001b[0mEpoch 22 Step 300/351 lr 0.001000 loss 0.0086 (0.0097) acc@1 1.0000 (0.9983) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:54:59] __main__ INFO: \u001b[0mEpoch 22 Step 351/351 lr 0.001000 loss 0.0047 (0.0095) acc@1 1.0000 (0.9984) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:54:59] __main__ INFO: \u001b[0mElapsed 401.82\n",
      "\u001b[32m[2020-06-23 23:54:59] __main__ INFO: \u001b[0mVal 22\n",
      "\u001b[32m[2020-06-23 23:55:12] __main__ INFO: \u001b[0mEpoch 22 loss 0.2860 acc@1 0.9256 acc@5 0.9972\n",
      "\u001b[32m[2020-06-23 23:55:12] __main__ INFO: \u001b[0mElapsed 13.50\n",
      "\u001b[32m[2020-06-23 23:55:12] __main__ INFO: \u001b[0mTrain 23 7722\n",
      "\u001b[32m[2020-06-23 23:57:07] __main__ INFO: \u001b[0mEpoch 23 Step 100/351 lr 0.001000 loss 0.0067 (0.0091) acc@1 1.0000 (0.9981) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:59:01] __main__ INFO: \u001b[0mEpoch 23 Step 200/351 lr 0.001000 loss 0.0058 (0.0093) acc@1 1.0000 (0.9982) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:00:56] __main__ INFO: \u001b[0mEpoch 23 Step 300/351 lr 0.001000 loss 0.0050 (0.0092) acc@1 1.0000 (0.9982) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:01:54] __main__ INFO: \u001b[0mEpoch 23 Step 351/351 lr 0.001000 loss 0.0046 (0.0092) acc@1 1.0000 (0.9982) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:01:54] __main__ INFO: \u001b[0mElapsed 401.77\n",
      "\u001b[32m[2020-06-24 00:01:54] __main__ INFO: \u001b[0mVal 23\n",
      "\u001b[32m[2020-06-24 00:02:08] __main__ INFO: \u001b[0mEpoch 23 loss 0.2863 acc@1 0.9262 acc@5 0.9974\n",
      "\u001b[32m[2020-06-24 00:02:08] __main__ INFO: \u001b[0mElapsed 13.49\n",
      "\u001b[32m[2020-06-24 00:02:08] __main__ INFO: \u001b[0mTrain 24 8073\n",
      "\u001b[32m[2020-06-24 00:04:02] __main__ INFO: \u001b[0mEpoch 24 Step 100/351 lr 0.001000 loss 0.0101 (0.0100) acc@1 1.0000 (0.9980) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:05:56] __main__ INFO: \u001b[0mEpoch 24 Step 200/351 lr 0.001000 loss 0.0102 (0.0105) acc@1 1.0000 (0.9981) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:07:51] __main__ INFO: \u001b[0mEpoch 24 Step 300/351 lr 0.001000 loss 0.0068 (0.0103) acc@1 1.0000 (0.9979) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:08:49] __main__ INFO: \u001b[0mEpoch 24 Step 351/351 lr 0.001000 loss 0.0038 (0.0104) acc@1 1.0000 (0.9979) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:08:49] __main__ INFO: \u001b[0mElapsed 401.62\n",
      "\u001b[32m[2020-06-24 00:08:49] __main__ INFO: \u001b[0mVal 24\n",
      "\u001b[32m[2020-06-24 00:09:03] __main__ INFO: \u001b[0mEpoch 24 loss 0.3071 acc@1 0.9232 acc@5 0.9968\n",
      "\u001b[32m[2020-06-24 00:09:03] __main__ INFO: \u001b[0mElapsed 13.49\n",
      "\u001b[32m[2020-06-24 00:09:03] __main__ INFO: \u001b[0mTrain 25 8424\n",
      "\u001b[32m[2020-06-24 00:10:57] __main__ INFO: \u001b[0mEpoch 25 Step 100/351 lr 0.001000 loss 0.0060 (0.0101) acc@1 1.0000 (0.9980) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:12:51] __main__ INFO: \u001b[0mEpoch 25 Step 200/351 lr 0.001000 loss 0.0129 (0.0098) acc@1 1.0000 (0.9980) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:14:46] __main__ INFO: \u001b[0mEpoch 25 Step 300/351 lr 0.001000 loss 0.0461 (0.0101) acc@1 0.9922 (0.9980) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:15:44] __main__ INFO: \u001b[0mEpoch 25 Step 351/351 lr 0.001000 loss 0.0051 (0.0100) acc@1 1.0000 (0.9980) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:15:44] __main__ INFO: \u001b[0mElapsed 401.46\n",
      "\u001b[32m[2020-06-24 00:15:44] __main__ INFO: \u001b[0mVal 25\n",
      "\u001b[32m[2020-06-24 00:15:58] __main__ INFO: \u001b[0mEpoch 25 loss 0.2934 acc@1 0.9286 acc@5 0.9964\n",
      "\u001b[32m[2020-06-24 00:15:58] __main__ INFO: \u001b[0mElapsed 13.50\n",
      "\u001b[32m[2020-06-24 00:15:58] __main__ INFO: \u001b[0mTrain 26 8775\n",
      "\u001b[32m[2020-06-24 00:17:52] __main__ INFO: \u001b[0mEpoch 26 Step 100/351 lr 0.001000 loss 0.0069 (0.0084) acc@1 1.0000 (0.9990) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:19:46] __main__ INFO: \u001b[0mEpoch 26 Step 200/351 lr 0.001000 loss 0.0053 (0.0079) acc@1 1.0000 (0.9988) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:21:41] __main__ INFO: \u001b[0mEpoch 26 Step 300/351 lr 0.001000 loss 0.0044 (0.0081) acc@1 1.0000 (0.9988) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:22:39] __main__ INFO: \u001b[0mEpoch 26 Step 351/351 lr 0.001000 loss 0.0043 (0.0078) acc@1 1.0000 (0.9988) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:22:39] __main__ INFO: \u001b[0mElapsed 401.16\n",
      "\u001b[32m[2020-06-24 00:22:39] __main__ INFO: \u001b[0mVal 26\n",
      "\u001b[32m[2020-06-24 00:22:52] __main__ INFO: \u001b[0mEpoch 26 loss 0.2789 acc@1 0.9272 acc@5 0.9972\n",
      "\u001b[32m[2020-06-24 00:22:52] __main__ INFO: \u001b[0mElapsed 13.48\n",
      "\u001b[32m[2020-06-24 00:22:52] __main__ INFO: \u001b[0mTrain 27 9126\n",
      "\u001b[32m[2020-06-24 00:24:47] __main__ INFO: \u001b[0mEpoch 27 Step 100/351 lr 0.001000 loss 0.0102 (0.0075) acc@1 0.9922 (0.9988) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:26:41] __main__ INFO: \u001b[0mEpoch 27 Step 200/351 lr 0.001000 loss 0.0060 (0.0072) acc@1 1.0000 (0.9989) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:28:35] __main__ INFO: \u001b[0mEpoch 27 Step 300/351 lr 0.001000 loss 0.0283 (0.0074) acc@1 0.9922 (0.9988) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:29:33] __main__ INFO: \u001b[0mEpoch 27 Step 351/351 lr 0.001000 loss 0.0208 (0.0074) acc@1 0.9844 (0.9988) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:29:33] __main__ INFO: \u001b[0mElapsed 401.09\n",
      "\u001b[32m[2020-06-24 00:29:33] __main__ INFO: \u001b[0mVal 27\n",
      "\u001b[32m[2020-06-24 00:29:47] __main__ INFO: \u001b[0mEpoch 27 loss 0.2946 acc@1 0.9270 acc@5 0.9968\n",
      "\u001b[32m[2020-06-24 00:29:47] __main__ INFO: \u001b[0mElapsed 13.47\n",
      "\u001b[32m[2020-06-24 00:29:47] __main__ INFO: \u001b[0mTrain 28 9477\n",
      "\u001b[32m[2020-06-24 00:31:41] __main__ INFO: \u001b[0mEpoch 28 Step 100/351 lr 0.001000 loss 0.0064 (0.0070) acc@1 1.0000 (0.9990) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:33:36] __main__ INFO: \u001b[0mEpoch 28 Step 200/351 lr 0.001000 loss 0.0050 (0.0077) acc@1 1.0000 (0.9988) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:35:30] __main__ INFO: \u001b[0mEpoch 28 Step 300/351 lr 0.001000 loss 0.0029 (0.0071) acc@1 1.0000 (0.9991) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:36:28] __main__ INFO: \u001b[0mEpoch 28 Step 351/351 lr 0.001000 loss 0.0052 (0.0072) acc@1 1.0000 (0.9990) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:36:28] __main__ INFO: \u001b[0mElapsed 401.25\n",
      "\u001b[32m[2020-06-24 00:36:28] __main__ INFO: \u001b[0mVal 28\n",
      "\u001b[32m[2020-06-24 00:36:42] __main__ INFO: \u001b[0mEpoch 28 loss 0.2939 acc@1 0.9270 acc@5 0.9964\n",
      "\u001b[32m[2020-06-24 00:36:42] __main__ INFO: \u001b[0mElapsed 13.49\n",
      "\u001b[32m[2020-06-24 00:36:42] __main__ INFO: \u001b[0mTrain 29 9828\n",
      "\u001b[32m[2020-06-24 00:38:36] __main__ INFO: \u001b[0mEpoch 29 Step 100/351 lr 0.001000 loss 0.0036 (0.0067) acc@1 1.0000 (0.9990) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:40:30] __main__ INFO: \u001b[0mEpoch 29 Step 200/351 lr 0.001000 loss 0.0110 (0.0073) acc@1 0.9922 (0.9987) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:42:25] __main__ INFO: \u001b[0mEpoch 29 Step 300/351 lr 0.001000 loss 0.0071 (0.0070) acc@1 1.0000 (0.9989) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:43:23] __main__ INFO: \u001b[0mEpoch 29 Step 351/351 lr 0.001000 loss 0.0047 (0.0071) acc@1 1.0000 (0.9988) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:43:23] __main__ INFO: \u001b[0mElapsed 401.24\n",
      "\u001b[32m[2020-06-24 00:43:23] __main__ INFO: \u001b[0mVal 29\n",
      "\u001b[32m[2020-06-24 00:43:36] __main__ INFO: \u001b[0mEpoch 29 loss 0.2891 acc@1 0.9296 acc@5 0.9968\n",
      "\u001b[32m[2020-06-24 00:43:36] __main__ INFO: \u001b[0mElapsed 13.48\n",
      "\u001b[32m[2020-06-24 00:43:36] __main__ INFO: \u001b[0mTrain 30 10179\n",
      "\u001b[32m[2020-06-24 00:45:31] __main__ INFO: \u001b[0mEpoch 30 Step 100/351 lr 0.001000 loss 0.0112 (0.0066) acc@1 0.9922 (0.9989) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:47:25] __main__ INFO: \u001b[0mEpoch 30 Step 200/351 lr 0.001000 loss 0.0043 (0.0063) acc@1 1.0000 (0.9990) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:49:19] __main__ INFO: \u001b[0mEpoch 30 Step 300/351 lr 0.001000 loss 0.0041 (0.0060) acc@1 1.0000 (0.9991) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:50:17] __main__ INFO: \u001b[0mEpoch 30 Step 351/351 lr 0.001000 loss 0.0026 (0.0059) acc@1 1.0000 (0.9992) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:50:17] __main__ INFO: \u001b[0mElapsed 401.18\n",
      "\u001b[32m[2020-06-24 00:50:17] __main__ INFO: \u001b[0mVal 30\n",
      "\u001b[32m[2020-06-24 00:50:31] __main__ INFO: \u001b[0mEpoch 30 loss 0.2761 acc@1 0.9312 acc@5 0.9974\n",
      "\u001b[32m[2020-06-24 00:50:31] __main__ INFO: \u001b[0mElapsed 13.50\n",
      "\u001b[32m[2020-06-24 00:50:31] __main__ INFO: \u001b[0mTrain 31 10530\n",
      "\u001b[32m[2020-06-24 00:52:25] __main__ INFO: \u001b[0mEpoch 31 Step 100/351 lr 0.001000 loss 0.0038 (0.0053) acc@1 1.0000 (0.9994) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:54:20] __main__ INFO: \u001b[0mEpoch 31 Step 200/351 lr 0.001000 loss 0.0038 (0.0059) acc@1 1.0000 (0.9991) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:56:14] __main__ INFO: \u001b[0mEpoch 31 Step 300/351 lr 0.001000 loss 0.0038 (0.0058) acc@1 1.0000 (0.9992) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:57:12] __main__ INFO: \u001b[0mEpoch 31 Step 351/351 lr 0.001000 loss 0.0029 (0.0060) acc@1 1.0000 (0.9992) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:57:12] __main__ INFO: \u001b[0mElapsed 401.16\n",
      "\u001b[32m[2020-06-24 00:57:12] __main__ INFO: \u001b[0mVal 31\n",
      "\u001b[32m[2020-06-24 00:57:26] __main__ INFO: \u001b[0mEpoch 31 loss 0.2894 acc@1 0.9292 acc@5 0.9968\n",
      "\u001b[32m[2020-06-24 00:57:26] __main__ INFO: \u001b[0mElapsed 13.51\n",
      "\u001b[32m[2020-06-24 00:57:26] __main__ INFO: \u001b[0mTrain 32 10881\n",
      "\u001b[32m[2020-06-24 00:59:20] __main__ INFO: \u001b[0mEpoch 32 Step 100/351 lr 0.001000 loss 0.0067 (0.0069) acc@1 1.0000 (0.9988) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 01:01:14] __main__ INFO: \u001b[0mEpoch 32 Step 200/351 lr 0.001000 loss 0.0049 (0.0067) acc@1 1.0000 (0.9990) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 01:03:09] __main__ INFO: \u001b[0mEpoch 32 Step 300/351 lr 0.001000 loss 0.0052 (0.0068) acc@1 1.0000 (0.9988) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 01:04:07] __main__ INFO: \u001b[0mEpoch 32 Step 351/351 lr 0.001000 loss 0.0297 (0.0068) acc@1 0.9844 (0.9988) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 01:04:07] __main__ INFO: \u001b[0mElapsed 401.17\n",
      "\u001b[32m[2020-06-24 01:04:07] __main__ INFO: \u001b[0mVal 32\n",
      "\u001b[32m[2020-06-24 01:04:20] __main__ INFO: \u001b[0mEpoch 32 loss 0.2817 acc@1 0.9296 acc@5 0.9976\n",
      "\u001b[32m[2020-06-24 01:04:20] __main__ INFO: \u001b[0mElapsed 13.47\n",
      "\u001b[32m[2020-06-24 01:04:20] __main__ INFO: \u001b[0mTrain 33 11232\n",
      "\u001b[32m[2020-06-24 01:06:15] __main__ INFO: \u001b[0mEpoch 33 Step 100/351 lr 0.001000 loss 0.0046 (0.0061) acc@1 1.0000 (0.9992) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 01:08:09] __main__ INFO: \u001b[0mEpoch 33 Step 200/351 lr 0.001000 loss 0.0032 (0.0059) acc@1 1.0000 (0.9991) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 01:10:03] __main__ INFO: \u001b[0mEpoch 33 Step 300/351 lr 0.001000 loss 0.0059 (0.0057) acc@1 1.0000 (0.9993) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 01:11:01] __main__ INFO: \u001b[0mEpoch 33 Step 351/351 lr 0.001000 loss 0.0022 (0.0056) acc@1 1.0000 (0.9993) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 01:11:01] __main__ INFO: \u001b[0mElapsed 401.06\n",
      "\u001b[32m[2020-06-24 01:11:01] __main__ INFO: \u001b[0mVal 33\n",
      "\u001b[32m[2020-06-24 01:11:15] __main__ INFO: \u001b[0mEpoch 33 loss 0.2903 acc@1 0.9288 acc@5 0.9976\n",
      "\u001b[32m[2020-06-24 01:11:15] __main__ INFO: \u001b[0mElapsed 13.49\n",
      "\u001b[32m[2020-06-24 01:11:15] __main__ INFO: \u001b[0mTrain 34 11583\n",
      "\u001b[32m[2020-06-24 01:13:09] __main__ INFO: \u001b[0mEpoch 34 Step 100/351 lr 0.001000 loss 0.0054 (0.0052) acc@1 1.0000 (0.9993) acc@5 1.0000 (1.0000)\n"
     ]
    }
   ],
   "source": [
    "# Resume training with the un-augmented data\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "#!python train.py --config configs/cifar/resnet.yaml \\\n",
    "!python train.py --config /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_1_20/exp00/config.yaml \\\n",
    "    train.checkpoint /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_1_20/exp00/checkpoint_00400.pth \\\n",
    "    dataset.name CIFAR10 \\\n",
    "    train.base_lr .001 \\\n",
    "    train.output_dir /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_1_20/exp00_resume400_50 \\\n",
    "    scheduler.epochs 50\n",
    "\n",
    "#### Set LEARNING RATE based on ending LR\n",
    "#    train.resume True \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-25 22:12:09] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_20/exp00_resume400_50/checkpoint_00050.pth\n",
      "Files already downloaded and verified\n",
      "100%|███████████████████████████████████████████| 79/79 [00:27<00:00,  2.89it/s]\n",
      "\u001b[32m[2020-06-25 22:12:37] __main__ INFO: \u001b[0mElapsed 27.31\n",
      "\u001b[32m[2020-06-25 22:12:37] __main__ INFO: \u001b[0mLoss 0.2885 Accuracy 0.9315\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/wrn.yaml \\\n",
    "   model.resnet.depth 32 \\\n",
    "   test.batch_size 128 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_1_20/exp00_resume400_50/checkpoint_00050.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_1_20/exp00_resume400_50/test_results_0050_cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-25 22:14:31] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_20/exp00_resume400_50/checkpoint_00050.pth\n",
      "CIFAR 10.1\n",
      "100%|███████████████████████████████████████████| 16/16 [00:05<00:00,  2.77it/s]\n",
      "\u001b[32m[2020-06-25 22:14:38] __main__ INFO: \u001b[0mElapsed 5.77\n",
      "\u001b[32m[2020-06-25 22:14:38] __main__ INFO: \u001b[0mLoss 0.6531 Accuracy 0.8470\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10.1 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/wrn.yaml \\\n",
    "   model.resnet.depth 32 \\\n",
    "   test.batch_size 128 \\\n",
    "   dataset.name CIFAR101 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_1_20/exp00_resume400_50/checkpoint_00050.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_1_20/exp00_resume400_50/test_results_0050_cifar101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-25 22:16:42] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_20/exp00/checkpoint_00400.pth\n",
      "Files already downloaded and verified\n",
      "100%|███████████████████████████████████████████| 79/79 [00:27<00:00,  2.90it/s]\n",
      "\u001b[32m[2020-06-25 22:17:11] __main__ INFO: \u001b[0mElapsed 27.29\n",
      "\u001b[32m[2020-06-25 22:17:11] __main__ INFO: \u001b[0mLoss 0.6982 Accuracy 0.8337\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/wrn.yaml \\\n",
    "   model.resnet.depth 32 \\\n",
    "   test.batch_size 128 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_1_20/exp00/checkpoint_00400.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_1_20/exp00/test_results_0400_cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-25 22:18:26] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_20/exp00/checkpoint_00400.pth\n",
      "CIFAR 10.1\n",
      "100%|███████████████████████████████████████████| 16/16 [00:05<00:00,  2.78it/s]\n",
      "\u001b[32m[2020-06-25 22:18:33] __main__ INFO: \u001b[0mElapsed 5.75\n",
      "\u001b[32m[2020-06-25 22:18:33] __main__ INFO: \u001b[0mLoss 1.3437 Accuracy 0.6940\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10.1 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "!python evaluate.py --config configs/cifar/wrn.yaml \\\n",
    "   model.resnet.depth 32 \\\n",
    "   test.batch_size 128 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_1_20/exp00/checkpoint_00400.pth \\\n",
    "   dataset.name CIFAR101 \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_1_20/exp00/test_results_0400_cifar101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Testset</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Original_Accuracy</th>\n",
       "      <th>Original_CI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wrn_28_10_ra_2_20</td>\n",
       "      <td>400</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>1.6982</td>\n",
       "      <td>0.8337</td>\n",
       "      <td>95.9</td>\n",
       "      <td>(95.5, 96.3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wrn_28_10_ra_2_20</td>\n",
       "      <td>400</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>1.3437</td>\n",
       "      <td>0.694</td>\n",
       "      <td>89.7</td>\n",
       "      <td>(88.3, 91.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wrn_28_10_ra_2_20_refined400</td>\n",
       "      <td>50</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>0.6531</td>\n",
       "      <td>0.847</td>\n",
       "      <td>89.7</td>\n",
       "      <td>(88.3, 91.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wrn_28_10_ra_2_20_refined400</td>\n",
       "      <td>50</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.2885</td>\n",
       "      <td>0.9315</td>\n",
       "      <td>95.9</td>\n",
       "      <td>(95.5, 96.3)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model Epoch    Testset    Loss Accuracy  \\\n",
       "0             wrn_28_10_ra_2_20   400    cifar10  1.6982   0.8337   \n",
       "1             wrn_28_10_ra_2_20   400  cifar10.1  1.3437    0.694   \n",
       "2  wrn_28_10_ra_2_20_refined400    50  cifar10.1  0.6531    0.847   \n",
       "3  wrn_28_10_ra_2_20_refined400    50    cifar10  0.2885   0.9315   \n",
       "\n",
       "   Original_Accuracy   Original_CI  \n",
       "0               95.9  (95.5, 96.3)  \n",
       "1               89.7  (88.3, 91.0)  \n",
       "2               89.7  (88.3, 91.0)  \n",
       "3               95.9  (95.5, 96.3)  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "a = pd.Series(['wrn_28_10_ra_1_20', 400, 'cifar10', 1.6982, 0.8337]) \n",
    "c = pd.Series(['wrn_28_10_ra_1_20', 400, 'cifar10.1', 1.3437, 0.6940]) \n",
    "\n",
    "\n",
    "e = pd.Series(['wrn_28_10_ra_1_20_refined400', 50, 'cifar10.1', 0.6531, 0.8470]) \n",
    "f = pd.Series(['wrn_28_10_ra_1_20_refined400', 50, 'cifar10', 0.2885,0.9315]) \n",
    "               \n",
    "df_results = pd.concat([a,c,e,f], axis=1).T\n",
    "df_results.columns = ['Model', 'Epoch', 'Testset', 'Loss', 'Accuracy']\n",
    "\n",
    "df_results['Original_Accuracy'] = df_results.apply((lambda row: 95.9 if row[2] == 'cifar10' else 89.7), axis=1)\n",
    "df_results['Original_CI'] = df_results.apply((lambda row: (95.5, 96.3) if row[2] == 'cifar10' else (88.3, 91.0)), axis=1)\n",
    "\n",
    "df_results.to_csv('/home/ec2-user/SageMaker/experiments/wrn_28_10_ra_1_20/results.csv')\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['preds', 'probs', 'labels', 'loss', 'acc']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ -7.153804  ,  -0.1832159 ,  -0.69570637, ...,  -0.50926757,\n",
       "         -5.526208  , -12.987257  ],\n",
       "       [  2.862379  ,   7.963458  ,  -6.603018  , ...,  -4.740323  ,\n",
       "         25.90399   ,  -0.52988565],\n",
       "       [  4.25749   ,   8.408992  ,  -4.3299227 , ...,  -2.3715498 ,\n",
       "         13.468082  ,   4.5792727 ],\n",
       "       ...,\n",
       "       [ -4.7270765 ,  -1.2400844 ,   1.3852903 , ...,  -0.51062894,\n",
       "         -3.399443  ,  -2.4969094 ],\n",
       "       [ -2.7640457 ,  14.635863  ,   6.7449965 , ...,  -1.3011913 ,\n",
       "         -3.036379  ,  -7.061736  ],\n",
       "       [ -2.6933427 ,   1.8961854 ,  -3.6396854 , ...,  18.63456   ,\n",
       "         -2.7524152 ,  -3.2204888 ]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Peak inside the output file for predictions\n",
    "import numpy as np\n",
    "output = '/home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_20/exp00/test_results_0160/predictions.npz'\n",
    "npzfile = np.load(output)\n",
    "print(npzfile.files)\n",
    "npzfile['preds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the model checkpoints, configs, and results to S3 \n",
    "bucket='sagemaker-may29'\n",
    "prefix = 'sagemaker/results/original-models/wrn_28_10_ra_1_20'\n",
    "path = '/home/ec2-user/SageMaker/experiments/wrn_28_10_ra_1_20'\n",
    "\n",
    "s3_resource = boto3.resource(\"s3\", region_name=\"us-east-2\")\n",
    "\n",
    "def uploadDirectory(local_path,bucket_name,s3_prefix):\n",
    "\n",
    "    my_bucket = s3_resource.Bucket(bucket_name)\n",
    "    \n",
    "    for path, subdirs, files in os.walk(local_path):\n",
    "        path = path.replace(\"\\\\\",\"/\")\n",
    "        directory_name = path.replace(local_path,\"\")\n",
    "        for file in files:\n",
    "            #print(\"Local File:\", os.path.join(path, file))\n",
    "            #print(\"      Dest:\", s3_prefix+directory_name+'/'+file)\n",
    "            my_bucket.upload_file(os.path.join(path, file), s3_prefix+directory_name+'/'+file)\n",
    "    \n",
    "uploadDirectory(path,bucket,prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
