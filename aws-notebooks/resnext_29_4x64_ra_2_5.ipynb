{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNext 29 4x64\n",
    "\n",
    " - Training Dataset:  RandAugment, N=2, M=5\n",
    " - Sagemaker Notebook must be of type, conda_pytorch_p36\n",
    " \n",
    "#### Install Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import numpy \n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESNEXT_29_4x64D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without Containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 1)) (1.18.1)\n",
      "Requirement already satisfied: torch>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 2)) (1.4.0)\n",
      "Requirement already satisfied: torchvision in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (0.5.0)\n",
      "Collecting fvcore\n",
      "  Downloading fvcore-0.1.1.post20200623.tar.gz (32 kB)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 5)) (4.44.1)\n",
      "Collecting yacs\n",
      "  Downloading yacs-0.1.7-py3-none-any.whl (14 kB)\n",
      "Collecting apex\n",
      "  Cloning https://github.com/NVIDIA/apex.git to /tmp/pip-install-nyqnqino/apex\n",
      "  Running command git clone -q https://github.com/NVIDIA/apex.git /tmp/pip-install-nyqnqino/apex\n",
      "  Running command git submodule update --init --recursive -q\n",
      "Collecting termcolor\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting thop<0.0.31.post2004070130\n",
      "  Downloading thop-0.0.31.post2001170342-py3-none-any.whl (7.5 kB)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (1.14.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (7.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (5.3.1)\n",
      "Collecting portalocker\n",
      "  Downloading portalocker-1.7.0-py2.py3-none-any.whl (14 kB)\n",
      "Collecting tabulate\n",
      "  Downloading tabulate-0.8.7-py3-none-any.whl (24 kB)\n",
      "Building wheels for collected packages: fvcore, apex, termcolor\n",
      "  Building wheel for fvcore (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fvcore: filename=fvcore-0.1.1.post20200623-py3-none-any.whl size=41179 sha256=29ea8fe5e36b186ccd4fdcae487cfd71708084adf0e9e08a3df0ed0dd745177f\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/27/fc/86/b29aea030f5468db673ec86033a9579cc50e02979aa0c78ebe\n",
      "  Building wheel for apex (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for apex: filename=apex-0.1-py3-none-any.whl size=192130 sha256=61d4ae3a9db07dda87ad2c3ee8d4ce64ba366c7625874f4121dde6344515ad8b\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-8yyjymph/wheels/2a/45/23/6b4f2d6323a65ee0022d22a96d7bf580138e689f17cc48235c\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=3d31b3d3eab9309bcd258d4ea87ae67d714a3f50ec4a667a07b0c21399e75e17\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/93/2a/eb/e58dbcbc963549ee4f065ff80a59f274cc7210b6eab962acdc\n",
      "Successfully built fvcore apex termcolor\n",
      "Installing collected packages: yacs, portalocker, termcolor, tabulate, fvcore, apex, thop\n",
      "Successfully installed apex-0.1 fvcore-0.1.1.post20200623 portalocker-1.7.0 tabulate-0.8.7 termcolor-1.1.0 thop-0.0.31.post2001170342 yacs-0.1.7\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Sagemaker Notebook must be of type, conda_pytorch_p36\n",
    "\n",
    "!pip install -r '/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorboard\n",
      "  Downloading tensorboard-2.2.2-py3-none-any.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 3.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (2.23.0)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.2.2-py3-none-any.whl (88 kB)\n",
      "\u001b[K     |████████████████████████████████| 88 kB 10.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779 kB)\n",
      "\u001b[K     |████████████████████████████████| 779 kB 27.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio>=1.24.3\n",
      "  Downloading grpcio-1.30.0-cp36-cp36m-manylinux2010_x86_64.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 50.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.12.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.18.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.0.1)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (0.34.2)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.14.0)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.18.0-py2.py3-none-any.whl (90 kB)\n",
      "\u001b[K     |████████████████████████████████| 90 kB 16.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (3.12.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (46.1.3.post20200330)\n",
      "Collecting absl-py>=0.4\n",
      "  Downloading absl-py-0.9.0.tar.gz (104 kB)\n",
      "\u001b[K     |████████████████████████████████| 104 kB 52.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (2020.4.5.2)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (1.25.8)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard) (1.5.0)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 49.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.1.1-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (3.4.2)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "\u001b[K     |████████████████████████████████| 147 kB 48.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard) (2.2.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard) (0.4.8)\n",
      "Building wheels for collected packages: absl-py\n",
      "  Building wheel for absl-py (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for absl-py: filename=absl_py-0.9.0-py3-none-any.whl size=121931 sha256=d0619e52173cff294fd316f8fbfe283d28148cdf9bc83922e2ce897b922b148a\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/c3/af/84/3962a6af7b4ab336e951b7877dcfb758cf94548bb1771e0679\n",
      "Successfully built absl-py\n",
      "Installing collected packages: oauthlib, requests-oauthlib, pyasn1-modules, cachetools, google-auth, google-auth-oauthlib, markdown, tensorboard-plugin-wit, grpcio, absl-py, tensorboard\n",
      "Successfully installed absl-py-0.9.0 cachetools-4.1.1 google-auth-1.18.0 google-auth-oauthlib-0.4.1 grpcio-1.30.0 markdown-3.2.2 oauthlib-3.1.0 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 tensorboard-2.2.2 tensorboard-plugin-wit-1.7.0\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Need to add this to requirements.txt\n",
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-28 20:42:20] __main__ INFO: \u001b[0mdevice: cuda\n",
      "cudnn:\n",
      "  benchmark: True\n",
      "  deterministic: False\n",
      "dataset:\n",
      "  name: CIFAR10_RA_2_5\n",
      "  dataset_dir: ''\n",
      "  image_size: 32\n",
      "  n_channels: 3\n",
      "  n_classes: 10\n",
      "model:\n",
      "  type: cifar\n",
      "  name: resnext\n",
      "  init_mode: kaiming_fan_out\n",
      "  vgg:\n",
      "    n_channels: [64, 128, 256, 512, 512]\n",
      "    n_layers: [2, 2, 3, 3, 3]\n",
      "    use_bn: True\n",
      "  resnet:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "  resnet_preact:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "  wrn:\n",
      "    depth: 28\n",
      "    initial_channels: 16\n",
      "    widening_factor: 10\n",
      "    drop_rate: 0.0\n",
      "  densenet:\n",
      "    depth: 100\n",
      "    n_blocks: [6, 12, 24, 16]\n",
      "    block_type: bottleneck\n",
      "    growth_rate: 12\n",
      "    drop_rate: 0.0\n",
      "    compression_rate: 0.5\n",
      "  pyramidnet:\n",
      "    depth: 272\n",
      "    n_blocks: [3, 24, 36, 3]\n",
      "    initial_channels: 16\n",
      "    block_type: bottleneck\n",
      "    alpha: 200\n",
      "  resnext:\n",
      "    depth: 29\n",
      "    n_blocks: [3, 4, 6, 3]\n",
      "    initial_channels: 64\n",
      "    cardinality: 4\n",
      "    base_channels: 64\n",
      "  shake_shake:\n",
      "    depth: 26\n",
      "    initial_channels: 96\n",
      "    shake_forward: True\n",
      "    shake_backward: True\n",
      "    shake_image: True\n",
      "  se_resnet_preact:\n",
      "    depth: 110\n",
      "    initial_channels: 16\n",
      "    se_reduction: 16\n",
      "    block_type: basic\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "train:\n",
      "  checkpoint: ''\n",
      "  resume: False\n",
      "  precision: O0\n",
      "  batch_size: 128\n",
      "  subdivision: 1\n",
      "  optimizer: sgd\n",
      "  base_lr: 0.1\n",
      "  momentum: 0.9\n",
      "  nesterov: True\n",
      "  weight_decay: 0.0005\n",
      "  no_weight_decay_on_bn: False\n",
      "  gradient_clip: 0\n",
      "  start_epoch: 0\n",
      "  seed: 0\n",
      "  val_first: True\n",
      "  val_period: 1\n",
      "  val_ratio: 0.1\n",
      "  use_test_as_val: False\n",
      "  output_dir: /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_5/exp00\n",
      "  log_period: 100\n",
      "  checkpoint_period: 100\n",
      "  use_tensorboard: True\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: True\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "  distributed: False\n",
      "  dist:\n",
      "    backend: nccl\n",
      "    init_method: env://\n",
      "    world_size: -1\n",
      "    node_rank: -1\n",
      "    local_rank: 0\n",
      "    use_sync_bn: False\n",
      "tensorboard:\n",
      "  train_images: False\n",
      "  val_images: False\n",
      "  model_params: False\n",
      "optim:\n",
      "  adam:\n",
      "    betas: (0.9, 0.999)\n",
      "  lars:\n",
      "    eps: 1e-09\n",
      "    threshold: 0.01\n",
      "  adabound:\n",
      "    betas: (0.9, 0.999)\n",
      "    final_lr: 0.1\n",
      "    gamma: 0.001\n",
      "scheduler:\n",
      "  epochs: 400\n",
      "  warmup:\n",
      "    type: none\n",
      "    epochs: 0\n",
      "    start_factor: 0.001\n",
      "    exponent: 4\n",
      "  type: multistep\n",
      "  milestones: [150, 225]\n",
      "  lr_decay: 0.1\n",
      "  lr_min_factor: 0.001\n",
      "  T0: 10\n",
      "  T_mul: 1.0\n",
      "validation:\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: False\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "augmentation:\n",
      "  use_random_crop: True\n",
      "  use_random_horizontal_flip: True\n",
      "  use_cutout: False\n",
      "  use_random_erasing: False\n",
      "  use_dual_cutout: False\n",
      "  use_mixup: False\n",
      "  use_ricap: False\n",
      "  use_cutmix: False\n",
      "  use_label_smoothing: False\n",
      "  random_crop:\n",
      "    padding: 4\n",
      "    fill: 0\n",
      "    padding_mode: constant\n",
      "  random_horizontal_flip:\n",
      "    prob: 0.5\n",
      "  cutout:\n",
      "    prob: 1.0\n",
      "    mask_size: 16\n",
      "    cut_inside: False\n",
      "    mask_color: 0\n",
      "    dual_cutout_alpha: 0.1\n",
      "  random_erasing:\n",
      "    prob: 0.5\n",
      "    area_ratio_range: [0.02, 0.4]\n",
      "    min_aspect_ratio: 0.3\n",
      "    max_attempt: 20\n",
      "  mixup:\n",
      "    alpha: 1.0\n",
      "  ricap:\n",
      "    beta: 0.3\n",
      "  cutmix:\n",
      "    alpha: 1.0\n",
      "  label_smoothing:\n",
      "    epsilon: 0.1\n",
      "tta:\n",
      "  use_resize: False\n",
      "  use_center_crop: False\n",
      "  resize: 256\n",
      "test:\n",
      "  checkpoint: ''\n",
      "  output_dir: ''\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    pin_memory: False\n",
      "\u001b[32m[2020-06-28 20:42:20] __main__ INFO: \u001b[0menv_info:\n",
      "  pytorch_version: 1.4.0\n",
      "  cuda_version: 10.1\n",
      "  cudnn_version: 7603\n",
      "  num_gpus: 1\n",
      "  gpu_name: Tesla K80\n",
      "  gpu_capability: 3.7\n",
      "(50000, 32, 32, 3)\n",
      "\u001b[32m[2020-06-28 20:42:27] __main__ INFO: \u001b[0mMACs  : 2.75G\n",
      "\u001b[32m[2020-06-28 20:42:27] __main__ INFO: \u001b[0m#params: 17.56M\n",
      "Selected optimization level O0:  Pure FP32 training.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\",)\n",
      "\u001b[32m[2020-06-28 20:42:27] __main__ INFO: \u001b[0mVal 0\n",
      "\u001b[32m[2020-06-28 20:42:53] __main__ INFO: \u001b[0mEpoch 0 loss 4.4518 acc@1 0.1116 acc@5 0.5028\n",
      "\u001b[32m[2020-06-28 20:42:53] __main__ INFO: \u001b[0mElapsed 25.88\n",
      "\u001b[32m[2020-06-28 20:42:53] __main__ INFO: \u001b[0mTrain 1 0\n",
      "\u001b[32m[2020-06-28 20:45:23] __main__ INFO: \u001b[0mEpoch 1 Step 100/351 lr 0.100000 loss 3.3691 (8.6738) acc@1 0.0859 (0.1062) acc@5 0.4297 (0.5128)\n",
      "\u001b[32m[2020-06-28 20:47:45] __main__ INFO: \u001b[0mEpoch 1 Step 200/351 lr 0.100000 loss 2.5073 (5.6763) acc@1 0.1172 (0.1095) acc@5 0.4766 (0.5173)\n",
      "\u001b[32m[2020-06-28 20:50:08] __main__ INFO: \u001b[0mEpoch 1 Step 300/351 lr 0.100000 loss 2.4049 (4.6270) acc@1 0.1797 (0.1108) acc@5 0.5312 (0.5220)\n",
      "\u001b[32m[2020-06-28 20:51:21] __main__ INFO: \u001b[0mEpoch 1 Step 351/351 lr 0.100000 loss 2.4138 (4.3160) acc@1 0.0703 (0.1119) acc@5 0.5156 (0.5239)\n",
      "\u001b[32m[2020-06-28 20:51:21] __main__ INFO: \u001b[0mElapsed 507.82\n",
      "\u001b[32m[2020-06-28 20:51:21] __main__ INFO: \u001b[0mVal 1\n",
      "\u001b[32m[2020-06-28 20:51:38] __main__ INFO: \u001b[0mEpoch 1 loss 2.3758 acc@1 0.1168 acc@5 0.5526\n",
      "\u001b[32m[2020-06-28 20:51:38] __main__ INFO: \u001b[0mElapsed 17.77\n",
      "\u001b[32m[2020-06-28 20:51:38] __main__ INFO: \u001b[0mTrain 2 351\n",
      "\u001b[32m[2020-06-28 20:54:01] __main__ INFO: \u001b[0mEpoch 2 Step 100/351 lr 0.100000 loss 2.3630 (2.4220) acc@1 0.0859 (0.1155) acc@5 0.5547 (0.5507)\n",
      "\u001b[32m[2020-06-28 20:56:24] __main__ INFO: \u001b[0mEpoch 2 Step 200/351 lr 0.100000 loss 2.2689 (2.3989) acc@1 0.1719 (0.1195) acc@5 0.5312 (0.5467)\n",
      "\u001b[32m[2020-06-28 20:58:46] __main__ INFO: \u001b[0mEpoch 2 Step 300/351 lr 0.100000 loss 2.3387 (2.3838) acc@1 0.1250 (0.1217) acc@5 0.5469 (0.5462)\n",
      "\u001b[32m[2020-06-28 20:59:59] __main__ INFO: \u001b[0mEpoch 2 Step 351/351 lr 0.100000 loss 2.4615 (2.3766) acc@1 0.1406 (0.1229) acc@5 0.5234 (0.5470)\n",
      "\u001b[32m[2020-06-28 20:59:59] __main__ INFO: \u001b[0mElapsed 500.97\n",
      "\u001b[32m[2020-06-28 20:59:59] __main__ INFO: \u001b[0mVal 2\n",
      "\u001b[32m[2020-06-28 21:00:17] __main__ INFO: \u001b[0mEpoch 2 loss 2.3580 acc@1 0.1240 acc@5 0.5464\n",
      "\u001b[32m[2020-06-28 21:00:17] __main__ INFO: \u001b[0mElapsed 17.76\n",
      "\u001b[32m[2020-06-28 21:00:17] __main__ INFO: \u001b[0mTrain 3 702\n",
      "\u001b[32m[2020-06-28 21:02:40] __main__ INFO: \u001b[0mEpoch 3 Step 100/351 lr 0.100000 loss 2.3362 (2.3294) acc@1 0.1172 (0.1277) acc@5 0.5781 (0.5570)\n",
      "\u001b[32m[2020-06-28 21:05:03] __main__ INFO: \u001b[0mEpoch 3 Step 200/351 lr 0.100000 loss 2.3567 (2.3223) acc@1 0.1094 (0.1263) acc@5 0.5156 (0.5672)\n",
      "\u001b[32m[2020-06-28 21:07:25] __main__ INFO: \u001b[0mEpoch 3 Step 300/351 lr 0.100000 loss 2.2586 (2.3101) acc@1 0.1641 (0.1298) acc@5 0.6094 (0.5763)\n",
      "\u001b[32m[2020-06-28 21:08:38] __main__ INFO: \u001b[0mEpoch 3 Step 351/351 lr 0.100000 loss 2.2601 (2.3012) acc@1 0.1562 (0.1333) acc@5 0.5391 (0.5824)\n",
      "\u001b[32m[2020-06-28 21:08:38] __main__ INFO: \u001b[0mElapsed 501.05\n",
      "\u001b[32m[2020-06-28 21:08:38] __main__ INFO: \u001b[0mVal 3\n",
      "\u001b[32m[2020-06-28 21:08:56] __main__ INFO: \u001b[0mEpoch 3 loss 2.2553 acc@1 0.1406 acc@5 0.6110\n",
      "\u001b[32m[2020-06-28 21:08:56] __main__ INFO: \u001b[0mElapsed 17.74\n",
      "\u001b[32m[2020-06-28 21:08:56] __main__ INFO: \u001b[0mTrain 4 1053\n",
      "\u001b[32m[2020-06-28 21:11:19] __main__ INFO: \u001b[0mEpoch 4 Step 100/351 lr 0.100000 loss 2.1358 (2.2258) acc@1 0.2344 (0.1551) acc@5 0.6406 (0.6306)\n",
      "\u001b[32m[2020-06-28 21:13:41] __main__ INFO: \u001b[0mEpoch 4 Step 200/351 lr 0.100000 loss 2.1524 (2.2123) acc@1 0.1719 (0.1588) acc@5 0.6328 (0.6383)\n",
      "\u001b[32m[2020-06-28 21:16:04] __main__ INFO: \u001b[0mEpoch 4 Step 300/351 lr 0.100000 loss 2.1146 (2.1926) acc@1 0.1328 (0.1670) acc@5 0.6641 (0.6496)\n",
      "\u001b[32m[2020-06-28 21:17:17] __main__ INFO: \u001b[0mEpoch 4 Step 351/351 lr 0.100000 loss 2.1266 (2.1827) acc@1 0.2031 (0.1706) acc@5 0.6797 (0.6539)\n",
      "\u001b[32m[2020-06-28 21:17:17] __main__ INFO: \u001b[0mElapsed 501.34\n",
      "\u001b[32m[2020-06-28 21:17:17] __main__ INFO: \u001b[0mVal 4\n",
      "\u001b[32m[2020-06-28 21:17:35] __main__ INFO: \u001b[0mEpoch 4 loss 2.1315 acc@1 0.1976 acc@5 0.6762\n",
      "\u001b[32m[2020-06-28 21:17:35] __main__ INFO: \u001b[0mElapsed 17.79\n",
      "\u001b[32m[2020-06-28 21:17:35] __main__ INFO: \u001b[0mTrain 5 1404\n",
      "\u001b[32m[2020-06-28 21:19:58] __main__ INFO: \u001b[0mEpoch 5 Step 100/351 lr 0.100000 loss 2.0682 (2.1145) acc@1 0.2031 (0.1881) acc@5 0.6953 (0.6889)\n",
      "\u001b[32m[2020-06-28 21:22:21] __main__ INFO: \u001b[0mEpoch 5 Step 200/351 lr 0.100000 loss 2.1001 (2.1024) acc@1 0.1953 (0.1975) acc@5 0.7344 (0.6954)\n",
      "\u001b[32m[2020-06-28 21:24:44] __main__ INFO: \u001b[0mEpoch 5 Step 300/351 lr 0.100000 loss 1.9436 (2.0905) acc@1 0.2266 (0.2061) acc@5 0.7266 (0.7006)\n",
      "\u001b[32m[2020-06-28 21:25:56] __main__ INFO: \u001b[0mEpoch 5 Step 351/351 lr 0.100000 loss 1.9930 (2.0857) acc@1 0.2188 (0.2087) acc@5 0.7266 (0.7019)\n",
      "\u001b[32m[2020-06-28 21:25:56] __main__ INFO: \u001b[0mElapsed 501.51\n",
      "\u001b[32m[2020-06-28 21:25:56] __main__ INFO: \u001b[0mVal 5\n",
      "\u001b[32m[2020-06-28 21:26:14] __main__ INFO: \u001b[0mEpoch 5 loss 2.0315 acc@1 0.2276 acc@5 0.7218\n",
      "\u001b[32m[2020-06-28 21:26:14] __main__ INFO: \u001b[0mElapsed 17.77\n",
      "\u001b[32m[2020-06-28 21:26:14] __main__ INFO: \u001b[0mTrain 6 1755\n",
      "\u001b[32m[2020-06-28 21:28:37] __main__ INFO: \u001b[0mEpoch 6 Step 100/351 lr 0.100000 loss 2.1083 (2.0133) acc@1 0.1719 (0.2402) acc@5 0.7031 (0.7289)\n",
      "\u001b[32m[2020-06-28 21:31:00] __main__ INFO: \u001b[0mEpoch 6 Step 200/351 lr 0.100000 loss 1.9436 (2.0000) acc@1 0.2422 (0.2471) acc@5 0.8125 (0.7341)\n",
      "\u001b[32m[2020-06-28 21:33:22] __main__ INFO: \u001b[0mEpoch 6 Step 300/351 lr 0.100000 loss 1.9483 (1.9845) acc@1 0.2266 (0.2546) acc@5 0.7734 (0.7375)\n",
      "\u001b[32m[2020-06-28 21:34:35] __main__ INFO: \u001b[0mEpoch 6 Step 351/351 lr 0.100000 loss 1.9964 (1.9762) acc@1 0.2500 (0.2585) acc@5 0.7734 (0.7396)\n",
      "\u001b[32m[2020-06-28 21:34:35] __main__ INFO: \u001b[0mElapsed 500.72\n",
      "\u001b[32m[2020-06-28 21:34:35] __main__ INFO: \u001b[0mVal 6\n",
      "\u001b[32m[2020-06-28 21:34:53] __main__ INFO: \u001b[0mEpoch 6 loss 2.0144 acc@1 0.2616 acc@5 0.7194\n",
      "\u001b[32m[2020-06-28 21:34:53] __main__ INFO: \u001b[0mElapsed 17.74\n",
      "\u001b[32m[2020-06-28 21:34:53] __main__ INFO: \u001b[0mTrain 7 2106\n",
      "\u001b[32m[2020-06-28 21:37:15] __main__ INFO: \u001b[0mEpoch 7 Step 100/351 lr 0.100000 loss 1.8285 (1.9017) acc@1 0.3047 (0.2931) acc@5 0.7969 (0.7520)\n",
      "\u001b[32m[2020-06-28 21:39:38] __main__ INFO: \u001b[0mEpoch 7 Step 200/351 lr 0.100000 loss 1.7816 (1.8804) acc@1 0.3203 (0.3000) acc@5 0.7578 (0.7616)\n",
      "\u001b[32m[2020-06-28 21:42:01] __main__ INFO: \u001b[0mEpoch 7 Step 300/351 lr 0.100000 loss 1.8196 (1.8675) acc@1 0.3359 (0.3063) acc@5 0.7969 (0.7646)\n",
      "\u001b[32m[2020-06-28 21:43:13] __main__ INFO: \u001b[0mEpoch 7 Step 351/351 lr 0.100000 loss 1.7687 (1.8564) acc@1 0.3281 (0.3105) acc@5 0.8203 (0.7675)\n",
      "\u001b[32m[2020-06-28 21:43:14] __main__ INFO: \u001b[0mElapsed 500.81\n",
      "\u001b[32m[2020-06-28 21:43:14] __main__ INFO: \u001b[0mVal 7\n",
      "\u001b[32m[2020-06-28 21:43:31] __main__ INFO: \u001b[0mEpoch 7 loss 1.8513 acc@1 0.3186 acc@5 0.7780\n",
      "\u001b[32m[2020-06-28 21:43:31] __main__ INFO: \u001b[0mElapsed 17.74\n",
      "\u001b[32m[2020-06-28 21:43:31] __main__ INFO: \u001b[0mTrain 8 2457\n",
      "\u001b[32m[2020-06-28 21:45:54] __main__ INFO: \u001b[0mEpoch 8 Step 100/351 lr 0.100000 loss 1.8900 (1.7834) acc@1 0.3125 (0.3383) acc@5 0.6797 (0.7798)\n",
      "\u001b[32m[2020-06-28 21:48:17] __main__ INFO: \u001b[0mEpoch 8 Step 200/351 lr 0.100000 loss 1.6857 (1.7670) acc@1 0.3984 (0.3504) acc@5 0.7812 (0.7857)\n",
      "\u001b[32m[2020-06-28 21:50:40] __main__ INFO: \u001b[0mEpoch 8 Step 300/351 lr 0.100000 loss 1.7533 (1.7537) acc@1 0.3906 (0.3528) acc@5 0.7734 (0.7889)\n",
      "\u001b[32m[2020-06-28 21:51:53] __main__ INFO: \u001b[0mEpoch 8 Step 351/351 lr 0.100000 loss 1.6736 (1.7490) acc@1 0.3281 (0.3542) acc@5 0.7812 (0.7894)\n",
      "\u001b[32m[2020-06-28 21:51:53] __main__ INFO: \u001b[0mElapsed 501.33\n",
      "\u001b[32m[2020-06-28 21:51:53] __main__ INFO: \u001b[0mVal 8\n",
      "\u001b[32m[2020-06-28 21:52:10] __main__ INFO: \u001b[0mEpoch 8 loss 1.7842 acc@1 0.3374 acc@5 0.7854\n",
      "\u001b[32m[2020-06-28 21:52:10] __main__ INFO: \u001b[0mElapsed 17.75\n",
      "\u001b[32m[2020-06-28 21:52:10] __main__ INFO: \u001b[0mTrain 9 2808\n",
      "\u001b[32m[2020-06-28 21:54:33] __main__ INFO: \u001b[0mEpoch 9 Step 100/351 lr 0.100000 loss 1.6914 (1.7033) acc@1 0.3906 (0.3724) acc@5 0.7891 (0.7934)\n",
      "\u001b[32m[2020-06-28 21:56:56] __main__ INFO: \u001b[0mEpoch 9 Step 200/351 lr 0.100000 loss 1.6239 (1.6934) acc@1 0.3984 (0.3759) acc@5 0.8359 (0.7932)\n",
      "\u001b[32m[2020-06-28 21:59:19] __main__ INFO: \u001b[0mEpoch 9 Step 300/351 lr 0.100000 loss 1.5795 (1.6770) acc@1 0.4531 (0.3830) acc@5 0.7891 (0.7997)\n",
      "\u001b[32m[2020-06-28 22:00:32] __main__ INFO: \u001b[0mEpoch 9 Step 351/351 lr 0.100000 loss 1.7142 (1.6759) acc@1 0.3984 (0.3827) acc@5 0.8281 (0.8007)\n",
      "\u001b[32m[2020-06-28 22:00:32] __main__ INFO: \u001b[0mElapsed 501.50\n",
      "\u001b[32m[2020-06-28 22:00:32] __main__ INFO: \u001b[0mVal 9\n",
      "\u001b[32m[2020-06-28 22:00:50] __main__ INFO: \u001b[0mEpoch 9 loss 1.6173 acc@1 0.4082 acc@5 0.8050\n",
      "\u001b[32m[2020-06-28 22:00:50] __main__ INFO: \u001b[0mElapsed 17.78\n",
      "\u001b[32m[2020-06-28 22:00:50] __main__ INFO: \u001b[0mTrain 10 3159\n",
      "\u001b[32m[2020-06-28 22:03:13] __main__ INFO: \u001b[0mEpoch 10 Step 100/351 lr 0.100000 loss 1.6624 (1.6282) acc@1 0.3906 (0.3981) acc@5 0.7734 (0.8046)\n",
      "\u001b[32m[2020-06-28 22:05:35] __main__ INFO: \u001b[0mEpoch 10 Step 200/351 lr 0.100000 loss 1.5248 (1.6265) acc@1 0.4062 (0.4001) acc@5 0.8594 (0.8083)\n",
      "\u001b[32m[2020-06-28 22:07:58] __main__ INFO: \u001b[0mEpoch 10 Step 300/351 lr 0.100000 loss 1.5396 (1.6232) acc@1 0.4609 (0.4026) acc@5 0.8203 (0.8078)\n",
      "\u001b[32m[2020-06-28 22:09:11] __main__ INFO: \u001b[0mEpoch 10 Step 351/351 lr 0.100000 loss 1.6406 (1.6194) acc@1 0.3984 (0.4029) acc@5 0.7969 (0.8068)\n",
      "\u001b[32m[2020-06-28 22:09:11] __main__ INFO: \u001b[0mElapsed 501.34\n",
      "\u001b[32m[2020-06-28 22:09:11] __main__ INFO: \u001b[0mVal 10\n",
      "\u001b[32m[2020-06-28 22:09:29] __main__ INFO: \u001b[0mEpoch 10 loss 1.6611 acc@1 0.3936 acc@5 0.7968\n",
      "\u001b[32m[2020-06-28 22:09:29] __main__ INFO: \u001b[0mElapsed 17.77\n",
      "\u001b[32m[2020-06-28 22:09:29] __main__ INFO: \u001b[0mTrain 11 3510\n",
      "\u001b[32m[2020-06-28 22:11:52] __main__ INFO: \u001b[0mEpoch 11 Step 100/351 lr 0.100000 loss 1.5350 (1.5881) acc@1 0.4375 (0.4159) acc@5 0.7734 (0.8091)\n",
      "\u001b[32m[2020-06-28 22:14:14] __main__ INFO: \u001b[0mEpoch 11 Step 200/351 lr 0.100000 loss 1.4386 (1.5816) acc@1 0.4688 (0.4175) acc@5 0.7656 (0.8117)\n",
      "\u001b[32m[2020-06-28 22:16:37] __main__ INFO: \u001b[0mEpoch 11 Step 300/351 lr 0.100000 loss 1.7343 (1.5762) acc@1 0.4062 (0.4194) acc@5 0.7109 (0.8142)\n",
      "\u001b[32m[2020-06-28 22:17:50] __main__ INFO: \u001b[0mEpoch 11 Step 351/351 lr 0.100000 loss 1.5726 (1.5722) acc@1 0.4375 (0.4205) acc@5 0.7656 (0.8148)\n",
      "\u001b[32m[2020-06-28 22:17:50] __main__ INFO: \u001b[0mElapsed 500.87\n",
      "\u001b[32m[2020-06-28 22:17:50] __main__ INFO: \u001b[0mVal 11\n",
      "\u001b[32m[2020-06-28 22:18:07] __main__ INFO: \u001b[0mEpoch 11 loss 1.9558 acc@1 0.3420 acc@5 0.7658\n",
      "\u001b[32m[2020-06-28 22:18:07] __main__ INFO: \u001b[0mElapsed 17.81\n",
      "\u001b[32m[2020-06-28 22:18:07] __main__ INFO: \u001b[0mTrain 12 3861\n",
      "\u001b[32m[2020-06-28 22:20:30] __main__ INFO: \u001b[0mEpoch 12 Step 100/351 lr 0.100000 loss 1.5995 (1.5601) acc@1 0.4688 (0.4292) acc@5 0.8438 (0.8166)\n",
      "\u001b[32m[2020-06-28 22:22:53] __main__ INFO: \u001b[0mEpoch 12 Step 200/351 lr 0.100000 loss 1.4881 (1.5518) acc@1 0.4219 (0.4304) acc@5 0.8594 (0.8177)\n",
      "\u001b[32m[2020-06-28 22:25:16] __main__ INFO: \u001b[0mEpoch 12 Step 300/351 lr 0.100000 loss 1.6441 (1.5401) acc@1 0.4688 (0.4341) acc@5 0.8203 (0.8187)\n",
      "\u001b[32m[2020-06-28 22:26:29] __main__ INFO: \u001b[0mEpoch 12 Step 351/351 lr 0.100000 loss 1.3379 (1.5371) acc@1 0.5156 (0.4355) acc@5 0.8359 (0.8187)\n",
      "\u001b[32m[2020-06-28 22:26:29] __main__ INFO: \u001b[0mElapsed 501.51\n",
      "\u001b[32m[2020-06-28 22:26:29] __main__ INFO: \u001b[0mVal 12\n",
      "\u001b[32m[2020-06-28 22:26:47] __main__ INFO: \u001b[0mEpoch 12 loss 1.6640 acc@1 0.3918 acc@5 0.8032\n",
      "\u001b[32m[2020-06-28 22:26:47] __main__ INFO: \u001b[0mElapsed 17.78\n",
      "\u001b[32m[2020-06-28 22:26:47] __main__ INFO: \u001b[0mTrain 13 4212\n",
      "\u001b[32m[2020-06-28 22:29:10] __main__ INFO: \u001b[0mEpoch 13 Step 100/351 lr 0.100000 loss 1.5768 (1.5023) acc@1 0.3906 (0.4444) acc@5 0.8438 (0.8230)\n",
      "\u001b[32m[2020-06-28 22:31:33] __main__ INFO: \u001b[0mEpoch 13 Step 200/351 lr 0.100000 loss 1.5644 (1.4997) acc@1 0.4141 (0.4465) acc@5 0.8438 (0.8240)\n",
      "\u001b[32m[2020-06-28 22:33:56] __main__ INFO: \u001b[0mEpoch 13 Step 300/351 lr 0.100000 loss 1.5837 (1.5044) acc@1 0.3984 (0.4455) acc@5 0.7812 (0.8209)\n",
      "\u001b[32m[2020-06-28 22:35:08] __main__ INFO: \u001b[0mEpoch 13 Step 351/351 lr 0.100000 loss 1.3390 (1.5015) acc@1 0.5312 (0.4464) acc@5 0.7969 (0.8215)\n",
      "\u001b[32m[2020-06-28 22:35:08] __main__ INFO: \u001b[0mElapsed 501.79\n",
      "\u001b[32m[2020-06-28 22:35:08] __main__ INFO: \u001b[0mVal 13\n",
      "\u001b[32m[2020-06-28 22:35:26] __main__ INFO: \u001b[0mEpoch 13 loss 1.8007 acc@1 0.3616 acc@5 0.7960\n",
      "\u001b[32m[2020-06-28 22:35:26] __main__ INFO: \u001b[0mElapsed 17.79\n",
      "\u001b[32m[2020-06-28 22:35:26] __main__ INFO: \u001b[0mTrain 14 4563\n",
      "\u001b[32m[2020-06-28 22:37:49] __main__ INFO: \u001b[0mEpoch 14 Step 100/351 lr 0.100000 loss 1.6614 (1.4766) acc@1 0.3750 (0.4552) acc@5 0.7500 (0.8248)\n",
      "\u001b[32m[2020-06-28 22:40:12] __main__ INFO: \u001b[0mEpoch 14 Step 200/351 lr 0.100000 loss 1.5066 (1.4770) acc@1 0.4375 (0.4580) acc@5 0.8203 (0.8243)\n",
      "\u001b[32m[2020-06-28 22:42:35] __main__ INFO: \u001b[0mEpoch 14 Step 300/351 lr 0.100000 loss 1.5025 (1.4744) acc@1 0.4531 (0.4579) acc@5 0.8203 (0.8257)\n",
      "\u001b[32m[2020-06-28 22:43:48] __main__ INFO: \u001b[0mEpoch 14 Step 351/351 lr 0.100000 loss 1.3745 (1.4731) acc@1 0.4766 (0.4582) acc@5 0.8438 (0.8266)\n",
      "\u001b[32m[2020-06-28 22:43:48] __main__ INFO: \u001b[0mElapsed 501.80\n",
      "\u001b[32m[2020-06-28 22:43:48] __main__ INFO: \u001b[0mVal 14\n",
      "\u001b[32m[2020-06-28 22:44:06] __main__ INFO: \u001b[0mEpoch 14 loss 1.5485 acc@1 0.4452 acc@5 0.8130\n",
      "\u001b[32m[2020-06-28 22:44:06] __main__ INFO: \u001b[0mElapsed 17.75\n",
      "\u001b[32m[2020-06-28 22:44:06] __main__ INFO: \u001b[0mTrain 15 4914\n",
      "\u001b[32m[2020-06-28 22:46:29] __main__ INFO: \u001b[0mEpoch 15 Step 100/351 lr 0.100000 loss 1.3965 (1.4277) acc@1 0.5078 (0.4770) acc@5 0.8359 (0.8322)\n",
      "\u001b[32m[2020-06-28 22:48:52] __main__ INFO: \u001b[0mEpoch 15 Step 200/351 lr 0.100000 loss 1.7750 (1.4502) acc@1 0.3203 (0.4674) acc@5 0.7578 (0.8259)\n",
      "\u001b[32m[2020-06-28 22:51:14] __main__ INFO: \u001b[0mEpoch 15 Step 300/351 lr 0.100000 loss 1.6003 (1.4522) acc@1 0.4219 (0.4669) acc@5 0.8359 (0.8255)\n",
      "\u001b[32m[2020-06-28 22:52:27] __main__ INFO: \u001b[0mEpoch 15 Step 351/351 lr 0.100000 loss 1.3235 (1.4482) acc@1 0.5156 (0.4685) acc@5 0.8672 (0.8257)\n",
      "\u001b[32m[2020-06-28 22:52:27] __main__ INFO: \u001b[0mElapsed 501.55\n",
      "\u001b[32m[2020-06-28 22:52:27] __main__ INFO: \u001b[0mVal 15\n",
      "\u001b[32m[2020-06-28 22:52:45] __main__ INFO: \u001b[0mEpoch 15 loss 1.6036 acc@1 0.4332 acc@5 0.8140\n",
      "\u001b[32m[2020-06-28 22:52:45] __main__ INFO: \u001b[0mElapsed 17.75\n",
      "\u001b[32m[2020-06-28 22:52:45] __main__ INFO: \u001b[0mTrain 16 5265\n",
      "\u001b[32m[2020-06-28 22:55:08] __main__ INFO: \u001b[0mEpoch 16 Step 100/351 lr 0.100000 loss 1.3196 (1.4411) acc@1 0.5781 (0.4667) acc@5 0.8438 (0.8298)\n",
      "\u001b[32m[2020-06-28 22:57:31] __main__ INFO: \u001b[0mEpoch 16 Step 200/351 lr 0.100000 loss 1.4886 (1.4318) acc@1 0.4766 (0.4735) acc@5 0.8203 (0.8321)\n",
      "\u001b[32m[2020-06-28 22:59:54] __main__ INFO: \u001b[0mEpoch 16 Step 300/351 lr 0.100000 loss 1.4936 (1.4301) acc@1 0.4688 (0.4726) acc@5 0.8438 (0.8311)\n",
      "\u001b[32m[2020-06-28 23:01:07] __main__ INFO: \u001b[0mEpoch 16 Step 351/351 lr 0.100000 loss 1.5978 (1.4310) acc@1 0.3906 (0.4727) acc@5 0.8281 (0.8316)\n",
      "\u001b[32m[2020-06-28 23:01:07] __main__ INFO: \u001b[0mElapsed 501.74\n",
      "\u001b[32m[2020-06-28 23:01:07] __main__ INFO: \u001b[0mVal 16\n",
      "\u001b[32m[2020-06-28 23:01:25] __main__ INFO: \u001b[0mEpoch 16 loss 1.5392 acc@1 0.4396 acc@5 0.8214\n",
      "\u001b[32m[2020-06-28 23:01:25] __main__ INFO: \u001b[0mElapsed 17.78\n",
      "\u001b[32m[2020-06-28 23:01:25] __main__ INFO: \u001b[0mTrain 17 5616\n",
      "\u001b[32m[2020-06-28 23:03:48] __main__ INFO: \u001b[0mEpoch 17 Step 100/351 lr 0.100000 loss 1.6164 (1.4138) acc@1 0.4219 (0.4795) acc@5 0.8047 (0.8303)\n",
      "\u001b[32m[2020-06-28 23:06:10] __main__ INFO: \u001b[0mEpoch 17 Step 200/351 lr 0.100000 loss 1.3723 (1.4057) acc@1 0.4844 (0.4855) acc@5 0.8125 (0.8295)\n",
      "\u001b[32m[2020-06-28 23:08:33] __main__ INFO: \u001b[0mEpoch 17 Step 300/351 lr 0.100000 loss 1.4949 (1.4054) acc@1 0.4453 (0.4836) acc@5 0.7734 (0.8303)\n",
      "\u001b[32m[2020-06-28 23:09:46] __main__ INFO: \u001b[0mEpoch 17 Step 351/351 lr 0.100000 loss 1.4657 (1.4026) acc@1 0.4453 (0.4850) acc@5 0.8125 (0.8310)\n",
      "\u001b[32m[2020-06-28 23:09:46] __main__ INFO: \u001b[0mElapsed 501.57\n",
      "\u001b[32m[2020-06-28 23:09:46] __main__ INFO: \u001b[0mVal 17\n",
      "\u001b[32m[2020-06-28 23:10:04] __main__ INFO: \u001b[0mEpoch 17 loss 1.8393 acc@1 0.3660 acc@5 0.7838\n",
      "\u001b[32m[2020-06-28 23:10:04] __main__ INFO: \u001b[0mElapsed 17.77\n",
      "\u001b[32m[2020-06-28 23:10:04] __main__ INFO: \u001b[0mTrain 18 5967\n",
      "\u001b[32m[2020-06-28 23:12:27] __main__ INFO: \u001b[0mEpoch 18 Step 100/351 lr 0.100000 loss 1.3549 (1.3924) acc@1 0.5156 (0.4862) acc@5 0.8125 (0.8332)\n",
      "\u001b[32m[2020-06-28 23:14:50] __main__ INFO: \u001b[0mEpoch 18 Step 200/351 lr 0.100000 loss 1.4769 (1.3863) acc@1 0.4297 (0.4886) acc@5 0.8828 (0.8336)\n",
      "\u001b[32m[2020-06-28 23:17:13] __main__ INFO: \u001b[0mEpoch 18 Step 300/351 lr 0.100000 loss 1.4025 (1.3832) acc@1 0.4531 (0.4904) acc@5 0.8359 (0.8352)\n",
      "\u001b[32m[2020-06-28 23:18:26] __main__ INFO: \u001b[0mEpoch 18 Step 351/351 lr 0.100000 loss 1.2152 (1.3849) acc@1 0.5781 (0.4895) acc@5 0.8203 (0.8334)\n",
      "\u001b[32m[2020-06-28 23:18:26] __main__ INFO: \u001b[0mElapsed 501.57\n",
      "\u001b[32m[2020-06-28 23:18:26] __main__ INFO: \u001b[0mVal 18\n",
      "\u001b[32m[2020-06-28 23:18:43] __main__ INFO: \u001b[0mEpoch 18 loss 1.7105 acc@1 0.4090 acc@5 0.7936\n",
      "\u001b[32m[2020-06-28 23:18:43] __main__ INFO: \u001b[0mElapsed 17.78\n",
      "\u001b[32m[2020-06-28 23:18:43] __main__ INFO: \u001b[0mTrain 19 6318\n",
      "\u001b[32m[2020-06-28 23:21:06] __main__ INFO: \u001b[0mEpoch 19 Step 100/351 lr 0.100000 loss 1.5367 (1.3570) acc@1 0.4297 (0.5024) acc@5 0.7734 (0.8404)\n",
      "\u001b[32m[2020-06-28 23:23:29] __main__ INFO: \u001b[0mEpoch 19 Step 200/351 lr 0.100000 loss 1.5534 (1.3700) acc@1 0.4531 (0.4939) acc@5 0.8438 (0.8374)\n",
      "\u001b[32m[2020-06-28 23:25:52] __main__ INFO: \u001b[0mEpoch 19 Step 300/351 lr 0.100000 loss 1.4024 (1.3680) acc@1 0.4609 (0.4959) acc@5 0.7891 (0.8363)\n",
      "\u001b[32m[2020-06-28 23:27:05] __main__ INFO: \u001b[0mEpoch 19 Step 351/351 lr 0.100000 loss 1.4107 (1.3669) acc@1 0.4922 (0.4965) acc@5 0.8516 (0.8373)\n",
      "\u001b[32m[2020-06-28 23:27:05] __main__ INFO: \u001b[0mElapsed 501.52\n",
      "\u001b[32m[2020-06-28 23:27:05] __main__ INFO: \u001b[0mVal 19\n",
      "\u001b[32m[2020-06-28 23:27:23] __main__ INFO: \u001b[0mEpoch 19 loss 1.8808 acc@1 0.4358 acc@5 0.8058\n",
      "\u001b[32m[2020-06-28 23:27:23] __main__ INFO: \u001b[0mElapsed 17.79\n",
      "\u001b[32m[2020-06-28 23:27:23] __main__ INFO: \u001b[0mTrain 20 6669\n",
      "\u001b[32m[2020-06-28 23:29:46] __main__ INFO: \u001b[0mEpoch 20 Step 100/351 lr 0.100000 loss 1.5675 (1.3688) acc@1 0.4141 (0.4984) acc@5 0.8203 (0.8403)\n",
      "\u001b[32m[2020-06-28 23:32:09] __main__ INFO: \u001b[0mEpoch 20 Step 200/351 lr 0.100000 loss 1.4081 (1.3518) acc@1 0.5078 (0.5042) acc@5 0.8516 (0.8380)\n",
      "\u001b[32m[2020-06-28 23:34:32] __main__ INFO: \u001b[0mEpoch 20 Step 300/351 lr 0.100000 loss 1.2572 (1.3512) acc@1 0.5312 (0.5039) acc@5 0.8750 (0.8377)\n",
      "\u001b[32m[2020-06-28 23:35:44] __main__ INFO: \u001b[0mEpoch 20 Step 351/351 lr 0.100000 loss 1.3320 (1.3563) acc@1 0.5078 (0.5020) acc@5 0.8359 (0.8371)\n",
      "\u001b[32m[2020-06-28 23:35:44] __main__ INFO: \u001b[0mElapsed 501.76\n",
      "\u001b[32m[2020-06-28 23:35:44] __main__ INFO: \u001b[0mVal 20\n",
      "\u001b[32m[2020-06-28 23:36:02] __main__ INFO: \u001b[0mEpoch 20 loss 1.5464 acc@1 0.4484 acc@5 0.8222\n",
      "\u001b[32m[2020-06-28 23:36:02] __main__ INFO: \u001b[0mElapsed 17.79\n",
      "\u001b[32m[2020-06-28 23:36:02] __main__ INFO: \u001b[0mTrain 21 7020\n",
      "\u001b[32m[2020-06-28 23:38:25] __main__ INFO: \u001b[0mEpoch 21 Step 100/351 lr 0.100000 loss 1.3101 (1.3224) acc@1 0.5391 (0.5122) acc@5 0.8359 (0.8355)\n",
      "\u001b[32m[2020-06-28 23:40:48] __main__ INFO: \u001b[0mEpoch 21 Step 200/351 lr 0.100000 loss 1.2508 (1.3314) acc@1 0.5625 (0.5098) acc@5 0.8594 (0.8369)\n",
      "\u001b[32m[2020-06-28 23:43:10] __main__ INFO: \u001b[0mEpoch 21 Step 300/351 lr 0.100000 loss 1.3556 (1.3291) acc@1 0.5000 (0.5112) acc@5 0.8672 (0.8381)\n",
      "\u001b[32m[2020-06-28 23:44:23] __main__ INFO: \u001b[0mEpoch 21 Step 351/351 lr 0.100000 loss 1.3930 (1.3360) acc@1 0.5078 (0.5085) acc@5 0.8750 (0.8377)\n",
      "\u001b[32m[2020-06-28 23:44:23] __main__ INFO: \u001b[0mElapsed 500.51\n",
      "\u001b[32m[2020-06-28 23:44:23] __main__ INFO: \u001b[0mVal 21\n",
      "\u001b[32m[2020-06-28 23:44:40] __main__ INFO: \u001b[0mEpoch 21 loss 1.5300 acc@1 0.4516 acc@5 0.8372\n",
      "\u001b[32m[2020-06-28 23:44:40] __main__ INFO: \u001b[0mElapsed 17.59\n",
      "\u001b[32m[2020-06-28 23:44:40] __main__ INFO: \u001b[0mTrain 22 7371\n",
      "\u001b[32m[2020-06-28 23:47:02] __main__ INFO: \u001b[0mEpoch 22 Step 100/351 lr 0.100000 loss 1.4660 (1.3219) acc@1 0.4453 (0.5128) acc@5 0.8281 (0.8379)\n",
      "\u001b[32m[2020-06-28 23:49:23] __main__ INFO: \u001b[0mEpoch 22 Step 200/351 lr 0.100000 loss 1.1991 (1.3202) acc@1 0.5547 (0.5132) acc@5 0.8750 (0.8389)\n",
      "\u001b[32m[2020-06-28 23:51:43] __main__ INFO: \u001b[0mEpoch 22 Step 300/351 lr 0.100000 loss 1.3220 (1.3260) acc@1 0.4922 (0.5120) acc@5 0.8203 (0.8381)\n",
      "\u001b[32m[2020-06-28 23:52:55] __main__ INFO: \u001b[0mEpoch 22 Step 351/351 lr 0.100000 loss 1.4379 (1.3256) acc@1 0.4531 (0.5122) acc@5 0.8438 (0.8384)\n",
      "\u001b[32m[2020-06-28 23:52:55] __main__ INFO: \u001b[0mElapsed 495.00\n",
      "\u001b[32m[2020-06-28 23:52:55] __main__ INFO: \u001b[0mVal 22\n",
      "\u001b[32m[2020-06-28 23:53:13] __main__ INFO: \u001b[0mEpoch 22 loss 1.5970 acc@1 0.4452 acc@5 0.8260\n",
      "\u001b[32m[2020-06-28 23:53:13] __main__ INFO: \u001b[0mElapsed 17.56\n",
      "\u001b[32m[2020-06-28 23:53:13] __main__ INFO: \u001b[0mTrain 23 7722\n",
      "\u001b[32m[2020-06-28 23:55:34] __main__ INFO: \u001b[0mEpoch 23 Step 100/351 lr 0.100000 loss 1.1249 (1.3235) acc@1 0.5703 (0.5091) acc@5 0.8906 (0.8384)\n",
      "\u001b[32m[2020-06-28 23:57:55] __main__ INFO: \u001b[0mEpoch 23 Step 200/351 lr 0.100000 loss 1.3360 (1.3179) acc@1 0.5312 (0.5142) acc@5 0.8203 (0.8418)\n",
      "\u001b[32m[2020-06-29 00:00:16] __main__ INFO: \u001b[0mEpoch 23 Step 300/351 lr 0.100000 loss 1.3648 (1.3217) acc@1 0.4688 (0.5132) acc@5 0.8438 (0.8399)\n",
      "\u001b[32m[2020-06-29 00:01:27] __main__ INFO: \u001b[0mEpoch 23 Step 351/351 lr 0.100000 loss 1.2259 (1.3215) acc@1 0.5312 (0.5130) acc@5 0.8438 (0.8407)\n",
      "\u001b[32m[2020-06-29 00:01:27] __main__ INFO: \u001b[0mElapsed 494.41\n",
      "\u001b[32m[2020-06-29 00:01:27] __main__ INFO: \u001b[0mVal 23\n",
      "\u001b[32m[2020-06-29 00:01:45] __main__ INFO: \u001b[0mEpoch 23 loss 1.4481 acc@1 0.4734 acc@5 0.8380\n",
      "\u001b[32m[2020-06-29 00:01:45] __main__ INFO: \u001b[0mElapsed 17.56\n",
      "\u001b[32m[2020-06-29 00:01:45] __main__ INFO: \u001b[0mTrain 24 8073\n",
      "\u001b[32m[2020-06-29 00:04:06] __main__ INFO: \u001b[0mEpoch 24 Step 100/351 lr 0.100000 loss 1.1948 (1.3065) acc@1 0.5391 (0.5198) acc@5 0.8984 (0.8387)\n",
      "\u001b[32m[2020-06-29 00:06:27] __main__ INFO: \u001b[0mEpoch 24 Step 200/351 lr 0.100000 loss 1.3547 (1.3050) acc@1 0.5078 (0.5206) acc@5 0.8359 (0.8420)\n",
      "\u001b[32m[2020-06-29 00:08:48] __main__ INFO: \u001b[0mEpoch 24 Step 300/351 lr 0.100000 loss 1.3709 (1.3094) acc@1 0.4688 (0.5175) acc@5 0.8359 (0.8408)\n",
      "\u001b[32m[2020-06-29 00:10:00] __main__ INFO: \u001b[0mEpoch 24 Step 351/351 lr 0.100000 loss 1.3862 (1.3088) acc@1 0.5234 (0.5178) acc@5 0.8359 (0.8405)\n",
      "\u001b[32m[2020-06-29 00:10:00] __main__ INFO: \u001b[0mElapsed 494.72\n",
      "\u001b[32m[2020-06-29 00:10:00] __main__ INFO: \u001b[0mVal 24\n",
      "\u001b[32m[2020-06-29 00:10:17] __main__ INFO: \u001b[0mEpoch 24 loss 1.4899 acc@1 0.4578 acc@5 0.8322\n",
      "\u001b[32m[2020-06-29 00:10:17] __main__ INFO: \u001b[0mElapsed 17.57\n",
      "\u001b[32m[2020-06-29 00:10:17] __main__ INFO: \u001b[0mTrain 25 8424\n",
      "\u001b[32m[2020-06-29 00:12:38] __main__ INFO: \u001b[0mEpoch 25 Step 100/351 lr 0.100000 loss 1.2511 (1.2920) acc@1 0.5391 (0.5229) acc@5 0.8906 (0.8447)\n",
      "\u001b[32m[2020-06-29 00:14:59] __main__ INFO: \u001b[0mEpoch 25 Step 200/351 lr 0.100000 loss 1.1610 (1.2966) acc@1 0.5625 (0.5232) acc@5 0.8672 (0.8428)\n",
      "\u001b[32m[2020-06-29 00:17:20] __main__ INFO: \u001b[0mEpoch 25 Step 300/351 lr 0.100000 loss 1.0776 (1.2950) acc@1 0.6016 (0.5226) acc@5 0.8672 (0.8435)\n",
      "\u001b[32m[2020-06-29 00:18:32] __main__ INFO: \u001b[0mEpoch 25 Step 351/351 lr 0.100000 loss 1.3227 (1.2968) acc@1 0.5469 (0.5225) acc@5 0.8125 (0.8434)\n",
      "\u001b[32m[2020-06-29 00:18:32] __main__ INFO: \u001b[0mElapsed 494.72\n",
      "\u001b[32m[2020-06-29 00:18:32] __main__ INFO: \u001b[0mVal 25\n",
      "\u001b[32m[2020-06-29 00:18:49] __main__ INFO: \u001b[0mEpoch 25 loss 1.5999 acc@1 0.4444 acc@5 0.8200\n",
      "\u001b[32m[2020-06-29 00:18:49] __main__ INFO: \u001b[0mElapsed 17.57\n",
      "\u001b[32m[2020-06-29 00:18:49] __main__ INFO: \u001b[0mTrain 26 8775\n",
      "\u001b[32m[2020-06-29 00:21:10] __main__ INFO: \u001b[0mEpoch 26 Step 100/351 lr 0.100000 loss 1.2261 (1.2705) acc@1 0.5547 (0.5312) acc@5 0.8594 (0.8429)\n",
      "\u001b[32m[2020-06-29 00:23:31] __main__ INFO: \u001b[0mEpoch 26 Step 200/351 lr 0.100000 loss 1.2626 (1.2900) acc@1 0.5234 (0.5254) acc@5 0.8281 (0.8404)\n",
      "\u001b[32m[2020-06-29 00:25:52] __main__ INFO: \u001b[0mEpoch 26 Step 300/351 lr 0.100000 loss 1.2944 (1.2912) acc@1 0.5156 (0.5253) acc@5 0.8672 (0.8407)\n",
      "\u001b[32m[2020-06-29 00:27:04] __main__ INFO: \u001b[0mEpoch 26 Step 351/351 lr 0.100000 loss 1.2006 (1.2914) acc@1 0.5234 (0.5242) acc@5 0.8594 (0.8403)\n",
      "\u001b[32m[2020-06-29 00:27:04] __main__ INFO: \u001b[0mElapsed 494.53\n",
      "\u001b[32m[2020-06-29 00:27:04] __main__ INFO: \u001b[0mVal 26\n",
      "\u001b[32m[2020-06-29 00:27:22] __main__ INFO: \u001b[0mEpoch 26 loss 1.5873 acc@1 0.4564 acc@5 0.8224\n",
      "\u001b[32m[2020-06-29 00:27:22] __main__ INFO: \u001b[0mElapsed 17.54\n",
      "\u001b[32m[2020-06-29 00:27:22] __main__ INFO: \u001b[0mTrain 27 9126\n",
      "\u001b[32m[2020-06-29 00:29:43] __main__ INFO: \u001b[0mEpoch 27 Step 100/351 lr 0.100000 loss 1.1291 (1.2831) acc@1 0.5781 (0.5257) acc@5 0.8438 (0.8417)\n",
      "\u001b[32m[2020-06-29 00:32:03] __main__ INFO: \u001b[0mEpoch 27 Step 200/351 lr 0.100000 loss 1.2474 (1.2817) acc@1 0.5469 (0.5268) acc@5 0.8438 (0.8415)\n",
      "\u001b[32m[2020-06-29 00:34:24] __main__ INFO: \u001b[0mEpoch 27 Step 300/351 lr 0.100000 loss 1.3055 (1.2806) acc@1 0.4766 (0.5286) acc@5 0.8125 (0.8412)\n",
      "\u001b[32m[2020-06-29 00:35:36] __main__ INFO: \u001b[0mEpoch 27 Step 351/351 lr 0.100000 loss 1.3595 (1.2838) acc@1 0.4531 (0.5276) acc@5 0.8438 (0.8414)\n",
      "\u001b[32m[2020-06-29 00:35:36] __main__ INFO: \u001b[0mElapsed 494.74\n",
      "\u001b[32m[2020-06-29 00:35:36] __main__ INFO: \u001b[0mVal 27\n",
      "\u001b[32m[2020-06-29 00:35:54] __main__ INFO: \u001b[0mEpoch 27 loss 1.4914 acc@1 0.4504 acc@5 0.8344\n",
      "\u001b[32m[2020-06-29 00:35:54] __main__ INFO: \u001b[0mElapsed 17.55\n",
      "\u001b[32m[2020-06-29 00:35:54] __main__ INFO: \u001b[0mTrain 28 9477\n",
      "\u001b[32m[2020-06-29 00:38:15] __main__ INFO: \u001b[0mEpoch 28 Step 100/351 lr 0.100000 loss 1.3607 (1.2704) acc@1 0.5078 (0.5309) acc@5 0.8203 (0.8463)\n",
      "\u001b[32m[2020-06-29 00:40:36] __main__ INFO: \u001b[0mEpoch 28 Step 200/351 lr 0.100000 loss 1.3096 (1.2820) acc@1 0.5391 (0.5262) acc@5 0.8125 (0.8437)\n",
      "\u001b[32m[2020-06-29 00:42:56] __main__ INFO: \u001b[0mEpoch 28 Step 300/351 lr 0.100000 loss 1.2578 (1.2780) acc@1 0.5391 (0.5298) acc@5 0.8438 (0.8440)\n",
      "\u001b[32m[2020-06-29 00:44:08] __main__ INFO: \u001b[0mEpoch 28 Step 351/351 lr 0.100000 loss 1.0279 (1.2789) acc@1 0.6250 (0.5293) acc@5 0.8906 (0.8443)\n",
      "\u001b[32m[2020-06-29 00:44:08] __main__ INFO: \u001b[0mElapsed 494.43\n",
      "\u001b[32m[2020-06-29 00:44:08] __main__ INFO: \u001b[0mVal 28\n",
      "\u001b[32m[2020-06-29 00:44:26] __main__ INFO: \u001b[0mEpoch 28 loss 1.8912 acc@1 0.3920 acc@5 0.8100\n",
      "\u001b[32m[2020-06-29 00:44:26] __main__ INFO: \u001b[0mElapsed 17.57\n",
      "\u001b[32m[2020-06-29 00:44:26] __main__ INFO: \u001b[0mTrain 29 9828\n",
      "\u001b[32m[2020-06-29 00:46:47] __main__ INFO: \u001b[0mEpoch 29 Step 100/351 lr 0.100000 loss 1.2803 (1.2657) acc@1 0.5312 (0.5305) acc@5 0.8828 (0.8455)\n",
      "\u001b[32m[2020-06-29 00:49:08] __main__ INFO: \u001b[0mEpoch 29 Step 200/351 lr 0.100000 loss 1.2255 (1.2672) acc@1 0.5469 (0.5321) acc@5 0.8672 (0.8447)\n",
      "\u001b[32m[2020-06-29 00:51:29] __main__ INFO: \u001b[0mEpoch 29 Step 300/351 lr 0.100000 loss 1.3517 (1.2744) acc@1 0.4453 (0.5280) acc@5 0.8203 (0.8427)\n",
      "\u001b[32m[2020-06-29 00:52:41] __main__ INFO: \u001b[0mEpoch 29 Step 351/351 lr 0.100000 loss 1.1567 (1.2729) acc@1 0.5859 (0.5283) acc@5 0.8359 (0.8429)\n",
      "\u001b[32m[2020-06-29 00:52:41] __main__ INFO: \u001b[0mElapsed 494.82\n",
      "\u001b[32m[2020-06-29 00:52:41] __main__ INFO: \u001b[0mVal 29\n",
      "\u001b[32m[2020-06-29 00:52:58] __main__ INFO: \u001b[0mEpoch 29 loss 1.4265 acc@1 0.4834 acc@5 0.8410\n",
      "\u001b[32m[2020-06-29 00:52:58] __main__ INFO: \u001b[0mElapsed 17.56\n",
      "\u001b[32m[2020-06-29 00:52:58] __main__ INFO: \u001b[0mTrain 30 10179\n",
      "\u001b[32m[2020-06-29 00:55:19] __main__ INFO: \u001b[0mEpoch 30 Step 100/351 lr 0.100000 loss 1.2747 (1.2702) acc@1 0.4922 (0.5331) acc@5 0.8594 (0.8432)\n",
      "\u001b[32m[2020-06-29 00:57:40] __main__ INFO: \u001b[0mEpoch 30 Step 200/351 lr 0.100000 loss 1.4168 (1.2708) acc@1 0.4922 (0.5338) acc@5 0.8203 (0.8418)\n",
      "\u001b[32m[2020-06-29 01:00:01] __main__ INFO: \u001b[0mEpoch 30 Step 300/351 lr 0.100000 loss 1.3721 (1.2666) acc@1 0.4844 (0.5352) acc@5 0.8125 (0.8418)\n",
      "\u001b[32m[2020-06-29 01:01:13] __main__ INFO: \u001b[0mEpoch 30 Step 351/351 lr 0.100000 loss 1.2670 (1.2663) acc@1 0.5156 (0.5347) acc@5 0.8750 (0.8417)\n",
      "\u001b[32m[2020-06-29 01:01:13] __main__ INFO: \u001b[0mElapsed 494.45\n",
      "\u001b[32m[2020-06-29 01:01:13] __main__ INFO: \u001b[0mVal 30\n",
      "\u001b[32m[2020-06-29 01:01:30] __main__ INFO: \u001b[0mEpoch 30 loss 1.3940 acc@1 0.5032 acc@5 0.8418\n",
      "\u001b[32m[2020-06-29 01:01:30] __main__ INFO: \u001b[0mElapsed 17.55\n",
      "\u001b[32m[2020-06-29 01:01:30] __main__ INFO: \u001b[0mTrain 31 10530\n",
      "\u001b[32m[2020-06-29 01:03:51] __main__ INFO: \u001b[0mEpoch 31 Step 100/351 lr 0.100000 loss 1.1272 (1.2340) acc@1 0.6094 (0.5486) acc@5 0.9141 (0.8484)\n",
      "\u001b[32m[2020-06-29 01:06:12] __main__ INFO: \u001b[0mEpoch 31 Step 200/351 lr 0.100000 loss 1.2305 (1.2517) acc@1 0.5312 (0.5399) acc@5 0.8516 (0.8457)\n",
      "\u001b[32m[2020-06-29 01:08:33] __main__ INFO: \u001b[0mEpoch 31 Step 300/351 lr 0.100000 loss 1.2764 (1.2583) acc@1 0.5156 (0.5347) acc@5 0.8047 (0.8448)\n",
      "\u001b[32m[2020-06-29 01:09:44] __main__ INFO: \u001b[0mEpoch 31 Step 351/351 lr 0.100000 loss 1.3747 (1.2612) acc@1 0.5000 (0.5336) acc@5 0.7969 (0.8446)\n",
      "\u001b[32m[2020-06-29 01:09:44] __main__ INFO: \u001b[0mElapsed 494.14\n",
      "\u001b[32m[2020-06-29 01:09:44] __main__ INFO: \u001b[0mVal 31\n",
      "\u001b[32m[2020-06-29 01:10:02] __main__ INFO: \u001b[0mEpoch 31 loss 1.5572 acc@1 0.4542 acc@5 0.8246\n",
      "\u001b[32m[2020-06-29 01:10:02] __main__ INFO: \u001b[0mElapsed 17.55\n",
      "\u001b[32m[2020-06-29 01:10:02] __main__ INFO: \u001b[0mTrain 32 10881\n",
      "\u001b[32m[2020-06-29 01:12:23] __main__ INFO: \u001b[0mEpoch 32 Step 100/351 lr 0.100000 loss 1.1038 (1.2360) acc@1 0.6719 (0.5466) acc@5 0.9375 (0.8507)\n",
      "\u001b[32m[2020-06-29 01:14:44] __main__ INFO: \u001b[0mEpoch 32 Step 200/351 lr 0.100000 loss 1.3437 (1.2438) acc@1 0.4844 (0.5430) acc@5 0.8594 (0.8484)\n",
      "\u001b[32m[2020-06-29 01:17:05] __main__ INFO: \u001b[0mEpoch 32 Step 300/351 lr 0.100000 loss 1.2436 (1.2521) acc@1 0.5078 (0.5406) acc@5 0.8672 (0.8467)\n",
      "\u001b[32m[2020-06-29 01:18:17] __main__ INFO: \u001b[0mEpoch 32 Step 351/351 lr 0.100000 loss 1.0390 (1.2526) acc@1 0.6172 (0.5409) acc@5 0.8984 (0.8464)\n",
      "\u001b[32m[2020-06-29 01:18:17] __main__ INFO: \u001b[0mElapsed 494.61\n",
      "\u001b[32m[2020-06-29 01:18:17] __main__ INFO: \u001b[0mVal 32\n",
      "\u001b[32m[2020-06-29 01:18:34] __main__ INFO: \u001b[0mEpoch 32 loss 1.3979 acc@1 0.5016 acc@5 0.8372\n",
      "\u001b[32m[2020-06-29 01:18:34] __main__ INFO: \u001b[0mElapsed 17.54\n",
      "\u001b[32m[2020-06-29 01:18:34] __main__ INFO: \u001b[0mTrain 33 11232\n",
      "\u001b[32m[2020-06-29 01:20:55] __main__ INFO: \u001b[0mEpoch 33 Step 100/351 lr 0.100000 loss 1.0644 (1.2281) acc@1 0.6328 (0.5450) acc@5 0.8359 (0.8500)\n",
      "\u001b[32m[2020-06-29 01:23:16] __main__ INFO: \u001b[0mEpoch 33 Step 200/351 lr 0.100000 loss 1.2821 (1.2384) acc@1 0.5000 (0.5397) acc@5 0.8594 (0.8486)\n",
      "\u001b[32m[2020-06-29 01:25:36] __main__ INFO: \u001b[0mEpoch 33 Step 300/351 lr 0.100000 loss 1.2063 (1.2452) acc@1 0.5391 (0.5382) acc@5 0.8359 (0.8457)\n",
      "\u001b[32m[2020-06-29 01:26:48] __main__ INFO: \u001b[0mEpoch 33 Step 351/351 lr 0.100000 loss 1.1115 (1.2457) acc@1 0.6016 (0.5392) acc@5 0.8672 (0.8450)\n",
      "\u001b[32m[2020-06-29 01:26:48] __main__ INFO: \u001b[0mElapsed 494.06\n",
      "\u001b[32m[2020-06-29 01:26:48] __main__ INFO: \u001b[0mVal 33\n",
      "\u001b[32m[2020-06-29 01:27:06] __main__ INFO: \u001b[0mEpoch 33 loss 1.5420 acc@1 0.4554 acc@5 0.8300\n",
      "\u001b[32m[2020-06-29 01:27:06] __main__ INFO: \u001b[0mElapsed 17.54\n",
      "\u001b[32m[2020-06-29 01:27:06] __main__ INFO: \u001b[0mTrain 34 11583\n",
      "\u001b[32m[2020-06-29 01:29:27] __main__ INFO: \u001b[0mEpoch 34 Step 100/351 lr 0.100000 loss 1.1372 (1.2361) acc@1 0.5938 (0.5444) acc@5 0.8359 (0.8464)\n",
      "\u001b[32m[2020-06-29 01:31:47] __main__ INFO: \u001b[0mEpoch 34 Step 200/351 lr 0.100000 loss 1.2244 (1.2438) acc@1 0.5391 (0.5414) acc@5 0.8281 (0.8437)\n",
      "\u001b[32m[2020-06-29 01:34:08] __main__ INFO: \u001b[0mEpoch 34 Step 300/351 lr 0.100000 loss 1.1532 (1.2478) acc@1 0.5312 (0.5404) acc@5 0.8906 (0.8417)\n",
      "\u001b[32m[2020-06-29 01:35:20] __main__ INFO: \u001b[0mEpoch 34 Step 351/351 lr 0.100000 loss 1.2161 (1.2468) acc@1 0.5547 (0.5399) acc@5 0.8438 (0.8425)\n",
      "\u001b[32m[2020-06-29 01:35:20] __main__ INFO: \u001b[0mElapsed 494.33\n",
      "\u001b[32m[2020-06-29 01:35:20] __main__ INFO: \u001b[0mVal 34\n",
      "\u001b[32m[2020-06-29 01:35:38] __main__ INFO: \u001b[0mEpoch 34 loss 1.7641 acc@1 0.4230 acc@5 0.8166\n",
      "\u001b[32m[2020-06-29 01:35:38] __main__ INFO: \u001b[0mElapsed 17.55\n",
      "\u001b[32m[2020-06-29 01:35:38] __main__ INFO: \u001b[0mTrain 35 11934\n",
      "\u001b[32m[2020-06-29 01:37:58] __main__ INFO: \u001b[0mEpoch 35 Step 100/351 lr 0.100000 loss 1.2256 (1.2263) acc@1 0.5312 (0.5481) acc@5 0.8438 (0.8438)\n",
      "\u001b[32m[2020-06-29 01:40:19] __main__ INFO: \u001b[0mEpoch 35 Step 200/351 lr 0.100000 loss 1.0866 (1.2368) acc@1 0.6016 (0.5441) acc@5 0.8594 (0.8439)\n",
      "\u001b[32m[2020-06-29 01:42:40] __main__ INFO: \u001b[0mEpoch 35 Step 300/351 lr 0.100000 loss 1.1871 (1.2457) acc@1 0.5781 (0.5416) acc@5 0.8594 (0.8432)\n",
      "\u001b[32m[2020-06-29 01:43:52] __main__ INFO: \u001b[0mEpoch 35 Step 351/351 lr 0.100000 loss 1.2487 (1.2472) acc@1 0.5391 (0.5406) acc@5 0.8672 (0.8439)\n",
      "\u001b[32m[2020-06-29 01:43:52] __main__ INFO: \u001b[0mElapsed 494.13\n",
      "\u001b[32m[2020-06-29 01:43:52] __main__ INFO: \u001b[0mVal 35\n",
      "\u001b[32m[2020-06-29 01:44:09] __main__ INFO: \u001b[0mEpoch 35 loss 1.4390 acc@1 0.4754 acc@5 0.8344\n",
      "\u001b[32m[2020-06-29 01:44:09] __main__ INFO: \u001b[0mElapsed 17.56\n",
      "\u001b[32m[2020-06-29 01:44:09] __main__ INFO: \u001b[0mTrain 36 12285\n",
      "\u001b[32m[2020-06-29 01:46:30] __main__ INFO: \u001b[0mEpoch 36 Step 100/351 lr 0.100000 loss 1.3122 (1.2251) acc@1 0.5469 (0.5500) acc@5 0.8203 (0.8480)\n",
      "\u001b[32m[2020-06-29 01:48:51] __main__ INFO: \u001b[0mEpoch 36 Step 200/351 lr 0.100000 loss 1.4268 (1.2325) acc@1 0.4453 (0.5450) acc@5 0.7812 (0.8450)\n",
      "\u001b[32m[2020-06-29 01:51:12] __main__ INFO: \u001b[0mEpoch 36 Step 300/351 lr 0.100000 loss 1.2882 (1.2367) acc@1 0.5234 (0.5431) acc@5 0.8359 (0.8456)\n",
      "\u001b[32m[2020-06-29 01:52:24] __main__ INFO: \u001b[0mEpoch 36 Step 351/351 lr 0.100000 loss 1.2281 (1.2385) acc@1 0.5469 (0.5418) acc@5 0.8750 (0.8448)\n",
      "\u001b[32m[2020-06-29 01:52:24] __main__ INFO: \u001b[0mElapsed 494.61\n",
      "\u001b[32m[2020-06-29 01:52:24] __main__ INFO: \u001b[0mVal 36\n",
      "\u001b[32m[2020-06-29 01:52:41] __main__ INFO: \u001b[0mEpoch 36 loss 1.5905 acc@1 0.4464 acc@5 0.8168\n",
      "\u001b[32m[2020-06-29 01:52:41] __main__ INFO: \u001b[0mElapsed 17.57\n",
      "\u001b[32m[2020-06-29 01:52:41] __main__ INFO: \u001b[0mTrain 37 12636\n",
      "\u001b[32m[2020-06-29 01:55:03] __main__ INFO: \u001b[0mEpoch 37 Step 100/351 lr 0.100000 loss 1.1681 (1.2207) acc@1 0.5469 (0.5516) acc@5 0.8359 (0.8456)\n",
      "\u001b[32m[2020-06-29 01:57:23] __main__ INFO: \u001b[0mEpoch 37 Step 200/351 lr 0.100000 loss 1.1341 (1.2255) acc@1 0.5547 (0.5487) acc@5 0.8672 (0.8459)\n",
      "\u001b[32m[2020-06-29 01:59:44] __main__ INFO: \u001b[0mEpoch 37 Step 300/351 lr 0.100000 loss 1.1994 (1.2335) acc@1 0.5391 (0.5472) acc@5 0.8125 (0.8453)\n",
      "\u001b[32m[2020-06-29 02:00:56] __main__ INFO: \u001b[0mEpoch 37 Step 351/351 lr 0.100000 loss 1.2477 (1.2336) acc@1 0.5234 (0.5469) acc@5 0.8438 (0.8459)\n",
      "\u001b[32m[2020-06-29 02:00:56] __main__ INFO: \u001b[0mElapsed 494.41\n",
      "\u001b[32m[2020-06-29 02:00:56] __main__ INFO: \u001b[0mVal 37\n",
      "\u001b[32m[2020-06-29 02:01:13] __main__ INFO: \u001b[0mEpoch 37 loss 1.4788 acc@1 0.4804 acc@5 0.8430\n",
      "\u001b[32m[2020-06-29 02:01:13] __main__ INFO: \u001b[0mElapsed 17.57\n",
      "\u001b[32m[2020-06-29 02:01:13] __main__ INFO: \u001b[0mTrain 38 12987\n",
      "\u001b[32m[2020-06-29 02:03:34] __main__ INFO: \u001b[0mEpoch 38 Step 100/351 lr 0.100000 loss 1.3002 (1.2236) acc@1 0.5312 (0.5527) acc@5 0.7812 (0.8478)\n",
      "\u001b[32m[2020-06-29 02:05:55] __main__ INFO: \u001b[0mEpoch 38 Step 200/351 lr 0.100000 loss 1.3613 (1.2336) acc@1 0.4844 (0.5479) acc@5 0.7891 (0.8457)\n",
      "\u001b[32m[2020-06-29 02:08:16] __main__ INFO: \u001b[0mEpoch 38 Step 300/351 lr 0.100000 loss 1.1778 (1.2345) acc@1 0.5625 (0.5476) acc@5 0.8359 (0.8442)\n",
      "\u001b[32m[2020-06-29 02:09:28] __main__ INFO: \u001b[0mEpoch 38 Step 351/351 lr 0.100000 loss 1.1729 (1.2335) acc@1 0.6016 (0.5475) acc@5 0.8359 (0.8441)\n",
      "\u001b[32m[2020-06-29 02:09:28] __main__ INFO: \u001b[0mElapsed 494.48\n",
      "\u001b[32m[2020-06-29 02:09:28] __main__ INFO: \u001b[0mVal 38\n",
      "\u001b[32m[2020-06-29 02:09:45] __main__ INFO: \u001b[0mEpoch 38 loss 1.4219 acc@1 0.4896 acc@5 0.8426\n",
      "\u001b[32m[2020-06-29 02:09:45] __main__ INFO: \u001b[0mElapsed 17.56\n",
      "\u001b[32m[2020-06-29 02:09:45] __main__ INFO: \u001b[0mTrain 39 13338\n",
      "\u001b[32m[2020-06-29 02:12:06] __main__ INFO: \u001b[0mEpoch 39 Step 100/351 lr 0.100000 loss 1.1194 (1.2152) acc@1 0.5859 (0.5483) acc@5 0.9141 (0.8498)\n",
      "\u001b[32m[2020-06-29 02:14:27] __main__ INFO: \u001b[0mEpoch 39 Step 200/351 lr 0.100000 loss 1.2492 (1.2262) acc@1 0.5625 (0.5479) acc@5 0.8438 (0.8475)\n",
      "\u001b[32m[2020-06-29 02:16:48] __main__ INFO: \u001b[0mEpoch 39 Step 300/351 lr 0.100000 loss 1.3764 (1.2261) acc@1 0.5234 (0.5460) acc@5 0.8281 (0.8480)\n",
      "\u001b[32m[2020-06-29 02:18:00] __main__ INFO: \u001b[0mEpoch 39 Step 351/351 lr 0.100000 loss 1.2715 (1.2279) acc@1 0.5000 (0.5454) acc@5 0.8203 (0.8480)\n",
      "\u001b[32m[2020-06-29 02:18:00] __main__ INFO: \u001b[0mElapsed 494.56\n",
      "\u001b[32m[2020-06-29 02:18:00] __main__ INFO: \u001b[0mVal 39\n",
      "\u001b[32m[2020-06-29 02:18:18] __main__ INFO: \u001b[0mEpoch 39 loss 1.4721 acc@1 0.4736 acc@5 0.8330\n",
      "\u001b[32m[2020-06-29 02:18:18] __main__ INFO: \u001b[0mElapsed 17.56\n",
      "\u001b[32m[2020-06-29 02:18:18] __main__ INFO: \u001b[0mTrain 40 13689\n",
      "\u001b[32m[2020-06-29 02:20:38] __main__ INFO: \u001b[0mEpoch 40 Step 100/351 lr 0.100000 loss 1.4328 (1.2255) acc@1 0.5156 (0.5444) acc@5 0.8516 (0.8403)\n",
      "\u001b[32m[2020-06-29 02:22:59] __main__ INFO: \u001b[0mEpoch 40 Step 200/351 lr 0.100000 loss 1.1571 (1.2206) acc@1 0.5859 (0.5483) acc@5 0.8281 (0.8441)\n",
      "\u001b[32m[2020-06-29 02:25:20] __main__ INFO: \u001b[0mEpoch 40 Step 300/351 lr 0.100000 loss 1.2839 (1.2222) acc@1 0.5234 (0.5479) acc@5 0.8594 (0.8437)\n",
      "\u001b[32m[2020-06-29 02:26:32] __main__ INFO: \u001b[0mEpoch 40 Step 351/351 lr 0.100000 loss 1.1902 (1.2220) acc@1 0.5703 (0.5487) acc@5 0.8672 (0.8444)\n",
      "\u001b[32m[2020-06-29 02:26:32] __main__ INFO: \u001b[0mElapsed 494.36\n",
      "\u001b[32m[2020-06-29 02:26:32] __main__ INFO: \u001b[0mVal 40\n",
      "\u001b[32m[2020-06-29 02:26:50] __main__ INFO: \u001b[0mEpoch 40 loss 1.5844 acc@1 0.4378 acc@5 0.8318\n",
      "\u001b[32m[2020-06-29 02:26:50] __main__ INFO: \u001b[0mElapsed 17.57\n",
      "\u001b[32m[2020-06-29 02:26:50] __main__ INFO: \u001b[0mTrain 41 14040\n",
      "\u001b[32m[2020-06-29 02:29:10] __main__ INFO: \u001b[0mEpoch 41 Step 100/351 lr 0.100000 loss 1.3924 (1.2001) acc@1 0.4297 (0.5560) acc@5 0.8828 (0.8503)\n",
      "\u001b[32m[2020-06-29 02:31:31] __main__ INFO: \u001b[0mEpoch 41 Step 200/351 lr 0.100000 loss 1.3324 (1.2107) acc@1 0.4844 (0.5541) acc@5 0.8203 (0.8499)\n",
      "\u001b[32m[2020-06-29 02:33:52] __main__ INFO: \u001b[0mEpoch 41 Step 300/351 lr 0.100000 loss 1.2895 (1.2167) acc@1 0.5469 (0.5523) acc@5 0.8516 (0.8498)\n",
      "\u001b[32m[2020-06-29 02:35:04] __main__ INFO: \u001b[0mEpoch 41 Step 351/351 lr 0.100000 loss 1.0720 (1.2216) acc@1 0.6250 (0.5495) acc@5 0.9219 (0.8491)\n",
      "\u001b[32m[2020-06-29 02:35:04] __main__ INFO: \u001b[0mElapsed 494.13\n",
      "\u001b[32m[2020-06-29 02:35:04] __main__ INFO: \u001b[0mVal 41\n",
      "\u001b[32m[2020-06-29 02:35:21] __main__ INFO: \u001b[0mEpoch 41 loss 1.4592 acc@1 0.4936 acc@5 0.8346\n",
      "\u001b[32m[2020-06-29 02:35:21] __main__ INFO: \u001b[0mElapsed 17.57\n",
      "\u001b[32m[2020-06-29 02:35:21] __main__ INFO: \u001b[0mTrain 42 14391\n",
      "\u001b[32m[2020-06-29 02:37:42] __main__ INFO: \u001b[0mEpoch 42 Step 100/351 lr 0.100000 loss 1.1173 (1.2053) acc@1 0.5859 (0.5557) acc@5 0.8594 (0.8480)\n",
      "\u001b[32m[2020-06-29 02:40:03] __main__ INFO: \u001b[0mEpoch 42 Step 200/351 lr 0.100000 loss 1.2812 (1.2224) acc@1 0.5000 (0.5472) acc@5 0.7969 (0.8455)\n",
      "\u001b[32m[2020-06-29 02:42:24] __main__ INFO: \u001b[0mEpoch 42 Step 300/351 lr 0.100000 loss 1.2692 (1.2155) acc@1 0.5078 (0.5505) acc@5 0.8516 (0.8448)\n",
      "\u001b[32m[2020-06-29 02:43:35] __main__ INFO: \u001b[0mEpoch 42 Step 351/351 lr 0.100000 loss 1.4073 (1.2168) acc@1 0.4609 (0.5502) acc@5 0.8594 (0.8453)\n",
      "\u001b[32m[2020-06-29 02:43:35] __main__ INFO: \u001b[0mElapsed 494.18\n",
      "\u001b[32m[2020-06-29 02:43:35] __main__ INFO: \u001b[0mVal 42\n",
      "\u001b[32m[2020-06-29 02:43:53] __main__ INFO: \u001b[0mEpoch 42 loss 1.4738 acc@1 0.4914 acc@5 0.8376\n",
      "\u001b[32m[2020-06-29 02:43:53] __main__ INFO: \u001b[0mElapsed 17.54\n",
      "\u001b[32m[2020-06-29 02:43:53] __main__ INFO: \u001b[0mTrain 43 14742\n",
      "\u001b[32m[2020-06-29 02:46:14] __main__ INFO: \u001b[0mEpoch 43 Step 100/351 lr 0.100000 loss 1.2439 (1.2212) acc@1 0.5000 (0.5470) acc@5 0.7969 (0.8458)\n",
      "\u001b[32m[2020-06-29 02:48:35] __main__ INFO: \u001b[0mEpoch 43 Step 200/351 lr 0.100000 loss 1.1128 (1.2160) acc@1 0.5703 (0.5494) acc@5 0.8984 (0.8475)\n",
      "\u001b[32m[2020-06-29 02:50:55] __main__ INFO: \u001b[0mEpoch 43 Step 300/351 lr 0.100000 loss 1.1621 (1.2116) acc@1 0.5703 (0.5522) acc@5 0.8750 (0.8484)\n",
      "\u001b[32m[2020-06-29 02:52:07] __main__ INFO: \u001b[0mEpoch 43 Step 351/351 lr 0.100000 loss 1.0388 (1.2118) acc@1 0.6172 (0.5522) acc@5 0.8594 (0.8480)\n",
      "\u001b[32m[2020-06-29 02:52:07] __main__ INFO: \u001b[0mElapsed 494.10\n",
      "\u001b[32m[2020-06-29 02:52:07] __main__ INFO: \u001b[0mVal 43\n",
      "\u001b[32m[2020-06-29 02:52:25] __main__ INFO: \u001b[0mEpoch 43 loss 1.4805 acc@1 0.4722 acc@5 0.8318\n",
      "\u001b[32m[2020-06-29 02:52:25] __main__ INFO: \u001b[0mElapsed 17.58\n",
      "\u001b[32m[2020-06-29 02:52:25] __main__ INFO: \u001b[0mTrain 44 15093\n",
      "\u001b[32m[2020-06-29 02:54:45] __main__ INFO: \u001b[0mEpoch 44 Step 100/351 lr 0.100000 loss 1.1180 (1.1861) acc@1 0.5938 (0.5607) acc@5 0.8828 (0.8500)\n",
      "\u001b[32m[2020-06-29 02:57:06] __main__ INFO: \u001b[0mEpoch 44 Step 200/351 lr 0.100000 loss 1.1251 (1.2076) acc@1 0.5703 (0.5529) acc@5 0.8750 (0.8488)\n",
      "\u001b[32m[2020-06-29 02:59:27] __main__ INFO: \u001b[0mEpoch 44 Step 300/351 lr 0.100000 loss 1.3951 (1.2133) acc@1 0.4609 (0.5511) acc@5 0.8438 (0.8482)\n",
      "\u001b[32m[2020-06-29 03:00:39] __main__ INFO: \u001b[0mEpoch 44 Step 351/351 lr 0.100000 loss 1.2194 (1.2131) acc@1 0.5625 (0.5509) acc@5 0.8750 (0.8473)\n",
      "\u001b[32m[2020-06-29 03:00:39] __main__ INFO: \u001b[0mElapsed 494.39\n",
      "\u001b[32m[2020-06-29 03:00:39] __main__ INFO: \u001b[0mVal 44\n",
      "\u001b[32m[2020-06-29 03:00:57] __main__ INFO: \u001b[0mEpoch 44 loss 1.3255 acc@1 0.5128 acc@5 0.8442\n",
      "\u001b[32m[2020-06-29 03:00:57] __main__ INFO: \u001b[0mElapsed 17.56\n",
      "\u001b[32m[2020-06-29 03:00:57] __main__ INFO: \u001b[0mTrain 45 15444\n",
      "\u001b[32m[2020-06-29 03:03:17] __main__ INFO: \u001b[0mEpoch 45 Step 100/351 lr 0.100000 loss 1.0561 (1.1992) acc@1 0.6094 (0.5584) acc@5 0.8438 (0.8470)\n",
      "\u001b[32m[2020-06-29 03:05:38] __main__ INFO: \u001b[0mEpoch 45 Step 200/351 lr 0.100000 loss 1.1511 (1.2025) acc@1 0.5703 (0.5561) acc@5 0.8516 (0.8499)\n",
      "\u001b[32m[2020-06-29 03:07:59] __main__ INFO: \u001b[0mEpoch 45 Step 300/351 lr 0.100000 loss 1.2755 (1.2099) acc@1 0.5000 (0.5552) acc@5 0.8047 (0.8490)\n",
      "\u001b[32m[2020-06-29 03:09:11] __main__ INFO: \u001b[0mEpoch 45 Step 351/351 lr 0.100000 loss 1.2186 (1.2085) acc@1 0.5391 (0.5561) acc@5 0.8203 (0.8476)\n",
      "\u001b[32m[2020-06-29 03:09:11] __main__ INFO: \u001b[0mElapsed 494.10\n",
      "\u001b[32m[2020-06-29 03:09:11] __main__ INFO: \u001b[0mVal 45\n",
      "\u001b[32m[2020-06-29 03:09:28] __main__ INFO: \u001b[0mEpoch 45 loss 1.4840 acc@1 0.4760 acc@5 0.8290\n",
      "\u001b[32m[2020-06-29 03:09:28] __main__ INFO: \u001b[0mElapsed 17.55\n",
      "\u001b[32m[2020-06-29 03:09:28] __main__ INFO: \u001b[0mTrain 46 15795\n",
      "\u001b[32m[2020-06-29 03:11:49] __main__ INFO: \u001b[0mEpoch 46 Step 100/351 lr 0.100000 loss 1.1739 (1.1942) acc@1 0.5547 (0.5547) acc@5 0.8672 (0.8491)\n",
      "\u001b[32m[2020-06-29 03:14:10] __main__ INFO: \u001b[0mEpoch 46 Step 200/351 lr 0.100000 loss 1.2717 (1.2063) acc@1 0.5312 (0.5527) acc@5 0.8594 (0.8477)\n",
      "\u001b[32m[2020-06-29 03:16:30] __main__ INFO: \u001b[0mEpoch 46 Step 300/351 lr 0.100000 loss 1.1938 (1.2023) acc@1 0.5781 (0.5552) acc@5 0.8281 (0.8489)\n",
      "\u001b[32m[2020-06-29 03:17:42] __main__ INFO: \u001b[0mEpoch 46 Step 351/351 lr 0.100000 loss 1.3683 (1.2024) acc@1 0.4375 (0.5549) acc@5 0.7734 (0.8490)\n",
      "\u001b[32m[2020-06-29 03:17:42] __main__ INFO: \u001b[0mElapsed 494.03\n",
      "\u001b[32m[2020-06-29 03:17:42] __main__ INFO: \u001b[0mVal 46\n",
      "\u001b[32m[2020-06-29 03:18:00] __main__ INFO: \u001b[0mEpoch 46 loss 1.5279 acc@1 0.4728 acc@5 0.8356\n",
      "\u001b[32m[2020-06-29 03:18:00] __main__ INFO: \u001b[0mElapsed 17.54\n",
      "\u001b[32m[2020-06-29 03:18:00] __main__ INFO: \u001b[0mTrain 47 16146\n",
      "\u001b[32m[2020-06-29 03:20:21] __main__ INFO: \u001b[0mEpoch 47 Step 100/351 lr 0.100000 loss 1.2577 (1.1900) acc@1 0.5156 (0.5580) acc@5 0.8047 (0.8514)\n",
      "\u001b[32m[2020-06-29 03:22:41] __main__ INFO: \u001b[0mEpoch 47 Step 200/351 lr 0.100000 loss 1.1155 (1.1962) acc@1 0.5703 (0.5571) acc@5 0.8516 (0.8496)\n"
     ]
    }
   ],
   "source": [
    "# Train the model per the settings specified for ResNext 29_4x64d in the original paper\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "!python train.py --config configs/cifar/resnext.yaml \\\n",
    "    model.resnext.cardinality 4 \\\n",
    "    train.batch_size 128 \\\n",
    "    dataset.name CIFAR10_RA_2_5 \\\n",
    "    train.base_lr 0.1 \\\n",
    "    train.output_dir /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_5/exp00 \\\n",
    "    scheduler.epochs 400\n",
    "\n",
    "# Number of epochs should be 300!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-01 13:12:32] __main__ INFO: \u001b[0mdevice: cuda\n",
      "cudnn:\n",
      "  benchmark: True\n",
      "  deterministic: False\n",
      "dataset:\n",
      "  name: CIFAR10\n",
      "  dataset_dir: ~/.torch/datasets/CIFAR10\n",
      "  image_size: 32\n",
      "  n_channels: 3\n",
      "  n_classes: 10\n",
      "model:\n",
      "  type: cifar\n",
      "  name: resnext\n",
      "  init_mode: kaiming_fan_out\n",
      "  vgg:\n",
      "    n_channels: [64, 128, 256, 512, 512]\n",
      "    n_layers: [2, 2, 3, 3, 3]\n",
      "    use_bn: True\n",
      "  resnet:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "  resnet_preact:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "  wrn:\n",
      "    depth: 28\n",
      "    initial_channels: 16\n",
      "    widening_factor: 10\n",
      "    drop_rate: 0.0\n",
      "  densenet:\n",
      "    depth: 100\n",
      "    n_blocks: [6, 12, 24, 16]\n",
      "    block_type: bottleneck\n",
      "    growth_rate: 12\n",
      "    drop_rate: 0.0\n",
      "    compression_rate: 0.5\n",
      "  pyramidnet:\n",
      "    depth: 272\n",
      "    n_blocks: [3, 24, 36, 3]\n",
      "    initial_channels: 16\n",
      "    block_type: bottleneck\n",
      "    alpha: 200\n",
      "  resnext:\n",
      "    depth: 29\n",
      "    n_blocks: [3, 4, 6, 3]\n",
      "    initial_channels: 64\n",
      "    cardinality: 4\n",
      "    base_channels: 64\n",
      "  shake_shake:\n",
      "    depth: 26\n",
      "    initial_channels: 96\n",
      "    shake_forward: True\n",
      "    shake_backward: True\n",
      "    shake_image: True\n",
      "  se_resnet_preact:\n",
      "    depth: 110\n",
      "    initial_channels: 16\n",
      "    se_reduction: 16\n",
      "    block_type: basic\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "train:\n",
      "  checkpoint: /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_5/exp00/checkpoint_00400.pth\n",
      "  resume: False\n",
      "  precision: O0\n",
      "  batch_size: 128\n",
      "  subdivision: 1\n",
      "  optimizer: sgd\n",
      "  base_lr: 0.001\n",
      "  momentum: 0.9\n",
      "  nesterov: True\n",
      "  weight_decay: 0.0005\n",
      "  no_weight_decay_on_bn: False\n",
      "  gradient_clip: 0\n",
      "  start_epoch: 0\n",
      "  seed: 0\n",
      "  val_first: True\n",
      "  val_period: 1\n",
      "  val_ratio: 0.1\n",
      "  use_test_as_val: False\n",
      "  output_dir: /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_5/exp00_resume400_50\n",
      "  log_period: 100\n",
      "  checkpoint_period: 100\n",
      "  use_tensorboard: True\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: True\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "  distributed: False\n",
      "  dist:\n",
      "    backend: nccl\n",
      "    init_method: env://\n",
      "    world_size: -1\n",
      "    node_rank: -1\n",
      "    local_rank: 0\n",
      "    use_sync_bn: False\n",
      "tensorboard:\n",
      "  train_images: False\n",
      "  val_images: False\n",
      "  model_params: False\n",
      "optim:\n",
      "  adam:\n",
      "    betas: (0.9, 0.999)\n",
      "  lars:\n",
      "    eps: 1e-09\n",
      "    threshold: 0.01\n",
      "  adabound:\n",
      "    betas: (0.9, 0.999)\n",
      "    final_lr: 0.1\n",
      "    gamma: 0.001\n",
      "scheduler:\n",
      "  epochs: 50\n",
      "  warmup:\n",
      "    type: none\n",
      "    epochs: 0\n",
      "    start_factor: 0.001\n",
      "    exponent: 4\n",
      "  type: multistep\n",
      "  milestones: [150, 225]\n",
      "  lr_decay: 0.1\n",
      "  lr_min_factor: 0.001\n",
      "  T0: 10\n",
      "  T_mul: 1.0\n",
      "validation:\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: False\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "augmentation:\n",
      "  use_random_crop: True\n",
      "  use_random_horizontal_flip: True\n",
      "  use_cutout: False\n",
      "  use_random_erasing: False\n",
      "  use_dual_cutout: False\n",
      "  use_mixup: False\n",
      "  use_ricap: False\n",
      "  use_cutmix: False\n",
      "  use_label_smoothing: False\n",
      "  random_crop:\n",
      "    padding: 4\n",
      "    fill: 0\n",
      "    padding_mode: constant\n",
      "  random_horizontal_flip:\n",
      "    prob: 0.5\n",
      "  cutout:\n",
      "    prob: 1.0\n",
      "    mask_size: 16\n",
      "    cut_inside: False\n",
      "    mask_color: 0\n",
      "    dual_cutout_alpha: 0.1\n",
      "  random_erasing:\n",
      "    prob: 0.5\n",
      "    area_ratio_range: [0.02, 0.4]\n",
      "    min_aspect_ratio: 0.3\n",
      "    max_attempt: 20\n",
      "  mixup:\n",
      "    alpha: 1.0\n",
      "  ricap:\n",
      "    beta: 0.3\n",
      "  cutmix:\n",
      "    alpha: 1.0\n",
      "  label_smoothing:\n",
      "    epsilon: 0.1\n",
      "tta:\n",
      "  use_resize: False\n",
      "  use_center_crop: False\n",
      "  resize: 256\n",
      "test:\n",
      "  checkpoint: ''\n",
      "  output_dir: ''\n",
      "  batch_size: 128\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    pin_memory: False\n",
      "\u001b[32m[2020-07-01 13:12:32] __main__ INFO: \u001b[0menv_info:\n",
      "  pytorch_version: 1.4.0\n",
      "  cuda_version: 10.1\n",
      "  cudnn_version: 7603\n",
      "  num_gpus: 1\n",
      "  gpu_name: Tesla K80\n",
      "  gpu_capability: 3.7\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /home/ec2-user/.torch/datasets/CIFAR10/cifar-10-python.tar.gz\n",
      "170500096it [00:02, 71032290.57it/s]                                            \n",
      "Extracting /home/ec2-user/.torch/datasets/CIFAR10/cifar-10-python.tar.gz to /home/ec2-user/.torch/datasets/CIFAR10\n",
      "\u001b[32m[2020-07-01 13:12:41] __main__ INFO: \u001b[0mMACs  : 2.75G\n",
      "\u001b[32m[2020-07-01 13:12:41] __main__ INFO: \u001b[0m#params: 17.56M\n",
      "Selected optimization level O0:  Pure FP32 training.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\",)\n",
      "\u001b[32m[2020-07-01 13:12:41] __main__ INFO: \u001b[0mVal 0\n",
      "\u001b[32m[2020-07-01 13:13:06] __main__ INFO: \u001b[0mEpoch 0 loss 0.4004 acc@1 0.8850 acc@5 0.9950\n",
      "\u001b[32m[2020-07-01 13:13:06] __main__ INFO: \u001b[0mElapsed 25.51\n",
      "\u001b[32m[2020-07-01 13:13:06] __main__ INFO: \u001b[0mTrain 1 0\n",
      "\u001b[32m[2020-07-01 13:15:35] __main__ INFO: \u001b[0mEpoch 1 Step 100/351 lr 0.001000 loss 0.0915 (0.1493) acc@1 0.9609 (0.9555) acc@5 1.0000 (0.9990)\n",
      "\u001b[32m[2020-07-01 13:17:55] __main__ INFO: \u001b[0mEpoch 1 Step 200/351 lr 0.001000 loss 0.1350 (0.1458) acc@1 0.9609 (0.9562) acc@5 1.0000 (0.9989)\n",
      "\u001b[32m[2020-07-01 13:20:16] __main__ INFO: \u001b[0mEpoch 1 Step 300/351 lr 0.001000 loss 0.0747 (0.1413) acc@1 0.9766 (0.9558) acc@5 1.0000 (0.9989)\n",
      "\u001b[32m[2020-07-01 13:21:28] __main__ INFO: \u001b[0mEpoch 1 Step 351/351 lr 0.001000 loss 0.1007 (0.1406) acc@1 0.9688 (0.9559) acc@5 1.0000 (0.9989)\n",
      "\u001b[32m[2020-07-01 13:21:28] __main__ INFO: \u001b[0mElapsed 502.15\n",
      "\u001b[32m[2020-07-01 13:21:28] __main__ INFO: \u001b[0mVal 1\n",
      "\u001b[32m[2020-07-01 13:21:46] __main__ INFO: \u001b[0mEpoch 1 loss 0.2523 acc@1 0.9202 acc@5 0.9978\n",
      "\u001b[32m[2020-07-01 13:21:46] __main__ INFO: \u001b[0mElapsed 17.63\n",
      "\u001b[32m[2020-07-01 13:21:46] __main__ INFO: \u001b[0mTrain 2 351\n",
      "\u001b[32m[2020-07-01 13:24:07] __main__ INFO: \u001b[0mEpoch 2 Step 100/351 lr 0.001000 loss 0.0467 (0.0908) acc@1 1.0000 (0.9729) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-07-01 13:26:28] __main__ INFO: \u001b[0mEpoch 2 Step 200/351 lr 0.001000 loss 0.1204 (0.0919) acc@1 0.9531 (0.9730) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-07-01 13:28:49] __main__ INFO: \u001b[0mEpoch 2 Step 300/351 lr 0.001000 loss 0.0306 (0.0931) acc@1 0.9922 (0.9724) acc@5 1.0000 (0.9995)\n",
      "\u001b[32m[2020-07-01 13:30:01] __main__ INFO: \u001b[0mEpoch 2 Step 351/351 lr 0.001000 loss 0.1446 (0.0929) acc@1 0.9609 (0.9724) acc@5 1.0000 (0.9994)\n",
      "\u001b[32m[2020-07-01 13:30:01] __main__ INFO: \u001b[0mElapsed 495.37\n",
      "\u001b[32m[2020-07-01 13:30:01] __main__ INFO: \u001b[0mVal 2\n",
      "\u001b[32m[2020-07-01 13:30:19] __main__ INFO: \u001b[0mEpoch 2 loss 0.2415 acc@1 0.9226 acc@5 0.9982\n",
      "\u001b[32m[2020-07-01 13:30:19] __main__ INFO: \u001b[0mElapsed 17.66\n",
      "\u001b[32m[2020-07-01 13:30:19] __main__ INFO: \u001b[0mTrain 3 702\n",
      "\u001b[32m[2020-07-01 13:32:40] __main__ INFO: \u001b[0mEpoch 3 Step 100/351 lr 0.001000 loss 0.0721 (0.0730) acc@1 0.9688 (0.9801) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-01 13:35:01] __main__ INFO: \u001b[0mEpoch 3 Step 200/351 lr 0.001000 loss 0.1263 (0.0716) acc@1 0.9609 (0.9796) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-01 13:37:22] __main__ INFO: \u001b[0mEpoch 3 Step 300/351 lr 0.001000 loss 0.0592 (0.0709) acc@1 0.9688 (0.9796) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-07-01 13:38:34] __main__ INFO: \u001b[0mEpoch 3 Step 351/351 lr 0.001000 loss 0.0654 (0.0706) acc@1 0.9766 (0.9797) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-01 13:38:34] __main__ INFO: \u001b[0mElapsed 494.67\n",
      "\u001b[32m[2020-07-01 13:38:34] __main__ INFO: \u001b[0mVal 3\n",
      "\u001b[32m[2020-07-01 13:38:51] __main__ INFO: \u001b[0mEpoch 3 loss 0.2482 acc@1 0.9228 acc@5 0.9982\n",
      "\u001b[32m[2020-07-01 13:38:51] __main__ INFO: \u001b[0mElapsed 17.64\n",
      "\u001b[32m[2020-07-01 13:38:51] __main__ INFO: \u001b[0mTrain 4 1053\n",
      "\u001b[32m[2020-07-01 13:41:13] __main__ INFO: \u001b[0mEpoch 4 Step 100/351 lr 0.001000 loss 0.0343 (0.0566) acc@1 1.0000 (0.9859) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-01 13:43:34] __main__ INFO: \u001b[0mEpoch 4 Step 200/351 lr 0.001000 loss 0.0268 (0.0538) acc@1 1.0000 (0.9873) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-01 13:45:55] __main__ INFO: \u001b[0mEpoch 4 Step 300/351 lr 0.001000 loss 0.0592 (0.0533) acc@1 0.9844 (0.9867) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-01 13:47:07] __main__ INFO: \u001b[0mEpoch 4 Step 351/351 lr 0.001000 loss 0.0446 (0.0536) acc@1 0.9922 (0.9866) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-01 13:47:07] __main__ INFO: \u001b[0mElapsed 495.34\n",
      "\u001b[32m[2020-07-01 13:47:07] __main__ INFO: \u001b[0mVal 4\n",
      "\u001b[32m[2020-07-01 13:47:24] __main__ INFO: \u001b[0mEpoch 4 loss 0.2383 acc@1 0.9250 acc@5 0.9984\n",
      "\u001b[32m[2020-07-01 13:47:24] __main__ INFO: \u001b[0mElapsed 17.66\n",
      "\u001b[32m[2020-07-01 13:47:24] __main__ INFO: \u001b[0mTrain 5 1404\n",
      "\u001b[32m[2020-07-01 13:49:46] __main__ INFO: \u001b[0mEpoch 5 Step 100/351 lr 0.001000 loss 0.0208 (0.0413) acc@1 1.0000 (0.9916) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-01 13:52:07] __main__ INFO: \u001b[0mEpoch 5 Step 200/351 lr 0.001000 loss 0.0549 (0.0419) acc@1 0.9922 (0.9911) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-01 13:54:28] __main__ INFO: \u001b[0mEpoch 5 Step 300/351 lr 0.001000 loss 0.0223 (0.0429) acc@1 1.0000 (0.9907) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-01 13:55:40] __main__ INFO: \u001b[0mEpoch 5 Step 351/351 lr 0.001000 loss 0.0293 (0.0436) acc@1 1.0000 (0.9904) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-01 13:55:40] __main__ INFO: \u001b[0mElapsed 495.43\n",
      "\u001b[32m[2020-07-01 13:55:40] __main__ INFO: \u001b[0mVal 5\n",
      "\u001b[32m[2020-07-01 13:55:57] __main__ INFO: \u001b[0mEpoch 5 loss 0.2430 acc@1 0.9258 acc@5 0.9980\n",
      "\u001b[32m[2020-07-01 13:55:57] __main__ INFO: \u001b[0mElapsed 17.63\n",
      "\u001b[32m[2020-07-01 13:55:57] __main__ INFO: \u001b[0mTrain 6 1755\n",
      "\u001b[32m[2020-07-01 13:58:18] __main__ INFO: \u001b[0mEpoch 6 Step 100/351 lr 0.001000 loss 0.0329 (0.0345) acc@1 0.9922 (0.9946) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 14:00:39] __main__ INFO: \u001b[0mEpoch 6 Step 200/351 lr 0.001000 loss 0.0289 (0.0357) acc@1 1.0000 (0.9936) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-01 14:03:00] __main__ INFO: \u001b[0mEpoch 6 Step 300/351 lr 0.001000 loss 0.0374 (0.0366) acc@1 1.0000 (0.9932) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-01 14:04:12] __main__ INFO: \u001b[0mEpoch 6 Step 351/351 lr 0.001000 loss 0.0534 (0.0362) acc@1 0.9922 (0.9933) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-01 14:04:12] __main__ INFO: \u001b[0mElapsed 494.63\n",
      "\u001b[32m[2020-07-01 14:04:12] __main__ INFO: \u001b[0mVal 6\n",
      "\u001b[32m[2020-07-01 14:04:30] __main__ INFO: \u001b[0mEpoch 6 loss 0.2378 acc@1 0.9258 acc@5 0.9980\n",
      "\u001b[32m[2020-07-01 14:04:30] __main__ INFO: \u001b[0mElapsed 17.64\n",
      "\u001b[32m[2020-07-01 14:04:30] __main__ INFO: \u001b[0mTrain 7 2106\n",
      "\u001b[32m[2020-07-01 14:06:51] __main__ INFO: \u001b[0mEpoch 7 Step 100/351 lr 0.001000 loss 0.0258 (0.0298) acc@1 1.0000 (0.9950) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 14:09:12] __main__ INFO: \u001b[0mEpoch 7 Step 200/351 lr 0.001000 loss 0.0208 (0.0301) acc@1 1.0000 (0.9947) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 14:11:34] __main__ INFO: \u001b[0mEpoch 7 Step 300/351 lr 0.001000 loss 0.0170 (0.0300) acc@1 1.0000 (0.9948) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 14:12:46] __main__ INFO: \u001b[0mEpoch 7 Step 351/351 lr 0.001000 loss 0.0262 (0.0304) acc@1 0.9922 (0.9947) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 14:12:46] __main__ INFO: \u001b[0mElapsed 496.28\n",
      "\u001b[32m[2020-07-01 14:12:46] __main__ INFO: \u001b[0mVal 7\n",
      "\u001b[32m[2020-07-01 14:13:04] __main__ INFO: \u001b[0mEpoch 7 loss 0.2396 acc@1 0.9266 acc@5 0.9982\n",
      "\u001b[32m[2020-07-01 14:13:04] __main__ INFO: \u001b[0mElapsed 17.80\n",
      "\u001b[32m[2020-07-01 14:13:04] __main__ INFO: \u001b[0mTrain 8 2457\n",
      "\u001b[32m[2020-07-01 14:15:26] __main__ INFO: \u001b[0mEpoch 8 Step 100/351 lr 0.001000 loss 0.0389 (0.0267) acc@1 0.9922 (0.9952) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 14:17:47] __main__ INFO: \u001b[0mEpoch 8 Step 200/351 lr 0.001000 loss 0.0286 (0.0266) acc@1 1.0000 (0.9955) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 14:20:09] __main__ INFO: \u001b[0mEpoch 8 Step 300/351 lr 0.001000 loss 0.0319 (0.0266) acc@1 0.9922 (0.9955) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 14:21:21] __main__ INFO: \u001b[0mEpoch 8 Step 351/351 lr 0.001000 loss 0.0235 (0.0270) acc@1 0.9922 (0.9953) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 14:21:21] __main__ INFO: \u001b[0mElapsed 497.63\n",
      "\u001b[32m[2020-07-01 14:21:21] __main__ INFO: \u001b[0mVal 8\n",
      "\u001b[32m[2020-07-01 14:21:39] __main__ INFO: \u001b[0mEpoch 8 loss 0.2364 acc@1 0.9290 acc@5 0.9976\n",
      "\u001b[32m[2020-07-01 14:21:39] __main__ INFO: \u001b[0mElapsed 17.77\n",
      "\u001b[32m[2020-07-01 14:21:39] __main__ INFO: \u001b[0mTrain 9 2808\n",
      "\u001b[32m[2020-07-01 14:24:01] __main__ INFO: \u001b[0mEpoch 9 Step 100/351 lr 0.001000 loss 0.0218 (0.0222) acc@1 1.0000 (0.9977) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 14:26:23] __main__ INFO: \u001b[0mEpoch 9 Step 200/351 lr 0.001000 loss 0.0287 (0.0224) acc@1 0.9922 (0.9973) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 14:28:44] __main__ INFO: \u001b[0mEpoch 9 Step 300/351 lr 0.001000 loss 0.0203 (0.0224) acc@1 1.0000 (0.9971) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 14:29:57] __main__ INFO: \u001b[0mEpoch 9 Step 351/351 lr 0.001000 loss 0.0339 (0.0229) acc@1 0.9922 (0.9969) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 14:29:57] __main__ INFO: \u001b[0mElapsed 497.56\n",
      "\u001b[32m[2020-07-01 14:29:57] __main__ INFO: \u001b[0mVal 9\n",
      "\u001b[32m[2020-07-01 14:30:14] __main__ INFO: \u001b[0mEpoch 9 loss 0.2374 acc@1 0.9244 acc@5 0.9980\n",
      "\u001b[32m[2020-07-01 14:30:14] __main__ INFO: \u001b[0mElapsed 17.75\n",
      "\u001b[32m[2020-07-01 14:30:14] __main__ INFO: \u001b[0mTrain 10 3159\n",
      "\u001b[32m[2020-07-01 14:32:36] __main__ INFO: \u001b[0mEpoch 10 Step 100/351 lr 0.001000 loss 0.0197 (0.0189) acc@1 1.0000 (0.9977) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 14:34:58] __main__ INFO: \u001b[0mEpoch 10 Step 200/351 lr 0.001000 loss 0.0222 (0.0189) acc@1 1.0000 (0.9977) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 14:37:20] __main__ INFO: \u001b[0mEpoch 10 Step 300/351 lr 0.001000 loss 0.0146 (0.0193) acc@1 1.0000 (0.9977) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 14:38:32] __main__ INFO: \u001b[0mEpoch 10 Step 351/351 lr 0.001000 loss 0.0338 (0.0199) acc@1 0.9922 (0.9974) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 14:38:32] __main__ INFO: \u001b[0mElapsed 497.53\n",
      "\u001b[32m[2020-07-01 14:38:32] __main__ INFO: \u001b[0mVal 10\n",
      "\u001b[32m[2020-07-01 14:38:50] __main__ INFO: \u001b[0mEpoch 10 loss 0.2376 acc@1 0.9280 acc@5 0.9976\n",
      "\u001b[32m[2020-07-01 14:38:50] __main__ INFO: \u001b[0mElapsed 17.74\n",
      "\u001b[32m[2020-07-01 14:38:50] __main__ INFO: \u001b[0mTrain 11 3510\n",
      "\u001b[32m[2020-07-01 14:41:12] __main__ INFO: \u001b[0mEpoch 11 Step 100/351 lr 0.001000 loss 0.0126 (0.0166) acc@1 1.0000 (0.9985) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 14:43:33] __main__ INFO: \u001b[0mEpoch 11 Step 200/351 lr 0.001000 loss 0.0254 (0.0173) acc@1 0.9844 (0.9983) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 14:45:55] __main__ INFO: \u001b[0mEpoch 11 Step 300/351 lr 0.001000 loss 0.0330 (0.0170) acc@1 0.9844 (0.9984) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 14:47:07] __main__ INFO: \u001b[0mEpoch 11 Step 351/351 lr 0.001000 loss 0.0090 (0.0176) acc@1 1.0000 (0.9982) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 14:47:07] __main__ INFO: \u001b[0mElapsed 497.75\n",
      "\u001b[32m[2020-07-01 14:47:07] __main__ INFO: \u001b[0mVal 11\n",
      "\u001b[32m[2020-07-01 14:47:25] __main__ INFO: \u001b[0mEpoch 11 loss 0.2408 acc@1 0.9262 acc@5 0.9980\n",
      "\u001b[32m[2020-07-01 14:47:25] __main__ INFO: \u001b[0mElapsed 17.76\n",
      "\u001b[32m[2020-07-01 14:47:25] __main__ INFO: \u001b[0mTrain 12 3861\n",
      "\u001b[32m[2020-07-01 14:49:47] __main__ INFO: \u001b[0mEpoch 12 Step 100/351 lr 0.001000 loss 0.0135 (0.0170) acc@1 1.0000 (0.9982) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 14:52:09] __main__ INFO: \u001b[0mEpoch 12 Step 200/351 lr 0.001000 loss 0.0127 (0.0167) acc@1 1.0000 (0.9984) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 14:54:31] __main__ INFO: \u001b[0mEpoch 12 Step 300/351 lr 0.001000 loss 0.0196 (0.0171) acc@1 1.0000 (0.9982) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 14:55:43] __main__ INFO: \u001b[0mEpoch 12 Step 351/351 lr 0.001000 loss 0.0119 (0.0171) acc@1 0.9922 (0.9982) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 14:55:44] __main__ INFO: \u001b[0mElapsed 498.28\n",
      "\u001b[32m[2020-07-01 14:55:44] __main__ INFO: \u001b[0mVal 12\n",
      "\u001b[32m[2020-07-01 14:56:01] __main__ INFO: \u001b[0mEpoch 12 loss 0.2430 acc@1 0.9268 acc@5 0.9980\n",
      "\u001b[32m[2020-07-01 14:56:01] __main__ INFO: \u001b[0mElapsed 17.75\n",
      "\u001b[32m[2020-07-01 14:56:01] __main__ INFO: \u001b[0mTrain 13 4212\n",
      "\u001b[32m[2020-07-01 14:58:23] __main__ INFO: \u001b[0mEpoch 13 Step 100/351 lr 0.001000 loss 0.0173 (0.0148) acc@1 1.0000 (0.9986) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 15:00:45] __main__ INFO: \u001b[0mEpoch 13 Step 200/351 lr 0.001000 loss 0.0125 (0.0149) acc@1 1.0000 (0.9987) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 15:03:07] __main__ INFO: \u001b[0mEpoch 13 Step 300/351 lr 0.001000 loss 0.0132 (0.0149) acc@1 1.0000 (0.9988) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 15:04:20] __main__ INFO: \u001b[0mEpoch 13 Step 351/351 lr 0.001000 loss 0.0120 (0.0149) acc@1 1.0000 (0.9988) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 15:04:20] __main__ INFO: \u001b[0mElapsed 498.35\n",
      "\u001b[32m[2020-07-01 15:04:20] __main__ INFO: \u001b[0mVal 13\n",
      "\u001b[32m[2020-07-01 15:04:37] __main__ INFO: \u001b[0mEpoch 13 loss 0.2481 acc@1 0.9258 acc@5 0.9984\n",
      "\u001b[32m[2020-07-01 15:04:37] __main__ INFO: \u001b[0mElapsed 17.77\n",
      "\u001b[32m[2020-07-01 15:04:37] __main__ INFO: \u001b[0mTrain 14 4563\n",
      "\u001b[32m[2020-07-01 15:06:59] __main__ INFO: \u001b[0mEpoch 14 Step 100/351 lr 0.001000 loss 0.0127 (0.0130) acc@1 1.0000 (0.9991) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 15:09:20] __main__ INFO: \u001b[0mEpoch 14 Step 200/351 lr 0.001000 loss 0.0204 (0.0135) acc@1 1.0000 (0.9990) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 15:11:42] __main__ INFO: \u001b[0mEpoch 14 Step 300/351 lr 0.001000 loss 0.0134 (0.0140) acc@1 1.0000 (0.9988) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 15:12:54] __main__ INFO: \u001b[0mEpoch 14 Step 351/351 lr 0.001000 loss 0.0113 (0.0139) acc@1 1.0000 (0.9987) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 15:12:54] __main__ INFO: \u001b[0mElapsed 496.35\n",
      "\u001b[32m[2020-07-01 15:12:54] __main__ INFO: \u001b[0mVal 14\n",
      "\u001b[32m[2020-07-01 15:13:11] __main__ INFO: \u001b[0mEpoch 14 loss 0.2422 acc@1 0.9278 acc@5 0.9988\n",
      "\u001b[32m[2020-07-01 15:13:11] __main__ INFO: \u001b[0mElapsed 17.70\n",
      "\u001b[32m[2020-07-01 15:13:11] __main__ INFO: \u001b[0mTrain 15 4914\n",
      "\u001b[32m[2020-07-01 15:15:33] __main__ INFO: \u001b[0mEpoch 15 Step 100/351 lr 0.001000 loss 0.0094 (0.0121) acc@1 1.0000 (0.9989) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 15:17:55] __main__ INFO: \u001b[0mEpoch 15 Step 200/351 lr 0.001000 loss 0.0083 (0.0128) acc@1 1.0000 (0.9989) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 15:20:17] __main__ INFO: \u001b[0mEpoch 15 Step 300/351 lr 0.001000 loss 0.0101 (0.0130) acc@1 1.0000 (0.9989) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 15:21:29] __main__ INFO: \u001b[0mEpoch 15 Step 351/351 lr 0.001000 loss 0.0113 (0.0128) acc@1 1.0000 (0.9989) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 15:21:29] __main__ INFO: \u001b[0mElapsed 497.98\n",
      "\u001b[32m[2020-07-01 15:21:29] __main__ INFO: \u001b[0mVal 15\n",
      "\u001b[32m[2020-07-01 15:21:47] __main__ INFO: \u001b[0mEpoch 15 loss 0.2401 acc@1 0.9288 acc@5 0.9984\n",
      "\u001b[32m[2020-07-01 15:21:47] __main__ INFO: \u001b[0mElapsed 17.77\n",
      "\u001b[32m[2020-07-01 15:21:47] __main__ INFO: \u001b[0mTrain 16 5265\n",
      "\u001b[32m[2020-07-01 15:24:09] __main__ INFO: \u001b[0mEpoch 16 Step 100/351 lr 0.001000 loss 0.0148 (0.0118) acc@1 0.9922 (0.9989) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 15:26:30] __main__ INFO: \u001b[0mEpoch 16 Step 200/351 lr 0.001000 loss 0.0177 (0.0122) acc@1 1.0000 (0.9990) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 15:28:51] __main__ INFO: \u001b[0mEpoch 16 Step 300/351 lr 0.001000 loss 0.0067 (0.0120) acc@1 1.0000 (0.9989) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 15:30:03] __main__ INFO: \u001b[0mEpoch 16 Step 351/351 lr 0.001000 loss 0.0152 (0.0120) acc@1 1.0000 (0.9989) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 15:30:03] __main__ INFO: \u001b[0mElapsed 495.49\n",
      "\u001b[32m[2020-07-01 15:30:03] __main__ INFO: \u001b[0mVal 16\n",
      "\u001b[32m[2020-07-01 15:30:20] __main__ INFO: \u001b[0mEpoch 16 loss 0.2467 acc@1 0.9264 acc@5 0.9970\n",
      "\u001b[32m[2020-07-01 15:30:20] __main__ INFO: \u001b[0mElapsed 17.61\n",
      "\u001b[32m[2020-07-01 15:30:20] __main__ INFO: \u001b[0mTrain 17 5616\n",
      "\u001b[32m[2020-07-01 15:32:41] __main__ INFO: \u001b[0mEpoch 17 Step 100/351 lr 0.001000 loss 0.0050 (0.0115) acc@1 1.0000 (0.9992) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 15:35:02] __main__ INFO: \u001b[0mEpoch 17 Step 200/351 lr 0.001000 loss 0.0085 (0.0111) acc@1 1.0000 (0.9994) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 15:37:23] __main__ INFO: \u001b[0mEpoch 17 Step 300/351 lr 0.001000 loss 0.0067 (0.0108) acc@1 1.0000 (0.9995) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 15:38:35] __main__ INFO: \u001b[0mEpoch 17 Step 351/351 lr 0.001000 loss 0.0179 (0.0109) acc@1 1.0000 (0.9994) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 15:38:35] __main__ INFO: \u001b[0mElapsed 495.18\n",
      "\u001b[32m[2020-07-01 15:38:35] __main__ INFO: \u001b[0mVal 17\n",
      "\u001b[32m[2020-07-01 15:38:53] __main__ INFO: \u001b[0mEpoch 17 loss 0.2430 acc@1 0.9284 acc@5 0.9982\n",
      "\u001b[32m[2020-07-01 15:38:53] __main__ INFO: \u001b[0mElapsed 17.70\n",
      "\u001b[32m[2020-07-01 15:38:53] __main__ INFO: \u001b[0mTrain 18 5967\n",
      "\u001b[32m[2020-07-01 15:41:14] __main__ INFO: \u001b[0mEpoch 18 Step 100/351 lr 0.001000 loss 0.0256 (0.0105) acc@1 0.9922 (0.9995) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 15:43:36] __main__ INFO: \u001b[0mEpoch 18 Step 200/351 lr 0.001000 loss 0.0073 (0.0105) acc@1 1.0000 (0.9993) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 15:45:57] __main__ INFO: \u001b[0mEpoch 18 Step 300/351 lr 0.001000 loss 0.0058 (0.0104) acc@1 1.0000 (0.9992) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 15:47:09] __main__ INFO: \u001b[0mEpoch 18 Step 351/351 lr 0.001000 loss 0.0084 (0.0104) acc@1 1.0000 (0.9992) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 15:47:09] __main__ INFO: \u001b[0mElapsed 495.57\n",
      "\u001b[32m[2020-07-01 15:47:09] __main__ INFO: \u001b[0mVal 18\n",
      "\u001b[32m[2020-07-01 15:47:26] __main__ INFO: \u001b[0mEpoch 18 loss 0.2415 acc@1 0.9274 acc@5 0.9982\n",
      "\u001b[32m[2020-07-01 15:47:26] __main__ INFO: \u001b[0mElapsed 17.66\n",
      "\u001b[32m[2020-07-01 15:47:26] __main__ INFO: \u001b[0mTrain 19 6318\n",
      "\u001b[32m[2020-07-01 15:49:47] __main__ INFO: \u001b[0mEpoch 19 Step 100/351 lr 0.001000 loss 0.0057 (0.0089) acc@1 1.0000 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 15:52:08] __main__ INFO: \u001b[0mEpoch 19 Step 200/351 lr 0.001000 loss 0.0087 (0.0090) acc@1 1.0000 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 15:54:29] __main__ INFO: \u001b[0mEpoch 19 Step 300/351 lr 0.001000 loss 0.0544 (0.0095) acc@1 0.9844 (0.9996) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 15:55:41] __main__ INFO: \u001b[0mEpoch 19 Step 351/351 lr 0.001000 loss 0.0083 (0.0097) acc@1 1.0000 (0.9995) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 15:55:41] __main__ INFO: \u001b[0mElapsed 494.67\n",
      "\u001b[32m[2020-07-01 15:55:41] __main__ INFO: \u001b[0mVal 19\n",
      "\u001b[32m[2020-07-01 15:55:59] __main__ INFO: \u001b[0mEpoch 19 loss 0.2391 acc@1 0.9284 acc@5 0.9980\n",
      "\u001b[32m[2020-07-01 15:55:59] __main__ INFO: \u001b[0mElapsed 17.65\n",
      "\u001b[32m[2020-07-01 15:55:59] __main__ INFO: \u001b[0mTrain 20 6669\n",
      "\u001b[32m[2020-07-01 15:58:20] __main__ INFO: \u001b[0mEpoch 20 Step 100/351 lr 0.001000 loss 0.0160 (0.0098) acc@1 0.9922 (0.9994) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 16:00:41] __main__ INFO: \u001b[0mEpoch 20 Step 200/351 lr 0.001000 loss 0.0092 (0.0093) acc@1 1.0000 (0.9995) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 16:03:02] __main__ INFO: \u001b[0mEpoch 20 Step 300/351 lr 0.001000 loss 0.0128 (0.0094) acc@1 1.0000 (0.9995) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 16:04:14] __main__ INFO: \u001b[0mEpoch 20 Step 351/351 lr 0.001000 loss 0.0123 (0.0094) acc@1 1.0000 (0.9995) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 16:04:14] __main__ INFO: \u001b[0mElapsed 494.85\n",
      "\u001b[32m[2020-07-01 16:04:14] __main__ INFO: \u001b[0mVal 20\n",
      "\u001b[32m[2020-07-01 16:04:31] __main__ INFO: \u001b[0mEpoch 20 loss 0.2422 acc@1 0.9278 acc@5 0.9976\n",
      "\u001b[32m[2020-07-01 16:04:31] __main__ INFO: \u001b[0mElapsed 17.65\n",
      "\u001b[32m[2020-07-01 16:04:31] __main__ INFO: \u001b[0mTrain 21 7020\n",
      "\u001b[32m[2020-07-01 16:06:52] __main__ INFO: \u001b[0mEpoch 21 Step 100/351 lr 0.001000 loss 0.0062 (0.0083) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 16:09:13] __main__ INFO: \u001b[0mEpoch 21 Step 200/351 lr 0.001000 loss 0.0176 (0.0080) acc@1 0.9922 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 16:11:34] __main__ INFO: \u001b[0mEpoch 21 Step 300/351 lr 0.001000 loss 0.0057 (0.0083) acc@1 1.0000 (0.9996) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 16:12:46] __main__ INFO: \u001b[0mEpoch 21 Step 351/351 lr 0.001000 loss 0.0062 (0.0084) acc@1 1.0000 (0.9996) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 16:12:46] __main__ INFO: \u001b[0mElapsed 494.53\n",
      "\u001b[32m[2020-07-01 16:12:46] __main__ INFO: \u001b[0mVal 21\n",
      "\u001b[32m[2020-07-01 16:13:03] __main__ INFO: \u001b[0mEpoch 21 loss 0.2414 acc@1 0.9294 acc@5 0.9980\n",
      "\u001b[32m[2020-07-01 16:13:03] __main__ INFO: \u001b[0mElapsed 17.63\n",
      "\u001b[32m[2020-07-01 16:13:03] __main__ INFO: \u001b[0mTrain 22 7371\n",
      "\u001b[32m[2020-07-01 16:15:24] __main__ INFO: \u001b[0mEpoch 22 Step 100/351 lr 0.001000 loss 0.0064 (0.0082) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 16:17:45] __main__ INFO: \u001b[0mEpoch 22 Step 200/351 lr 0.001000 loss 0.0068 (0.0084) acc@1 1.0000 (0.9996) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 16:20:06] __main__ INFO: \u001b[0mEpoch 22 Step 300/351 lr 0.001000 loss 0.0073 (0.0084) acc@1 1.0000 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 16:21:18] __main__ INFO: \u001b[0mEpoch 22 Step 351/351 lr 0.001000 loss 0.0088 (0.0084) acc@1 1.0000 (0.9996) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 16:21:18] __main__ INFO: \u001b[0mElapsed 494.82\n",
      "\u001b[32m[2020-07-01 16:21:18] __main__ INFO: \u001b[0mVal 22\n",
      "\u001b[32m[2020-07-01 16:21:36] __main__ INFO: \u001b[0mEpoch 22 loss 0.2419 acc@1 0.9300 acc@5 0.9982\n",
      "\u001b[32m[2020-07-01 16:21:36] __main__ INFO: \u001b[0mElapsed 17.63\n",
      "\u001b[32m[2020-07-01 16:21:36] __main__ INFO: \u001b[0mTrain 23 7722\n",
      "\u001b[32m[2020-07-01 16:23:57] __main__ INFO: \u001b[0mEpoch 23 Step 100/351 lr 0.001000 loss 0.0184 (0.0078) acc@1 0.9922 (0.9995) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 16:26:18] __main__ INFO: \u001b[0mEpoch 23 Step 200/351 lr 0.001000 loss 0.0135 (0.0079) acc@1 1.0000 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 16:28:39] __main__ INFO: \u001b[0mEpoch 23 Step 300/351 lr 0.001000 loss 0.0092 (0.0080) acc@1 1.0000 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 16:29:51] __main__ INFO: \u001b[0mEpoch 23 Step 351/351 lr 0.001000 loss 0.0042 (0.0080) acc@1 1.0000 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 16:29:51] __main__ INFO: \u001b[0mElapsed 495.04\n",
      "\u001b[32m[2020-07-01 16:29:51] __main__ INFO: \u001b[0mVal 23\n",
      "\u001b[32m[2020-07-01 16:30:09] __main__ INFO: \u001b[0mEpoch 23 loss 0.2413 acc@1 0.9280 acc@5 0.9980\n",
      "\u001b[32m[2020-07-01 16:30:09] __main__ INFO: \u001b[0mElapsed 17.65\n",
      "\u001b[32m[2020-07-01 16:30:09] __main__ INFO: \u001b[0mTrain 24 8073\n",
      "\u001b[32m[2020-07-01 16:32:29] __main__ INFO: \u001b[0mEpoch 24 Step 100/351 lr 0.001000 loss 0.0130 (0.0076) acc@1 1.0000 (0.9995) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 16:34:50] __main__ INFO: \u001b[0mEpoch 24 Step 200/351 lr 0.001000 loss 0.0148 (0.0078) acc@1 1.0000 (0.9996) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 16:37:11] __main__ INFO: \u001b[0mEpoch 24 Step 300/351 lr 0.001000 loss 0.0060 (0.0077) acc@1 1.0000 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 16:38:23] __main__ INFO: \u001b[0mEpoch 24 Step 351/351 lr 0.001000 loss 0.0123 (0.0079) acc@1 1.0000 (0.9996) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 16:38:23] __main__ INFO: \u001b[0mElapsed 494.31\n",
      "\u001b[32m[2020-07-01 16:38:23] __main__ INFO: \u001b[0mVal 24\n",
      "\u001b[32m[2020-07-01 16:38:41] __main__ INFO: \u001b[0mEpoch 24 loss 0.2423 acc@1 0.9298 acc@5 0.9984\n",
      "\u001b[32m[2020-07-01 16:38:41] __main__ INFO: \u001b[0mElapsed 17.64\n",
      "\u001b[32m[2020-07-01 16:38:41] __main__ INFO: \u001b[0mTrain 25 8424\n",
      "\u001b[32m[2020-07-01 16:41:01] __main__ INFO: \u001b[0mEpoch 25 Step 100/351 lr 0.001000 loss 0.0046 (0.0072) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 16:43:22] __main__ INFO: \u001b[0mEpoch 25 Step 200/351 lr 0.001000 loss 0.0117 (0.0072) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 16:45:43] __main__ INFO: \u001b[0mEpoch 25 Step 300/351 lr 0.001000 loss 0.0035 (0.0072) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 16:46:55] __main__ INFO: \u001b[0mEpoch 25 Step 351/351 lr 0.001000 loss 0.0038 (0.0072) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 16:46:55] __main__ INFO: \u001b[0mElapsed 494.75\n",
      "\u001b[32m[2020-07-01 16:46:55] __main__ INFO: \u001b[0mVal 25\n",
      "\u001b[32m[2020-07-01 16:47:13] __main__ INFO: \u001b[0mEpoch 25 loss 0.2433 acc@1 0.9290 acc@5 0.9982\n",
      "\u001b[32m[2020-07-01 16:47:13] __main__ INFO: \u001b[0mElapsed 17.65\n",
      "\u001b[32m[2020-07-01 16:47:13] __main__ INFO: \u001b[0mTrain 26 8775\n",
      "\u001b[32m[2020-07-01 16:49:34] __main__ INFO: \u001b[0mEpoch 26 Step 100/351 lr 0.001000 loss 0.0042 (0.0068) acc@1 1.0000 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 16:51:55] __main__ INFO: \u001b[0mEpoch 26 Step 200/351 lr 0.001000 loss 0.0055 (0.0066) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 16:54:17] __main__ INFO: \u001b[0mEpoch 26 Step 300/351 lr 0.001000 loss 0.0092 (0.0067) acc@1 1.0000 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 16:55:29] __main__ INFO: \u001b[0mEpoch 26 Step 351/351 lr 0.001000 loss 0.0194 (0.0068) acc@1 0.9922 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 16:55:29] __main__ INFO: \u001b[0mElapsed 496.09\n",
      "\u001b[32m[2020-07-01 16:55:29] __main__ INFO: \u001b[0mVal 26\n",
      "\u001b[32m[2020-07-01 16:55:47] __main__ INFO: \u001b[0mEpoch 26 loss 0.2429 acc@1 0.9302 acc@5 0.9982\n",
      "\u001b[32m[2020-07-01 16:55:47] __main__ INFO: \u001b[0mElapsed 17.80\n",
      "\u001b[32m[2020-07-01 16:55:47] __main__ INFO: \u001b[0mTrain 27 9126\n",
      "\u001b[32m[2020-07-01 16:58:09] __main__ INFO: \u001b[0mEpoch 27 Step 100/351 lr 0.001000 loss 0.0040 (0.0062) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 17:00:30] __main__ INFO: \u001b[0mEpoch 27 Step 200/351 lr 0.001000 loss 0.0080 (0.0063) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 17:02:52] __main__ INFO: \u001b[0mEpoch 27 Step 300/351 lr 0.001000 loss 0.0056 (0.0063) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 17:04:04] __main__ INFO: \u001b[0mEpoch 27 Step 351/351 lr 0.001000 loss 0.0057 (0.0064) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 17:04:04] __main__ INFO: \u001b[0mElapsed 497.10\n",
      "\u001b[32m[2020-07-01 17:04:04] __main__ INFO: \u001b[0mVal 27\n",
      "\u001b[32m[2020-07-01 17:04:22] __main__ INFO: \u001b[0mEpoch 27 loss 0.2407 acc@1 0.9304 acc@5 0.9984\n",
      "\u001b[32m[2020-07-01 17:04:22] __main__ INFO: \u001b[0mElapsed 17.70\n",
      "\u001b[32m[2020-07-01 17:04:22] __main__ INFO: \u001b[0mTrain 28 9477\n",
      "\u001b[32m[2020-07-01 17:06:43] __main__ INFO: \u001b[0mEpoch 28 Step 100/351 lr 0.001000 loss 0.0057 (0.0063) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 17:09:05] __main__ INFO: \u001b[0mEpoch 28 Step 200/351 lr 0.001000 loss 0.0033 (0.0065) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 17:11:27] __main__ INFO: \u001b[0mEpoch 28 Step 300/351 lr 0.001000 loss 0.0048 (0.0065) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 17:12:39] __main__ INFO: \u001b[0mEpoch 28 Step 351/351 lr 0.001000 loss 0.0060 (0.0064) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 17:12:39] __main__ INFO: \u001b[0mElapsed 497.54\n",
      "\u001b[32m[2020-07-01 17:12:39] __main__ INFO: \u001b[0mVal 28\n",
      "\u001b[32m[2020-07-01 17:12:57] __main__ INFO: \u001b[0mEpoch 28 loss 0.2430 acc@1 0.9286 acc@5 0.9978\n",
      "\u001b[32m[2020-07-01 17:12:57] __main__ INFO: \u001b[0mElapsed 17.77\n",
      "\u001b[32m[2020-07-01 17:12:57] __main__ INFO: \u001b[0mTrain 29 9828\n",
      "\u001b[32m[2020-07-01 17:15:19] __main__ INFO: \u001b[0mEpoch 29 Step 100/351 lr 0.001000 loss 0.0081 (0.0069) acc@1 1.0000 (0.9995) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 17:17:40] __main__ INFO: \u001b[0mEpoch 29 Step 200/351 lr 0.001000 loss 0.0146 (0.0070) acc@1 1.0000 (0.9995) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 17:20:02] __main__ INFO: \u001b[0mEpoch 29 Step 300/351 lr 0.001000 loss 0.0044 (0.0070) acc@1 1.0000 (0.9996) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 17:21:14] __main__ INFO: \u001b[0mEpoch 29 Step 351/351 lr 0.001000 loss 0.0051 (0.0070) acc@1 1.0000 (0.9996) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 17:21:14] __main__ INFO: \u001b[0mElapsed 497.40\n",
      "\u001b[32m[2020-07-01 17:21:14] __main__ INFO: \u001b[0mVal 29\n",
      "\u001b[32m[2020-07-01 17:21:32] __main__ INFO: \u001b[0mEpoch 29 loss 0.2402 acc@1 0.9302 acc@5 0.9976\n",
      "\u001b[32m[2020-07-01 17:21:32] __main__ INFO: \u001b[0mElapsed 17.75\n",
      "\u001b[32m[2020-07-01 17:21:32] __main__ INFO: \u001b[0mTrain 30 10179\n",
      "\u001b[32m[2020-07-01 17:23:54] __main__ INFO: \u001b[0mEpoch 30 Step 100/351 lr 0.001000 loss 0.0104 (0.0065) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 17:26:15] __main__ INFO: \u001b[0mEpoch 30 Step 200/351 lr 0.001000 loss 0.0022 (0.0063) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 17:28:36] __main__ INFO: \u001b[0mEpoch 30 Step 300/351 lr 0.001000 loss 0.0081 (0.0062) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 17:29:48] __main__ INFO: \u001b[0mEpoch 30 Step 351/351 lr 0.001000 loss 0.0032 (0.0061) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 17:29:48] __main__ INFO: \u001b[0mElapsed 495.66\n",
      "\u001b[32m[2020-07-01 17:29:48] __main__ INFO: \u001b[0mVal 30\n",
      "\u001b[32m[2020-07-01 17:30:05] __main__ INFO: \u001b[0mEpoch 30 loss 0.2392 acc@1 0.9314 acc@5 0.9978\n",
      "\u001b[32m[2020-07-01 17:30:05] __main__ INFO: \u001b[0mElapsed 17.65\n",
      "\u001b[32m[2020-07-01 17:30:05] __main__ INFO: \u001b[0mTrain 31 10530\n",
      "\u001b[32m[2020-07-01 17:32:26] __main__ INFO: \u001b[0mEpoch 31 Step 100/351 lr 0.001000 loss 0.0056 (0.0056) acc@1 1.0000 (1.0000) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 17:34:47] __main__ INFO: \u001b[0mEpoch 31 Step 200/351 lr 0.001000 loss 0.0046 (0.0054) acc@1 1.0000 (1.0000) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 17:37:08] __main__ INFO: \u001b[0mEpoch 31 Step 300/351 lr 0.001000 loss 0.0036 (0.0054) acc@1 1.0000 (1.0000) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 17:38:20] __main__ INFO: \u001b[0mEpoch 31 Step 351/351 lr 0.001000 loss 0.0076 (0.0056) acc@1 1.0000 (1.0000) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 17:38:20] __main__ INFO: \u001b[0mElapsed 494.66\n",
      "\u001b[32m[2020-07-01 17:38:20] __main__ INFO: \u001b[0mVal 31\n",
      "\u001b[32m[2020-07-01 17:38:38] __main__ INFO: \u001b[0mEpoch 31 loss 0.2416 acc@1 0.9310 acc@5 0.9984\n",
      "\u001b[32m[2020-07-01 17:38:38] __main__ INFO: \u001b[0mElapsed 17.64\n",
      "\u001b[32m[2020-07-01 17:38:38] __main__ INFO: \u001b[0mTrain 32 10881\n",
      "\u001b[32m[2020-07-01 17:40:59] __main__ INFO: \u001b[0mEpoch 32 Step 100/351 lr 0.001000 loss 0.0058 (0.0057) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 17:43:20] __main__ INFO: \u001b[0mEpoch 32 Step 200/351 lr 0.001000 loss 0.0038 (0.0055) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 17:45:41] __main__ INFO: \u001b[0mEpoch 32 Step 300/351 lr 0.001000 loss 0.0039 (0.0056) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 17:46:53] __main__ INFO: \u001b[0mEpoch 32 Step 351/351 lr 0.001000 loss 0.0034 (0.0056) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 17:46:53] __main__ INFO: \u001b[0mElapsed 494.91\n",
      "\u001b[32m[2020-07-01 17:46:53] __main__ INFO: \u001b[0mVal 32\n",
      "\u001b[32m[2020-07-01 17:47:10] __main__ INFO: \u001b[0mEpoch 32 loss 0.2484 acc@1 0.9288 acc@5 0.9980\n",
      "\u001b[32m[2020-07-01 17:47:10] __main__ INFO: \u001b[0mElapsed 17.62\n",
      "\u001b[32m[2020-07-01 17:47:10] __main__ INFO: \u001b[0mTrain 33 11232\n",
      "\u001b[32m[2020-07-01 17:49:31] __main__ INFO: \u001b[0mEpoch 33 Step 100/351 lr 0.001000 loss 0.0041 (0.0049) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 17:51:52] __main__ INFO: \u001b[0mEpoch 33 Step 200/351 lr 0.001000 loss 0.0086 (0.0050) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 17:54:13] __main__ INFO: \u001b[0mEpoch 33 Step 300/351 lr 0.001000 loss 0.0036 (0.0052) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 17:55:25] __main__ INFO: \u001b[0mEpoch 33 Step 351/351 lr 0.001000 loss 0.0033 (0.0052) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 17:55:25] __main__ INFO: \u001b[0mElapsed 494.52\n",
      "\u001b[32m[2020-07-01 17:55:25] __main__ INFO: \u001b[0mVal 33\n",
      "\u001b[32m[2020-07-01 17:55:42] __main__ INFO: \u001b[0mEpoch 33 loss 0.2476 acc@1 0.9276 acc@5 0.9978\n",
      "\u001b[32m[2020-07-01 17:55:42] __main__ INFO: \u001b[0mElapsed 17.62\n",
      "\u001b[32m[2020-07-01 17:55:42] __main__ INFO: \u001b[0mTrain 34 11583\n",
      "\u001b[32m[2020-07-01 17:58:03] __main__ INFO: \u001b[0mEpoch 34 Step 100/351 lr 0.001000 loss 0.0029 (0.0049) acc@1 1.0000 (1.0000) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 18:00:24] __main__ INFO: \u001b[0mEpoch 34 Step 200/351 lr 0.001000 loss 0.0054 (0.0049) acc@1 1.0000 (1.0000) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 18:02:45] __main__ INFO: \u001b[0mEpoch 34 Step 300/351 lr 0.001000 loss 0.0053 (0.0049) acc@1 1.0000 (1.0000) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 18:03:57] __main__ INFO: \u001b[0mEpoch 34 Step 351/351 lr 0.001000 loss 0.0038 (0.0050) acc@1 1.0000 (1.0000) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 18:03:57] __main__ INFO: \u001b[0mElapsed 494.83\n",
      "\u001b[32m[2020-07-01 18:03:57] __main__ INFO: \u001b[0mVal 34\n",
      "\u001b[32m[2020-07-01 18:04:15] __main__ INFO: \u001b[0mEpoch 34 loss 0.2410 acc@1 0.9326 acc@5 0.9982\n",
      "\u001b[32m[2020-07-01 18:04:15] __main__ INFO: \u001b[0mElapsed 17.64\n",
      "\u001b[32m[2020-07-01 18:04:15] __main__ INFO: \u001b[0mTrain 35 11934\n",
      "\u001b[32m[2020-07-01 18:06:36] __main__ INFO: \u001b[0mEpoch 35 Step 100/351 lr 0.001000 loss 0.0036 (0.0051) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 18:08:57] __main__ INFO: \u001b[0mEpoch 35 Step 200/351 lr 0.001000 loss 0.0018 (0.0052) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 18:11:18] __main__ INFO: \u001b[0mEpoch 35 Step 300/351 lr 0.001000 loss 0.0110 (0.0053) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 18:12:30] __main__ INFO: \u001b[0mEpoch 35 Step 351/351 lr 0.001000 loss 0.0070 (0.0054) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 18:12:30] __main__ INFO: \u001b[0mElapsed 494.90\n",
      "\u001b[32m[2020-07-01 18:12:30] __main__ INFO: \u001b[0mVal 35\n",
      "\u001b[32m[2020-07-01 18:12:47] __main__ INFO: \u001b[0mEpoch 35 loss 0.2448 acc@1 0.9322 acc@5 0.9980\n",
      "\u001b[32m[2020-07-01 18:12:47] __main__ INFO: \u001b[0mElapsed 17.63\n",
      "\u001b[32m[2020-07-01 18:12:47] __main__ INFO: \u001b[0mTrain 36 12285\n",
      "\u001b[32m[2020-07-01 18:15:08] __main__ INFO: \u001b[0mEpoch 36 Step 100/351 lr 0.001000 loss 0.0055 (0.0049) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 18:17:29] __main__ INFO: \u001b[0mEpoch 36 Step 200/351 lr 0.001000 loss 0.0035 (0.0055) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 18:19:50] __main__ INFO: \u001b[0mEpoch 36 Step 300/351 lr 0.001000 loss 0.0030 (0.0055) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 18:21:02] __main__ INFO: \u001b[0mEpoch 36 Step 351/351 lr 0.001000 loss 0.0044 (0.0056) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 18:21:02] __main__ INFO: \u001b[0mElapsed 494.92\n",
      "\u001b[32m[2020-07-01 18:21:02] __main__ INFO: \u001b[0mVal 36\n",
      "\u001b[32m[2020-07-01 18:21:20] __main__ INFO: \u001b[0mEpoch 36 loss 0.2439 acc@1 0.9296 acc@5 0.9982\n",
      "\u001b[32m[2020-07-01 18:21:20] __main__ INFO: \u001b[0mElapsed 17.65\n",
      "\u001b[32m[2020-07-01 18:21:20] __main__ INFO: \u001b[0mTrain 37 12636\n",
      "\u001b[32m[2020-07-01 18:23:41] __main__ INFO: \u001b[0mEpoch 37 Step 100/351 lr 0.001000 loss 0.0038 (0.0049) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 18:26:02] __main__ INFO: \u001b[0mEpoch 37 Step 200/351 lr 0.001000 loss 0.0052 (0.0048) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 18:28:23] __main__ INFO: \u001b[0mEpoch 37 Step 300/351 lr 0.001000 loss 0.0065 (0.0049) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 18:29:35] __main__ INFO: \u001b[0mEpoch 37 Step 351/351 lr 0.001000 loss 0.0051 (0.0049) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 18:29:35] __main__ INFO: \u001b[0mElapsed 494.96\n",
      "\u001b[32m[2020-07-01 18:29:35] __main__ INFO: \u001b[0mVal 37\n",
      "\u001b[32m[2020-07-01 18:29:53] __main__ INFO: \u001b[0mEpoch 37 loss 0.2441 acc@1 0.9308 acc@5 0.9982\n",
      "\u001b[32m[2020-07-01 18:29:53] __main__ INFO: \u001b[0mElapsed 17.66\n",
      "\u001b[32m[2020-07-01 18:29:53] __main__ INFO: \u001b[0mTrain 38 12987\n",
      "\u001b[32m[2020-07-01 18:32:14] __main__ INFO: \u001b[0mEpoch 38 Step 100/351 lr 0.001000 loss 0.0037 (0.0045) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 18:34:35] __main__ INFO: \u001b[0mEpoch 38 Step 200/351 lr 0.001000 loss 0.0042 (0.0048) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 18:36:55] __main__ INFO: \u001b[0mEpoch 38 Step 300/351 lr 0.001000 loss 0.0067 (0.0047) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 18:38:07] __main__ INFO: \u001b[0mEpoch 38 Step 351/351 lr 0.001000 loss 0.0033 (0.0047) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 18:38:07] __main__ INFO: \u001b[0mElapsed 494.81\n",
      "\u001b[32m[2020-07-01 18:38:07] __main__ INFO: \u001b[0mVal 38\n",
      "\u001b[32m[2020-07-01 18:38:25] __main__ INFO: \u001b[0mEpoch 38 loss 0.2389 acc@1 0.9304 acc@5 0.9982\n",
      "\u001b[32m[2020-07-01 18:38:25] __main__ INFO: \u001b[0mElapsed 17.61\n",
      "\u001b[32m[2020-07-01 18:38:25] __main__ INFO: \u001b[0mTrain 39 13338\n",
      "\u001b[32m[2020-07-01 18:40:46] __main__ INFO: \u001b[0mEpoch 39 Step 100/351 lr 0.001000 loss 0.0035 (0.0047) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 18:43:07] __main__ INFO: \u001b[0mEpoch 39 Step 200/351 lr 0.001000 loss 0.0037 (0.0048) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 18:45:27] __main__ INFO: \u001b[0mEpoch 39 Step 300/351 lr 0.001000 loss 0.0055 (0.0049) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 18:46:39] __main__ INFO: \u001b[0mEpoch 39 Step 351/351 lr 0.001000 loss 0.0030 (0.0049) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 18:46:39] __main__ INFO: \u001b[0mElapsed 494.38\n",
      "\u001b[32m[2020-07-01 18:46:39] __main__ INFO: \u001b[0mVal 39\n",
      "\u001b[32m[2020-07-01 18:46:57] __main__ INFO: \u001b[0mEpoch 39 loss 0.2421 acc@1 0.9300 acc@5 0.9976\n",
      "\u001b[32m[2020-07-01 18:46:57] __main__ INFO: \u001b[0mElapsed 17.65\n",
      "\u001b[32m[2020-07-01 18:46:57] __main__ INFO: \u001b[0mTrain 40 13689\n",
      "\u001b[32m[2020-07-01 18:49:18] __main__ INFO: \u001b[0mEpoch 40 Step 100/351 lr 0.001000 loss 0.0085 (0.0044) acc@1 1.0000 (1.0000) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 18:51:39] __main__ INFO: \u001b[0mEpoch 40 Step 200/351 lr 0.001000 loss 0.0034 (0.0043) acc@1 1.0000 (1.0000) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 18:54:00] __main__ INFO: \u001b[0mEpoch 40 Step 300/351 lr 0.001000 loss 0.0035 (0.0044) acc@1 1.0000 (1.0000) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 18:55:12] __main__ INFO: \u001b[0mEpoch 40 Step 351/351 lr 0.001000 loss 0.0046 (0.0044) acc@1 1.0000 (1.0000) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 18:55:12] __main__ INFO: \u001b[0mElapsed 494.96\n",
      "\u001b[32m[2020-07-01 18:55:12] __main__ INFO: \u001b[0mVal 40\n",
      "\u001b[32m[2020-07-01 18:55:30] __main__ INFO: \u001b[0mEpoch 40 loss 0.2406 acc@1 0.9298 acc@5 0.9974\n",
      "\u001b[32m[2020-07-01 18:55:30] __main__ INFO: \u001b[0mElapsed 17.66\n",
      "\u001b[32m[2020-07-01 18:55:30] __main__ INFO: \u001b[0mTrain 41 14040\n",
      "\u001b[32m[2020-07-01 18:57:51] __main__ INFO: \u001b[0mEpoch 41 Step 100/351 lr 0.001000 loss 0.0044 (0.0042) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 19:00:12] __main__ INFO: \u001b[0mEpoch 41 Step 200/351 lr 0.001000 loss 0.0035 (0.0043) acc@1 1.0000 (1.0000) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 19:02:32] __main__ INFO: \u001b[0mEpoch 41 Step 300/351 lr 0.001000 loss 0.0049 (0.0044) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 19:03:44] __main__ INFO: \u001b[0mEpoch 41 Step 351/351 lr 0.001000 loss 0.0042 (0.0045) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 19:03:44] __main__ INFO: \u001b[0mElapsed 494.61\n",
      "\u001b[32m[2020-07-01 19:03:44] __main__ INFO: \u001b[0mVal 41\n",
      "\u001b[32m[2020-07-01 19:04:02] __main__ INFO: \u001b[0mEpoch 41 loss 0.2434 acc@1 0.9310 acc@5 0.9978\n",
      "\u001b[32m[2020-07-01 19:04:02] __main__ INFO: \u001b[0mElapsed 17.65\n",
      "\u001b[32m[2020-07-01 19:04:02] __main__ INFO: \u001b[0mTrain 42 14391\n",
      "\u001b[32m[2020-07-01 19:06:23] __main__ INFO: \u001b[0mEpoch 42 Step 100/351 lr 0.001000 loss 0.0081 (0.0042) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 19:08:44] __main__ INFO: \u001b[0mEpoch 42 Step 200/351 lr 0.001000 loss 0.0037 (0.0043) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 19:11:04] __main__ INFO: \u001b[0mEpoch 42 Step 300/351 lr 0.001000 loss 0.0124 (0.0043) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 19:12:16] __main__ INFO: \u001b[0mEpoch 42 Step 351/351 lr 0.001000 loss 0.0097 (0.0043) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 19:12:16] __main__ INFO: \u001b[0mElapsed 494.42\n",
      "\u001b[32m[2020-07-01 19:12:16] __main__ INFO: \u001b[0mVal 42\n",
      "\u001b[32m[2020-07-01 19:12:34] __main__ INFO: \u001b[0mEpoch 42 loss 0.2430 acc@1 0.9316 acc@5 0.9984\n",
      "\u001b[32m[2020-07-01 19:12:34] __main__ INFO: \u001b[0mElapsed 17.66\n",
      "\u001b[32m[2020-07-01 19:12:34] __main__ INFO: \u001b[0mTrain 43 14742\n",
      "\u001b[32m[2020-07-01 19:14:55] __main__ INFO: \u001b[0mEpoch 43 Step 100/351 lr 0.001000 loss 0.0038 (0.0038) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 19:17:16] __main__ INFO: \u001b[0mEpoch 43 Step 200/351 lr 0.001000 loss 0.0033 (0.0042) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 19:19:36] __main__ INFO: \u001b[0mEpoch 43 Step 300/351 lr 0.001000 loss 0.0036 (0.0043) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 19:20:49] __main__ INFO: \u001b[0mEpoch 43 Step 351/351 lr 0.001000 loss 0.0032 (0.0043) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 19:20:49] __main__ INFO: \u001b[0mElapsed 494.84\n",
      "\u001b[32m[2020-07-01 19:20:49] __main__ INFO: \u001b[0mVal 43\n",
      "\u001b[32m[2020-07-01 19:21:07] __main__ INFO: \u001b[0mEpoch 43 loss 0.2431 acc@1 0.9326 acc@5 0.9976\n",
      "\u001b[32m[2020-07-01 19:21:07] __main__ INFO: \u001b[0mElapsed 17.76\n",
      "\u001b[32m[2020-07-01 19:21:07] __main__ INFO: \u001b[0mTrain 44 15093\n",
      "\u001b[32m[2020-07-01 19:23:29] __main__ INFO: \u001b[0mEpoch 44 Step 100/351 lr 0.001000 loss 0.0039 (0.0039) acc@1 1.0000 (1.0000) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 19:25:51] __main__ INFO: \u001b[0mEpoch 44 Step 200/351 lr 0.001000 loss 0.0029 (0.0041) acc@1 1.0000 (1.0000) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 19:28:13] __main__ INFO: \u001b[0mEpoch 44 Step 300/351 lr 0.001000 loss 0.0040 (0.0042) acc@1 1.0000 (1.0000) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 19:29:26] __main__ INFO: \u001b[0mEpoch 44 Step 351/351 lr 0.001000 loss 0.0043 (0.0042) acc@1 1.0000 (1.0000) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 19:29:26] __main__ INFO: \u001b[0mElapsed 498.94\n",
      "\u001b[32m[2020-07-01 19:29:26] __main__ INFO: \u001b[0mVal 44\n",
      "\u001b[32m[2020-07-01 19:29:43] __main__ INFO: \u001b[0mEpoch 44 loss 0.2399 acc@1 0.9322 acc@5 0.9984\n",
      "\u001b[32m[2020-07-01 19:29:43] __main__ INFO: \u001b[0mElapsed 17.79\n",
      "\u001b[32m[2020-07-01 19:29:43] __main__ INFO: \u001b[0mTrain 45 15444\n",
      "\u001b[32m[2020-07-01 19:32:06] __main__ INFO: \u001b[0mEpoch 45 Step 100/351 lr 0.001000 loss 0.0023 (0.0043) acc@1 1.0000 (1.0000) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 19:34:28] __main__ INFO: \u001b[0mEpoch 45 Step 200/351 lr 0.001000 loss 0.0073 (0.0045) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 19:36:50] __main__ INFO: \u001b[0mEpoch 45 Step 300/351 lr 0.001000 loss 0.0039 (0.0043) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 19:38:02] __main__ INFO: \u001b[0mEpoch 45 Step 351/351 lr 0.001000 loss 0.0040 (0.0043) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 19:38:02] __main__ INFO: \u001b[0mElapsed 499.10\n",
      "\u001b[32m[2020-07-01 19:38:02] __main__ INFO: \u001b[0mVal 45\n",
      "\u001b[32m[2020-07-01 19:38:20] __main__ INFO: \u001b[0mEpoch 45 loss 0.2429 acc@1 0.9318 acc@5 0.9982\n",
      "\u001b[32m[2020-07-01 19:38:20] __main__ INFO: \u001b[0mElapsed 17.79\n",
      "\u001b[32m[2020-07-01 19:38:20] __main__ INFO: \u001b[0mTrain 46 15795\n",
      "\u001b[32m[2020-07-01 19:40:43] __main__ INFO: \u001b[0mEpoch 46 Step 100/351 lr 0.001000 loss 0.0028 (0.0044) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 19:43:05] __main__ INFO: \u001b[0mEpoch 46 Step 200/351 lr 0.001000 loss 0.0040 (0.0042) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 19:45:27] __main__ INFO: \u001b[0mEpoch 46 Step 300/351 lr 0.001000 loss 0.0038 (0.0042) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 19:46:39] __main__ INFO: \u001b[0mEpoch 46 Step 351/351 lr 0.001000 loss 0.0103 (0.0042) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 19:46:39] __main__ INFO: \u001b[0mElapsed 499.21\n",
      "\u001b[32m[2020-07-01 19:46:39] __main__ INFO: \u001b[0mVal 46\n",
      "\u001b[32m[2020-07-01 19:46:57] __main__ INFO: \u001b[0mEpoch 46 loss 0.2407 acc@1 0.9326 acc@5 0.9978\n",
      "\u001b[32m[2020-07-01 19:46:57] __main__ INFO: \u001b[0mElapsed 17.80\n",
      "\u001b[32m[2020-07-01 19:46:57] __main__ INFO: \u001b[0mTrain 47 16146\n",
      "\u001b[32m[2020-07-01 19:49:19] __main__ INFO: \u001b[0mEpoch 47 Step 100/351 lr 0.001000 loss 0.0031 (0.0040) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 19:51:40] __main__ INFO: \u001b[0mEpoch 47 Step 200/351 lr 0.001000 loss 0.0027 (0.0042) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 19:54:01] __main__ INFO: \u001b[0mEpoch 47 Step 300/351 lr 0.001000 loss 0.0028 (0.0043) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 19:55:13] __main__ INFO: \u001b[0mEpoch 47 Step 351/351 lr 0.001000 loss 0.0038 (0.0043) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 19:55:13] __main__ INFO: \u001b[0mElapsed 496.03\n",
      "\u001b[32m[2020-07-01 19:55:13] __main__ INFO: \u001b[0mVal 47\n",
      "\u001b[32m[2020-07-01 19:55:31] __main__ INFO: \u001b[0mEpoch 47 loss 0.2437 acc@1 0.9300 acc@5 0.9980\n",
      "\u001b[32m[2020-07-01 19:55:31] __main__ INFO: \u001b[0mElapsed 17.66\n",
      "\u001b[32m[2020-07-01 19:55:31] __main__ INFO: \u001b[0mTrain 48 16497\n",
      "\u001b[32m[2020-07-01 19:57:52] __main__ INFO: \u001b[0mEpoch 48 Step 100/351 lr 0.001000 loss 0.0044 (0.0040) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 20:00:13] __main__ INFO: \u001b[0mEpoch 48 Step 200/351 lr 0.001000 loss 0.0034 (0.0041) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 20:02:34] __main__ INFO: \u001b[0mEpoch 48 Step 300/351 lr 0.001000 loss 0.0026 (0.0042) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 20:03:46] __main__ INFO: \u001b[0mEpoch 48 Step 351/351 lr 0.001000 loss 0.0038 (0.0041) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 20:03:46] __main__ INFO: \u001b[0mElapsed 495.20\n",
      "\u001b[32m[2020-07-01 20:03:46] __main__ INFO: \u001b[0mVal 48\n",
      "\u001b[32m[2020-07-01 20:04:04] __main__ INFO: \u001b[0mEpoch 48 loss 0.2421 acc@1 0.9312 acc@5 0.9982\n",
      "\u001b[32m[2020-07-01 20:04:04] __main__ INFO: \u001b[0mElapsed 17.67\n",
      "\u001b[32m[2020-07-01 20:04:04] __main__ INFO: \u001b[0mTrain 49 16848\n",
      "\u001b[32m[2020-07-01 20:06:25] __main__ INFO: \u001b[0mEpoch 49 Step 100/351 lr 0.001000 loss 0.0056 (0.0042) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 20:08:46] __main__ INFO: \u001b[0mEpoch 49 Step 200/351 lr 0.001000 loss 0.0054 (0.0040) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 20:11:07] __main__ INFO: \u001b[0mEpoch 49 Step 300/351 lr 0.001000 loss 0.0035 (0.0041) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 20:12:19] __main__ INFO: \u001b[0mEpoch 49 Step 351/351 lr 0.001000 loss 0.0034 (0.0040) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 20:12:19] __main__ INFO: \u001b[0mElapsed 494.94\n",
      "\u001b[32m[2020-07-01 20:12:19] __main__ INFO: \u001b[0mVal 49\n",
      "\u001b[32m[2020-07-01 20:12:36] __main__ INFO: \u001b[0mEpoch 49 loss 0.2434 acc@1 0.9312 acc@5 0.9976\n",
      "\u001b[32m[2020-07-01 20:12:36] __main__ INFO: \u001b[0mElapsed 17.68\n",
      "\u001b[32m[2020-07-01 20:12:36] __main__ INFO: \u001b[0mTrain 50 17199\n",
      "\u001b[32m[2020-07-01 20:14:57] __main__ INFO: \u001b[0mEpoch 50 Step 100/351 lr 0.001000 loss 0.0025 (0.0040) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 20:17:18] __main__ INFO: \u001b[0mEpoch 50 Step 200/351 lr 0.001000 loss 0.0036 (0.0039) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 20:19:39] __main__ INFO: \u001b[0mEpoch 50 Step 300/351 lr 0.001000 loss 0.0046 (0.0040) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 20:20:51] __main__ INFO: \u001b[0mEpoch 50 Step 351/351 lr 0.001000 loss 0.0031 (0.0039) acc@1 1.0000 (1.0000) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 20:20:51] __main__ INFO: \u001b[0mElapsed 494.68\n",
      "\u001b[32m[2020-07-01 20:20:51] __main__ INFO: \u001b[0mVal 50\n",
      "\u001b[32m[2020-07-01 20:21:09] __main__ INFO: \u001b[0mEpoch 50 loss 0.2387 acc@1 0.9314 acc@5 0.9982\n",
      "\u001b[32m[2020-07-01 20:21:09] __main__ INFO: \u001b[0mElapsed 17.65\n",
      "\u001b[32m[2020-07-01 20:21:09] fvcore.common.checkpoint INFO: \u001b[0mSaving checkpoint to /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_5/exp00_resume400_50/checkpoint_00050.pth\n"
     ]
    }
   ],
   "source": [
    "# Resume training with the un-augmented data\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "#!python train.py --config configs/cifar/resnet.yaml \\\n",
    "!python train.py --config /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_5/exp00/config.yaml \\\n",
    "    train.checkpoint /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_5/exp00/checkpoint_00400.pth \\\n",
    "    model.resnext.cardinality 4 \\\n",
    "    test.batch_size 128 \\\n",
    "    dataset.name CIFAR10 \\\n",
    "    train.base_lr .001 \\\n",
    "    train.output_dir /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_5/exp00_resume400_50 \\\n",
    "    scheduler.epochs 50\n",
    "\n",
    "#### Set LEARNING RATE based on ending LR\n",
    "#    train.resume True \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-02 17:09:21] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_5/exp00/checkpoint_00400.pth\n",
      "Files already downloaded and verified\n",
      "100%|███████████████████████████████████████████| 79/79 [00:35<00:00,  2.21it/s]\n",
      "\u001b[32m[2020-07-02 17:09:58] __main__ INFO: \u001b[0mElapsed 35.78\n",
      "\u001b[32m[2020-07-02 17:09:58] __main__ INFO: \u001b[0mLoss 0.3952 Accuracy 0.8857\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10 test dataset - without training on unaugmented data\n",
    "# write the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/resnext.yaml \\\n",
    "   model.resnext.cardinality 4 \\\n",
    "   test.batch_size 128 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_5/exp00/checkpoint_00400.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_5/exp00/test_results_0400_cifar10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-02 17:11:20] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_5/exp00_resume400_50/checkpoint_00050.pth\n",
      "Files already downloaded and verified\n",
      "100%|███████████████████████████████████████████| 79/79 [00:35<00:00,  2.20it/s]\n",
      "\u001b[32m[2020-07-02 17:11:57] __main__ INFO: \u001b[0mElapsed 35.90\n",
      "\u001b[32m[2020-07-02 17:11:57] __main__ INFO: \u001b[0mLoss 0.2333 Accuracy 0.9354\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10 test dataset - using model refined on 50 epochs\n",
    "# write the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/resnext.yaml \\\n",
    "   model.resnext.cardinality 4 \\\n",
    "   test.batch_size 128 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_5/exp00_resume400_50/checkpoint_00050.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_5/exp00_resume400_50/test_results_0050_cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-02 17:17:48] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_5/exp00/checkpoint_00400.pth\n",
      "CIFAR 10.1\n",
      "100%|███████████████████████████████████████████| 16/16 [00:07<00:00,  2.15it/s]\n",
      "\u001b[32m[2020-07-02 17:17:56] __main__ INFO: \u001b[0mElapsed 7.43\n",
      "\u001b[32m[2020-07-02 17:17:56] __main__ INFO: \u001b[0mLoss 0.7954 Accuracy 0.7755\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10.1 test dataset \n",
    "# write the results to the test output directory specified.\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "!python evaluate.py --config configs/cifar/resnext.yaml \\\n",
    "    model.resnext.cardinality 4 \\\n",
    "    test.batch_size 128 \\\n",
    "    dataset.name CIFAR101 \\\n",
    "    test.checkpoint /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_5/exp00/checkpoint_00400.pth \\\n",
    "    test.output_dir /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_5/exp00/test_results_0400_CIFAR101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-02 17:18:50] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_5/exp00_resume400_50/checkpoint_00050.pth\n",
      "CIFAR 10.1\n",
      "100%|███████████████████████████████████████████| 16/16 [00:07<00:00,  2.16it/s]\n",
      "\u001b[32m[2020-07-02 17:18:58] __main__ INFO: \u001b[0mElapsed 7.39\n",
      "\u001b[32m[2020-07-02 17:18:58] __main__ INFO: \u001b[0mLoss 0.5154 Accuracy 0.8455\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10.1 test dataset - using model refined on 50 epochs\n",
    "# write the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/resnext.yaml \\\n",
    "   model.resnext.cardinality 4 \\\n",
    "   test.batch_size 128 \\\n",
    "   dataset.name CIFAR101 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_5/exp00_resume400_50/checkpoint_00050.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_5/exp00_resume400_50/test_results_0050_cifar101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Testset</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Original_Accuracy_300epochs</th>\n",
       "      <th>Original_CI_300epochs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>resnext_29_4x64d_ra_2_5</td>\n",
       "      <td>400</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.3952</td>\n",
       "      <td>0.8857</td>\n",
       "      <td>96.4</td>\n",
       "      <td>(96.0, 96.7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>resnext_29_4x64d_ra_2_5</td>\n",
       "      <td>400</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>0.7954</td>\n",
       "      <td>0.7755</td>\n",
       "      <td>89.6</td>\n",
       "      <td>(88.2, 90.9)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>resnext_29_4x64d_ra_2_5_refined400</td>\n",
       "      <td>50</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.2333</td>\n",
       "      <td>0.9354</td>\n",
       "      <td>96.4</td>\n",
       "      <td>(96.0, 96.7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>resnext_29_4x64d_ra_2_5_refined400</td>\n",
       "      <td>50</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>0.5154</td>\n",
       "      <td>0.8455</td>\n",
       "      <td>89.6</td>\n",
       "      <td>(88.2, 90.9)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Model Epoch    Testset    Loss Accuracy  \\\n",
       "0             resnext_29_4x64d_ra_2_5   400    cifar10  0.3952   0.8857   \n",
       "1             resnext_29_4x64d_ra_2_5   400  cifar10.1  0.7954   0.7755   \n",
       "2  resnext_29_4x64d_ra_2_5_refined400    50    cifar10  0.2333   0.9354   \n",
       "3  resnext_29_4x64d_ra_2_5_refined400    50  cifar10.1  0.5154   0.8455   \n",
       "\n",
       "   Original_Accuracy_300epochs Original_CI_300epochs  \n",
       "0                         96.4          (96.0, 96.7)  \n",
       "1                         89.6          (88.2, 90.9)  \n",
       "2                         96.4          (96.0, 96.7)  \n",
       "3                         89.6          (88.2, 90.9)  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "a = pd.Series(['resnext_29_4x64d_ra_2_5', 400, 'cifar10', 0.3952, 0.8857]) #Loss 0.3952 Accuracy 0.8857\n",
    "b = pd.Series(['resnext_29_4x64d_ra_2_5', 400, 'cifar10.1', 0.7954, 0.7755]) #0.7954 Accuracy 0.7755\n",
    "\n",
    "c = pd.Series(['resnext_29_4x64d_ra_2_5_refined400', 50, 'cifar10', 0.2333, 0.9354]) #0.2333 Accuracy 0.9354\n",
    "d = pd.Series(['resnext_29_4x64d_ra_2_5_refined400', 50, 'cifar10.1', 0.5154, 0.8455]) #0.5154 Accuracy 0.8455\n",
    "\n",
    "df_results = pd.concat([a,b,c,d], axis=1).T\n",
    "df_results.columns = ['Model', 'Epoch', 'Testset', 'Loss', 'Accuracy']\n",
    "\n",
    "df_results['Original_Accuracy_300epochs'] = df_results.apply((lambda row: 96.4 if row[2] == 'cifar10' else 89.6), axis=1)\n",
    "df_results['Original_CI_300epochs'] = df_results.apply((lambda row: (96.0, 96.7) if row[2] == 'cifar10' else (88.2, 90.9)), axis=1)\n",
    "\n",
    "df_results.to_csv('/home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_5/results.csv')\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Testset</th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Original_Accuracy</th>\n",
       "      <th>Original_CI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>resnext_29_4x64d</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>100</td>\n",
       "      <td>0.6746</td>\n",
       "      <td>0.8019</td>\n",
       "      <td>96.4</td>\n",
       "      <td>(96.0, 96.7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>resnext_29_4x64d</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>200</td>\n",
       "      <td>0.2311</td>\n",
       "      <td>0.9321</td>\n",
       "      <td>96.4</td>\n",
       "      <td>(96.0, 96.7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>resnext_29_4x64d</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>300</td>\n",
       "      <td>0.1517</td>\n",
       "      <td>0.9535</td>\n",
       "      <td>96.4</td>\n",
       "      <td>(96.0, 96.7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>resnext_29_4x64d</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>300</td>\n",
       "      <td>0.3742</td>\n",
       "      <td>0.8905</td>\n",
       "      <td>89.6</td>\n",
       "      <td>(88.2, 90.9)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Model    Testset  Epoch    Loss  Accuracy  Original_Accuracy  \\\n",
       "0  resnext_29_4x64d    cifar10    100  0.6746    0.8019               96.4   \n",
       "1  resnext_29_4x64d    cifar10    200  0.2311    0.9321               96.4   \n",
       "2  resnext_29_4x64d    cifar10    300  0.1517    0.9535               96.4   \n",
       "3  resnext_29_4x64d  cifar10.1    300  0.3742    0.8905               89.6   \n",
       "\n",
       "    Original_CI  \n",
       "0  (96.0, 96.7)  \n",
       "1  (96.0, 96.7)  \n",
       "2  (96.0, 96.7)  \n",
       "3  (88.2, 90.9)  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write the results to a CSV file so that we can analyze later.\n",
    "import pandas as pd\n",
    "\n",
    "results = {'Model': ['resnext_29_4x64d', 'resnext_29_4x64d', 'resnext_29_4x64d', 'resnext_29_4x64d'],\n",
    "           'Testset': ['cifar10', 'cifar10', 'cifar10', 'cifar10.1'],\n",
    "           'Epoch': [100, 200, 300, 300],\n",
    "           'Loss': [0.6746, 0.2311, 0.1517, 0.3742],\n",
    "           'Accuracy': [0.8019, 0.9321, 0.9535, 0.8905],\n",
    "           'Original_Accuracy': [96.4, 96.4, 96.4, 89.6],\n",
    "           'Original_CI': [(96.0, 96.7), (96.0, 96.7), (96.0, 96.7), (88.2, 90.9)]\n",
    "           }\n",
    "\n",
    "df = pd.DataFrame(results, columns = ['Model', 'Testset', 'Epoch', 'Loss', 'Accuracy', \n",
    "                                      'Original_Accuracy', 'Original_CI'])\n",
    "\n",
    "\n",
    "df.to_csv('/home/ec2-user/SageMaker/experiments/resnext_29_4x64d/exp00/results.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['preds', 'probs', 'labels', 'loss', 'acc']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-1.2041907 , -1.8999833 , -0.24285015, ..., -1.5008752 ,\n",
       "        -1.8426697 , -2.8560946 ],\n",
       "       [ 0.5460079 ,  2.220384  , -1.9393705 , ..., -2.6070693 ,\n",
       "        11.327686  , -1.2085156 ],\n",
       "       [-1.3446747 ,  2.1730833 , -1.1615647 , ..., -2.2299995 ,\n",
       "        10.984515  , -0.75660706],\n",
       "       ...,\n",
       "       [-2.4790986 , -1.3337001 ,  0.61669415, ..., -0.83421385,\n",
       "        -1.8529658 , -1.7280097 ],\n",
       "       [-0.90489024,  9.350766  ,  1.0618937 , ..., -2.3210623 ,\n",
       "        -0.9061641 , -1.8115013 ],\n",
       "       [-1.4560711 , -1.0518838 , -1.4613396 , ..., 12.668192  ,\n",
       "        -2.1191459 , -0.8881919 ]], dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Peak inside the output file for predictions\n",
    "import numpy as np\n",
    "output = '/home/ec2-user/SageMaker/experiments/resnext_29_4x64d/exp00/test_results_0300/predictions.npz'\n",
    "npzfile = np.load(output)\n",
    "print(npzfile.files)\n",
    "npzfile['preds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the model checkpoints, configs, and results to S3 \n",
    "bucket='sagemaker-may29'\n",
    "prefix = 'sagemaker/results/original-models/'\n",
    "path = '/home/ec2-user/SageMaker/experiments/'\n",
    "\n",
    "s3_resource = boto3.resource(\"s3\", region_name=\"us-east-2\")\n",
    "\n",
    "def uploadDirectory(local_path,bucket_name,s3_prefix):\n",
    "\n",
    "    my_bucket = s3_resource.Bucket(bucket_name)\n",
    "    \n",
    "    for path, subdirs, files in os.walk(local_path):\n",
    "        path = path.replace(\"\\\\\",\"/\")\n",
    "        directory_name = path.replace(local_path,\"\")\n",
    "        for file in files:\n",
    "            #print(\"Local File:\", os.path.join(path, file))\n",
    "            #print(\"      Dest:\", s3_prefix+directory_name+'/'+file)\n",
    "            my_bucket.upload_file(os.path.join(path, file), s3_prefix+directory_name+'/'+file)\n",
    "    \n",
    "uploadDirectory(path,bucket,prefix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
