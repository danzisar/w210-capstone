{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import numpy \n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESNEXT_29_4x64D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without Containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 1)) (1.18.1)\n",
      "Requirement already satisfied: torch>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 2)) (1.4.0)\n",
      "Requirement already satisfied: torchvision in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (0.5.0)\n",
      "Collecting fvcore\n",
      "  Downloading fvcore-0.1.1.post20200623.tar.gz (32 kB)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 5)) (4.44.1)\n",
      "Collecting yacs\n",
      "  Downloading yacs-0.1.7-py3-none-any.whl (14 kB)\n",
      "Collecting apex\n",
      "  Cloning https://github.com/NVIDIA/apex.git to /tmp/pip-install-nyqnqino/apex\n",
      "  Running command git clone -q https://github.com/NVIDIA/apex.git /tmp/pip-install-nyqnqino/apex\n",
      "  Running command git submodule update --init --recursive -q\n",
      "Collecting termcolor\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting thop<0.0.31.post2004070130\n",
      "  Downloading thop-0.0.31.post2001170342-py3-none-any.whl (7.5 kB)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (1.14.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (7.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (5.3.1)\n",
      "Collecting portalocker\n",
      "  Downloading portalocker-1.7.0-py2.py3-none-any.whl (14 kB)\n",
      "Collecting tabulate\n",
      "  Downloading tabulate-0.8.7-py3-none-any.whl (24 kB)\n",
      "Building wheels for collected packages: fvcore, apex, termcolor\n",
      "  Building wheel for fvcore (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fvcore: filename=fvcore-0.1.1.post20200623-py3-none-any.whl size=41179 sha256=29ea8fe5e36b186ccd4fdcae487cfd71708084adf0e9e08a3df0ed0dd745177f\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/27/fc/86/b29aea030f5468db673ec86033a9579cc50e02979aa0c78ebe\n",
      "  Building wheel for apex (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for apex: filename=apex-0.1-py3-none-any.whl size=192130 sha256=61d4ae3a9db07dda87ad2c3ee8d4ce64ba366c7625874f4121dde6344515ad8b\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-8yyjymph/wheels/2a/45/23/6b4f2d6323a65ee0022d22a96d7bf580138e689f17cc48235c\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=3d31b3d3eab9309bcd258d4ea87ae67d714a3f50ec4a667a07b0c21399e75e17\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/93/2a/eb/e58dbcbc963549ee4f065ff80a59f274cc7210b6eab962acdc\n",
      "Successfully built fvcore apex termcolor\n",
      "Installing collected packages: yacs, portalocker, termcolor, tabulate, fvcore, apex, thop\n",
      "Successfully installed apex-0.1 fvcore-0.1.1.post20200623 portalocker-1.7.0 tabulate-0.8.7 termcolor-1.1.0 thop-0.0.31.post2001170342 yacs-0.1.7\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Sagemaker Notebook must be of type, conda_pytorch_p36\n",
    "\n",
    "!pip install -r '/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorboard\n",
      "  Downloading tensorboard-2.2.2-py3-none-any.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 3.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (2.23.0)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.2.2-py3-none-any.whl (88 kB)\n",
      "\u001b[K     |████████████████████████████████| 88 kB 10.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779 kB)\n",
      "\u001b[K     |████████████████████████████████| 779 kB 27.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio>=1.24.3\n",
      "  Downloading grpcio-1.30.0-cp36-cp36m-manylinux2010_x86_64.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 50.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.12.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.18.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.0.1)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (0.34.2)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.14.0)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.18.0-py2.py3-none-any.whl (90 kB)\n",
      "\u001b[K     |████████████████████████████████| 90 kB 16.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (3.12.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (46.1.3.post20200330)\n",
      "Collecting absl-py>=0.4\n",
      "  Downloading absl-py-0.9.0.tar.gz (104 kB)\n",
      "\u001b[K     |████████████████████████████████| 104 kB 52.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (2020.4.5.2)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (1.25.8)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard) (1.5.0)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 49.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.1.1-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (3.4.2)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "\u001b[K     |████████████████████████████████| 147 kB 48.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard) (2.2.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard) (0.4.8)\n",
      "Building wheels for collected packages: absl-py\n",
      "  Building wheel for absl-py (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for absl-py: filename=absl_py-0.9.0-py3-none-any.whl size=121931 sha256=d0619e52173cff294fd316f8fbfe283d28148cdf9bc83922e2ce897b922b148a\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/c3/af/84/3962a6af7b4ab336e951b7877dcfb758cf94548bb1771e0679\n",
      "Successfully built absl-py\n",
      "Installing collected packages: oauthlib, requests-oauthlib, pyasn1-modules, cachetools, google-auth, google-auth-oauthlib, markdown, tensorboard-plugin-wit, grpcio, absl-py, tensorboard\n",
      "Successfully installed absl-py-0.9.0 cachetools-4.1.1 google-auth-1.18.0 google-auth-oauthlib-0.4.1 grpcio-1.30.0 markdown-3.2.2 oauthlib-3.1.0 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 tensorboard-2.2.2 tensorboard-plugin-wit-1.7.0\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Need to add this to requirements.txt\n",
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-28 20:42:20] __main__ INFO: \u001b[0mdevice: cuda\n",
      "cudnn:\n",
      "  benchmark: True\n",
      "  deterministic: False\n",
      "dataset:\n",
      "  name: CIFAR10_RA_2_5\n",
      "  dataset_dir: ''\n",
      "  image_size: 32\n",
      "  n_channels: 3\n",
      "  n_classes: 10\n",
      "model:\n",
      "  type: cifar\n",
      "  name: resnext\n",
      "  init_mode: kaiming_fan_out\n",
      "  vgg:\n",
      "    n_channels: [64, 128, 256, 512, 512]\n",
      "    n_layers: [2, 2, 3, 3, 3]\n",
      "    use_bn: True\n",
      "  resnet:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "  resnet_preact:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "  wrn:\n",
      "    depth: 28\n",
      "    initial_channels: 16\n",
      "    widening_factor: 10\n",
      "    drop_rate: 0.0\n",
      "  densenet:\n",
      "    depth: 100\n",
      "    n_blocks: [6, 12, 24, 16]\n",
      "    block_type: bottleneck\n",
      "    growth_rate: 12\n",
      "    drop_rate: 0.0\n",
      "    compression_rate: 0.5\n",
      "  pyramidnet:\n",
      "    depth: 272\n",
      "    n_blocks: [3, 24, 36, 3]\n",
      "    initial_channels: 16\n",
      "    block_type: bottleneck\n",
      "    alpha: 200\n",
      "  resnext:\n",
      "    depth: 29\n",
      "    n_blocks: [3, 4, 6, 3]\n",
      "    initial_channels: 64\n",
      "    cardinality: 4\n",
      "    base_channels: 64\n",
      "  shake_shake:\n",
      "    depth: 26\n",
      "    initial_channels: 96\n",
      "    shake_forward: True\n",
      "    shake_backward: True\n",
      "    shake_image: True\n",
      "  se_resnet_preact:\n",
      "    depth: 110\n",
      "    initial_channels: 16\n",
      "    se_reduction: 16\n",
      "    block_type: basic\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "train:\n",
      "  checkpoint: ''\n",
      "  resume: False\n",
      "  precision: O0\n",
      "  batch_size: 128\n",
      "  subdivision: 1\n",
      "  optimizer: sgd\n",
      "  base_lr: 0.1\n",
      "  momentum: 0.9\n",
      "  nesterov: True\n",
      "  weight_decay: 0.0005\n",
      "  no_weight_decay_on_bn: False\n",
      "  gradient_clip: 0\n",
      "  start_epoch: 0\n",
      "  seed: 0\n",
      "  val_first: True\n",
      "  val_period: 1\n",
      "  val_ratio: 0.1\n",
      "  use_test_as_val: False\n",
      "  output_dir: /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_5/exp00\n",
      "  log_period: 100\n",
      "  checkpoint_period: 100\n",
      "  use_tensorboard: True\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: True\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "  distributed: False\n",
      "  dist:\n",
      "    backend: nccl\n",
      "    init_method: env://\n",
      "    world_size: -1\n",
      "    node_rank: -1\n",
      "    local_rank: 0\n",
      "    use_sync_bn: False\n",
      "tensorboard:\n",
      "  train_images: False\n",
      "  val_images: False\n",
      "  model_params: False\n",
      "optim:\n",
      "  adam:\n",
      "    betas: (0.9, 0.999)\n",
      "  lars:\n",
      "    eps: 1e-09\n",
      "    threshold: 0.01\n",
      "  adabound:\n",
      "    betas: (0.9, 0.999)\n",
      "    final_lr: 0.1\n",
      "    gamma: 0.001\n",
      "scheduler:\n",
      "  epochs: 400\n",
      "  warmup:\n",
      "    type: none\n",
      "    epochs: 0\n",
      "    start_factor: 0.001\n",
      "    exponent: 4\n",
      "  type: multistep\n",
      "  milestones: [150, 225]\n",
      "  lr_decay: 0.1\n",
      "  lr_min_factor: 0.001\n",
      "  T0: 10\n",
      "  T_mul: 1.0\n",
      "validation:\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: False\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "augmentation:\n",
      "  use_random_crop: True\n",
      "  use_random_horizontal_flip: True\n",
      "  use_cutout: False\n",
      "  use_random_erasing: False\n",
      "  use_dual_cutout: False\n",
      "  use_mixup: False\n",
      "  use_ricap: False\n",
      "  use_cutmix: False\n",
      "  use_label_smoothing: False\n",
      "  random_crop:\n",
      "    padding: 4\n",
      "    fill: 0\n",
      "    padding_mode: constant\n",
      "  random_horizontal_flip:\n",
      "    prob: 0.5\n",
      "  cutout:\n",
      "    prob: 1.0\n",
      "    mask_size: 16\n",
      "    cut_inside: False\n",
      "    mask_color: 0\n",
      "    dual_cutout_alpha: 0.1\n",
      "  random_erasing:\n",
      "    prob: 0.5\n",
      "    area_ratio_range: [0.02, 0.4]\n",
      "    min_aspect_ratio: 0.3\n",
      "    max_attempt: 20\n",
      "  mixup:\n",
      "    alpha: 1.0\n",
      "  ricap:\n",
      "    beta: 0.3\n",
      "  cutmix:\n",
      "    alpha: 1.0\n",
      "  label_smoothing:\n",
      "    epsilon: 0.1\n",
      "tta:\n",
      "  use_resize: False\n",
      "  use_center_crop: False\n",
      "  resize: 256\n",
      "test:\n",
      "  checkpoint: ''\n",
      "  output_dir: ''\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    pin_memory: False\n",
      "\u001b[32m[2020-06-28 20:42:20] __main__ INFO: \u001b[0menv_info:\n",
      "  pytorch_version: 1.4.0\n",
      "  cuda_version: 10.1\n",
      "  cudnn_version: 7603\n",
      "  num_gpus: 1\n",
      "  gpu_name: Tesla K80\n",
      "  gpu_capability: 3.7\n",
      "(50000, 32, 32, 3)\n",
      "\u001b[32m[2020-06-28 20:42:27] __main__ INFO: \u001b[0mMACs  : 2.75G\n",
      "\u001b[32m[2020-06-28 20:42:27] __main__ INFO: \u001b[0m#params: 17.56M\n",
      "Selected optimization level O0:  Pure FP32 training.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\",)\n",
      "\u001b[32m[2020-06-28 20:42:27] __main__ INFO: \u001b[0mVal 0\n",
      "\u001b[32m[2020-06-28 20:42:53] __main__ INFO: \u001b[0mEpoch 0 loss 4.4518 acc@1 0.1116 acc@5 0.5028\n",
      "\u001b[32m[2020-06-28 20:42:53] __main__ INFO: \u001b[0mElapsed 25.88\n",
      "\u001b[32m[2020-06-28 20:42:53] __main__ INFO: \u001b[0mTrain 1 0\n",
      "\u001b[32m[2020-06-28 20:45:23] __main__ INFO: \u001b[0mEpoch 1 Step 100/351 lr 0.100000 loss 3.3691 (8.6738) acc@1 0.0859 (0.1062) acc@5 0.4297 (0.5128)\n",
      "\u001b[32m[2020-06-28 20:47:45] __main__ INFO: \u001b[0mEpoch 1 Step 200/351 lr 0.100000 loss 2.5073 (5.6763) acc@1 0.1172 (0.1095) acc@5 0.4766 (0.5173)\n",
      "\u001b[32m[2020-06-28 20:50:08] __main__ INFO: \u001b[0mEpoch 1 Step 300/351 lr 0.100000 loss 2.4049 (4.6270) acc@1 0.1797 (0.1108) acc@5 0.5312 (0.5220)\n",
      "\u001b[32m[2020-06-28 20:51:21] __main__ INFO: \u001b[0mEpoch 1 Step 351/351 lr 0.100000 loss 2.4138 (4.3160) acc@1 0.0703 (0.1119) acc@5 0.5156 (0.5239)\n",
      "\u001b[32m[2020-06-28 20:51:21] __main__ INFO: \u001b[0mElapsed 507.82\n",
      "\u001b[32m[2020-06-28 20:51:21] __main__ INFO: \u001b[0mVal 1\n",
      "\u001b[32m[2020-06-28 20:51:38] __main__ INFO: \u001b[0mEpoch 1 loss 2.3758 acc@1 0.1168 acc@5 0.5526\n",
      "\u001b[32m[2020-06-28 20:51:38] __main__ INFO: \u001b[0mElapsed 17.77\n",
      "\u001b[32m[2020-06-28 20:51:38] __main__ INFO: \u001b[0mTrain 2 351\n",
      "\u001b[32m[2020-06-28 20:54:01] __main__ INFO: \u001b[0mEpoch 2 Step 100/351 lr 0.100000 loss 2.3630 (2.4220) acc@1 0.0859 (0.1155) acc@5 0.5547 (0.5507)\n",
      "\u001b[32m[2020-06-28 20:56:24] __main__ INFO: \u001b[0mEpoch 2 Step 200/351 lr 0.100000 loss 2.2689 (2.3989) acc@1 0.1719 (0.1195) acc@5 0.5312 (0.5467)\n",
      "\u001b[32m[2020-06-28 20:58:46] __main__ INFO: \u001b[0mEpoch 2 Step 300/351 lr 0.100000 loss 2.3387 (2.3838) acc@1 0.1250 (0.1217) acc@5 0.5469 (0.5462)\n",
      "\u001b[32m[2020-06-28 20:59:59] __main__ INFO: \u001b[0mEpoch 2 Step 351/351 lr 0.100000 loss 2.4615 (2.3766) acc@1 0.1406 (0.1229) acc@5 0.5234 (0.5470)\n",
      "\u001b[32m[2020-06-28 20:59:59] __main__ INFO: \u001b[0mElapsed 500.97\n",
      "\u001b[32m[2020-06-28 20:59:59] __main__ INFO: \u001b[0mVal 2\n",
      "\u001b[32m[2020-06-28 21:00:17] __main__ INFO: \u001b[0mEpoch 2 loss 2.3580 acc@1 0.1240 acc@5 0.5464\n",
      "\u001b[32m[2020-06-28 21:00:17] __main__ INFO: \u001b[0mElapsed 17.76\n",
      "\u001b[32m[2020-06-28 21:00:17] __main__ INFO: \u001b[0mTrain 3 702\n",
      "\u001b[32m[2020-06-28 21:02:40] __main__ INFO: \u001b[0mEpoch 3 Step 100/351 lr 0.100000 loss 2.3362 (2.3294) acc@1 0.1172 (0.1277) acc@5 0.5781 (0.5570)\n",
      "\u001b[32m[2020-06-28 21:05:03] __main__ INFO: \u001b[0mEpoch 3 Step 200/351 lr 0.100000 loss 2.3567 (2.3223) acc@1 0.1094 (0.1263) acc@5 0.5156 (0.5672)\n",
      "\u001b[32m[2020-06-28 21:07:25] __main__ INFO: \u001b[0mEpoch 3 Step 300/351 lr 0.100000 loss 2.2586 (2.3101) acc@1 0.1641 (0.1298) acc@5 0.6094 (0.5763)\n",
      "\u001b[32m[2020-06-28 21:08:38] __main__ INFO: \u001b[0mEpoch 3 Step 351/351 lr 0.100000 loss 2.2601 (2.3012) acc@1 0.1562 (0.1333) acc@5 0.5391 (0.5824)\n",
      "\u001b[32m[2020-06-28 21:08:38] __main__ INFO: \u001b[0mElapsed 501.05\n",
      "\u001b[32m[2020-06-28 21:08:38] __main__ INFO: \u001b[0mVal 3\n",
      "\u001b[32m[2020-06-28 21:08:56] __main__ INFO: \u001b[0mEpoch 3 loss 2.2553 acc@1 0.1406 acc@5 0.6110\n",
      "\u001b[32m[2020-06-28 21:08:56] __main__ INFO: \u001b[0mElapsed 17.74\n",
      "\u001b[32m[2020-06-28 21:08:56] __main__ INFO: \u001b[0mTrain 4 1053\n",
      "\u001b[32m[2020-06-28 21:11:19] __main__ INFO: \u001b[0mEpoch 4 Step 100/351 lr 0.100000 loss 2.1358 (2.2258) acc@1 0.2344 (0.1551) acc@5 0.6406 (0.6306)\n",
      "\u001b[32m[2020-06-28 21:13:41] __main__ INFO: \u001b[0mEpoch 4 Step 200/351 lr 0.100000 loss 2.1524 (2.2123) acc@1 0.1719 (0.1588) acc@5 0.6328 (0.6383)\n",
      "\u001b[32m[2020-06-28 21:16:04] __main__ INFO: \u001b[0mEpoch 4 Step 300/351 lr 0.100000 loss 2.1146 (2.1926) acc@1 0.1328 (0.1670) acc@5 0.6641 (0.6496)\n",
      "\u001b[32m[2020-06-28 21:17:17] __main__ INFO: \u001b[0mEpoch 4 Step 351/351 lr 0.100000 loss 2.1266 (2.1827) acc@1 0.2031 (0.1706) acc@5 0.6797 (0.6539)\n",
      "\u001b[32m[2020-06-28 21:17:17] __main__ INFO: \u001b[0mElapsed 501.34\n",
      "\u001b[32m[2020-06-28 21:17:17] __main__ INFO: \u001b[0mVal 4\n",
      "\u001b[32m[2020-06-28 21:17:35] __main__ INFO: \u001b[0mEpoch 4 loss 2.1315 acc@1 0.1976 acc@5 0.6762\n",
      "\u001b[32m[2020-06-28 21:17:35] __main__ INFO: \u001b[0mElapsed 17.79\n",
      "\u001b[32m[2020-06-28 21:17:35] __main__ INFO: \u001b[0mTrain 5 1404\n",
      "\u001b[32m[2020-06-28 21:19:58] __main__ INFO: \u001b[0mEpoch 5 Step 100/351 lr 0.100000 loss 2.0682 (2.1145) acc@1 0.2031 (0.1881) acc@5 0.6953 (0.6889)\n",
      "\u001b[32m[2020-06-28 21:22:21] __main__ INFO: \u001b[0mEpoch 5 Step 200/351 lr 0.100000 loss 2.1001 (2.1024) acc@1 0.1953 (0.1975) acc@5 0.7344 (0.6954)\n",
      "\u001b[32m[2020-06-28 21:24:44] __main__ INFO: \u001b[0mEpoch 5 Step 300/351 lr 0.100000 loss 1.9436 (2.0905) acc@1 0.2266 (0.2061) acc@5 0.7266 (0.7006)\n",
      "\u001b[32m[2020-06-28 21:25:56] __main__ INFO: \u001b[0mEpoch 5 Step 351/351 lr 0.100000 loss 1.9930 (2.0857) acc@1 0.2188 (0.2087) acc@5 0.7266 (0.7019)\n",
      "\u001b[32m[2020-06-28 21:25:56] __main__ INFO: \u001b[0mElapsed 501.51\n",
      "\u001b[32m[2020-06-28 21:25:56] __main__ INFO: \u001b[0mVal 5\n",
      "\u001b[32m[2020-06-28 21:26:14] __main__ INFO: \u001b[0mEpoch 5 loss 2.0315 acc@1 0.2276 acc@5 0.7218\n",
      "\u001b[32m[2020-06-28 21:26:14] __main__ INFO: \u001b[0mElapsed 17.77\n",
      "\u001b[32m[2020-06-28 21:26:14] __main__ INFO: \u001b[0mTrain 6 1755\n",
      "\u001b[32m[2020-06-28 21:28:37] __main__ INFO: \u001b[0mEpoch 6 Step 100/351 lr 0.100000 loss 2.1083 (2.0133) acc@1 0.1719 (0.2402) acc@5 0.7031 (0.7289)\n",
      "\u001b[32m[2020-06-28 21:31:00] __main__ INFO: \u001b[0mEpoch 6 Step 200/351 lr 0.100000 loss 1.9436 (2.0000) acc@1 0.2422 (0.2471) acc@5 0.8125 (0.7341)\n",
      "\u001b[32m[2020-06-28 21:33:22] __main__ INFO: \u001b[0mEpoch 6 Step 300/351 lr 0.100000 loss 1.9483 (1.9845) acc@1 0.2266 (0.2546) acc@5 0.7734 (0.7375)\n",
      "\u001b[32m[2020-06-28 21:34:35] __main__ INFO: \u001b[0mEpoch 6 Step 351/351 lr 0.100000 loss 1.9964 (1.9762) acc@1 0.2500 (0.2585) acc@5 0.7734 (0.7396)\n",
      "\u001b[32m[2020-06-28 21:34:35] __main__ INFO: \u001b[0mElapsed 500.72\n",
      "\u001b[32m[2020-06-28 21:34:35] __main__ INFO: \u001b[0mVal 6\n",
      "\u001b[32m[2020-06-28 21:34:53] __main__ INFO: \u001b[0mEpoch 6 loss 2.0144 acc@1 0.2616 acc@5 0.7194\n",
      "\u001b[32m[2020-06-28 21:34:53] __main__ INFO: \u001b[0mElapsed 17.74\n",
      "\u001b[32m[2020-06-28 21:34:53] __main__ INFO: \u001b[0mTrain 7 2106\n",
      "\u001b[32m[2020-06-28 21:37:15] __main__ INFO: \u001b[0mEpoch 7 Step 100/351 lr 0.100000 loss 1.8285 (1.9017) acc@1 0.3047 (0.2931) acc@5 0.7969 (0.7520)\n",
      "\u001b[32m[2020-06-28 21:39:38] __main__ INFO: \u001b[0mEpoch 7 Step 200/351 lr 0.100000 loss 1.7816 (1.8804) acc@1 0.3203 (0.3000) acc@5 0.7578 (0.7616)\n",
      "\u001b[32m[2020-06-28 21:42:01] __main__ INFO: \u001b[0mEpoch 7 Step 300/351 lr 0.100000 loss 1.8196 (1.8675) acc@1 0.3359 (0.3063) acc@5 0.7969 (0.7646)\n",
      "\u001b[32m[2020-06-28 21:43:13] __main__ INFO: \u001b[0mEpoch 7 Step 351/351 lr 0.100000 loss 1.7687 (1.8564) acc@1 0.3281 (0.3105) acc@5 0.8203 (0.7675)\n",
      "\u001b[32m[2020-06-28 21:43:14] __main__ INFO: \u001b[0mElapsed 500.81\n",
      "\u001b[32m[2020-06-28 21:43:14] __main__ INFO: \u001b[0mVal 7\n",
      "\u001b[32m[2020-06-28 21:43:31] __main__ INFO: \u001b[0mEpoch 7 loss 1.8513 acc@1 0.3186 acc@5 0.7780\n",
      "\u001b[32m[2020-06-28 21:43:31] __main__ INFO: \u001b[0mElapsed 17.74\n",
      "\u001b[32m[2020-06-28 21:43:31] __main__ INFO: \u001b[0mTrain 8 2457\n",
      "\u001b[32m[2020-06-28 21:45:54] __main__ INFO: \u001b[0mEpoch 8 Step 100/351 lr 0.100000 loss 1.8900 (1.7834) acc@1 0.3125 (0.3383) acc@5 0.6797 (0.7798)\n",
      "\u001b[32m[2020-06-28 21:48:17] __main__ INFO: \u001b[0mEpoch 8 Step 200/351 lr 0.100000 loss 1.6857 (1.7670) acc@1 0.3984 (0.3504) acc@5 0.7812 (0.7857)\n",
      "\u001b[32m[2020-06-28 21:50:40] __main__ INFO: \u001b[0mEpoch 8 Step 300/351 lr 0.100000 loss 1.7533 (1.7537) acc@1 0.3906 (0.3528) acc@5 0.7734 (0.7889)\n",
      "\u001b[32m[2020-06-28 21:51:53] __main__ INFO: \u001b[0mEpoch 8 Step 351/351 lr 0.100000 loss 1.6736 (1.7490) acc@1 0.3281 (0.3542) acc@5 0.7812 (0.7894)\n",
      "\u001b[32m[2020-06-28 21:51:53] __main__ INFO: \u001b[0mElapsed 501.33\n",
      "\u001b[32m[2020-06-28 21:51:53] __main__ INFO: \u001b[0mVal 8\n",
      "\u001b[32m[2020-06-28 21:52:10] __main__ INFO: \u001b[0mEpoch 8 loss 1.7842 acc@1 0.3374 acc@5 0.7854\n",
      "\u001b[32m[2020-06-28 21:52:10] __main__ INFO: \u001b[0mElapsed 17.75\n",
      "\u001b[32m[2020-06-28 21:52:10] __main__ INFO: \u001b[0mTrain 9 2808\n",
      "\u001b[32m[2020-06-28 21:54:33] __main__ INFO: \u001b[0mEpoch 9 Step 100/351 lr 0.100000 loss 1.6914 (1.7033) acc@1 0.3906 (0.3724) acc@5 0.7891 (0.7934)\n",
      "\u001b[32m[2020-06-28 21:56:56] __main__ INFO: \u001b[0mEpoch 9 Step 200/351 lr 0.100000 loss 1.6239 (1.6934) acc@1 0.3984 (0.3759) acc@5 0.8359 (0.7932)\n",
      "\u001b[32m[2020-06-28 21:59:19] __main__ INFO: \u001b[0mEpoch 9 Step 300/351 lr 0.100000 loss 1.5795 (1.6770) acc@1 0.4531 (0.3830) acc@5 0.7891 (0.7997)\n",
      "\u001b[32m[2020-06-28 22:00:32] __main__ INFO: \u001b[0mEpoch 9 Step 351/351 lr 0.100000 loss 1.7142 (1.6759) acc@1 0.3984 (0.3827) acc@5 0.8281 (0.8007)\n",
      "\u001b[32m[2020-06-28 22:00:32] __main__ INFO: \u001b[0mElapsed 501.50\n",
      "\u001b[32m[2020-06-28 22:00:32] __main__ INFO: \u001b[0mVal 9\n",
      "\u001b[32m[2020-06-28 22:00:50] __main__ INFO: \u001b[0mEpoch 9 loss 1.6173 acc@1 0.4082 acc@5 0.8050\n",
      "\u001b[32m[2020-06-28 22:00:50] __main__ INFO: \u001b[0mElapsed 17.78\n",
      "\u001b[32m[2020-06-28 22:00:50] __main__ INFO: \u001b[0mTrain 10 3159\n",
      "\u001b[32m[2020-06-28 22:03:13] __main__ INFO: \u001b[0mEpoch 10 Step 100/351 lr 0.100000 loss 1.6624 (1.6282) acc@1 0.3906 (0.3981) acc@5 0.7734 (0.8046)\n",
      "\u001b[32m[2020-06-28 22:05:35] __main__ INFO: \u001b[0mEpoch 10 Step 200/351 lr 0.100000 loss 1.5248 (1.6265) acc@1 0.4062 (0.4001) acc@5 0.8594 (0.8083)\n",
      "\u001b[32m[2020-06-28 22:07:58] __main__ INFO: \u001b[0mEpoch 10 Step 300/351 lr 0.100000 loss 1.5396 (1.6232) acc@1 0.4609 (0.4026) acc@5 0.8203 (0.8078)\n",
      "\u001b[32m[2020-06-28 22:09:11] __main__ INFO: \u001b[0mEpoch 10 Step 351/351 lr 0.100000 loss 1.6406 (1.6194) acc@1 0.3984 (0.4029) acc@5 0.7969 (0.8068)\n",
      "\u001b[32m[2020-06-28 22:09:11] __main__ INFO: \u001b[0mElapsed 501.34\n",
      "\u001b[32m[2020-06-28 22:09:11] __main__ INFO: \u001b[0mVal 10\n",
      "\u001b[32m[2020-06-28 22:09:29] __main__ INFO: \u001b[0mEpoch 10 loss 1.6611 acc@1 0.3936 acc@5 0.7968\n",
      "\u001b[32m[2020-06-28 22:09:29] __main__ INFO: \u001b[0mElapsed 17.77\n",
      "\u001b[32m[2020-06-28 22:09:29] __main__ INFO: \u001b[0mTrain 11 3510\n",
      "\u001b[32m[2020-06-28 22:11:52] __main__ INFO: \u001b[0mEpoch 11 Step 100/351 lr 0.100000 loss 1.5350 (1.5881) acc@1 0.4375 (0.4159) acc@5 0.7734 (0.8091)\n",
      "\u001b[32m[2020-06-28 22:14:14] __main__ INFO: \u001b[0mEpoch 11 Step 200/351 lr 0.100000 loss 1.4386 (1.5816) acc@1 0.4688 (0.4175) acc@5 0.7656 (0.8117)\n",
      "\u001b[32m[2020-06-28 22:16:37] __main__ INFO: \u001b[0mEpoch 11 Step 300/351 lr 0.100000 loss 1.7343 (1.5762) acc@1 0.4062 (0.4194) acc@5 0.7109 (0.8142)\n",
      "\u001b[32m[2020-06-28 22:17:50] __main__ INFO: \u001b[0mEpoch 11 Step 351/351 lr 0.100000 loss 1.5726 (1.5722) acc@1 0.4375 (0.4205) acc@5 0.7656 (0.8148)\n",
      "\u001b[32m[2020-06-28 22:17:50] __main__ INFO: \u001b[0mElapsed 500.87\n",
      "\u001b[32m[2020-06-28 22:17:50] __main__ INFO: \u001b[0mVal 11\n",
      "\u001b[32m[2020-06-28 22:18:07] __main__ INFO: \u001b[0mEpoch 11 loss 1.9558 acc@1 0.3420 acc@5 0.7658\n",
      "\u001b[32m[2020-06-28 22:18:07] __main__ INFO: \u001b[0mElapsed 17.81\n",
      "\u001b[32m[2020-06-28 22:18:07] __main__ INFO: \u001b[0mTrain 12 3861\n",
      "\u001b[32m[2020-06-28 22:20:30] __main__ INFO: \u001b[0mEpoch 12 Step 100/351 lr 0.100000 loss 1.5995 (1.5601) acc@1 0.4688 (0.4292) acc@5 0.8438 (0.8166)\n",
      "\u001b[32m[2020-06-28 22:22:53] __main__ INFO: \u001b[0mEpoch 12 Step 200/351 lr 0.100000 loss 1.4881 (1.5518) acc@1 0.4219 (0.4304) acc@5 0.8594 (0.8177)\n",
      "\u001b[32m[2020-06-28 22:25:16] __main__ INFO: \u001b[0mEpoch 12 Step 300/351 lr 0.100000 loss 1.6441 (1.5401) acc@1 0.4688 (0.4341) acc@5 0.8203 (0.8187)\n",
      "\u001b[32m[2020-06-28 22:26:29] __main__ INFO: \u001b[0mEpoch 12 Step 351/351 lr 0.100000 loss 1.3379 (1.5371) acc@1 0.5156 (0.4355) acc@5 0.8359 (0.8187)\n",
      "\u001b[32m[2020-06-28 22:26:29] __main__ INFO: \u001b[0mElapsed 501.51\n",
      "\u001b[32m[2020-06-28 22:26:29] __main__ INFO: \u001b[0mVal 12\n",
      "\u001b[32m[2020-06-28 22:26:47] __main__ INFO: \u001b[0mEpoch 12 loss 1.6640 acc@1 0.3918 acc@5 0.8032\n",
      "\u001b[32m[2020-06-28 22:26:47] __main__ INFO: \u001b[0mElapsed 17.78\n",
      "\u001b[32m[2020-06-28 22:26:47] __main__ INFO: \u001b[0mTrain 13 4212\n",
      "\u001b[32m[2020-06-28 22:29:10] __main__ INFO: \u001b[0mEpoch 13 Step 100/351 lr 0.100000 loss 1.5768 (1.5023) acc@1 0.3906 (0.4444) acc@5 0.8438 (0.8230)\n",
      "\u001b[32m[2020-06-28 22:31:33] __main__ INFO: \u001b[0mEpoch 13 Step 200/351 lr 0.100000 loss 1.5644 (1.4997) acc@1 0.4141 (0.4465) acc@5 0.8438 (0.8240)\n",
      "\u001b[32m[2020-06-28 22:33:56] __main__ INFO: \u001b[0mEpoch 13 Step 300/351 lr 0.100000 loss 1.5837 (1.5044) acc@1 0.3984 (0.4455) acc@5 0.7812 (0.8209)\n",
      "\u001b[32m[2020-06-28 22:35:08] __main__ INFO: \u001b[0mEpoch 13 Step 351/351 lr 0.100000 loss 1.3390 (1.5015) acc@1 0.5312 (0.4464) acc@5 0.7969 (0.8215)\n",
      "\u001b[32m[2020-06-28 22:35:08] __main__ INFO: \u001b[0mElapsed 501.79\n",
      "\u001b[32m[2020-06-28 22:35:08] __main__ INFO: \u001b[0mVal 13\n",
      "\u001b[32m[2020-06-28 22:35:26] __main__ INFO: \u001b[0mEpoch 13 loss 1.8007 acc@1 0.3616 acc@5 0.7960\n",
      "\u001b[32m[2020-06-28 22:35:26] __main__ INFO: \u001b[0mElapsed 17.79\n",
      "\u001b[32m[2020-06-28 22:35:26] __main__ INFO: \u001b[0mTrain 14 4563\n",
      "\u001b[32m[2020-06-28 22:37:49] __main__ INFO: \u001b[0mEpoch 14 Step 100/351 lr 0.100000 loss 1.6614 (1.4766) acc@1 0.3750 (0.4552) acc@5 0.7500 (0.8248)\n",
      "\u001b[32m[2020-06-28 22:40:12] __main__ INFO: \u001b[0mEpoch 14 Step 200/351 lr 0.100000 loss 1.5066 (1.4770) acc@1 0.4375 (0.4580) acc@5 0.8203 (0.8243)\n",
      "\u001b[32m[2020-06-28 22:42:35] __main__ INFO: \u001b[0mEpoch 14 Step 300/351 lr 0.100000 loss 1.5025 (1.4744) acc@1 0.4531 (0.4579) acc@5 0.8203 (0.8257)\n",
      "\u001b[32m[2020-06-28 22:43:48] __main__ INFO: \u001b[0mEpoch 14 Step 351/351 lr 0.100000 loss 1.3745 (1.4731) acc@1 0.4766 (0.4582) acc@5 0.8438 (0.8266)\n",
      "\u001b[32m[2020-06-28 22:43:48] __main__ INFO: \u001b[0mElapsed 501.80\n",
      "\u001b[32m[2020-06-28 22:43:48] __main__ INFO: \u001b[0mVal 14\n",
      "\u001b[32m[2020-06-28 22:44:06] __main__ INFO: \u001b[0mEpoch 14 loss 1.5485 acc@1 0.4452 acc@5 0.8130\n",
      "\u001b[32m[2020-06-28 22:44:06] __main__ INFO: \u001b[0mElapsed 17.75\n",
      "\u001b[32m[2020-06-28 22:44:06] __main__ INFO: \u001b[0mTrain 15 4914\n",
      "\u001b[32m[2020-06-28 22:46:29] __main__ INFO: \u001b[0mEpoch 15 Step 100/351 lr 0.100000 loss 1.3965 (1.4277) acc@1 0.5078 (0.4770) acc@5 0.8359 (0.8322)\n",
      "\u001b[32m[2020-06-28 22:48:52] __main__ INFO: \u001b[0mEpoch 15 Step 200/351 lr 0.100000 loss 1.7750 (1.4502) acc@1 0.3203 (0.4674) acc@5 0.7578 (0.8259)\n",
      "\u001b[32m[2020-06-28 22:51:14] __main__ INFO: \u001b[0mEpoch 15 Step 300/351 lr 0.100000 loss 1.6003 (1.4522) acc@1 0.4219 (0.4669) acc@5 0.8359 (0.8255)\n",
      "\u001b[32m[2020-06-28 22:52:27] __main__ INFO: \u001b[0mEpoch 15 Step 351/351 lr 0.100000 loss 1.3235 (1.4482) acc@1 0.5156 (0.4685) acc@5 0.8672 (0.8257)\n",
      "\u001b[32m[2020-06-28 22:52:27] __main__ INFO: \u001b[0mElapsed 501.55\n",
      "\u001b[32m[2020-06-28 22:52:27] __main__ INFO: \u001b[0mVal 15\n",
      "\u001b[32m[2020-06-28 22:52:45] __main__ INFO: \u001b[0mEpoch 15 loss 1.6036 acc@1 0.4332 acc@5 0.8140\n",
      "\u001b[32m[2020-06-28 22:52:45] __main__ INFO: \u001b[0mElapsed 17.75\n",
      "\u001b[32m[2020-06-28 22:52:45] __main__ INFO: \u001b[0mTrain 16 5265\n",
      "\u001b[32m[2020-06-28 22:55:08] __main__ INFO: \u001b[0mEpoch 16 Step 100/351 lr 0.100000 loss 1.3196 (1.4411) acc@1 0.5781 (0.4667) acc@5 0.8438 (0.8298)\n",
      "\u001b[32m[2020-06-28 22:57:31] __main__ INFO: \u001b[0mEpoch 16 Step 200/351 lr 0.100000 loss 1.4886 (1.4318) acc@1 0.4766 (0.4735) acc@5 0.8203 (0.8321)\n",
      "\u001b[32m[2020-06-28 22:59:54] __main__ INFO: \u001b[0mEpoch 16 Step 300/351 lr 0.100000 loss 1.4936 (1.4301) acc@1 0.4688 (0.4726) acc@5 0.8438 (0.8311)\n",
      "\u001b[32m[2020-06-28 23:01:07] __main__ INFO: \u001b[0mEpoch 16 Step 351/351 lr 0.100000 loss 1.5978 (1.4310) acc@1 0.3906 (0.4727) acc@5 0.8281 (0.8316)\n",
      "\u001b[32m[2020-06-28 23:01:07] __main__ INFO: \u001b[0mElapsed 501.74\n",
      "\u001b[32m[2020-06-28 23:01:07] __main__ INFO: \u001b[0mVal 16\n",
      "\u001b[32m[2020-06-28 23:01:25] __main__ INFO: \u001b[0mEpoch 16 loss 1.5392 acc@1 0.4396 acc@5 0.8214\n",
      "\u001b[32m[2020-06-28 23:01:25] __main__ INFO: \u001b[0mElapsed 17.78\n",
      "\u001b[32m[2020-06-28 23:01:25] __main__ INFO: \u001b[0mTrain 17 5616\n",
      "\u001b[32m[2020-06-28 23:03:48] __main__ INFO: \u001b[0mEpoch 17 Step 100/351 lr 0.100000 loss 1.6164 (1.4138) acc@1 0.4219 (0.4795) acc@5 0.8047 (0.8303)\n",
      "\u001b[32m[2020-06-28 23:06:10] __main__ INFO: \u001b[0mEpoch 17 Step 200/351 lr 0.100000 loss 1.3723 (1.4057) acc@1 0.4844 (0.4855) acc@5 0.8125 (0.8295)\n",
      "\u001b[32m[2020-06-28 23:08:33] __main__ INFO: \u001b[0mEpoch 17 Step 300/351 lr 0.100000 loss 1.4949 (1.4054) acc@1 0.4453 (0.4836) acc@5 0.7734 (0.8303)\n",
      "\u001b[32m[2020-06-28 23:09:46] __main__ INFO: \u001b[0mEpoch 17 Step 351/351 lr 0.100000 loss 1.4657 (1.4026) acc@1 0.4453 (0.4850) acc@5 0.8125 (0.8310)\n",
      "\u001b[32m[2020-06-28 23:09:46] __main__ INFO: \u001b[0mElapsed 501.57\n",
      "\u001b[32m[2020-06-28 23:09:46] __main__ INFO: \u001b[0mVal 17\n",
      "\u001b[32m[2020-06-28 23:10:04] __main__ INFO: \u001b[0mEpoch 17 loss 1.8393 acc@1 0.3660 acc@5 0.7838\n",
      "\u001b[32m[2020-06-28 23:10:04] __main__ INFO: \u001b[0mElapsed 17.77\n",
      "\u001b[32m[2020-06-28 23:10:04] __main__ INFO: \u001b[0mTrain 18 5967\n",
      "\u001b[32m[2020-06-28 23:12:27] __main__ INFO: \u001b[0mEpoch 18 Step 100/351 lr 0.100000 loss 1.3549 (1.3924) acc@1 0.5156 (0.4862) acc@5 0.8125 (0.8332)\n",
      "\u001b[32m[2020-06-28 23:14:50] __main__ INFO: \u001b[0mEpoch 18 Step 200/351 lr 0.100000 loss 1.4769 (1.3863) acc@1 0.4297 (0.4886) acc@5 0.8828 (0.8336)\n",
      "\u001b[32m[2020-06-28 23:17:13] __main__ INFO: \u001b[0mEpoch 18 Step 300/351 lr 0.100000 loss 1.4025 (1.3832) acc@1 0.4531 (0.4904) acc@5 0.8359 (0.8352)\n",
      "\u001b[32m[2020-06-28 23:18:26] __main__ INFO: \u001b[0mEpoch 18 Step 351/351 lr 0.100000 loss 1.2152 (1.3849) acc@1 0.5781 (0.4895) acc@5 0.8203 (0.8334)\n",
      "\u001b[32m[2020-06-28 23:18:26] __main__ INFO: \u001b[0mElapsed 501.57\n",
      "\u001b[32m[2020-06-28 23:18:26] __main__ INFO: \u001b[0mVal 18\n",
      "\u001b[32m[2020-06-28 23:18:43] __main__ INFO: \u001b[0mEpoch 18 loss 1.7105 acc@1 0.4090 acc@5 0.7936\n",
      "\u001b[32m[2020-06-28 23:18:43] __main__ INFO: \u001b[0mElapsed 17.78\n",
      "\u001b[32m[2020-06-28 23:18:43] __main__ INFO: \u001b[0mTrain 19 6318\n",
      "\u001b[32m[2020-06-28 23:21:06] __main__ INFO: \u001b[0mEpoch 19 Step 100/351 lr 0.100000 loss 1.5367 (1.3570) acc@1 0.4297 (0.5024) acc@5 0.7734 (0.8404)\n",
      "\u001b[32m[2020-06-28 23:23:29] __main__ INFO: \u001b[0mEpoch 19 Step 200/351 lr 0.100000 loss 1.5534 (1.3700) acc@1 0.4531 (0.4939) acc@5 0.8438 (0.8374)\n",
      "\u001b[32m[2020-06-28 23:25:52] __main__ INFO: \u001b[0mEpoch 19 Step 300/351 lr 0.100000 loss 1.4024 (1.3680) acc@1 0.4609 (0.4959) acc@5 0.7891 (0.8363)\n",
      "\u001b[32m[2020-06-28 23:27:05] __main__ INFO: \u001b[0mEpoch 19 Step 351/351 lr 0.100000 loss 1.4107 (1.3669) acc@1 0.4922 (0.4965) acc@5 0.8516 (0.8373)\n",
      "\u001b[32m[2020-06-28 23:27:05] __main__ INFO: \u001b[0mElapsed 501.52\n",
      "\u001b[32m[2020-06-28 23:27:05] __main__ INFO: \u001b[0mVal 19\n",
      "\u001b[32m[2020-06-28 23:27:23] __main__ INFO: \u001b[0mEpoch 19 loss 1.8808 acc@1 0.4358 acc@5 0.8058\n",
      "\u001b[32m[2020-06-28 23:27:23] __main__ INFO: \u001b[0mElapsed 17.79\n",
      "\u001b[32m[2020-06-28 23:27:23] __main__ INFO: \u001b[0mTrain 20 6669\n",
      "\u001b[32m[2020-06-28 23:29:46] __main__ INFO: \u001b[0mEpoch 20 Step 100/351 lr 0.100000 loss 1.5675 (1.3688) acc@1 0.4141 (0.4984) acc@5 0.8203 (0.8403)\n",
      "\u001b[32m[2020-06-28 23:32:09] __main__ INFO: \u001b[0mEpoch 20 Step 200/351 lr 0.100000 loss 1.4081 (1.3518) acc@1 0.5078 (0.5042) acc@5 0.8516 (0.8380)\n",
      "\u001b[32m[2020-06-28 23:34:32] __main__ INFO: \u001b[0mEpoch 20 Step 300/351 lr 0.100000 loss 1.2572 (1.3512) acc@1 0.5312 (0.5039) acc@5 0.8750 (0.8377)\n",
      "\u001b[32m[2020-06-28 23:35:44] __main__ INFO: \u001b[0mEpoch 20 Step 351/351 lr 0.100000 loss 1.3320 (1.3563) acc@1 0.5078 (0.5020) acc@5 0.8359 (0.8371)\n",
      "\u001b[32m[2020-06-28 23:35:44] __main__ INFO: \u001b[0mElapsed 501.76\n",
      "\u001b[32m[2020-06-28 23:35:44] __main__ INFO: \u001b[0mVal 20\n",
      "\u001b[32m[2020-06-28 23:36:02] __main__ INFO: \u001b[0mEpoch 20 loss 1.5464 acc@1 0.4484 acc@5 0.8222\n",
      "\u001b[32m[2020-06-28 23:36:02] __main__ INFO: \u001b[0mElapsed 17.79\n",
      "\u001b[32m[2020-06-28 23:36:02] __main__ INFO: \u001b[0mTrain 21 7020\n",
      "\u001b[32m[2020-06-28 23:38:25] __main__ INFO: \u001b[0mEpoch 21 Step 100/351 lr 0.100000 loss 1.3101 (1.3224) acc@1 0.5391 (0.5122) acc@5 0.8359 (0.8355)\n",
      "\u001b[32m[2020-06-28 23:40:48] __main__ INFO: \u001b[0mEpoch 21 Step 200/351 lr 0.100000 loss 1.2508 (1.3314) acc@1 0.5625 (0.5098) acc@5 0.8594 (0.8369)\n",
      "\u001b[32m[2020-06-28 23:43:10] __main__ INFO: \u001b[0mEpoch 21 Step 300/351 lr 0.100000 loss 1.3556 (1.3291) acc@1 0.5000 (0.5112) acc@5 0.8672 (0.8381)\n",
      "\u001b[32m[2020-06-28 23:44:23] __main__ INFO: \u001b[0mEpoch 21 Step 351/351 lr 0.100000 loss 1.3930 (1.3360) acc@1 0.5078 (0.5085) acc@5 0.8750 (0.8377)\n",
      "\u001b[32m[2020-06-28 23:44:23] __main__ INFO: \u001b[0mElapsed 500.51\n",
      "\u001b[32m[2020-06-28 23:44:23] __main__ INFO: \u001b[0mVal 21\n",
      "\u001b[32m[2020-06-28 23:44:40] __main__ INFO: \u001b[0mEpoch 21 loss 1.5300 acc@1 0.4516 acc@5 0.8372\n",
      "\u001b[32m[2020-06-28 23:44:40] __main__ INFO: \u001b[0mElapsed 17.59\n",
      "\u001b[32m[2020-06-28 23:44:40] __main__ INFO: \u001b[0mTrain 22 7371\n",
      "\u001b[32m[2020-06-28 23:47:02] __main__ INFO: \u001b[0mEpoch 22 Step 100/351 lr 0.100000 loss 1.4660 (1.3219) acc@1 0.4453 (0.5128) acc@5 0.8281 (0.8379)\n",
      "\u001b[32m[2020-06-28 23:49:23] __main__ INFO: \u001b[0mEpoch 22 Step 200/351 lr 0.100000 loss 1.1991 (1.3202) acc@1 0.5547 (0.5132) acc@5 0.8750 (0.8389)\n",
      "\u001b[32m[2020-06-28 23:51:43] __main__ INFO: \u001b[0mEpoch 22 Step 300/351 lr 0.100000 loss 1.3220 (1.3260) acc@1 0.4922 (0.5120) acc@5 0.8203 (0.8381)\n",
      "\u001b[32m[2020-06-28 23:52:55] __main__ INFO: \u001b[0mEpoch 22 Step 351/351 lr 0.100000 loss 1.4379 (1.3256) acc@1 0.4531 (0.5122) acc@5 0.8438 (0.8384)\n",
      "\u001b[32m[2020-06-28 23:52:55] __main__ INFO: \u001b[0mElapsed 495.00\n",
      "\u001b[32m[2020-06-28 23:52:55] __main__ INFO: \u001b[0mVal 22\n",
      "\u001b[32m[2020-06-28 23:53:13] __main__ INFO: \u001b[0mEpoch 22 loss 1.5970 acc@1 0.4452 acc@5 0.8260\n",
      "\u001b[32m[2020-06-28 23:53:13] __main__ INFO: \u001b[0mElapsed 17.56\n",
      "\u001b[32m[2020-06-28 23:53:13] __main__ INFO: \u001b[0mTrain 23 7722\n",
      "\u001b[32m[2020-06-28 23:55:34] __main__ INFO: \u001b[0mEpoch 23 Step 100/351 lr 0.100000 loss 1.1249 (1.3235) acc@1 0.5703 (0.5091) acc@5 0.8906 (0.8384)\n",
      "\u001b[32m[2020-06-28 23:57:55] __main__ INFO: \u001b[0mEpoch 23 Step 200/351 lr 0.100000 loss 1.3360 (1.3179) acc@1 0.5312 (0.5142) acc@5 0.8203 (0.8418)\n",
      "\u001b[32m[2020-06-29 00:00:16] __main__ INFO: \u001b[0mEpoch 23 Step 300/351 lr 0.100000 loss 1.3648 (1.3217) acc@1 0.4688 (0.5132) acc@5 0.8438 (0.8399)\n",
      "\u001b[32m[2020-06-29 00:01:27] __main__ INFO: \u001b[0mEpoch 23 Step 351/351 lr 0.100000 loss 1.2259 (1.3215) acc@1 0.5312 (0.5130) acc@5 0.8438 (0.8407)\n",
      "\u001b[32m[2020-06-29 00:01:27] __main__ INFO: \u001b[0mElapsed 494.41\n",
      "\u001b[32m[2020-06-29 00:01:27] __main__ INFO: \u001b[0mVal 23\n",
      "\u001b[32m[2020-06-29 00:01:45] __main__ INFO: \u001b[0mEpoch 23 loss 1.4481 acc@1 0.4734 acc@5 0.8380\n",
      "\u001b[32m[2020-06-29 00:01:45] __main__ INFO: \u001b[0mElapsed 17.56\n",
      "\u001b[32m[2020-06-29 00:01:45] __main__ INFO: \u001b[0mTrain 24 8073\n",
      "\u001b[32m[2020-06-29 00:04:06] __main__ INFO: \u001b[0mEpoch 24 Step 100/351 lr 0.100000 loss 1.1948 (1.3065) acc@1 0.5391 (0.5198) acc@5 0.8984 (0.8387)\n",
      "\u001b[32m[2020-06-29 00:06:27] __main__ INFO: \u001b[0mEpoch 24 Step 200/351 lr 0.100000 loss 1.3547 (1.3050) acc@1 0.5078 (0.5206) acc@5 0.8359 (0.8420)\n",
      "\u001b[32m[2020-06-29 00:08:48] __main__ INFO: \u001b[0mEpoch 24 Step 300/351 lr 0.100000 loss 1.3709 (1.3094) acc@1 0.4688 (0.5175) acc@5 0.8359 (0.8408)\n",
      "\u001b[32m[2020-06-29 00:10:00] __main__ INFO: \u001b[0mEpoch 24 Step 351/351 lr 0.100000 loss 1.3862 (1.3088) acc@1 0.5234 (0.5178) acc@5 0.8359 (0.8405)\n",
      "\u001b[32m[2020-06-29 00:10:00] __main__ INFO: \u001b[0mElapsed 494.72\n",
      "\u001b[32m[2020-06-29 00:10:00] __main__ INFO: \u001b[0mVal 24\n",
      "\u001b[32m[2020-06-29 00:10:17] __main__ INFO: \u001b[0mEpoch 24 loss 1.4899 acc@1 0.4578 acc@5 0.8322\n",
      "\u001b[32m[2020-06-29 00:10:17] __main__ INFO: \u001b[0mElapsed 17.57\n",
      "\u001b[32m[2020-06-29 00:10:17] __main__ INFO: \u001b[0mTrain 25 8424\n",
      "\u001b[32m[2020-06-29 00:12:38] __main__ INFO: \u001b[0mEpoch 25 Step 100/351 lr 0.100000 loss 1.2511 (1.2920) acc@1 0.5391 (0.5229) acc@5 0.8906 (0.8447)\n",
      "\u001b[32m[2020-06-29 00:14:59] __main__ INFO: \u001b[0mEpoch 25 Step 200/351 lr 0.100000 loss 1.1610 (1.2966) acc@1 0.5625 (0.5232) acc@5 0.8672 (0.8428)\n",
      "\u001b[32m[2020-06-29 00:17:20] __main__ INFO: \u001b[0mEpoch 25 Step 300/351 lr 0.100000 loss 1.0776 (1.2950) acc@1 0.6016 (0.5226) acc@5 0.8672 (0.8435)\n",
      "\u001b[32m[2020-06-29 00:18:32] __main__ INFO: \u001b[0mEpoch 25 Step 351/351 lr 0.100000 loss 1.3227 (1.2968) acc@1 0.5469 (0.5225) acc@5 0.8125 (0.8434)\n",
      "\u001b[32m[2020-06-29 00:18:32] __main__ INFO: \u001b[0mElapsed 494.72\n",
      "\u001b[32m[2020-06-29 00:18:32] __main__ INFO: \u001b[0mVal 25\n",
      "\u001b[32m[2020-06-29 00:18:49] __main__ INFO: \u001b[0mEpoch 25 loss 1.5999 acc@1 0.4444 acc@5 0.8200\n",
      "\u001b[32m[2020-06-29 00:18:49] __main__ INFO: \u001b[0mElapsed 17.57\n",
      "\u001b[32m[2020-06-29 00:18:49] __main__ INFO: \u001b[0mTrain 26 8775\n",
      "\u001b[32m[2020-06-29 00:21:10] __main__ INFO: \u001b[0mEpoch 26 Step 100/351 lr 0.100000 loss 1.2261 (1.2705) acc@1 0.5547 (0.5312) acc@5 0.8594 (0.8429)\n",
      "\u001b[32m[2020-06-29 00:23:31] __main__ INFO: \u001b[0mEpoch 26 Step 200/351 lr 0.100000 loss 1.2626 (1.2900) acc@1 0.5234 (0.5254) acc@5 0.8281 (0.8404)\n",
      "\u001b[32m[2020-06-29 00:25:52] __main__ INFO: \u001b[0mEpoch 26 Step 300/351 lr 0.100000 loss 1.2944 (1.2912) acc@1 0.5156 (0.5253) acc@5 0.8672 (0.8407)\n",
      "\u001b[32m[2020-06-29 00:27:04] __main__ INFO: \u001b[0mEpoch 26 Step 351/351 lr 0.100000 loss 1.2006 (1.2914) acc@1 0.5234 (0.5242) acc@5 0.8594 (0.8403)\n",
      "\u001b[32m[2020-06-29 00:27:04] __main__ INFO: \u001b[0mElapsed 494.53\n",
      "\u001b[32m[2020-06-29 00:27:04] __main__ INFO: \u001b[0mVal 26\n",
      "\u001b[32m[2020-06-29 00:27:22] __main__ INFO: \u001b[0mEpoch 26 loss 1.5873 acc@1 0.4564 acc@5 0.8224\n",
      "\u001b[32m[2020-06-29 00:27:22] __main__ INFO: \u001b[0mElapsed 17.54\n",
      "\u001b[32m[2020-06-29 00:27:22] __main__ INFO: \u001b[0mTrain 27 9126\n",
      "\u001b[32m[2020-06-29 00:29:43] __main__ INFO: \u001b[0mEpoch 27 Step 100/351 lr 0.100000 loss 1.1291 (1.2831) acc@1 0.5781 (0.5257) acc@5 0.8438 (0.8417)\n",
      "\u001b[32m[2020-06-29 00:32:03] __main__ INFO: \u001b[0mEpoch 27 Step 200/351 lr 0.100000 loss 1.2474 (1.2817) acc@1 0.5469 (0.5268) acc@5 0.8438 (0.8415)\n",
      "\u001b[32m[2020-06-29 00:34:24] __main__ INFO: \u001b[0mEpoch 27 Step 300/351 lr 0.100000 loss 1.3055 (1.2806) acc@1 0.4766 (0.5286) acc@5 0.8125 (0.8412)\n",
      "\u001b[32m[2020-06-29 00:35:36] __main__ INFO: \u001b[0mEpoch 27 Step 351/351 lr 0.100000 loss 1.3595 (1.2838) acc@1 0.4531 (0.5276) acc@5 0.8438 (0.8414)\n",
      "\u001b[32m[2020-06-29 00:35:36] __main__ INFO: \u001b[0mElapsed 494.74\n",
      "\u001b[32m[2020-06-29 00:35:36] __main__ INFO: \u001b[0mVal 27\n",
      "\u001b[32m[2020-06-29 00:35:54] __main__ INFO: \u001b[0mEpoch 27 loss 1.4914 acc@1 0.4504 acc@5 0.8344\n",
      "\u001b[32m[2020-06-29 00:35:54] __main__ INFO: \u001b[0mElapsed 17.55\n",
      "\u001b[32m[2020-06-29 00:35:54] __main__ INFO: \u001b[0mTrain 28 9477\n",
      "\u001b[32m[2020-06-29 00:38:15] __main__ INFO: \u001b[0mEpoch 28 Step 100/351 lr 0.100000 loss 1.3607 (1.2704) acc@1 0.5078 (0.5309) acc@5 0.8203 (0.8463)\n",
      "\u001b[32m[2020-06-29 00:40:36] __main__ INFO: \u001b[0mEpoch 28 Step 200/351 lr 0.100000 loss 1.3096 (1.2820) acc@1 0.5391 (0.5262) acc@5 0.8125 (0.8437)\n",
      "\u001b[32m[2020-06-29 00:42:56] __main__ INFO: \u001b[0mEpoch 28 Step 300/351 lr 0.100000 loss 1.2578 (1.2780) acc@1 0.5391 (0.5298) acc@5 0.8438 (0.8440)\n",
      "\u001b[32m[2020-06-29 00:44:08] __main__ INFO: \u001b[0mEpoch 28 Step 351/351 lr 0.100000 loss 1.0279 (1.2789) acc@1 0.6250 (0.5293) acc@5 0.8906 (0.8443)\n",
      "\u001b[32m[2020-06-29 00:44:08] __main__ INFO: \u001b[0mElapsed 494.43\n",
      "\u001b[32m[2020-06-29 00:44:08] __main__ INFO: \u001b[0mVal 28\n",
      "\u001b[32m[2020-06-29 00:44:26] __main__ INFO: \u001b[0mEpoch 28 loss 1.8912 acc@1 0.3920 acc@5 0.8100\n",
      "\u001b[32m[2020-06-29 00:44:26] __main__ INFO: \u001b[0mElapsed 17.57\n",
      "\u001b[32m[2020-06-29 00:44:26] __main__ INFO: \u001b[0mTrain 29 9828\n",
      "\u001b[32m[2020-06-29 00:46:47] __main__ INFO: \u001b[0mEpoch 29 Step 100/351 lr 0.100000 loss 1.2803 (1.2657) acc@1 0.5312 (0.5305) acc@5 0.8828 (0.8455)\n",
      "\u001b[32m[2020-06-29 00:49:08] __main__ INFO: \u001b[0mEpoch 29 Step 200/351 lr 0.100000 loss 1.2255 (1.2672) acc@1 0.5469 (0.5321) acc@5 0.8672 (0.8447)\n",
      "\u001b[32m[2020-06-29 00:51:29] __main__ INFO: \u001b[0mEpoch 29 Step 300/351 lr 0.100000 loss 1.3517 (1.2744) acc@1 0.4453 (0.5280) acc@5 0.8203 (0.8427)\n",
      "\u001b[32m[2020-06-29 00:52:41] __main__ INFO: \u001b[0mEpoch 29 Step 351/351 lr 0.100000 loss 1.1567 (1.2729) acc@1 0.5859 (0.5283) acc@5 0.8359 (0.8429)\n",
      "\u001b[32m[2020-06-29 00:52:41] __main__ INFO: \u001b[0mElapsed 494.82\n",
      "\u001b[32m[2020-06-29 00:52:41] __main__ INFO: \u001b[0mVal 29\n",
      "\u001b[32m[2020-06-29 00:52:58] __main__ INFO: \u001b[0mEpoch 29 loss 1.4265 acc@1 0.4834 acc@5 0.8410\n",
      "\u001b[32m[2020-06-29 00:52:58] __main__ INFO: \u001b[0mElapsed 17.56\n",
      "\u001b[32m[2020-06-29 00:52:58] __main__ INFO: \u001b[0mTrain 30 10179\n",
      "\u001b[32m[2020-06-29 00:55:19] __main__ INFO: \u001b[0mEpoch 30 Step 100/351 lr 0.100000 loss 1.2747 (1.2702) acc@1 0.4922 (0.5331) acc@5 0.8594 (0.8432)\n",
      "\u001b[32m[2020-06-29 00:57:40] __main__ INFO: \u001b[0mEpoch 30 Step 200/351 lr 0.100000 loss 1.4168 (1.2708) acc@1 0.4922 (0.5338) acc@5 0.8203 (0.8418)\n",
      "\u001b[32m[2020-06-29 01:00:01] __main__ INFO: \u001b[0mEpoch 30 Step 300/351 lr 0.100000 loss 1.3721 (1.2666) acc@1 0.4844 (0.5352) acc@5 0.8125 (0.8418)\n",
      "\u001b[32m[2020-06-29 01:01:13] __main__ INFO: \u001b[0mEpoch 30 Step 351/351 lr 0.100000 loss 1.2670 (1.2663) acc@1 0.5156 (0.5347) acc@5 0.8750 (0.8417)\n",
      "\u001b[32m[2020-06-29 01:01:13] __main__ INFO: \u001b[0mElapsed 494.45\n",
      "\u001b[32m[2020-06-29 01:01:13] __main__ INFO: \u001b[0mVal 30\n",
      "\u001b[32m[2020-06-29 01:01:30] __main__ INFO: \u001b[0mEpoch 30 loss 1.3940 acc@1 0.5032 acc@5 0.8418\n",
      "\u001b[32m[2020-06-29 01:01:30] __main__ INFO: \u001b[0mElapsed 17.55\n",
      "\u001b[32m[2020-06-29 01:01:30] __main__ INFO: \u001b[0mTrain 31 10530\n",
      "\u001b[32m[2020-06-29 01:03:51] __main__ INFO: \u001b[0mEpoch 31 Step 100/351 lr 0.100000 loss 1.1272 (1.2340) acc@1 0.6094 (0.5486) acc@5 0.9141 (0.8484)\n",
      "\u001b[32m[2020-06-29 01:06:12] __main__ INFO: \u001b[0mEpoch 31 Step 200/351 lr 0.100000 loss 1.2305 (1.2517) acc@1 0.5312 (0.5399) acc@5 0.8516 (0.8457)\n",
      "\u001b[32m[2020-06-29 01:08:33] __main__ INFO: \u001b[0mEpoch 31 Step 300/351 lr 0.100000 loss 1.2764 (1.2583) acc@1 0.5156 (0.5347) acc@5 0.8047 (0.8448)\n",
      "\u001b[32m[2020-06-29 01:09:44] __main__ INFO: \u001b[0mEpoch 31 Step 351/351 lr 0.100000 loss 1.3747 (1.2612) acc@1 0.5000 (0.5336) acc@5 0.7969 (0.8446)\n",
      "\u001b[32m[2020-06-29 01:09:44] __main__ INFO: \u001b[0mElapsed 494.14\n",
      "\u001b[32m[2020-06-29 01:09:44] __main__ INFO: \u001b[0mVal 31\n",
      "\u001b[32m[2020-06-29 01:10:02] __main__ INFO: \u001b[0mEpoch 31 loss 1.5572 acc@1 0.4542 acc@5 0.8246\n",
      "\u001b[32m[2020-06-29 01:10:02] __main__ INFO: \u001b[0mElapsed 17.55\n",
      "\u001b[32m[2020-06-29 01:10:02] __main__ INFO: \u001b[0mTrain 32 10881\n",
      "\u001b[32m[2020-06-29 01:12:23] __main__ INFO: \u001b[0mEpoch 32 Step 100/351 lr 0.100000 loss 1.1038 (1.2360) acc@1 0.6719 (0.5466) acc@5 0.9375 (0.8507)\n",
      "\u001b[32m[2020-06-29 01:14:44] __main__ INFO: \u001b[0mEpoch 32 Step 200/351 lr 0.100000 loss 1.3437 (1.2438) acc@1 0.4844 (0.5430) acc@5 0.8594 (0.8484)\n",
      "\u001b[32m[2020-06-29 01:17:05] __main__ INFO: \u001b[0mEpoch 32 Step 300/351 lr 0.100000 loss 1.2436 (1.2521) acc@1 0.5078 (0.5406) acc@5 0.8672 (0.8467)\n",
      "\u001b[32m[2020-06-29 01:18:17] __main__ INFO: \u001b[0mEpoch 32 Step 351/351 lr 0.100000 loss 1.0390 (1.2526) acc@1 0.6172 (0.5409) acc@5 0.8984 (0.8464)\n",
      "\u001b[32m[2020-06-29 01:18:17] __main__ INFO: \u001b[0mElapsed 494.61\n",
      "\u001b[32m[2020-06-29 01:18:17] __main__ INFO: \u001b[0mVal 32\n",
      "\u001b[32m[2020-06-29 01:18:34] __main__ INFO: \u001b[0mEpoch 32 loss 1.3979 acc@1 0.5016 acc@5 0.8372\n",
      "\u001b[32m[2020-06-29 01:18:34] __main__ INFO: \u001b[0mElapsed 17.54\n",
      "\u001b[32m[2020-06-29 01:18:34] __main__ INFO: \u001b[0mTrain 33 11232\n",
      "\u001b[32m[2020-06-29 01:20:55] __main__ INFO: \u001b[0mEpoch 33 Step 100/351 lr 0.100000 loss 1.0644 (1.2281) acc@1 0.6328 (0.5450) acc@5 0.8359 (0.8500)\n",
      "\u001b[32m[2020-06-29 01:23:16] __main__ INFO: \u001b[0mEpoch 33 Step 200/351 lr 0.100000 loss 1.2821 (1.2384) acc@1 0.5000 (0.5397) acc@5 0.8594 (0.8486)\n",
      "\u001b[32m[2020-06-29 01:25:36] __main__ INFO: \u001b[0mEpoch 33 Step 300/351 lr 0.100000 loss 1.2063 (1.2452) acc@1 0.5391 (0.5382) acc@5 0.8359 (0.8457)\n",
      "\u001b[32m[2020-06-29 01:26:48] __main__ INFO: \u001b[0mEpoch 33 Step 351/351 lr 0.100000 loss 1.1115 (1.2457) acc@1 0.6016 (0.5392) acc@5 0.8672 (0.8450)\n",
      "\u001b[32m[2020-06-29 01:26:48] __main__ INFO: \u001b[0mElapsed 494.06\n",
      "\u001b[32m[2020-06-29 01:26:48] __main__ INFO: \u001b[0mVal 33\n",
      "\u001b[32m[2020-06-29 01:27:06] __main__ INFO: \u001b[0mEpoch 33 loss 1.5420 acc@1 0.4554 acc@5 0.8300\n",
      "\u001b[32m[2020-06-29 01:27:06] __main__ INFO: \u001b[0mElapsed 17.54\n",
      "\u001b[32m[2020-06-29 01:27:06] __main__ INFO: \u001b[0mTrain 34 11583\n",
      "\u001b[32m[2020-06-29 01:29:27] __main__ INFO: \u001b[0mEpoch 34 Step 100/351 lr 0.100000 loss 1.1372 (1.2361) acc@1 0.5938 (0.5444) acc@5 0.8359 (0.8464)\n",
      "\u001b[32m[2020-06-29 01:31:47] __main__ INFO: \u001b[0mEpoch 34 Step 200/351 lr 0.100000 loss 1.2244 (1.2438) acc@1 0.5391 (0.5414) acc@5 0.8281 (0.8437)\n",
      "\u001b[32m[2020-06-29 01:34:08] __main__ INFO: \u001b[0mEpoch 34 Step 300/351 lr 0.100000 loss 1.1532 (1.2478) acc@1 0.5312 (0.5404) acc@5 0.8906 (0.8417)\n",
      "\u001b[32m[2020-06-29 01:35:20] __main__ INFO: \u001b[0mEpoch 34 Step 351/351 lr 0.100000 loss 1.2161 (1.2468) acc@1 0.5547 (0.5399) acc@5 0.8438 (0.8425)\n",
      "\u001b[32m[2020-06-29 01:35:20] __main__ INFO: \u001b[0mElapsed 494.33\n",
      "\u001b[32m[2020-06-29 01:35:20] __main__ INFO: \u001b[0mVal 34\n",
      "\u001b[32m[2020-06-29 01:35:38] __main__ INFO: \u001b[0mEpoch 34 loss 1.7641 acc@1 0.4230 acc@5 0.8166\n",
      "\u001b[32m[2020-06-29 01:35:38] __main__ INFO: \u001b[0mElapsed 17.55\n",
      "\u001b[32m[2020-06-29 01:35:38] __main__ INFO: \u001b[0mTrain 35 11934\n",
      "\u001b[32m[2020-06-29 01:37:58] __main__ INFO: \u001b[0mEpoch 35 Step 100/351 lr 0.100000 loss 1.2256 (1.2263) acc@1 0.5312 (0.5481) acc@5 0.8438 (0.8438)\n",
      "\u001b[32m[2020-06-29 01:40:19] __main__ INFO: \u001b[0mEpoch 35 Step 200/351 lr 0.100000 loss 1.0866 (1.2368) acc@1 0.6016 (0.5441) acc@5 0.8594 (0.8439)\n",
      "\u001b[32m[2020-06-29 01:42:40] __main__ INFO: \u001b[0mEpoch 35 Step 300/351 lr 0.100000 loss 1.1871 (1.2457) acc@1 0.5781 (0.5416) acc@5 0.8594 (0.8432)\n",
      "\u001b[32m[2020-06-29 01:43:52] __main__ INFO: \u001b[0mEpoch 35 Step 351/351 lr 0.100000 loss 1.2487 (1.2472) acc@1 0.5391 (0.5406) acc@5 0.8672 (0.8439)\n",
      "\u001b[32m[2020-06-29 01:43:52] __main__ INFO: \u001b[0mElapsed 494.13\n",
      "\u001b[32m[2020-06-29 01:43:52] __main__ INFO: \u001b[0mVal 35\n",
      "\u001b[32m[2020-06-29 01:44:09] __main__ INFO: \u001b[0mEpoch 35 loss 1.4390 acc@1 0.4754 acc@5 0.8344\n",
      "\u001b[32m[2020-06-29 01:44:09] __main__ INFO: \u001b[0mElapsed 17.56\n",
      "\u001b[32m[2020-06-29 01:44:09] __main__ INFO: \u001b[0mTrain 36 12285\n",
      "\u001b[32m[2020-06-29 01:46:30] __main__ INFO: \u001b[0mEpoch 36 Step 100/351 lr 0.100000 loss 1.3122 (1.2251) acc@1 0.5469 (0.5500) acc@5 0.8203 (0.8480)\n",
      "\u001b[32m[2020-06-29 01:48:51] __main__ INFO: \u001b[0mEpoch 36 Step 200/351 lr 0.100000 loss 1.4268 (1.2325) acc@1 0.4453 (0.5450) acc@5 0.7812 (0.8450)\n",
      "\u001b[32m[2020-06-29 01:51:12] __main__ INFO: \u001b[0mEpoch 36 Step 300/351 lr 0.100000 loss 1.2882 (1.2367) acc@1 0.5234 (0.5431) acc@5 0.8359 (0.8456)\n",
      "\u001b[32m[2020-06-29 01:52:24] __main__ INFO: \u001b[0mEpoch 36 Step 351/351 lr 0.100000 loss 1.2281 (1.2385) acc@1 0.5469 (0.5418) acc@5 0.8750 (0.8448)\n",
      "\u001b[32m[2020-06-29 01:52:24] __main__ INFO: \u001b[0mElapsed 494.61\n",
      "\u001b[32m[2020-06-29 01:52:24] __main__ INFO: \u001b[0mVal 36\n",
      "\u001b[32m[2020-06-29 01:52:41] __main__ INFO: \u001b[0mEpoch 36 loss 1.5905 acc@1 0.4464 acc@5 0.8168\n",
      "\u001b[32m[2020-06-29 01:52:41] __main__ INFO: \u001b[0mElapsed 17.57\n",
      "\u001b[32m[2020-06-29 01:52:41] __main__ INFO: \u001b[0mTrain 37 12636\n",
      "\u001b[32m[2020-06-29 01:55:03] __main__ INFO: \u001b[0mEpoch 37 Step 100/351 lr 0.100000 loss 1.1681 (1.2207) acc@1 0.5469 (0.5516) acc@5 0.8359 (0.8456)\n",
      "\u001b[32m[2020-06-29 01:57:23] __main__ INFO: \u001b[0mEpoch 37 Step 200/351 lr 0.100000 loss 1.1341 (1.2255) acc@1 0.5547 (0.5487) acc@5 0.8672 (0.8459)\n",
      "\u001b[32m[2020-06-29 01:59:44] __main__ INFO: \u001b[0mEpoch 37 Step 300/351 lr 0.100000 loss 1.1994 (1.2335) acc@1 0.5391 (0.5472) acc@5 0.8125 (0.8453)\n",
      "\u001b[32m[2020-06-29 02:00:56] __main__ INFO: \u001b[0mEpoch 37 Step 351/351 lr 0.100000 loss 1.2477 (1.2336) acc@1 0.5234 (0.5469) acc@5 0.8438 (0.8459)\n",
      "\u001b[32m[2020-06-29 02:00:56] __main__ INFO: \u001b[0mElapsed 494.41\n",
      "\u001b[32m[2020-06-29 02:00:56] __main__ INFO: \u001b[0mVal 37\n",
      "\u001b[32m[2020-06-29 02:01:13] __main__ INFO: \u001b[0mEpoch 37 loss 1.4788 acc@1 0.4804 acc@5 0.8430\n",
      "\u001b[32m[2020-06-29 02:01:13] __main__ INFO: \u001b[0mElapsed 17.57\n",
      "\u001b[32m[2020-06-29 02:01:13] __main__ INFO: \u001b[0mTrain 38 12987\n",
      "\u001b[32m[2020-06-29 02:03:34] __main__ INFO: \u001b[0mEpoch 38 Step 100/351 lr 0.100000 loss 1.3002 (1.2236) acc@1 0.5312 (0.5527) acc@5 0.7812 (0.8478)\n",
      "\u001b[32m[2020-06-29 02:05:55] __main__ INFO: \u001b[0mEpoch 38 Step 200/351 lr 0.100000 loss 1.3613 (1.2336) acc@1 0.4844 (0.5479) acc@5 0.7891 (0.8457)\n",
      "\u001b[32m[2020-06-29 02:08:16] __main__ INFO: \u001b[0mEpoch 38 Step 300/351 lr 0.100000 loss 1.1778 (1.2345) acc@1 0.5625 (0.5476) acc@5 0.8359 (0.8442)\n",
      "\u001b[32m[2020-06-29 02:09:28] __main__ INFO: \u001b[0mEpoch 38 Step 351/351 lr 0.100000 loss 1.1729 (1.2335) acc@1 0.6016 (0.5475) acc@5 0.8359 (0.8441)\n",
      "\u001b[32m[2020-06-29 02:09:28] __main__ INFO: \u001b[0mElapsed 494.48\n",
      "\u001b[32m[2020-06-29 02:09:28] __main__ INFO: \u001b[0mVal 38\n",
      "\u001b[32m[2020-06-29 02:09:45] __main__ INFO: \u001b[0mEpoch 38 loss 1.4219 acc@1 0.4896 acc@5 0.8426\n",
      "\u001b[32m[2020-06-29 02:09:45] __main__ INFO: \u001b[0mElapsed 17.56\n",
      "\u001b[32m[2020-06-29 02:09:45] __main__ INFO: \u001b[0mTrain 39 13338\n",
      "\u001b[32m[2020-06-29 02:12:06] __main__ INFO: \u001b[0mEpoch 39 Step 100/351 lr 0.100000 loss 1.1194 (1.2152) acc@1 0.5859 (0.5483) acc@5 0.9141 (0.8498)\n",
      "\u001b[32m[2020-06-29 02:14:27] __main__ INFO: \u001b[0mEpoch 39 Step 200/351 lr 0.100000 loss 1.2492 (1.2262) acc@1 0.5625 (0.5479) acc@5 0.8438 (0.8475)\n",
      "\u001b[32m[2020-06-29 02:16:48] __main__ INFO: \u001b[0mEpoch 39 Step 300/351 lr 0.100000 loss 1.3764 (1.2261) acc@1 0.5234 (0.5460) acc@5 0.8281 (0.8480)\n",
      "\u001b[32m[2020-06-29 02:18:00] __main__ INFO: \u001b[0mEpoch 39 Step 351/351 lr 0.100000 loss 1.2715 (1.2279) acc@1 0.5000 (0.5454) acc@5 0.8203 (0.8480)\n",
      "\u001b[32m[2020-06-29 02:18:00] __main__ INFO: \u001b[0mElapsed 494.56\n",
      "\u001b[32m[2020-06-29 02:18:00] __main__ INFO: \u001b[0mVal 39\n",
      "\u001b[32m[2020-06-29 02:18:18] __main__ INFO: \u001b[0mEpoch 39 loss 1.4721 acc@1 0.4736 acc@5 0.8330\n",
      "\u001b[32m[2020-06-29 02:18:18] __main__ INFO: \u001b[0mElapsed 17.56\n",
      "\u001b[32m[2020-06-29 02:18:18] __main__ INFO: \u001b[0mTrain 40 13689\n",
      "\u001b[32m[2020-06-29 02:20:38] __main__ INFO: \u001b[0mEpoch 40 Step 100/351 lr 0.100000 loss 1.4328 (1.2255) acc@1 0.5156 (0.5444) acc@5 0.8516 (0.8403)\n",
      "\u001b[32m[2020-06-29 02:22:59] __main__ INFO: \u001b[0mEpoch 40 Step 200/351 lr 0.100000 loss 1.1571 (1.2206) acc@1 0.5859 (0.5483) acc@5 0.8281 (0.8441)\n",
      "\u001b[32m[2020-06-29 02:25:20] __main__ INFO: \u001b[0mEpoch 40 Step 300/351 lr 0.100000 loss 1.2839 (1.2222) acc@1 0.5234 (0.5479) acc@5 0.8594 (0.8437)\n",
      "\u001b[32m[2020-06-29 02:26:32] __main__ INFO: \u001b[0mEpoch 40 Step 351/351 lr 0.100000 loss 1.1902 (1.2220) acc@1 0.5703 (0.5487) acc@5 0.8672 (0.8444)\n",
      "\u001b[32m[2020-06-29 02:26:32] __main__ INFO: \u001b[0mElapsed 494.36\n",
      "\u001b[32m[2020-06-29 02:26:32] __main__ INFO: \u001b[0mVal 40\n",
      "\u001b[32m[2020-06-29 02:26:50] __main__ INFO: \u001b[0mEpoch 40 loss 1.5844 acc@1 0.4378 acc@5 0.8318\n",
      "\u001b[32m[2020-06-29 02:26:50] __main__ INFO: \u001b[0mElapsed 17.57\n",
      "\u001b[32m[2020-06-29 02:26:50] __main__ INFO: \u001b[0mTrain 41 14040\n",
      "\u001b[32m[2020-06-29 02:29:10] __main__ INFO: \u001b[0mEpoch 41 Step 100/351 lr 0.100000 loss 1.3924 (1.2001) acc@1 0.4297 (0.5560) acc@5 0.8828 (0.8503)\n",
      "\u001b[32m[2020-06-29 02:31:31] __main__ INFO: \u001b[0mEpoch 41 Step 200/351 lr 0.100000 loss 1.3324 (1.2107) acc@1 0.4844 (0.5541) acc@5 0.8203 (0.8499)\n",
      "\u001b[32m[2020-06-29 02:33:52] __main__ INFO: \u001b[0mEpoch 41 Step 300/351 lr 0.100000 loss 1.2895 (1.2167) acc@1 0.5469 (0.5523) acc@5 0.8516 (0.8498)\n",
      "\u001b[32m[2020-06-29 02:35:04] __main__ INFO: \u001b[0mEpoch 41 Step 351/351 lr 0.100000 loss 1.0720 (1.2216) acc@1 0.6250 (0.5495) acc@5 0.9219 (0.8491)\n",
      "\u001b[32m[2020-06-29 02:35:04] __main__ INFO: \u001b[0mElapsed 494.13\n",
      "\u001b[32m[2020-06-29 02:35:04] __main__ INFO: \u001b[0mVal 41\n",
      "\u001b[32m[2020-06-29 02:35:21] __main__ INFO: \u001b[0mEpoch 41 loss 1.4592 acc@1 0.4936 acc@5 0.8346\n",
      "\u001b[32m[2020-06-29 02:35:21] __main__ INFO: \u001b[0mElapsed 17.57\n",
      "\u001b[32m[2020-06-29 02:35:21] __main__ INFO: \u001b[0mTrain 42 14391\n",
      "\u001b[32m[2020-06-29 02:37:42] __main__ INFO: \u001b[0mEpoch 42 Step 100/351 lr 0.100000 loss 1.1173 (1.2053) acc@1 0.5859 (0.5557) acc@5 0.8594 (0.8480)\n",
      "\u001b[32m[2020-06-29 02:40:03] __main__ INFO: \u001b[0mEpoch 42 Step 200/351 lr 0.100000 loss 1.2812 (1.2224) acc@1 0.5000 (0.5472) acc@5 0.7969 (0.8455)\n",
      "\u001b[32m[2020-06-29 02:42:24] __main__ INFO: \u001b[0mEpoch 42 Step 300/351 lr 0.100000 loss 1.2692 (1.2155) acc@1 0.5078 (0.5505) acc@5 0.8516 (0.8448)\n",
      "\u001b[32m[2020-06-29 02:43:35] __main__ INFO: \u001b[0mEpoch 42 Step 351/351 lr 0.100000 loss 1.4073 (1.2168) acc@1 0.4609 (0.5502) acc@5 0.8594 (0.8453)\n",
      "\u001b[32m[2020-06-29 02:43:35] __main__ INFO: \u001b[0mElapsed 494.18\n",
      "\u001b[32m[2020-06-29 02:43:35] __main__ INFO: \u001b[0mVal 42\n",
      "\u001b[32m[2020-06-29 02:43:53] __main__ INFO: \u001b[0mEpoch 42 loss 1.4738 acc@1 0.4914 acc@5 0.8376\n",
      "\u001b[32m[2020-06-29 02:43:53] __main__ INFO: \u001b[0mElapsed 17.54\n",
      "\u001b[32m[2020-06-29 02:43:53] __main__ INFO: \u001b[0mTrain 43 14742\n",
      "\u001b[32m[2020-06-29 02:46:14] __main__ INFO: \u001b[0mEpoch 43 Step 100/351 lr 0.100000 loss 1.2439 (1.2212) acc@1 0.5000 (0.5470) acc@5 0.7969 (0.8458)\n",
      "\u001b[32m[2020-06-29 02:48:35] __main__ INFO: \u001b[0mEpoch 43 Step 200/351 lr 0.100000 loss 1.1128 (1.2160) acc@1 0.5703 (0.5494) acc@5 0.8984 (0.8475)\n",
      "\u001b[32m[2020-06-29 02:50:55] __main__ INFO: \u001b[0mEpoch 43 Step 300/351 lr 0.100000 loss 1.1621 (1.2116) acc@1 0.5703 (0.5522) acc@5 0.8750 (0.8484)\n",
      "\u001b[32m[2020-06-29 02:52:07] __main__ INFO: \u001b[0mEpoch 43 Step 351/351 lr 0.100000 loss 1.0388 (1.2118) acc@1 0.6172 (0.5522) acc@5 0.8594 (0.8480)\n",
      "\u001b[32m[2020-06-29 02:52:07] __main__ INFO: \u001b[0mElapsed 494.10\n",
      "\u001b[32m[2020-06-29 02:52:07] __main__ INFO: \u001b[0mVal 43\n",
      "\u001b[32m[2020-06-29 02:52:25] __main__ INFO: \u001b[0mEpoch 43 loss 1.4805 acc@1 0.4722 acc@5 0.8318\n",
      "\u001b[32m[2020-06-29 02:52:25] __main__ INFO: \u001b[0mElapsed 17.58\n",
      "\u001b[32m[2020-06-29 02:52:25] __main__ INFO: \u001b[0mTrain 44 15093\n",
      "\u001b[32m[2020-06-29 02:54:45] __main__ INFO: \u001b[0mEpoch 44 Step 100/351 lr 0.100000 loss 1.1180 (1.1861) acc@1 0.5938 (0.5607) acc@5 0.8828 (0.8500)\n",
      "\u001b[32m[2020-06-29 02:57:06] __main__ INFO: \u001b[0mEpoch 44 Step 200/351 lr 0.100000 loss 1.1251 (1.2076) acc@1 0.5703 (0.5529) acc@5 0.8750 (0.8488)\n",
      "\u001b[32m[2020-06-29 02:59:27] __main__ INFO: \u001b[0mEpoch 44 Step 300/351 lr 0.100000 loss 1.3951 (1.2133) acc@1 0.4609 (0.5511) acc@5 0.8438 (0.8482)\n",
      "\u001b[32m[2020-06-29 03:00:39] __main__ INFO: \u001b[0mEpoch 44 Step 351/351 lr 0.100000 loss 1.2194 (1.2131) acc@1 0.5625 (0.5509) acc@5 0.8750 (0.8473)\n",
      "\u001b[32m[2020-06-29 03:00:39] __main__ INFO: \u001b[0mElapsed 494.39\n",
      "\u001b[32m[2020-06-29 03:00:39] __main__ INFO: \u001b[0mVal 44\n",
      "\u001b[32m[2020-06-29 03:00:57] __main__ INFO: \u001b[0mEpoch 44 loss 1.3255 acc@1 0.5128 acc@5 0.8442\n",
      "\u001b[32m[2020-06-29 03:00:57] __main__ INFO: \u001b[0mElapsed 17.56\n",
      "\u001b[32m[2020-06-29 03:00:57] __main__ INFO: \u001b[0mTrain 45 15444\n",
      "\u001b[32m[2020-06-29 03:03:17] __main__ INFO: \u001b[0mEpoch 45 Step 100/351 lr 0.100000 loss 1.0561 (1.1992) acc@1 0.6094 (0.5584) acc@5 0.8438 (0.8470)\n",
      "\u001b[32m[2020-06-29 03:05:38] __main__ INFO: \u001b[0mEpoch 45 Step 200/351 lr 0.100000 loss 1.1511 (1.2025) acc@1 0.5703 (0.5561) acc@5 0.8516 (0.8499)\n",
      "\u001b[32m[2020-06-29 03:07:59] __main__ INFO: \u001b[0mEpoch 45 Step 300/351 lr 0.100000 loss 1.2755 (1.2099) acc@1 0.5000 (0.5552) acc@5 0.8047 (0.8490)\n",
      "\u001b[32m[2020-06-29 03:09:11] __main__ INFO: \u001b[0mEpoch 45 Step 351/351 lr 0.100000 loss 1.2186 (1.2085) acc@1 0.5391 (0.5561) acc@5 0.8203 (0.8476)\n",
      "\u001b[32m[2020-06-29 03:09:11] __main__ INFO: \u001b[0mElapsed 494.10\n",
      "\u001b[32m[2020-06-29 03:09:11] __main__ INFO: \u001b[0mVal 45\n",
      "\u001b[32m[2020-06-29 03:09:28] __main__ INFO: \u001b[0mEpoch 45 loss 1.4840 acc@1 0.4760 acc@5 0.8290\n",
      "\u001b[32m[2020-06-29 03:09:28] __main__ INFO: \u001b[0mElapsed 17.55\n",
      "\u001b[32m[2020-06-29 03:09:28] __main__ INFO: \u001b[0mTrain 46 15795\n",
      "\u001b[32m[2020-06-29 03:11:49] __main__ INFO: \u001b[0mEpoch 46 Step 100/351 lr 0.100000 loss 1.1739 (1.1942) acc@1 0.5547 (0.5547) acc@5 0.8672 (0.8491)\n",
      "\u001b[32m[2020-06-29 03:14:10] __main__ INFO: \u001b[0mEpoch 46 Step 200/351 lr 0.100000 loss 1.2717 (1.2063) acc@1 0.5312 (0.5527) acc@5 0.8594 (0.8477)\n",
      "\u001b[32m[2020-06-29 03:16:30] __main__ INFO: \u001b[0mEpoch 46 Step 300/351 lr 0.100000 loss 1.1938 (1.2023) acc@1 0.5781 (0.5552) acc@5 0.8281 (0.8489)\n",
      "\u001b[32m[2020-06-29 03:17:42] __main__ INFO: \u001b[0mEpoch 46 Step 351/351 lr 0.100000 loss 1.3683 (1.2024) acc@1 0.4375 (0.5549) acc@5 0.7734 (0.8490)\n",
      "\u001b[32m[2020-06-29 03:17:42] __main__ INFO: \u001b[0mElapsed 494.03\n",
      "\u001b[32m[2020-06-29 03:17:42] __main__ INFO: \u001b[0mVal 46\n",
      "\u001b[32m[2020-06-29 03:18:00] __main__ INFO: \u001b[0mEpoch 46 loss 1.5279 acc@1 0.4728 acc@5 0.8356\n",
      "\u001b[32m[2020-06-29 03:18:00] __main__ INFO: \u001b[0mElapsed 17.54\n",
      "\u001b[32m[2020-06-29 03:18:00] __main__ INFO: \u001b[0mTrain 47 16146\n",
      "\u001b[32m[2020-06-29 03:20:21] __main__ INFO: \u001b[0mEpoch 47 Step 100/351 lr 0.100000 loss 1.2577 (1.1900) acc@1 0.5156 (0.5580) acc@5 0.8047 (0.8514)\n",
      "\u001b[32m[2020-06-29 03:22:41] __main__ INFO: \u001b[0mEpoch 47 Step 200/351 lr 0.100000 loss 1.1155 (1.1962) acc@1 0.5703 (0.5571) acc@5 0.8516 (0.8496)\n"
     ]
    }
   ],
   "source": [
    "# Train the model per the settings specified for ResNext 29_4x64d in the original paper\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "!python train.py --config configs/cifar/resnext.yaml \\\n",
    "    model.resnext.cardinality 4 \\\n",
    "    train.batch_size 128 \\\n",
    "    dataset.name CIFAR10_RA_2_5 \\\n",
    "    train.base_lr 0.1 \\\n",
    "    train.output_dir /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_5/exp00 \\\n",
    "    scheduler.epochs 400\n",
    "\n",
    "# Number of epochs should be 300!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-01 13:12:32] __main__ INFO: \u001b[0mdevice: cuda\n",
      "cudnn:\n",
      "  benchmark: True\n",
      "  deterministic: False\n",
      "dataset:\n",
      "  name: CIFAR10\n",
      "  dataset_dir: ~/.torch/datasets/CIFAR10\n",
      "  image_size: 32\n",
      "  n_channels: 3\n",
      "  n_classes: 10\n",
      "model:\n",
      "  type: cifar\n",
      "  name: resnext\n",
      "  init_mode: kaiming_fan_out\n",
      "  vgg:\n",
      "    n_channels: [64, 128, 256, 512, 512]\n",
      "    n_layers: [2, 2, 3, 3, 3]\n",
      "    use_bn: True\n",
      "  resnet:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "  resnet_preact:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "  wrn:\n",
      "    depth: 28\n",
      "    initial_channels: 16\n",
      "    widening_factor: 10\n",
      "    drop_rate: 0.0\n",
      "  densenet:\n",
      "    depth: 100\n",
      "    n_blocks: [6, 12, 24, 16]\n",
      "    block_type: bottleneck\n",
      "    growth_rate: 12\n",
      "    drop_rate: 0.0\n",
      "    compression_rate: 0.5\n",
      "  pyramidnet:\n",
      "    depth: 272\n",
      "    n_blocks: [3, 24, 36, 3]\n",
      "    initial_channels: 16\n",
      "    block_type: bottleneck\n",
      "    alpha: 200\n",
      "  resnext:\n",
      "    depth: 29\n",
      "    n_blocks: [3, 4, 6, 3]\n",
      "    initial_channels: 64\n",
      "    cardinality: 4\n",
      "    base_channels: 64\n",
      "  shake_shake:\n",
      "    depth: 26\n",
      "    initial_channels: 96\n",
      "    shake_forward: True\n",
      "    shake_backward: True\n",
      "    shake_image: True\n",
      "  se_resnet_preact:\n",
      "    depth: 110\n",
      "    initial_channels: 16\n",
      "    se_reduction: 16\n",
      "    block_type: basic\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "train:\n",
      "  checkpoint: /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_5/exp00/checkpoint_00400.pth\n",
      "  resume: False\n",
      "  precision: O0\n",
      "  batch_size: 128\n",
      "  subdivision: 1\n",
      "  optimizer: sgd\n",
      "  base_lr: 0.001\n",
      "  momentum: 0.9\n",
      "  nesterov: True\n",
      "  weight_decay: 0.0005\n",
      "  no_weight_decay_on_bn: False\n",
      "  gradient_clip: 0\n",
      "  start_epoch: 0\n",
      "  seed: 0\n",
      "  val_first: True\n",
      "  val_period: 1\n",
      "  val_ratio: 0.1\n",
      "  use_test_as_val: False\n",
      "  output_dir: /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_5/exp00_resume400_50\n",
      "  log_period: 100\n",
      "  checkpoint_period: 100\n",
      "  use_tensorboard: True\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: True\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "  distributed: False\n",
      "  dist:\n",
      "    backend: nccl\n",
      "    init_method: env://\n",
      "    world_size: -1\n",
      "    node_rank: -1\n",
      "    local_rank: 0\n",
      "    use_sync_bn: False\n",
      "tensorboard:\n",
      "  train_images: False\n",
      "  val_images: False\n",
      "  model_params: False\n",
      "optim:\n",
      "  adam:\n",
      "    betas: (0.9, 0.999)\n",
      "  lars:\n",
      "    eps: 1e-09\n",
      "    threshold: 0.01\n",
      "  adabound:\n",
      "    betas: (0.9, 0.999)\n",
      "    final_lr: 0.1\n",
      "    gamma: 0.001\n",
      "scheduler:\n",
      "  epochs: 50\n",
      "  warmup:\n",
      "    type: none\n",
      "    epochs: 0\n",
      "    start_factor: 0.001\n",
      "    exponent: 4\n",
      "  type: multistep\n",
      "  milestones: [150, 225]\n",
      "  lr_decay: 0.1\n",
      "  lr_min_factor: 0.001\n",
      "  T0: 10\n",
      "  T_mul: 1.0\n",
      "validation:\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: False\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "augmentation:\n",
      "  use_random_crop: True\n",
      "  use_random_horizontal_flip: True\n",
      "  use_cutout: False\n",
      "  use_random_erasing: False\n",
      "  use_dual_cutout: False\n",
      "  use_mixup: False\n",
      "  use_ricap: False\n",
      "  use_cutmix: False\n",
      "  use_label_smoothing: False\n",
      "  random_crop:\n",
      "    padding: 4\n",
      "    fill: 0\n",
      "    padding_mode: constant\n",
      "  random_horizontal_flip:\n",
      "    prob: 0.5\n",
      "  cutout:\n",
      "    prob: 1.0\n",
      "    mask_size: 16\n",
      "    cut_inside: False\n",
      "    mask_color: 0\n",
      "    dual_cutout_alpha: 0.1\n",
      "  random_erasing:\n",
      "    prob: 0.5\n",
      "    area_ratio_range: [0.02, 0.4]\n",
      "    min_aspect_ratio: 0.3\n",
      "    max_attempt: 20\n",
      "  mixup:\n",
      "    alpha: 1.0\n",
      "  ricap:\n",
      "    beta: 0.3\n",
      "  cutmix:\n",
      "    alpha: 1.0\n",
      "  label_smoothing:\n",
      "    epsilon: 0.1\n",
      "tta:\n",
      "  use_resize: False\n",
      "  use_center_crop: False\n",
      "  resize: 256\n",
      "test:\n",
      "  checkpoint: ''\n",
      "  output_dir: ''\n",
      "  batch_size: 128\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    pin_memory: False\n",
      "\u001b[32m[2020-07-01 13:12:32] __main__ INFO: \u001b[0menv_info:\n",
      "  pytorch_version: 1.4.0\n",
      "  cuda_version: 10.1\n",
      "  cudnn_version: 7603\n",
      "  num_gpus: 1\n",
      "  gpu_name: Tesla K80\n",
      "  gpu_capability: 3.7\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /home/ec2-user/.torch/datasets/CIFAR10/cifar-10-python.tar.gz\n",
      "170500096it [00:02, 71032290.57it/s]                                            \n",
      "Extracting /home/ec2-user/.torch/datasets/CIFAR10/cifar-10-python.tar.gz to /home/ec2-user/.torch/datasets/CIFAR10\n",
      "\u001b[32m[2020-07-01 13:12:41] __main__ INFO: \u001b[0mMACs  : 2.75G\n",
      "\u001b[32m[2020-07-01 13:12:41] __main__ INFO: \u001b[0m#params: 17.56M\n",
      "Selected optimization level O0:  Pure FP32 training.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\",)\n",
      "\u001b[32m[2020-07-01 13:12:41] __main__ INFO: \u001b[0mVal 0\n",
      "\u001b[32m[2020-07-01 13:13:06] __main__ INFO: \u001b[0mEpoch 0 loss 0.4004 acc@1 0.8850 acc@5 0.9950\n",
      "\u001b[32m[2020-07-01 13:13:06] __main__ INFO: \u001b[0mElapsed 25.51\n",
      "\u001b[32m[2020-07-01 13:13:06] __main__ INFO: \u001b[0mTrain 1 0\n",
      "\u001b[32m[2020-07-01 13:15:35] __main__ INFO: \u001b[0mEpoch 1 Step 100/351 lr 0.001000 loss 0.0915 (0.1493) acc@1 0.9609 (0.9555) acc@5 1.0000 (0.9990)\n",
      "\u001b[32m[2020-07-01 13:17:55] __main__ INFO: \u001b[0mEpoch 1 Step 200/351 lr 0.001000 loss 0.1350 (0.1458) acc@1 0.9609 (0.9562) acc@5 1.0000 (0.9989)\n",
      "\u001b[32m[2020-07-01 13:20:16] __main__ INFO: \u001b[0mEpoch 1 Step 300/351 lr 0.001000 loss 0.0747 (0.1413) acc@1 0.9766 (0.9558) acc@5 1.0000 (0.9989)\n",
      "\u001b[32m[2020-07-01 13:21:28] __main__ INFO: \u001b[0mEpoch 1 Step 351/351 lr 0.001000 loss 0.1007 (0.1406) acc@1 0.9688 (0.9559) acc@5 1.0000 (0.9989)\n",
      "\u001b[32m[2020-07-01 13:21:28] __main__ INFO: \u001b[0mElapsed 502.15\n",
      "\u001b[32m[2020-07-01 13:21:28] __main__ INFO: \u001b[0mVal 1\n",
      "\u001b[32m[2020-07-01 13:21:46] __main__ INFO: \u001b[0mEpoch 1 loss 0.2523 acc@1 0.9202 acc@5 0.9978\n",
      "\u001b[32m[2020-07-01 13:21:46] __main__ INFO: \u001b[0mElapsed 17.63\n",
      "\u001b[32m[2020-07-01 13:21:46] __main__ INFO: \u001b[0mTrain 2 351\n",
      "\u001b[32m[2020-07-01 13:24:07] __main__ INFO: \u001b[0mEpoch 2 Step 100/351 lr 0.001000 loss 0.0467 (0.0908) acc@1 1.0000 (0.9729) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-07-01 13:26:28] __main__ INFO: \u001b[0mEpoch 2 Step 200/351 lr 0.001000 loss 0.1204 (0.0919) acc@1 0.9531 (0.9730) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-07-01 13:28:49] __main__ INFO: \u001b[0mEpoch 2 Step 300/351 lr 0.001000 loss 0.0306 (0.0931) acc@1 0.9922 (0.9724) acc@5 1.0000 (0.9995)\n",
      "\u001b[32m[2020-07-01 13:30:01] __main__ INFO: \u001b[0mEpoch 2 Step 351/351 lr 0.001000 loss 0.1446 (0.0929) acc@1 0.9609 (0.9724) acc@5 1.0000 (0.9994)\n",
      "\u001b[32m[2020-07-01 13:30:01] __main__ INFO: \u001b[0mElapsed 495.37\n",
      "\u001b[32m[2020-07-01 13:30:01] __main__ INFO: \u001b[0mVal 2\n",
      "\u001b[32m[2020-07-01 13:30:19] __main__ INFO: \u001b[0mEpoch 2 loss 0.2415 acc@1 0.9226 acc@5 0.9982\n",
      "\u001b[32m[2020-07-01 13:30:19] __main__ INFO: \u001b[0mElapsed 17.66\n",
      "\u001b[32m[2020-07-01 13:30:19] __main__ INFO: \u001b[0mTrain 3 702\n",
      "\u001b[32m[2020-07-01 13:32:40] __main__ INFO: \u001b[0mEpoch 3 Step 100/351 lr 0.001000 loss 0.0721 (0.0730) acc@1 0.9688 (0.9801) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-01 13:35:01] __main__ INFO: \u001b[0mEpoch 3 Step 200/351 lr 0.001000 loss 0.1263 (0.0716) acc@1 0.9609 (0.9796) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-01 13:37:22] __main__ INFO: \u001b[0mEpoch 3 Step 300/351 lr 0.001000 loss 0.0592 (0.0709) acc@1 0.9688 (0.9796) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-07-01 13:38:34] __main__ INFO: \u001b[0mEpoch 3 Step 351/351 lr 0.001000 loss 0.0654 (0.0706) acc@1 0.9766 (0.9797) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-01 13:38:34] __main__ INFO: \u001b[0mElapsed 494.67\n",
      "\u001b[32m[2020-07-01 13:38:34] __main__ INFO: \u001b[0mVal 3\n",
      "\u001b[32m[2020-07-01 13:38:51] __main__ INFO: \u001b[0mEpoch 3 loss 0.2482 acc@1 0.9228 acc@5 0.9982\n",
      "\u001b[32m[2020-07-01 13:38:51] __main__ INFO: \u001b[0mElapsed 17.64\n",
      "\u001b[32m[2020-07-01 13:38:51] __main__ INFO: \u001b[0mTrain 4 1053\n",
      "\u001b[32m[2020-07-01 13:41:13] __main__ INFO: \u001b[0mEpoch 4 Step 100/351 lr 0.001000 loss 0.0343 (0.0566) acc@1 1.0000 (0.9859) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-01 13:43:34] __main__ INFO: \u001b[0mEpoch 4 Step 200/351 lr 0.001000 loss 0.0268 (0.0538) acc@1 1.0000 (0.9873) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-01 13:45:55] __main__ INFO: \u001b[0mEpoch 4 Step 300/351 lr 0.001000 loss 0.0592 (0.0533) acc@1 0.9844 (0.9867) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-01 13:47:07] __main__ INFO: \u001b[0mEpoch 4 Step 351/351 lr 0.001000 loss 0.0446 (0.0536) acc@1 0.9922 (0.9866) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-01 13:47:07] __main__ INFO: \u001b[0mElapsed 495.34\n",
      "\u001b[32m[2020-07-01 13:47:07] __main__ INFO: \u001b[0mVal 4\n",
      "\u001b[32m[2020-07-01 13:47:24] __main__ INFO: \u001b[0mEpoch 4 loss 0.2383 acc@1 0.9250 acc@5 0.9984\n",
      "\u001b[32m[2020-07-01 13:47:24] __main__ INFO: \u001b[0mElapsed 17.66\n",
      "\u001b[32m[2020-07-01 13:47:24] __main__ INFO: \u001b[0mTrain 5 1404\n",
      "\u001b[32m[2020-07-01 13:49:46] __main__ INFO: \u001b[0mEpoch 5 Step 100/351 lr 0.001000 loss 0.0208 (0.0413) acc@1 1.0000 (0.9916) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-01 13:52:07] __main__ INFO: \u001b[0mEpoch 5 Step 200/351 lr 0.001000 loss 0.0549 (0.0419) acc@1 0.9922 (0.9911) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-01 13:54:28] __main__ INFO: \u001b[0mEpoch 5 Step 300/351 lr 0.001000 loss 0.0223 (0.0429) acc@1 1.0000 (0.9907) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-01 13:55:40] __main__ INFO: \u001b[0mEpoch 5 Step 351/351 lr 0.001000 loss 0.0293 (0.0436) acc@1 1.0000 (0.9904) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-01 13:55:40] __main__ INFO: \u001b[0mElapsed 495.43\n",
      "\u001b[32m[2020-07-01 13:55:40] __main__ INFO: \u001b[0mVal 5\n",
      "\u001b[32m[2020-07-01 13:55:57] __main__ INFO: \u001b[0mEpoch 5 loss 0.2430 acc@1 0.9258 acc@5 0.9980\n",
      "\u001b[32m[2020-07-01 13:55:57] __main__ INFO: \u001b[0mElapsed 17.63\n",
      "\u001b[32m[2020-07-01 13:55:57] __main__ INFO: \u001b[0mTrain 6 1755\n",
      "\u001b[32m[2020-07-01 13:58:18] __main__ INFO: \u001b[0mEpoch 6 Step 100/351 lr 0.001000 loss 0.0329 (0.0345) acc@1 0.9922 (0.9946) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 14:00:39] __main__ INFO: \u001b[0mEpoch 6 Step 200/351 lr 0.001000 loss 0.0289 (0.0357) acc@1 1.0000 (0.9936) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-01 14:03:00] __main__ INFO: \u001b[0mEpoch 6 Step 300/351 lr 0.001000 loss 0.0374 (0.0366) acc@1 1.0000 (0.9932) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-01 14:04:12] __main__ INFO: \u001b[0mEpoch 6 Step 351/351 lr 0.001000 loss 0.0534 (0.0362) acc@1 0.9922 (0.9933) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-01 14:04:12] __main__ INFO: \u001b[0mElapsed 494.63\n",
      "\u001b[32m[2020-07-01 14:04:12] __main__ INFO: \u001b[0mVal 6\n",
      "\u001b[32m[2020-07-01 14:04:30] __main__ INFO: \u001b[0mEpoch 6 loss 0.2378 acc@1 0.9258 acc@5 0.9980\n",
      "\u001b[32m[2020-07-01 14:04:30] __main__ INFO: \u001b[0mElapsed 17.64\n",
      "\u001b[32m[2020-07-01 14:04:30] __main__ INFO: \u001b[0mTrain 7 2106\n",
      "\u001b[32m[2020-07-01 14:06:51] __main__ INFO: \u001b[0mEpoch 7 Step 100/351 lr 0.001000 loss 0.0258 (0.0298) acc@1 1.0000 (0.9950) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 14:09:12] __main__ INFO: \u001b[0mEpoch 7 Step 200/351 lr 0.001000 loss 0.0208 (0.0301) acc@1 1.0000 (0.9947) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 14:11:34] __main__ INFO: \u001b[0mEpoch 7 Step 300/351 lr 0.001000 loss 0.0170 (0.0300) acc@1 1.0000 (0.9948) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 14:12:46] __main__ INFO: \u001b[0mEpoch 7 Step 351/351 lr 0.001000 loss 0.0262 (0.0304) acc@1 0.9922 (0.9947) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 14:12:46] __main__ INFO: \u001b[0mElapsed 496.28\n",
      "\u001b[32m[2020-07-01 14:12:46] __main__ INFO: \u001b[0mVal 7\n",
      "\u001b[32m[2020-07-01 14:13:04] __main__ INFO: \u001b[0mEpoch 7 loss 0.2396 acc@1 0.9266 acc@5 0.9982\n",
      "\u001b[32m[2020-07-01 14:13:04] __main__ INFO: \u001b[0mElapsed 17.80\n",
      "\u001b[32m[2020-07-01 14:13:04] __main__ INFO: \u001b[0mTrain 8 2457\n",
      "\u001b[32m[2020-07-01 14:15:26] __main__ INFO: \u001b[0mEpoch 8 Step 100/351 lr 0.001000 loss 0.0389 (0.0267) acc@1 0.9922 (0.9952) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 14:17:47] __main__ INFO: \u001b[0mEpoch 8 Step 200/351 lr 0.001000 loss 0.0286 (0.0266) acc@1 1.0000 (0.9955) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 14:20:09] __main__ INFO: \u001b[0mEpoch 8 Step 300/351 lr 0.001000 loss 0.0319 (0.0266) acc@1 0.9922 (0.9955) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 14:21:21] __main__ INFO: \u001b[0mEpoch 8 Step 351/351 lr 0.001000 loss 0.0235 (0.0270) acc@1 0.9922 (0.9953) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 14:21:21] __main__ INFO: \u001b[0mElapsed 497.63\n",
      "\u001b[32m[2020-07-01 14:21:21] __main__ INFO: \u001b[0mVal 8\n",
      "\u001b[32m[2020-07-01 14:21:39] __main__ INFO: \u001b[0mEpoch 8 loss 0.2364 acc@1 0.9290 acc@5 0.9976\n",
      "\u001b[32m[2020-07-01 14:21:39] __main__ INFO: \u001b[0mElapsed 17.77\n",
      "\u001b[32m[2020-07-01 14:21:39] __main__ INFO: \u001b[0mTrain 9 2808\n",
      "\u001b[32m[2020-07-01 14:24:01] __main__ INFO: \u001b[0mEpoch 9 Step 100/351 lr 0.001000 loss 0.0218 (0.0222) acc@1 1.0000 (0.9977) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 14:26:23] __main__ INFO: \u001b[0mEpoch 9 Step 200/351 lr 0.001000 loss 0.0287 (0.0224) acc@1 0.9922 (0.9973) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 14:28:44] __main__ INFO: \u001b[0mEpoch 9 Step 300/351 lr 0.001000 loss 0.0203 (0.0224) acc@1 1.0000 (0.9971) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 14:29:57] __main__ INFO: \u001b[0mEpoch 9 Step 351/351 lr 0.001000 loss 0.0339 (0.0229) acc@1 0.9922 (0.9969) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 14:29:57] __main__ INFO: \u001b[0mElapsed 497.56\n",
      "\u001b[32m[2020-07-01 14:29:57] __main__ INFO: \u001b[0mVal 9\n",
      "\u001b[32m[2020-07-01 14:30:14] __main__ INFO: \u001b[0mEpoch 9 loss 0.2374 acc@1 0.9244 acc@5 0.9980\n",
      "\u001b[32m[2020-07-01 14:30:14] __main__ INFO: \u001b[0mElapsed 17.75\n",
      "\u001b[32m[2020-07-01 14:30:14] __main__ INFO: \u001b[0mTrain 10 3159\n",
      "\u001b[32m[2020-07-01 14:32:36] __main__ INFO: \u001b[0mEpoch 10 Step 100/351 lr 0.001000 loss 0.0197 (0.0189) acc@1 1.0000 (0.9977) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 14:34:58] __main__ INFO: \u001b[0mEpoch 10 Step 200/351 lr 0.001000 loss 0.0222 (0.0189) acc@1 1.0000 (0.9977) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 14:37:20] __main__ INFO: \u001b[0mEpoch 10 Step 300/351 lr 0.001000 loss 0.0146 (0.0193) acc@1 1.0000 (0.9977) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 14:38:32] __main__ INFO: \u001b[0mEpoch 10 Step 351/351 lr 0.001000 loss 0.0338 (0.0199) acc@1 0.9922 (0.9974) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 14:38:32] __main__ INFO: \u001b[0mElapsed 497.53\n",
      "\u001b[32m[2020-07-01 14:38:32] __main__ INFO: \u001b[0mVal 10\n",
      "\u001b[32m[2020-07-01 14:38:50] __main__ INFO: \u001b[0mEpoch 10 loss 0.2376 acc@1 0.9280 acc@5 0.9976\n",
      "\u001b[32m[2020-07-01 14:38:50] __main__ INFO: \u001b[0mElapsed 17.74\n",
      "\u001b[32m[2020-07-01 14:38:50] __main__ INFO: \u001b[0mTrain 11 3510\n",
      "\u001b[32m[2020-07-01 14:41:12] __main__ INFO: \u001b[0mEpoch 11 Step 100/351 lr 0.001000 loss 0.0126 (0.0166) acc@1 1.0000 (0.9985) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 14:43:33] __main__ INFO: \u001b[0mEpoch 11 Step 200/351 lr 0.001000 loss 0.0254 (0.0173) acc@1 0.9844 (0.9983) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 14:45:55] __main__ INFO: \u001b[0mEpoch 11 Step 300/351 lr 0.001000 loss 0.0330 (0.0170) acc@1 0.9844 (0.9984) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 14:47:07] __main__ INFO: \u001b[0mEpoch 11 Step 351/351 lr 0.001000 loss 0.0090 (0.0176) acc@1 1.0000 (0.9982) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 14:47:07] __main__ INFO: \u001b[0mElapsed 497.75\n",
      "\u001b[32m[2020-07-01 14:47:07] __main__ INFO: \u001b[0mVal 11\n",
      "\u001b[32m[2020-07-01 14:47:25] __main__ INFO: \u001b[0mEpoch 11 loss 0.2408 acc@1 0.9262 acc@5 0.9980\n",
      "\u001b[32m[2020-07-01 14:47:25] __main__ INFO: \u001b[0mElapsed 17.76\n",
      "\u001b[32m[2020-07-01 14:47:25] __main__ INFO: \u001b[0mTrain 12 3861\n",
      "\u001b[32m[2020-07-01 14:49:47] __main__ INFO: \u001b[0mEpoch 12 Step 100/351 lr 0.001000 loss 0.0135 (0.0170) acc@1 1.0000 (0.9982) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 14:52:09] __main__ INFO: \u001b[0mEpoch 12 Step 200/351 lr 0.001000 loss 0.0127 (0.0167) acc@1 1.0000 (0.9984) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 14:54:31] __main__ INFO: \u001b[0mEpoch 12 Step 300/351 lr 0.001000 loss 0.0196 (0.0171) acc@1 1.0000 (0.9982) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 14:55:43] __main__ INFO: \u001b[0mEpoch 12 Step 351/351 lr 0.001000 loss 0.0119 (0.0171) acc@1 0.9922 (0.9982) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 14:55:44] __main__ INFO: \u001b[0mElapsed 498.28\n",
      "\u001b[32m[2020-07-01 14:55:44] __main__ INFO: \u001b[0mVal 12\n",
      "\u001b[32m[2020-07-01 14:56:01] __main__ INFO: \u001b[0mEpoch 12 loss 0.2430 acc@1 0.9268 acc@5 0.9980\n",
      "\u001b[32m[2020-07-01 14:56:01] __main__ INFO: \u001b[0mElapsed 17.75\n",
      "\u001b[32m[2020-07-01 14:56:01] __main__ INFO: \u001b[0mTrain 13 4212\n",
      "\u001b[32m[2020-07-01 14:58:23] __main__ INFO: \u001b[0mEpoch 13 Step 100/351 lr 0.001000 loss 0.0173 (0.0148) acc@1 1.0000 (0.9986) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 15:00:45] __main__ INFO: \u001b[0mEpoch 13 Step 200/351 lr 0.001000 loss 0.0125 (0.0149) acc@1 1.0000 (0.9987) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 15:03:07] __main__ INFO: \u001b[0mEpoch 13 Step 300/351 lr 0.001000 loss 0.0132 (0.0149) acc@1 1.0000 (0.9988) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 15:04:20] __main__ INFO: \u001b[0mEpoch 13 Step 351/351 lr 0.001000 loss 0.0120 (0.0149) acc@1 1.0000 (0.9988) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 15:04:20] __main__ INFO: \u001b[0mElapsed 498.35\n",
      "\u001b[32m[2020-07-01 15:04:20] __main__ INFO: \u001b[0mVal 13\n",
      "\u001b[32m[2020-07-01 15:04:37] __main__ INFO: \u001b[0mEpoch 13 loss 0.2481 acc@1 0.9258 acc@5 0.9984\n",
      "\u001b[32m[2020-07-01 15:04:37] __main__ INFO: \u001b[0mElapsed 17.77\n",
      "\u001b[32m[2020-07-01 15:04:37] __main__ INFO: \u001b[0mTrain 14 4563\n",
      "\u001b[32m[2020-07-01 15:06:59] __main__ INFO: \u001b[0mEpoch 14 Step 100/351 lr 0.001000 loss 0.0127 (0.0130) acc@1 1.0000 (0.9991) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 15:09:20] __main__ INFO: \u001b[0mEpoch 14 Step 200/351 lr 0.001000 loss 0.0204 (0.0135) acc@1 1.0000 (0.9990) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 15:11:42] __main__ INFO: \u001b[0mEpoch 14 Step 300/351 lr 0.001000 loss 0.0134 (0.0140) acc@1 1.0000 (0.9988) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 15:12:54] __main__ INFO: \u001b[0mEpoch 14 Step 351/351 lr 0.001000 loss 0.0113 (0.0139) acc@1 1.0000 (0.9987) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 15:12:54] __main__ INFO: \u001b[0mElapsed 496.35\n",
      "\u001b[32m[2020-07-01 15:12:54] __main__ INFO: \u001b[0mVal 14\n",
      "\u001b[32m[2020-07-01 15:13:11] __main__ INFO: \u001b[0mEpoch 14 loss 0.2422 acc@1 0.9278 acc@5 0.9988\n",
      "\u001b[32m[2020-07-01 15:13:11] __main__ INFO: \u001b[0mElapsed 17.70\n",
      "\u001b[32m[2020-07-01 15:13:11] __main__ INFO: \u001b[0mTrain 15 4914\n",
      "\u001b[32m[2020-07-01 15:15:33] __main__ INFO: \u001b[0mEpoch 15 Step 100/351 lr 0.001000 loss 0.0094 (0.0121) acc@1 1.0000 (0.9989) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 15:17:55] __main__ INFO: \u001b[0mEpoch 15 Step 200/351 lr 0.001000 loss 0.0083 (0.0128) acc@1 1.0000 (0.9989) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 15:20:17] __main__ INFO: \u001b[0mEpoch 15 Step 300/351 lr 0.001000 loss 0.0101 (0.0130) acc@1 1.0000 (0.9989) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 15:21:29] __main__ INFO: \u001b[0mEpoch 15 Step 351/351 lr 0.001000 loss 0.0113 (0.0128) acc@1 1.0000 (0.9989) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 15:21:29] __main__ INFO: \u001b[0mElapsed 497.98\n",
      "\u001b[32m[2020-07-01 15:21:29] __main__ INFO: \u001b[0mVal 15\n",
      "\u001b[32m[2020-07-01 15:21:47] __main__ INFO: \u001b[0mEpoch 15 loss 0.2401 acc@1 0.9288 acc@5 0.9984\n",
      "\u001b[32m[2020-07-01 15:21:47] __main__ INFO: \u001b[0mElapsed 17.77\n",
      "\u001b[32m[2020-07-01 15:21:47] __main__ INFO: \u001b[0mTrain 16 5265\n",
      "\u001b[32m[2020-07-01 15:24:09] __main__ INFO: \u001b[0mEpoch 16 Step 100/351 lr 0.001000 loss 0.0148 (0.0118) acc@1 0.9922 (0.9989) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 15:26:30] __main__ INFO: \u001b[0mEpoch 16 Step 200/351 lr 0.001000 loss 0.0177 (0.0122) acc@1 1.0000 (0.9990) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 15:28:51] __main__ INFO: \u001b[0mEpoch 16 Step 300/351 lr 0.001000 loss 0.0067 (0.0120) acc@1 1.0000 (0.9989) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 15:30:03] __main__ INFO: \u001b[0mEpoch 16 Step 351/351 lr 0.001000 loss 0.0152 (0.0120) acc@1 1.0000 (0.9989) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 15:30:03] __main__ INFO: \u001b[0mElapsed 495.49\n",
      "\u001b[32m[2020-07-01 15:30:03] __main__ INFO: \u001b[0mVal 16\n",
      "\u001b[32m[2020-07-01 15:30:20] __main__ INFO: \u001b[0mEpoch 16 loss 0.2467 acc@1 0.9264 acc@5 0.9970\n",
      "\u001b[32m[2020-07-01 15:30:20] __main__ INFO: \u001b[0mElapsed 17.61\n",
      "\u001b[32m[2020-07-01 15:30:20] __main__ INFO: \u001b[0mTrain 17 5616\n",
      "\u001b[32m[2020-07-01 15:32:41] __main__ INFO: \u001b[0mEpoch 17 Step 100/351 lr 0.001000 loss 0.0050 (0.0115) acc@1 1.0000 (0.9992) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 15:35:02] __main__ INFO: \u001b[0mEpoch 17 Step 200/351 lr 0.001000 loss 0.0085 (0.0111) acc@1 1.0000 (0.9994) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 15:37:23] __main__ INFO: \u001b[0mEpoch 17 Step 300/351 lr 0.001000 loss 0.0067 (0.0108) acc@1 1.0000 (0.9995) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 15:38:35] __main__ INFO: \u001b[0mEpoch 17 Step 351/351 lr 0.001000 loss 0.0179 (0.0109) acc@1 1.0000 (0.9994) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 15:38:35] __main__ INFO: \u001b[0mElapsed 495.18\n",
      "\u001b[32m[2020-07-01 15:38:35] __main__ INFO: \u001b[0mVal 17\n",
      "\u001b[32m[2020-07-01 15:38:53] __main__ INFO: \u001b[0mEpoch 17 loss 0.2430 acc@1 0.9284 acc@5 0.9982\n",
      "\u001b[32m[2020-07-01 15:38:53] __main__ INFO: \u001b[0mElapsed 17.70\n",
      "\u001b[32m[2020-07-01 15:38:53] __main__ INFO: \u001b[0mTrain 18 5967\n",
      "\u001b[32m[2020-07-01 15:41:14] __main__ INFO: \u001b[0mEpoch 18 Step 100/351 lr 0.001000 loss 0.0256 (0.0105) acc@1 0.9922 (0.9995) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 15:43:36] __main__ INFO: \u001b[0mEpoch 18 Step 200/351 lr 0.001000 loss 0.0073 (0.0105) acc@1 1.0000 (0.9993) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 15:45:57] __main__ INFO: \u001b[0mEpoch 18 Step 300/351 lr 0.001000 loss 0.0058 (0.0104) acc@1 1.0000 (0.9992) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 15:47:09] __main__ INFO: \u001b[0mEpoch 18 Step 351/351 lr 0.001000 loss 0.0084 (0.0104) acc@1 1.0000 (0.9992) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 15:47:09] __main__ INFO: \u001b[0mElapsed 495.57\n",
      "\u001b[32m[2020-07-01 15:47:09] __main__ INFO: \u001b[0mVal 18\n",
      "\u001b[32m[2020-07-01 15:47:26] __main__ INFO: \u001b[0mEpoch 18 loss 0.2415 acc@1 0.9274 acc@5 0.9982\n",
      "\u001b[32m[2020-07-01 15:47:26] __main__ INFO: \u001b[0mElapsed 17.66\n",
      "\u001b[32m[2020-07-01 15:47:26] __main__ INFO: \u001b[0mTrain 19 6318\n",
      "\u001b[32m[2020-07-01 15:49:47] __main__ INFO: \u001b[0mEpoch 19 Step 100/351 lr 0.001000 loss 0.0057 (0.0089) acc@1 1.0000 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 15:52:08] __main__ INFO: \u001b[0mEpoch 19 Step 200/351 lr 0.001000 loss 0.0087 (0.0090) acc@1 1.0000 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 15:54:29] __main__ INFO: \u001b[0mEpoch 19 Step 300/351 lr 0.001000 loss 0.0544 (0.0095) acc@1 0.9844 (0.9996) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 15:55:41] __main__ INFO: \u001b[0mEpoch 19 Step 351/351 lr 0.001000 loss 0.0083 (0.0097) acc@1 1.0000 (0.9995) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 15:55:41] __main__ INFO: \u001b[0mElapsed 494.67\n",
      "\u001b[32m[2020-07-01 15:55:41] __main__ INFO: \u001b[0mVal 19\n",
      "\u001b[32m[2020-07-01 15:55:59] __main__ INFO: \u001b[0mEpoch 19 loss 0.2391 acc@1 0.9284 acc@5 0.9980\n",
      "\u001b[32m[2020-07-01 15:55:59] __main__ INFO: \u001b[0mElapsed 17.65\n",
      "\u001b[32m[2020-07-01 15:55:59] __main__ INFO: \u001b[0mTrain 20 6669\n",
      "\u001b[32m[2020-07-01 15:58:20] __main__ INFO: \u001b[0mEpoch 20 Step 100/351 lr 0.001000 loss 0.0160 (0.0098) acc@1 0.9922 (0.9994) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 16:00:41] __main__ INFO: \u001b[0mEpoch 20 Step 200/351 lr 0.001000 loss 0.0092 (0.0093) acc@1 1.0000 (0.9995) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 16:03:02] __main__ INFO: \u001b[0mEpoch 20 Step 300/351 lr 0.001000 loss 0.0128 (0.0094) acc@1 1.0000 (0.9995) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 16:04:14] __main__ INFO: \u001b[0mEpoch 20 Step 351/351 lr 0.001000 loss 0.0123 (0.0094) acc@1 1.0000 (0.9995) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 16:04:14] __main__ INFO: \u001b[0mElapsed 494.85\n",
      "\u001b[32m[2020-07-01 16:04:14] __main__ INFO: \u001b[0mVal 20\n",
      "\u001b[32m[2020-07-01 16:04:31] __main__ INFO: \u001b[0mEpoch 20 loss 0.2422 acc@1 0.9278 acc@5 0.9976\n",
      "\u001b[32m[2020-07-01 16:04:31] __main__ INFO: \u001b[0mElapsed 17.65\n",
      "\u001b[32m[2020-07-01 16:04:31] __main__ INFO: \u001b[0mTrain 21 7020\n",
      "\u001b[32m[2020-07-01 16:06:52] __main__ INFO: \u001b[0mEpoch 21 Step 100/351 lr 0.001000 loss 0.0062 (0.0083) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 16:09:13] __main__ INFO: \u001b[0mEpoch 21 Step 200/351 lr 0.001000 loss 0.0176 (0.0080) acc@1 0.9922 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 16:11:34] __main__ INFO: \u001b[0mEpoch 21 Step 300/351 lr 0.001000 loss 0.0057 (0.0083) acc@1 1.0000 (0.9996) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 16:12:46] __main__ INFO: \u001b[0mEpoch 21 Step 351/351 lr 0.001000 loss 0.0062 (0.0084) acc@1 1.0000 (0.9996) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 16:12:46] __main__ INFO: \u001b[0mElapsed 494.53\n",
      "\u001b[32m[2020-07-01 16:12:46] __main__ INFO: \u001b[0mVal 21\n",
      "\u001b[32m[2020-07-01 16:13:03] __main__ INFO: \u001b[0mEpoch 21 loss 0.2414 acc@1 0.9294 acc@5 0.9980\n",
      "\u001b[32m[2020-07-01 16:13:03] __main__ INFO: \u001b[0mElapsed 17.63\n",
      "\u001b[32m[2020-07-01 16:13:03] __main__ INFO: \u001b[0mTrain 22 7371\n",
      "\u001b[32m[2020-07-01 16:15:24] __main__ INFO: \u001b[0mEpoch 22 Step 100/351 lr 0.001000 loss 0.0064 (0.0082) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 16:17:45] __main__ INFO: \u001b[0mEpoch 22 Step 200/351 lr 0.001000 loss 0.0068 (0.0084) acc@1 1.0000 (0.9996) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 16:20:06] __main__ INFO: \u001b[0mEpoch 22 Step 300/351 lr 0.001000 loss 0.0073 (0.0084) acc@1 1.0000 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 16:21:18] __main__ INFO: \u001b[0mEpoch 22 Step 351/351 lr 0.001000 loss 0.0088 (0.0084) acc@1 1.0000 (0.9996) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 16:21:18] __main__ INFO: \u001b[0mElapsed 494.82\n",
      "\u001b[32m[2020-07-01 16:21:18] __main__ INFO: \u001b[0mVal 22\n",
      "\u001b[32m[2020-07-01 16:21:36] __main__ INFO: \u001b[0mEpoch 22 loss 0.2419 acc@1 0.9300 acc@5 0.9982\n",
      "\u001b[32m[2020-07-01 16:21:36] __main__ INFO: \u001b[0mElapsed 17.63\n",
      "\u001b[32m[2020-07-01 16:21:36] __main__ INFO: \u001b[0mTrain 23 7722\n",
      "\u001b[32m[2020-07-01 16:23:57] __main__ INFO: \u001b[0mEpoch 23 Step 100/351 lr 0.001000 loss 0.0184 (0.0078) acc@1 0.9922 (0.9995) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 16:26:18] __main__ INFO: \u001b[0mEpoch 23 Step 200/351 lr 0.001000 loss 0.0135 (0.0079) acc@1 1.0000 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 16:28:39] __main__ INFO: \u001b[0mEpoch 23 Step 300/351 lr 0.001000 loss 0.0092 (0.0080) acc@1 1.0000 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 16:29:51] __main__ INFO: \u001b[0mEpoch 23 Step 351/351 lr 0.001000 loss 0.0042 (0.0080) acc@1 1.0000 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 16:29:51] __main__ INFO: \u001b[0mElapsed 495.04\n",
      "\u001b[32m[2020-07-01 16:29:51] __main__ INFO: \u001b[0mVal 23\n",
      "\u001b[32m[2020-07-01 16:30:09] __main__ INFO: \u001b[0mEpoch 23 loss 0.2413 acc@1 0.9280 acc@5 0.9980\n",
      "\u001b[32m[2020-07-01 16:30:09] __main__ INFO: \u001b[0mElapsed 17.65\n",
      "\u001b[32m[2020-07-01 16:30:09] __main__ INFO: \u001b[0mTrain 24 8073\n",
      "\u001b[32m[2020-07-01 16:32:29] __main__ INFO: \u001b[0mEpoch 24 Step 100/351 lr 0.001000 loss 0.0130 (0.0076) acc@1 1.0000 (0.9995) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 16:34:50] __main__ INFO: \u001b[0mEpoch 24 Step 200/351 lr 0.001000 loss 0.0148 (0.0078) acc@1 1.0000 (0.9996) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 16:37:11] __main__ INFO: \u001b[0mEpoch 24 Step 300/351 lr 0.001000 loss 0.0060 (0.0077) acc@1 1.0000 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 16:38:23] __main__ INFO: \u001b[0mEpoch 24 Step 351/351 lr 0.001000 loss 0.0123 (0.0079) acc@1 1.0000 (0.9996) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 16:38:23] __main__ INFO: \u001b[0mElapsed 494.31\n",
      "\u001b[32m[2020-07-01 16:38:23] __main__ INFO: \u001b[0mVal 24\n",
      "\u001b[32m[2020-07-01 16:38:41] __main__ INFO: \u001b[0mEpoch 24 loss 0.2423 acc@1 0.9298 acc@5 0.9984\n",
      "\u001b[32m[2020-07-01 16:38:41] __main__ INFO: \u001b[0mElapsed 17.64\n",
      "\u001b[32m[2020-07-01 16:38:41] __main__ INFO: \u001b[0mTrain 25 8424\n",
      "\u001b[32m[2020-07-01 16:41:01] __main__ INFO: \u001b[0mEpoch 25 Step 100/351 lr 0.001000 loss 0.0046 (0.0072) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 16:43:22] __main__ INFO: \u001b[0mEpoch 25 Step 200/351 lr 0.001000 loss 0.0117 (0.0072) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 16:45:43] __main__ INFO: \u001b[0mEpoch 25 Step 300/351 lr 0.001000 loss 0.0035 (0.0072) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 16:46:55] __main__ INFO: \u001b[0mEpoch 25 Step 351/351 lr 0.001000 loss 0.0038 (0.0072) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 16:46:55] __main__ INFO: \u001b[0mElapsed 494.75\n",
      "\u001b[32m[2020-07-01 16:46:55] __main__ INFO: \u001b[0mVal 25\n",
      "\u001b[32m[2020-07-01 16:47:13] __main__ INFO: \u001b[0mEpoch 25 loss 0.2433 acc@1 0.9290 acc@5 0.9982\n",
      "\u001b[32m[2020-07-01 16:47:13] __main__ INFO: \u001b[0mElapsed 17.65\n",
      "\u001b[32m[2020-07-01 16:47:13] __main__ INFO: \u001b[0mTrain 26 8775\n",
      "\u001b[32m[2020-07-01 16:49:34] __main__ INFO: \u001b[0mEpoch 26 Step 100/351 lr 0.001000 loss 0.0042 (0.0068) acc@1 1.0000 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 16:51:55] __main__ INFO: \u001b[0mEpoch 26 Step 200/351 lr 0.001000 loss 0.0055 (0.0066) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 16:54:17] __main__ INFO: \u001b[0mEpoch 26 Step 300/351 lr 0.001000 loss 0.0092 (0.0067) acc@1 1.0000 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 16:55:29] __main__ INFO: \u001b[0mEpoch 26 Step 351/351 lr 0.001000 loss 0.0194 (0.0068) acc@1 0.9922 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 16:55:29] __main__ INFO: \u001b[0mElapsed 496.09\n",
      "\u001b[32m[2020-07-01 16:55:29] __main__ INFO: \u001b[0mVal 26\n",
      "\u001b[32m[2020-07-01 16:55:47] __main__ INFO: \u001b[0mEpoch 26 loss 0.2429 acc@1 0.9302 acc@5 0.9982\n",
      "\u001b[32m[2020-07-01 16:55:47] __main__ INFO: \u001b[0mElapsed 17.80\n",
      "\u001b[32m[2020-07-01 16:55:47] __main__ INFO: \u001b[0mTrain 27 9126\n",
      "\u001b[32m[2020-07-01 16:58:09] __main__ INFO: \u001b[0mEpoch 27 Step 100/351 lr 0.001000 loss 0.0040 (0.0062) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 17:00:30] __main__ INFO: \u001b[0mEpoch 27 Step 200/351 lr 0.001000 loss 0.0080 (0.0063) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 17:02:52] __main__ INFO: \u001b[0mEpoch 27 Step 300/351 lr 0.001000 loss 0.0056 (0.0063) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 17:04:04] __main__ INFO: \u001b[0mEpoch 27 Step 351/351 lr 0.001000 loss 0.0057 (0.0064) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 17:04:04] __main__ INFO: \u001b[0mElapsed 497.10\n",
      "\u001b[32m[2020-07-01 17:04:04] __main__ INFO: \u001b[0mVal 27\n",
      "\u001b[32m[2020-07-01 17:04:22] __main__ INFO: \u001b[0mEpoch 27 loss 0.2407 acc@1 0.9304 acc@5 0.9984\n",
      "\u001b[32m[2020-07-01 17:04:22] __main__ INFO: \u001b[0mElapsed 17.70\n",
      "\u001b[32m[2020-07-01 17:04:22] __main__ INFO: \u001b[0mTrain 28 9477\n",
      "\u001b[32m[2020-07-01 17:06:43] __main__ INFO: \u001b[0mEpoch 28 Step 100/351 lr 0.001000 loss 0.0057 (0.0063) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 17:09:05] __main__ INFO: \u001b[0mEpoch 28 Step 200/351 lr 0.001000 loss 0.0033 (0.0065) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 17:11:27] __main__ INFO: \u001b[0mEpoch 28 Step 300/351 lr 0.001000 loss 0.0048 (0.0065) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 17:12:39] __main__ INFO: \u001b[0mEpoch 28 Step 351/351 lr 0.001000 loss 0.0060 (0.0064) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 17:12:39] __main__ INFO: \u001b[0mElapsed 497.54\n",
      "\u001b[32m[2020-07-01 17:12:39] __main__ INFO: \u001b[0mVal 28\n",
      "\u001b[32m[2020-07-01 17:12:57] __main__ INFO: \u001b[0mEpoch 28 loss 0.2430 acc@1 0.9286 acc@5 0.9978\n",
      "\u001b[32m[2020-07-01 17:12:57] __main__ INFO: \u001b[0mElapsed 17.77\n",
      "\u001b[32m[2020-07-01 17:12:57] __main__ INFO: \u001b[0mTrain 29 9828\n",
      "\u001b[32m[2020-07-01 17:15:19] __main__ INFO: \u001b[0mEpoch 29 Step 100/351 lr 0.001000 loss 0.0081 (0.0069) acc@1 1.0000 (0.9995) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 17:17:40] __main__ INFO: \u001b[0mEpoch 29 Step 200/351 lr 0.001000 loss 0.0146 (0.0070) acc@1 1.0000 (0.9995) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 17:20:02] __main__ INFO: \u001b[0mEpoch 29 Step 300/351 lr 0.001000 loss 0.0044 (0.0070) acc@1 1.0000 (0.9996) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 17:21:14] __main__ INFO: \u001b[0mEpoch 29 Step 351/351 lr 0.001000 loss 0.0051 (0.0070) acc@1 1.0000 (0.9996) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 17:21:14] __main__ INFO: \u001b[0mElapsed 497.40\n",
      "\u001b[32m[2020-07-01 17:21:14] __main__ INFO: \u001b[0mVal 29\n",
      "\u001b[32m[2020-07-01 17:21:32] __main__ INFO: \u001b[0mEpoch 29 loss 0.2402 acc@1 0.9302 acc@5 0.9976\n",
      "\u001b[32m[2020-07-01 17:21:32] __main__ INFO: \u001b[0mElapsed 17.75\n",
      "\u001b[32m[2020-07-01 17:21:32] __main__ INFO: \u001b[0mTrain 30 10179\n",
      "\u001b[32m[2020-07-01 17:23:54] __main__ INFO: \u001b[0mEpoch 30 Step 100/351 lr 0.001000 loss 0.0104 (0.0065) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 17:26:15] __main__ INFO: \u001b[0mEpoch 30 Step 200/351 lr 0.001000 loss 0.0022 (0.0063) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 17:28:36] __main__ INFO: \u001b[0mEpoch 30 Step 300/351 lr 0.001000 loss 0.0081 (0.0062) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 17:29:48] __main__ INFO: \u001b[0mEpoch 30 Step 351/351 lr 0.001000 loss 0.0032 (0.0061) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 17:29:48] __main__ INFO: \u001b[0mElapsed 495.66\n",
      "\u001b[32m[2020-07-01 17:29:48] __main__ INFO: \u001b[0mVal 30\n",
      "\u001b[32m[2020-07-01 17:30:05] __main__ INFO: \u001b[0mEpoch 30 loss 0.2392 acc@1 0.9314 acc@5 0.9978\n",
      "\u001b[32m[2020-07-01 17:30:05] __main__ INFO: \u001b[0mElapsed 17.65\n",
      "\u001b[32m[2020-07-01 17:30:05] __main__ INFO: \u001b[0mTrain 31 10530\n",
      "\u001b[32m[2020-07-01 17:32:26] __main__ INFO: \u001b[0mEpoch 31 Step 100/351 lr 0.001000 loss 0.0056 (0.0056) acc@1 1.0000 (1.0000) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 17:34:47] __main__ INFO: \u001b[0mEpoch 31 Step 200/351 lr 0.001000 loss 0.0046 (0.0054) acc@1 1.0000 (1.0000) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 17:37:08] __main__ INFO: \u001b[0mEpoch 31 Step 300/351 lr 0.001000 loss 0.0036 (0.0054) acc@1 1.0000 (1.0000) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 17:38:20] __main__ INFO: \u001b[0mEpoch 31 Step 351/351 lr 0.001000 loss 0.0076 (0.0056) acc@1 1.0000 (1.0000) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 17:38:20] __main__ INFO: \u001b[0mElapsed 494.66\n",
      "\u001b[32m[2020-07-01 17:38:20] __main__ INFO: \u001b[0mVal 31\n",
      "\u001b[32m[2020-07-01 17:38:38] __main__ INFO: \u001b[0mEpoch 31 loss 0.2416 acc@1 0.9310 acc@5 0.9984\n",
      "\u001b[32m[2020-07-01 17:38:38] __main__ INFO: \u001b[0mElapsed 17.64\n",
      "\u001b[32m[2020-07-01 17:38:38] __main__ INFO: \u001b[0mTrain 32 10881\n",
      "\u001b[32m[2020-07-01 17:40:59] __main__ INFO: \u001b[0mEpoch 32 Step 100/351 lr 0.001000 loss 0.0058 (0.0057) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 17:43:20] __main__ INFO: \u001b[0mEpoch 32 Step 200/351 lr 0.001000 loss 0.0038 (0.0055) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 17:45:41] __main__ INFO: \u001b[0mEpoch 32 Step 300/351 lr 0.001000 loss 0.0039 (0.0056) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 17:46:53] __main__ INFO: \u001b[0mEpoch 32 Step 351/351 lr 0.001000 loss 0.0034 (0.0056) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 17:46:53] __main__ INFO: \u001b[0mElapsed 494.91\n",
      "\u001b[32m[2020-07-01 17:46:53] __main__ INFO: \u001b[0mVal 32\n",
      "\u001b[32m[2020-07-01 17:47:10] __main__ INFO: \u001b[0mEpoch 32 loss 0.2484 acc@1 0.9288 acc@5 0.9980\n",
      "\u001b[32m[2020-07-01 17:47:10] __main__ INFO: \u001b[0mElapsed 17.62\n",
      "\u001b[32m[2020-07-01 17:47:10] __main__ INFO: \u001b[0mTrain 33 11232\n",
      "\u001b[32m[2020-07-01 17:49:31] __main__ INFO: \u001b[0mEpoch 33 Step 100/351 lr 0.001000 loss 0.0041 (0.0049) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 17:51:52] __main__ INFO: \u001b[0mEpoch 33 Step 200/351 lr 0.001000 loss 0.0086 (0.0050) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 17:54:13] __main__ INFO: \u001b[0mEpoch 33 Step 300/351 lr 0.001000 loss 0.0036 (0.0052) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 17:55:25] __main__ INFO: \u001b[0mEpoch 33 Step 351/351 lr 0.001000 loss 0.0033 (0.0052) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 17:55:25] __main__ INFO: \u001b[0mElapsed 494.52\n",
      "\u001b[32m[2020-07-01 17:55:25] __main__ INFO: \u001b[0mVal 33\n",
      "\u001b[32m[2020-07-01 17:55:42] __main__ INFO: \u001b[0mEpoch 33 loss 0.2476 acc@1 0.9276 acc@5 0.9978\n",
      "\u001b[32m[2020-07-01 17:55:42] __main__ INFO: \u001b[0mElapsed 17.62\n",
      "\u001b[32m[2020-07-01 17:55:42] __main__ INFO: \u001b[0mTrain 34 11583\n",
      "\u001b[32m[2020-07-01 17:58:03] __main__ INFO: \u001b[0mEpoch 34 Step 100/351 lr 0.001000 loss 0.0029 (0.0049) acc@1 1.0000 (1.0000) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 18:00:24] __main__ INFO: \u001b[0mEpoch 34 Step 200/351 lr 0.001000 loss 0.0054 (0.0049) acc@1 1.0000 (1.0000) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 18:02:45] __main__ INFO: \u001b[0mEpoch 34 Step 300/351 lr 0.001000 loss 0.0053 (0.0049) acc@1 1.0000 (1.0000) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 18:03:57] __main__ INFO: \u001b[0mEpoch 34 Step 351/351 lr 0.001000 loss 0.0038 (0.0050) acc@1 1.0000 (1.0000) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 18:03:57] __main__ INFO: \u001b[0mElapsed 494.83\n",
      "\u001b[32m[2020-07-01 18:03:57] __main__ INFO: \u001b[0mVal 34\n",
      "\u001b[32m[2020-07-01 18:04:15] __main__ INFO: \u001b[0mEpoch 34 loss 0.2410 acc@1 0.9326 acc@5 0.9982\n",
      "\u001b[32m[2020-07-01 18:04:15] __main__ INFO: \u001b[0mElapsed 17.64\n",
      "\u001b[32m[2020-07-01 18:04:15] __main__ INFO: \u001b[0mTrain 35 11934\n",
      "\u001b[32m[2020-07-01 18:06:36] __main__ INFO: \u001b[0mEpoch 35 Step 100/351 lr 0.001000 loss 0.0036 (0.0051) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 18:08:57] __main__ INFO: \u001b[0mEpoch 35 Step 200/351 lr 0.001000 loss 0.0018 (0.0052) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 18:11:18] __main__ INFO: \u001b[0mEpoch 35 Step 300/351 lr 0.001000 loss 0.0110 (0.0053) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 18:12:30] __main__ INFO: \u001b[0mEpoch 35 Step 351/351 lr 0.001000 loss 0.0070 (0.0054) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 18:12:30] __main__ INFO: \u001b[0mElapsed 494.90\n",
      "\u001b[32m[2020-07-01 18:12:30] __main__ INFO: \u001b[0mVal 35\n",
      "\u001b[32m[2020-07-01 18:12:47] __main__ INFO: \u001b[0mEpoch 35 loss 0.2448 acc@1 0.9322 acc@5 0.9980\n",
      "\u001b[32m[2020-07-01 18:12:47] __main__ INFO: \u001b[0mElapsed 17.63\n",
      "\u001b[32m[2020-07-01 18:12:47] __main__ INFO: \u001b[0mTrain 36 12285\n",
      "\u001b[32m[2020-07-01 18:15:08] __main__ INFO: \u001b[0mEpoch 36 Step 100/351 lr 0.001000 loss 0.0055 (0.0049) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 18:17:29] __main__ INFO: \u001b[0mEpoch 36 Step 200/351 lr 0.001000 loss 0.0035 (0.0055) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 18:19:50] __main__ INFO: \u001b[0mEpoch 36 Step 300/351 lr 0.001000 loss 0.0030 (0.0055) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 18:21:02] __main__ INFO: \u001b[0mEpoch 36 Step 351/351 lr 0.001000 loss 0.0044 (0.0056) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 18:21:02] __main__ INFO: \u001b[0mElapsed 494.92\n",
      "\u001b[32m[2020-07-01 18:21:02] __main__ INFO: \u001b[0mVal 36\n",
      "\u001b[32m[2020-07-01 18:21:20] __main__ INFO: \u001b[0mEpoch 36 loss 0.2439 acc@1 0.9296 acc@5 0.9982\n",
      "\u001b[32m[2020-07-01 18:21:20] __main__ INFO: \u001b[0mElapsed 17.65\n",
      "\u001b[32m[2020-07-01 18:21:20] __main__ INFO: \u001b[0mTrain 37 12636\n",
      "\u001b[32m[2020-07-01 18:23:41] __main__ INFO: \u001b[0mEpoch 37 Step 100/351 lr 0.001000 loss 0.0038 (0.0049) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 18:26:02] __main__ INFO: \u001b[0mEpoch 37 Step 200/351 lr 0.001000 loss 0.0052 (0.0048) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 18:28:23] __main__ INFO: \u001b[0mEpoch 37 Step 300/351 lr 0.001000 loss 0.0065 (0.0049) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 18:29:35] __main__ INFO: \u001b[0mEpoch 37 Step 351/351 lr 0.001000 loss 0.0051 (0.0049) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 18:29:35] __main__ INFO: \u001b[0mElapsed 494.96\n",
      "\u001b[32m[2020-07-01 18:29:35] __main__ INFO: \u001b[0mVal 37\n",
      "\u001b[32m[2020-07-01 18:29:53] __main__ INFO: \u001b[0mEpoch 37 loss 0.2441 acc@1 0.9308 acc@5 0.9982\n",
      "\u001b[32m[2020-07-01 18:29:53] __main__ INFO: \u001b[0mElapsed 17.66\n",
      "\u001b[32m[2020-07-01 18:29:53] __main__ INFO: \u001b[0mTrain 38 12987\n",
      "\u001b[32m[2020-07-01 18:32:14] __main__ INFO: \u001b[0mEpoch 38 Step 100/351 lr 0.001000 loss 0.0037 (0.0045) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 18:34:35] __main__ INFO: \u001b[0mEpoch 38 Step 200/351 lr 0.001000 loss 0.0042 (0.0048) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 18:36:55] __main__ INFO: \u001b[0mEpoch 38 Step 300/351 lr 0.001000 loss 0.0067 (0.0047) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 18:38:07] __main__ INFO: \u001b[0mEpoch 38 Step 351/351 lr 0.001000 loss 0.0033 (0.0047) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 18:38:07] __main__ INFO: \u001b[0mElapsed 494.81\n",
      "\u001b[32m[2020-07-01 18:38:07] __main__ INFO: \u001b[0mVal 38\n",
      "\u001b[32m[2020-07-01 18:38:25] __main__ INFO: \u001b[0mEpoch 38 loss 0.2389 acc@1 0.9304 acc@5 0.9982\n",
      "\u001b[32m[2020-07-01 18:38:25] __main__ INFO: \u001b[0mElapsed 17.61\n",
      "\u001b[32m[2020-07-01 18:38:25] __main__ INFO: \u001b[0mTrain 39 13338\n",
      "\u001b[32m[2020-07-01 18:40:46] __main__ INFO: \u001b[0mEpoch 39 Step 100/351 lr 0.001000 loss 0.0035 (0.0047) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 18:43:07] __main__ INFO: \u001b[0mEpoch 39 Step 200/351 lr 0.001000 loss 0.0037 (0.0048) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 18:45:27] __main__ INFO: \u001b[0mEpoch 39 Step 300/351 lr 0.001000 loss 0.0055 (0.0049) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 18:46:39] __main__ INFO: \u001b[0mEpoch 39 Step 351/351 lr 0.001000 loss 0.0030 (0.0049) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 18:46:39] __main__ INFO: \u001b[0mElapsed 494.38\n",
      "\u001b[32m[2020-07-01 18:46:39] __main__ INFO: \u001b[0mVal 39\n",
      "\u001b[32m[2020-07-01 18:46:57] __main__ INFO: \u001b[0mEpoch 39 loss 0.2421 acc@1 0.9300 acc@5 0.9976\n",
      "\u001b[32m[2020-07-01 18:46:57] __main__ INFO: \u001b[0mElapsed 17.65\n",
      "\u001b[32m[2020-07-01 18:46:57] __main__ INFO: \u001b[0mTrain 40 13689\n",
      "\u001b[32m[2020-07-01 18:49:18] __main__ INFO: \u001b[0mEpoch 40 Step 100/351 lr 0.001000 loss 0.0085 (0.0044) acc@1 1.0000 (1.0000) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 18:51:39] __main__ INFO: \u001b[0mEpoch 40 Step 200/351 lr 0.001000 loss 0.0034 (0.0043) acc@1 1.0000 (1.0000) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 18:54:00] __main__ INFO: \u001b[0mEpoch 40 Step 300/351 lr 0.001000 loss 0.0035 (0.0044) acc@1 1.0000 (1.0000) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 18:55:12] __main__ INFO: \u001b[0mEpoch 40 Step 351/351 lr 0.001000 loss 0.0046 (0.0044) acc@1 1.0000 (1.0000) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 18:55:12] __main__ INFO: \u001b[0mElapsed 494.96\n",
      "\u001b[32m[2020-07-01 18:55:12] __main__ INFO: \u001b[0mVal 40\n",
      "\u001b[32m[2020-07-01 18:55:30] __main__ INFO: \u001b[0mEpoch 40 loss 0.2406 acc@1 0.9298 acc@5 0.9974\n",
      "\u001b[32m[2020-07-01 18:55:30] __main__ INFO: \u001b[0mElapsed 17.66\n",
      "\u001b[32m[2020-07-01 18:55:30] __main__ INFO: \u001b[0mTrain 41 14040\n",
      "\u001b[32m[2020-07-01 18:57:51] __main__ INFO: \u001b[0mEpoch 41 Step 100/351 lr 0.001000 loss 0.0044 (0.0042) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 19:00:12] __main__ INFO: \u001b[0mEpoch 41 Step 200/351 lr 0.001000 loss 0.0035 (0.0043) acc@1 1.0000 (1.0000) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 19:02:32] __main__ INFO: \u001b[0mEpoch 41 Step 300/351 lr 0.001000 loss 0.0049 (0.0044) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 19:03:44] __main__ INFO: \u001b[0mEpoch 41 Step 351/351 lr 0.001000 loss 0.0042 (0.0045) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 19:03:44] __main__ INFO: \u001b[0mElapsed 494.61\n",
      "\u001b[32m[2020-07-01 19:03:44] __main__ INFO: \u001b[0mVal 41\n",
      "\u001b[32m[2020-07-01 19:04:02] __main__ INFO: \u001b[0mEpoch 41 loss 0.2434 acc@1 0.9310 acc@5 0.9978\n",
      "\u001b[32m[2020-07-01 19:04:02] __main__ INFO: \u001b[0mElapsed 17.65\n",
      "\u001b[32m[2020-07-01 19:04:02] __main__ INFO: \u001b[0mTrain 42 14391\n",
      "\u001b[32m[2020-07-01 19:06:23] __main__ INFO: \u001b[0mEpoch 42 Step 100/351 lr 0.001000 loss 0.0081 (0.0042) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 19:08:44] __main__ INFO: \u001b[0mEpoch 42 Step 200/351 lr 0.001000 loss 0.0037 (0.0043) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 19:11:04] __main__ INFO: \u001b[0mEpoch 42 Step 300/351 lr 0.001000 loss 0.0124 (0.0043) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 19:12:16] __main__ INFO: \u001b[0mEpoch 42 Step 351/351 lr 0.001000 loss 0.0097 (0.0043) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 19:12:16] __main__ INFO: \u001b[0mElapsed 494.42\n",
      "\u001b[32m[2020-07-01 19:12:16] __main__ INFO: \u001b[0mVal 42\n",
      "\u001b[32m[2020-07-01 19:12:34] __main__ INFO: \u001b[0mEpoch 42 loss 0.2430 acc@1 0.9316 acc@5 0.9984\n",
      "\u001b[32m[2020-07-01 19:12:34] __main__ INFO: \u001b[0mElapsed 17.66\n",
      "\u001b[32m[2020-07-01 19:12:34] __main__ INFO: \u001b[0mTrain 43 14742\n",
      "\u001b[32m[2020-07-01 19:14:55] __main__ INFO: \u001b[0mEpoch 43 Step 100/351 lr 0.001000 loss 0.0038 (0.0038) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 19:17:16] __main__ INFO: \u001b[0mEpoch 43 Step 200/351 lr 0.001000 loss 0.0033 (0.0042) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 19:19:36] __main__ INFO: \u001b[0mEpoch 43 Step 300/351 lr 0.001000 loss 0.0036 (0.0043) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 19:20:49] __main__ INFO: \u001b[0mEpoch 43 Step 351/351 lr 0.001000 loss 0.0032 (0.0043) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 19:20:49] __main__ INFO: \u001b[0mElapsed 494.84\n",
      "\u001b[32m[2020-07-01 19:20:49] __main__ INFO: \u001b[0mVal 43\n",
      "\u001b[32m[2020-07-01 19:21:07] __main__ INFO: \u001b[0mEpoch 43 loss 0.2431 acc@1 0.9326 acc@5 0.9976\n",
      "\u001b[32m[2020-07-01 19:21:07] __main__ INFO: \u001b[0mElapsed 17.76\n",
      "\u001b[32m[2020-07-01 19:21:07] __main__ INFO: \u001b[0mTrain 44 15093\n",
      "\u001b[32m[2020-07-01 19:23:29] __main__ INFO: \u001b[0mEpoch 44 Step 100/351 lr 0.001000 loss 0.0039 (0.0039) acc@1 1.0000 (1.0000) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 19:25:51] __main__ INFO: \u001b[0mEpoch 44 Step 200/351 lr 0.001000 loss 0.0029 (0.0041) acc@1 1.0000 (1.0000) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 19:28:13] __main__ INFO: \u001b[0mEpoch 44 Step 300/351 lr 0.001000 loss 0.0040 (0.0042) acc@1 1.0000 (1.0000) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 19:29:26] __main__ INFO: \u001b[0mEpoch 44 Step 351/351 lr 0.001000 loss 0.0043 (0.0042) acc@1 1.0000 (1.0000) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 19:29:26] __main__ INFO: \u001b[0mElapsed 498.94\n",
      "\u001b[32m[2020-07-01 19:29:26] __main__ INFO: \u001b[0mVal 44\n",
      "\u001b[32m[2020-07-01 19:29:43] __main__ INFO: \u001b[0mEpoch 44 loss 0.2399 acc@1 0.9322 acc@5 0.9984\n",
      "\u001b[32m[2020-07-01 19:29:43] __main__ INFO: \u001b[0mElapsed 17.79\n",
      "\u001b[32m[2020-07-01 19:29:43] __main__ INFO: \u001b[0mTrain 45 15444\n",
      "\u001b[32m[2020-07-01 19:32:06] __main__ INFO: \u001b[0mEpoch 45 Step 100/351 lr 0.001000 loss 0.0023 (0.0043) acc@1 1.0000 (1.0000) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 19:34:28] __main__ INFO: \u001b[0mEpoch 45 Step 200/351 lr 0.001000 loss 0.0073 (0.0045) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 19:36:50] __main__ INFO: \u001b[0mEpoch 45 Step 300/351 lr 0.001000 loss 0.0039 (0.0043) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 19:38:02] __main__ INFO: \u001b[0mEpoch 45 Step 351/351 lr 0.001000 loss 0.0040 (0.0043) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 19:38:02] __main__ INFO: \u001b[0mElapsed 499.10\n",
      "\u001b[32m[2020-07-01 19:38:02] __main__ INFO: \u001b[0mVal 45\n",
      "\u001b[32m[2020-07-01 19:38:20] __main__ INFO: \u001b[0mEpoch 45 loss 0.2429 acc@1 0.9318 acc@5 0.9982\n",
      "\u001b[32m[2020-07-01 19:38:20] __main__ INFO: \u001b[0mElapsed 17.79\n",
      "\u001b[32m[2020-07-01 19:38:20] __main__ INFO: \u001b[0mTrain 46 15795\n",
      "\u001b[32m[2020-07-01 19:40:43] __main__ INFO: \u001b[0mEpoch 46 Step 100/351 lr 0.001000 loss 0.0028 (0.0044) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 19:43:05] __main__ INFO: \u001b[0mEpoch 46 Step 200/351 lr 0.001000 loss 0.0040 (0.0042) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 19:45:27] __main__ INFO: \u001b[0mEpoch 46 Step 300/351 lr 0.001000 loss 0.0038 (0.0042) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 19:46:39] __main__ INFO: \u001b[0mEpoch 46 Step 351/351 lr 0.001000 loss 0.0103 (0.0042) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 19:46:39] __main__ INFO: \u001b[0mElapsed 499.21\n",
      "\u001b[32m[2020-07-01 19:46:39] __main__ INFO: \u001b[0mVal 46\n",
      "\u001b[32m[2020-07-01 19:46:57] __main__ INFO: \u001b[0mEpoch 46 loss 0.2407 acc@1 0.9326 acc@5 0.9978\n",
      "\u001b[32m[2020-07-01 19:46:57] __main__ INFO: \u001b[0mElapsed 17.80\n",
      "\u001b[32m[2020-07-01 19:46:57] __main__ INFO: \u001b[0mTrain 47 16146\n",
      "\u001b[32m[2020-07-01 19:49:19] __main__ INFO: \u001b[0mEpoch 47 Step 100/351 lr 0.001000 loss 0.0031 (0.0040) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 19:51:40] __main__ INFO: \u001b[0mEpoch 47 Step 200/351 lr 0.001000 loss 0.0027 (0.0042) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 19:54:01] __main__ INFO: \u001b[0mEpoch 47 Step 300/351 lr 0.001000 loss 0.0028 (0.0043) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 19:55:13] __main__ INFO: \u001b[0mEpoch 47 Step 351/351 lr 0.001000 loss 0.0038 (0.0043) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 19:55:13] __main__ INFO: \u001b[0mElapsed 496.03\n",
      "\u001b[32m[2020-07-01 19:55:13] __main__ INFO: \u001b[0mVal 47\n",
      "\u001b[32m[2020-07-01 19:55:31] __main__ INFO: \u001b[0mEpoch 47 loss 0.2437 acc@1 0.9300 acc@5 0.9980\n",
      "\u001b[32m[2020-07-01 19:55:31] __main__ INFO: \u001b[0mElapsed 17.66\n",
      "\u001b[32m[2020-07-01 19:55:31] __main__ INFO: \u001b[0mTrain 48 16497\n",
      "\u001b[32m[2020-07-01 19:57:52] __main__ INFO: \u001b[0mEpoch 48 Step 100/351 lr 0.001000 loss 0.0044 (0.0040) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 20:00:13] __main__ INFO: \u001b[0mEpoch 48 Step 200/351 lr 0.001000 loss 0.0034 (0.0041) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 20:02:34] __main__ INFO: \u001b[0mEpoch 48 Step 300/351 lr 0.001000 loss 0.0026 (0.0042) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 20:03:46] __main__ INFO: \u001b[0mEpoch 48 Step 351/351 lr 0.001000 loss 0.0038 (0.0041) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 20:03:46] __main__ INFO: \u001b[0mElapsed 495.20\n",
      "\u001b[32m[2020-07-01 20:03:46] __main__ INFO: \u001b[0mVal 48\n",
      "\u001b[32m[2020-07-01 20:04:04] __main__ INFO: \u001b[0mEpoch 48 loss 0.2421 acc@1 0.9312 acc@5 0.9982\n",
      "\u001b[32m[2020-07-01 20:04:04] __main__ INFO: \u001b[0mElapsed 17.67\n",
      "\u001b[32m[2020-07-01 20:04:04] __main__ INFO: \u001b[0mTrain 49 16848\n",
      "\u001b[32m[2020-07-01 20:06:25] __main__ INFO: \u001b[0mEpoch 49 Step 100/351 lr 0.001000 loss 0.0056 (0.0042) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 20:08:46] __main__ INFO: \u001b[0mEpoch 49 Step 200/351 lr 0.001000 loss 0.0054 (0.0040) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 20:11:07] __main__ INFO: \u001b[0mEpoch 49 Step 300/351 lr 0.001000 loss 0.0035 (0.0041) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 20:12:19] __main__ INFO: \u001b[0mEpoch 49 Step 351/351 lr 0.001000 loss 0.0034 (0.0040) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 20:12:19] __main__ INFO: \u001b[0mElapsed 494.94\n",
      "\u001b[32m[2020-07-01 20:12:19] __main__ INFO: \u001b[0mVal 49\n",
      "\u001b[32m[2020-07-01 20:12:36] __main__ INFO: \u001b[0mEpoch 49 loss 0.2434 acc@1 0.9312 acc@5 0.9976\n",
      "\u001b[32m[2020-07-01 20:12:36] __main__ INFO: \u001b[0mElapsed 17.68\n",
      "\u001b[32m[2020-07-01 20:12:36] __main__ INFO: \u001b[0mTrain 50 17199\n",
      "\u001b[32m[2020-07-01 20:14:57] __main__ INFO: \u001b[0mEpoch 50 Step 100/351 lr 0.001000 loss 0.0025 (0.0040) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 20:17:18] __main__ INFO: \u001b[0mEpoch 50 Step 200/351 lr 0.001000 loss 0.0036 (0.0039) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 20:19:39] __main__ INFO: \u001b[0mEpoch 50 Step 300/351 lr 0.001000 loss 0.0046 (0.0040) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 20:20:51] __main__ INFO: \u001b[0mEpoch 50 Step 351/351 lr 0.001000 loss 0.0031 (0.0039) acc@1 1.0000 (1.0000) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-01 20:20:51] __main__ INFO: \u001b[0mElapsed 494.68\n",
      "\u001b[32m[2020-07-01 20:20:51] __main__ INFO: \u001b[0mVal 50\n",
      "\u001b[32m[2020-07-01 20:21:09] __main__ INFO: \u001b[0mEpoch 50 loss 0.2387 acc@1 0.9314 acc@5 0.9982\n",
      "\u001b[32m[2020-07-01 20:21:09] __main__ INFO: \u001b[0mElapsed 17.65\n",
      "\u001b[32m[2020-07-01 20:21:09] fvcore.common.checkpoint INFO: \u001b[0mSaving checkpoint to /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_5/exp00_resume400_50/checkpoint_00050.pth\n"
     ]
    }
   ],
   "source": [
    "# Resume training with the un-augmented data\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "#!python train.py --config configs/cifar/resnet.yaml \\\n",
    "!python train.py --config /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_5/exp00/config.yaml \\\n",
    "    train.checkpoint /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_5/exp00/checkpoint_00400.pth \\\n",
    "    model.resnext.cardinality 4 \\\n",
    "    test.batch_size 128 \\\n",
    "    dataset.name CIFAR10 \\\n",
    "    train.base_lr .001 \\\n",
    "    train.output_dir /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_5/exp00_resume400_50 \\\n",
    "    scheduler.epochs 50\n",
    "\n",
    "#### Set LEARNING RATE based on ending LR\n",
    "#    train.resume True \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-02 17:09:21] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_5/exp00/checkpoint_00400.pth\n",
      "Files already downloaded and verified\n",
      "100%|███████████████████████████████████████████| 79/79 [00:35<00:00,  2.21it/s]\n",
      "\u001b[32m[2020-07-02 17:09:58] __main__ INFO: \u001b[0mElapsed 35.78\n",
      "\u001b[32m[2020-07-02 17:09:58] __main__ INFO: \u001b[0mLoss 0.3952 Accuracy 0.8857\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10 test dataset - without training on unaugmented data\n",
    "# write the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/resnext.yaml \\\n",
    "   model.resnext.cardinality 4 \\\n",
    "   test.batch_size 128 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_5/exp00/checkpoint_00400.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_5/exp00/test_results_0400_cifar10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-02 17:11:20] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_5/exp00_resume400_50/checkpoint_00050.pth\n",
      "Files already downloaded and verified\n",
      "100%|███████████████████████████████████████████| 79/79 [00:35<00:00,  2.20it/s]\n",
      "\u001b[32m[2020-07-02 17:11:57] __main__ INFO: \u001b[0mElapsed 35.90\n",
      "\u001b[32m[2020-07-02 17:11:57] __main__ INFO: \u001b[0mLoss 0.2333 Accuracy 0.9354\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10 test dataset - using model refined on 50 epochs\n",
    "# write the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/resnext.yaml \\\n",
    "   model.resnext.cardinality 4 \\\n",
    "   test.batch_size 128 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_5/exp00_resume400_50/checkpoint_00050.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_5/exp00_resume400_50/test_results_0050_cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-02 17:17:48] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_5/exp00/checkpoint_00400.pth\n",
      "CIFAR 10.1\n",
      "100%|███████████████████████████████████████████| 16/16 [00:07<00:00,  2.15it/s]\n",
      "\u001b[32m[2020-07-02 17:17:56] __main__ INFO: \u001b[0mElapsed 7.43\n",
      "\u001b[32m[2020-07-02 17:17:56] __main__ INFO: \u001b[0mLoss 0.7954 Accuracy 0.7755\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10.1 test dataset \n",
    "# write the results to the test output directory specified.\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "!python evaluate.py --config configs/cifar/resnext.yaml \\\n",
    "    model.resnext.cardinality 4 \\\n",
    "    test.batch_size 128 \\\n",
    "    dataset.name CIFAR101 \\\n",
    "    test.checkpoint /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_5/exp00/checkpoint_00400.pth \\\n",
    "    test.output_dir /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_5/exp00/test_results_0400_CIFAR101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-02 17:18:50] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_5/exp00_resume400_50/checkpoint_00050.pth\n",
      "CIFAR 10.1\n",
      "100%|███████████████████████████████████████████| 16/16 [00:07<00:00,  2.16it/s]\n",
      "\u001b[32m[2020-07-02 17:18:58] __main__ INFO: \u001b[0mElapsed 7.39\n",
      "\u001b[32m[2020-07-02 17:18:58] __main__ INFO: \u001b[0mLoss 0.5154 Accuracy 0.8455\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10.1 test dataset - using model refined on 50 epochs\n",
    "# write the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/resnext.yaml \\\n",
    "   model.resnext.cardinality 4 \\\n",
    "   test.batch_size 128 \\\n",
    "   dataset.name CIFAR101 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_5/exp00_resume400_50/checkpoint_00050.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_5/exp00_resume400_50/test_results_0050_cifar101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Testset</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Original_Accuracy_300epochs</th>\n",
       "      <th>Original_CI_300epochs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>resnext_29_4x64d_ra_2_5</td>\n",
       "      <td>400</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.3952</td>\n",
       "      <td>0.8857</td>\n",
       "      <td>96.4</td>\n",
       "      <td>(96.0, 96.7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>resnext_29_4x64d_ra_2_5</td>\n",
       "      <td>400</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>0.7954</td>\n",
       "      <td>0.7755</td>\n",
       "      <td>89.6</td>\n",
       "      <td>(88.2, 90.9)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>resnext_29_4x64d_ra_2_5_refined400</td>\n",
       "      <td>50</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.2333</td>\n",
       "      <td>0.9354</td>\n",
       "      <td>96.4</td>\n",
       "      <td>(96.0, 96.7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>resnext_29_4x64d_ra_2_5_refined400</td>\n",
       "      <td>50</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>0.5154</td>\n",
       "      <td>0.8455</td>\n",
       "      <td>89.6</td>\n",
       "      <td>(88.2, 90.9)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Model Epoch    Testset    Loss Accuracy  \\\n",
       "0             resnext_29_4x64d_ra_2_5   400    cifar10  0.3952   0.8857   \n",
       "1             resnext_29_4x64d_ra_2_5   400  cifar10.1  0.7954   0.7755   \n",
       "2  resnext_29_4x64d_ra_2_5_refined400    50    cifar10  0.2333   0.9354   \n",
       "3  resnext_29_4x64d_ra_2_5_refined400    50  cifar10.1  0.5154   0.8455   \n",
       "\n",
       "   Original_Accuracy_300epochs Original_CI_300epochs  \n",
       "0                         96.4          (96.0, 96.7)  \n",
       "1                         89.6          (88.2, 90.9)  \n",
       "2                         96.4          (96.0, 96.7)  \n",
       "3                         89.6          (88.2, 90.9)  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "a = pd.Series(['resnext_29_4x64d_ra_2_5', 400, 'cifar10', 0.3952, 0.8857]) #Loss 0.3952 Accuracy 0.8857\n",
    "b = pd.Series(['resnext_29_4x64d_ra_2_5', 400, 'cifar10.1', 0.7954, 0.7755]) #0.7954 Accuracy 0.7755\n",
    "\n",
    "c = pd.Series(['resnext_29_4x64d_ra_2_5_refined400', 50, 'cifar10', 0.2333, 0.9354]) #0.2333 Accuracy 0.9354\n",
    "d = pd.Series(['resnext_29_4x64d_ra_2_5_refined400', 50, 'cifar10.1', 0.5154, 0.8455]) #0.5154 Accuracy 0.8455\n",
    "\n",
    "df_results = pd.concat([a,b,c,d], axis=1).T\n",
    "df_results.columns = ['Model', 'Epoch', 'Testset', 'Loss', 'Accuracy']\n",
    "\n",
    "df_results['Original_Accuracy_300epochs'] = df_results.apply((lambda row: 96.4 if row[2] == 'cifar10' else 89.6), axis=1)\n",
    "df_results['Original_CI_300epochs'] = df_results.apply((lambda row: (96.0, 96.7) if row[2] == 'cifar10' else (88.2, 90.9)), axis=1)\n",
    "\n",
    "df_results.to_csv('/home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_5/results.csv')\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Testset</th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Original_Accuracy</th>\n",
       "      <th>Original_CI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>resnext_29_4x64d</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>100</td>\n",
       "      <td>0.6746</td>\n",
       "      <td>0.8019</td>\n",
       "      <td>96.4</td>\n",
       "      <td>(96.0, 96.7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>resnext_29_4x64d</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>200</td>\n",
       "      <td>0.2311</td>\n",
       "      <td>0.9321</td>\n",
       "      <td>96.4</td>\n",
       "      <td>(96.0, 96.7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>resnext_29_4x64d</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>300</td>\n",
       "      <td>0.1517</td>\n",
       "      <td>0.9535</td>\n",
       "      <td>96.4</td>\n",
       "      <td>(96.0, 96.7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>resnext_29_4x64d</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>300</td>\n",
       "      <td>0.3742</td>\n",
       "      <td>0.8905</td>\n",
       "      <td>89.6</td>\n",
       "      <td>(88.2, 90.9)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Model    Testset  Epoch    Loss  Accuracy  Original_Accuracy  \\\n",
       "0  resnext_29_4x64d    cifar10    100  0.6746    0.8019               96.4   \n",
       "1  resnext_29_4x64d    cifar10    200  0.2311    0.9321               96.4   \n",
       "2  resnext_29_4x64d    cifar10    300  0.1517    0.9535               96.4   \n",
       "3  resnext_29_4x64d  cifar10.1    300  0.3742    0.8905               89.6   \n",
       "\n",
       "    Original_CI  \n",
       "0  (96.0, 96.7)  \n",
       "1  (96.0, 96.7)  \n",
       "2  (96.0, 96.7)  \n",
       "3  (88.2, 90.9)  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write the results to a CSV file so that we can analyze later.\n",
    "import pandas as pd\n",
    "\n",
    "results = {'Model': ['resnext_29_4x64d', 'resnext_29_4x64d', 'resnext_29_4x64d', 'resnext_29_4x64d'],\n",
    "           'Testset': ['cifar10', 'cifar10', 'cifar10', 'cifar10.1'],\n",
    "           'Epoch': [100, 200, 300, 300],\n",
    "           'Loss': [0.6746, 0.2311, 0.1517, 0.3742],\n",
    "           'Accuracy': [0.8019, 0.9321, 0.9535, 0.8905],\n",
    "           'Original_Accuracy': [96.4, 96.4, 96.4, 89.6],\n",
    "           'Original_CI': [(96.0, 96.7), (96.0, 96.7), (96.0, 96.7), (88.2, 90.9)]\n",
    "           }\n",
    "\n",
    "df = pd.DataFrame(results, columns = ['Model', 'Testset', 'Epoch', 'Loss', 'Accuracy', \n",
    "                                      'Original_Accuracy', 'Original_CI'])\n",
    "\n",
    "\n",
    "df.to_csv('/home/ec2-user/SageMaker/experiments/resnext_29_4x64d/exp00/results.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['preds', 'probs', 'labels', 'loss', 'acc']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-1.2041907 , -1.8999833 , -0.24285015, ..., -1.5008752 ,\n",
       "        -1.8426697 , -2.8560946 ],\n",
       "       [ 0.5460079 ,  2.220384  , -1.9393705 , ..., -2.6070693 ,\n",
       "        11.327686  , -1.2085156 ],\n",
       "       [-1.3446747 ,  2.1730833 , -1.1615647 , ..., -2.2299995 ,\n",
       "        10.984515  , -0.75660706],\n",
       "       ...,\n",
       "       [-2.4790986 , -1.3337001 ,  0.61669415, ..., -0.83421385,\n",
       "        -1.8529658 , -1.7280097 ],\n",
       "       [-0.90489024,  9.350766  ,  1.0618937 , ..., -2.3210623 ,\n",
       "        -0.9061641 , -1.8115013 ],\n",
       "       [-1.4560711 , -1.0518838 , -1.4613396 , ..., 12.668192  ,\n",
       "        -2.1191459 , -0.8881919 ]], dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Peak inside the output file for predictions\n",
    "import numpy as np\n",
    "output = '/home/ec2-user/SageMaker/experiments/resnext_29_4x64d/exp00/test_results_0300/predictions.npz'\n",
    "npzfile = np.load(output)\n",
    "print(npzfile.files)\n",
    "npzfile['preds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the model checkpoints, configs, and results to S3 \n",
    "bucket='sagemaker-may29'\n",
    "prefix = 'sagemaker/results/original-models/'\n",
    "path = '/home/ec2-user/SageMaker/experiments/'\n",
    "\n",
    "s3_resource = boto3.resource(\"s3\", region_name=\"us-east-2\")\n",
    "\n",
    "def uploadDirectory(local_path,bucket_name,s3_prefix):\n",
    "\n",
    "    my_bucket = s3_resource.Bucket(bucket_name)\n",
    "    \n",
    "    for path, subdirs, files in os.walk(local_path):\n",
    "        path = path.replace(\"\\\\\",\"/\")\n",
    "        directory_name = path.replace(local_path,\"\")\n",
    "        for file in files:\n",
    "            #print(\"Local File:\", os.path.join(path, file))\n",
    "            #print(\"      Dest:\", s3_prefix+directory_name+'/'+file)\n",
    "            my_bucket.upload_file(os.path.join(path, file), s3_prefix+directory_name+'/'+file)\n",
    "    \n",
    "uploadDirectory(path,bucket,prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Everything Below Is In-Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SageMaker Way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 Variable Definitions \n",
    "bucket='sagemaker-may29'\n",
    "prefix = '/sagemaker-may29/sagemaker/cifar102/data/cifar10_10k'\n",
    "model_path = '/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/'\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "#bucket = sagemaker.Session().default_bucket(\n",
    "#sagemaker_session = sagemaker.Session()\n",
    "bucket='sagemaker-may29'\n",
    "sagemaker_session = sagemaker.Session(default_bucket=bucket)\n",
    "\n",
    "\n",
    "# Set S3 dataset path \n",
    "#inputs = 's3://' + bucket + '/sagemaker/{}'.format(args.train_data)\n",
    "inputs = 's3://' + bucket + '/sagemaker-may29/sagemaker/cifar102/data/cifar10_10k/cifar10_10k_test'\n",
    "inputs = 's3://' + bucket + '/sagemaker-may29/sagemaker/cifar102/data/cifar10_10k/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/'\n",
    "container_data_dir = '/opt/ml/input/data/training'\n",
    "container_model_dir = '/opt/ml/model'\n",
    "\n",
    "parameters = {\n",
    "    'config': 'resnext.yaml',\n",
    "    'resnext.depth': 29,\n",
    "    'train.batch_size': 128,\n",
    "    'train.base_lr': 0.1,\n",
    "    #'data_dir': container_data_dir,\n",
    "    #'dataset.dataset_dir': container_data_dir\n",
    "    #'output_dir': container_model_dir,\n",
    "    #'num_train_epochs': 3,\n",
    "    #'per_gpu_train_batch_size': 64,\n",
    "    #'per_gpu_eval_batch_size': 64,\n",
    "    #'save_steps': 150,\n",
    "    #'logging_steps': 150\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the PyTorch class that enables the model script to run as a \n",
    "# training job on the SageMaker distributed, managed training infrastructure\n",
    "estimator = PyTorch(entry_point= model_path + 'train.py',\n",
    "                    role=role,\n",
    "                    framework_version='1.4.0',\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type='ml.p2.xlarge',\n",
    "                    hyperparameters=parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n",
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-08 14:49:34 Starting - Starting the training job...\n",
      "2020-06-08 14:49:36 Starting - Launching requested ML instances.........\n",
      "2020-06-08 14:51:06 Starting - Preparing the instances for training......\n",
      "2020-06-08 14:52:21 Downloading - Downloading input data...\n",
      "2020-06-08 14:53:01 Training - Downloading the training image...........\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-06-08 14:54:49,375 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-06-08 14:54:49,397 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-06-08 14:54:49,403 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-06-08 14:54:49,684 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-06-08 14:54:49,684 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-06-08 14:54:49,684 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-06-08 14:54:49,685 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m pip install . \u001b[0m\n",
      "\u001b[34mProcessing /tmp/tmprotlgjc8/module_dir\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: default-user-module-name\n",
      "  Building wheel for default-user-module-name (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for default-user-module-name (setup.py): finished with status 'done'\n",
      "  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=10386 sha256=4f15af3dee30a7116f4f4bdfc068a488b47ab4d7d874d41ddf7a13c58d82ad7a\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-hdo2srdp/wheels/c9/42/19/42704d4efcc75760cfda35a2b5e353ab766536836839b671fb\u001b[0m\n",
      "\u001b[34mSuccessfully built default-user-module-name\u001b[0m\n",
      "\u001b[34mInstalling collected packages: default-user-module-name\u001b[0m\n",
      "\u001b[34mSuccessfully installed default-user-module-name-1.0.0\u001b[0m\n",
      "\u001b[34m2020-06-08 14:54:51,994 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"resnext.depth\": 29,\n",
      "        \"train.base_lr\": 0.1,\n",
      "        \"train.batch_size\": 128,\n",
      "        \"config\": \"resnext.yaml\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2020-06-08-14-49-34-014\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-2-681306256852/pytorch-training-2020-06-08-14-49-34-014/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"config\":\"resnext.yaml\",\"resnext.depth\":29,\"train.base_lr\":0.1,\"train.batch_size\":128}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-2-681306256852/pytorch-training-2020-06-08-14-49-34-014/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"config\":\"resnext.yaml\",\"resnext.depth\":29,\"train.base_lr\":0.1,\"train.batch_size\":128},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2020-06-08-14-49-34-014\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-681306256852/pytorch-training-2020-06-08-14-49-34-014/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--config\",\"resnext.yaml\",\"--resnext.depth\",\"29\",\"--train.base_lr\",\"0.1\",\"--train.batch_size\",\"128\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_RESNEXT.DEPTH=29\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN.BASE_LR=0.1\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN.BATCH_SIZE=128\u001b[0m\n",
      "\u001b[34mSM_HP_CONFIG=resnext.yaml\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python train.py --config resnext.yaml --resnext.depth 29 --train.base_lr 0.1 --train.batch_size 128\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m2020-06-08 14:54:52,052 sagemaker-containers ERROR    ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[34mCommand \"/opt/conda/bin/python train.py --config resnext.yaml --resnext.depth 29 --train.base_lr 0.1 --train.batch_size 128\"\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"train.py\", line 7, in <module>\n",
      "    import apex\u001b[0m\n",
      "\u001b[34mModuleNotFoundError: No module named 'apex'\u001b[0m\n",
      "\n",
      "2020-06-08 14:55:00 Uploading - Uploading generated training model\n",
      "2020-06-08 14:55:00 Failed - Training job failed\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job pytorch-training-2020-06-08-14-49-34-014: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"/opt/conda/bin/python train.py --config resnext.yaml --resnext.depth 29 --train.base_lr 0.1 --train.batch_size 128\"\nTraceback (most recent call last):\n  File \"train.py\", line 7, in <module>\n    import apex\nModuleNotFoundError: No module named 'apex'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b5cd4c918894>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m's3://'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbucket\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/sagemaker-may29/sagemaker/cifar102/data/cifar10_10k/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#estimator.fit({'training': inputs})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compilation_job_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0;31m# If logs are requested, call logs_for_jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"None\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1109\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1110\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type)\u001b[0m\n\u001b[1;32m   3066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3067\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3068\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TrainingJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3069\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3070\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   2660\u001b[0m                 ),\n\u001b[1;32m   2661\u001b[0m                 \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Stopped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2662\u001b[0;31m                 \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2663\u001b[0m             )\n\u001b[1;32m   2664\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job pytorch-training-2020-06-08-14-49-34-014: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"/opt/conda/bin/python train.py --config resnext.yaml --resnext.depth 29 --train.base_lr 0.1 --train.batch_size 128\"\nTraceback (most recent call last):\n  File \"train.py\", line 7, in <module>\n    import apex\nModuleNotFoundError: No module named 'apex'"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "#inputs = 's3://' + bucket + '/sagemaker-may29/sagemaker/cifar102/data/cifar10_10k/cifar10_10k_test'\n",
    "inputs = 's3://' + bucket + '/sagemaker-may29/sagemaker/cifar102/data/cifar10_10k/'\n",
    "#estimator.fit({'training': inputs})\n",
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ec2-user/SageMaker'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Augmentation \n",
    "https://imgaug.readthedocs.io/en/latest/source/overview/collections.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imgaug in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (0.4.0)\n",
      "Requirement already satisfied: imageio in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from imgaug) (2.3.0)\n",
      "Requirement already satisfied: opencv-python in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from imgaug) (4.1.1.26)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from imgaug) (1.11.0)\n",
      "Requirement already satisfied: matplotlib in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from imgaug) (3.0.3)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from imgaug) (1.1.0)\n",
      "Requirement already satisfied: Pillow in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from imgaug) (5.4.1)\n",
      "Requirement already satisfied: Shapely in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from imgaug) (1.7.0)\n",
      "Requirement already satisfied: scikit-image>=0.14.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from imgaug) (0.17.2)\n",
      "Requirement already satisfied: numpy>=1.15 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from imgaug) (1.15.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from matplotlib->imgaug) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from matplotlib->imgaug) (1.0.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from matplotlib->imgaug) (2.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from matplotlib->imgaug) (2.7.3)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from scikit-image>=0.14.2->imgaug) (1.1.1)\n",
      "Requirement already satisfied: networkx>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from scikit-image>=0.14.2->imgaug) (2.1)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from scikit-image>=0.14.2->imgaug) (2020.6.3)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib->imgaug) (47.1.1)\n",
      "Requirement already satisfied: decorator>=4.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from networkx>=2.0->scikit-image>=0.14.2->imgaug) (4.3.0)\n",
      "\u001b[31mfastai 1.0.60 requires nvidia-ml-py3, which is not installed.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.2b1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install imgaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imgaug.augmenters as iaa\n",
    "aug = iaa.RandAugment(n=2, m=(0, 9))  \n",
    "  # n is the number of transformations to apply per image\n",
    "  # m is magnitude -> specifying a tuple will randomly select values between the min and max (max is 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3 \n",
    "from PIL import Image\n",
    "\n",
    "cifar10 = \"sagemaker/cifar-10/cifar/\"\n",
    "#cifar10_test_rec = \"s3://sagemaker-may29/sagemaker/cifar-10/cifar/test.rec\"\n",
    "\n",
    "def download_all_objects_in_folder(b, p):\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    my_bucket = s3_resource.Bucket(b)\n",
    "    objects = my_bucket.objects.filter(Prefix=p)\n",
    "    for obj in objects:\n",
    "        path, filename = os.path.split(obj.key)\n",
    "        my_bucket.download_file(obj.key, filename)    \n",
    "\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "# Download the unzipped data from the cifar10 folder\n",
    "download_all_objects_in_folder(bucket, cifar10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.rec: data\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandAugment \n",
    "https://arxiv.org/abs/1909.13719\n",
    "https://pypi.org/project/randaugment/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting randaugment\n",
      "  Using cached https://files.pythonhosted.org/packages/fb/ea/e24549f459800dc3bed21cd4e9c0d49d5b8deed65214b2444bd3e5a49f30/randaugment-1.0.2-py3-none-any.whl\n",
      "\u001b[31mfastai 1.0.60 requires nvidia-ml-py3, which is not installed.\u001b[0m\n",
      "Installing collected packages: randaugment\n",
      "Successfully installed randaugment-1.0.2\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.2b1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install randaugment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ImageFolder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-114a2729ff3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrandaugment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandAugment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageNetPolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m data = ImageFolder('/home/ec2-user/SageMaker/w210-capstone/data/cifar10/subset300', transform=transforms.Compose(\n\u001b[0m\u001b[1;32m      3\u001b[0m                         [\n\u001b[1;32m      4\u001b[0m                             \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomCrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# fill parameter needs torchvision installed from source\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                             \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomHorizontalFlip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ImageFolder' is not defined"
     ]
    }
   ],
   "source": [
    "from randaugment import RandAugment, ImageNetPolicy\n",
    "data = ImageFolder('/home/ec2-user/SageMaker/w210-capstone/data/cifar10/subset300', transform=transforms.Compose(\n",
    "                        [\n",
    "                            transforms.RandomCrop(32, padding=4, fill=128), # fill parameter needs torchvision installed from source\n",
    "                            transforms.RandomHorizontalFlip(), \n",
    "                            RandAugment(),\n",
    "                            #ImageNetPolicy(),\n",
    "                            transforms.ToTensor(), \n",
    "                            Cutout(size=16), # (https://github.com/uoguelph-mlrg/Cutout/blob/master/util/cutout.py)\n",
    "                            transforms.Normalize(...)\n",
    "                        ])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Augment\n",
    "https://blog.insightdatascience.com/automl-for-data-augmentation-e87cf692c366\n",
    "https://colab.research.google.com/drive/1KCAv2i_F3E3m_PKh56nbbZY8WnaASvgl#scrollTo=SuhR6Q3AMFy3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deepaugment\n",
      "  Downloading https://files.pythonhosted.org/packages/99/f9/40211d827039df475091639c6aded9a1786849f898b9c619e24c15efc82a/deepaugment-1.1.2-py2.py3-none-any.whl\n",
      "Collecting keras-applications==1.0.6 (from deepaugment)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/c4/2ff40221029f7098d58f8d7fb99b97e8100f3293f9856f0fb5834bef100b/Keras_Applications-1.0.6-py2.py3-none-any.whl (44kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 4.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pandas==0.23.4 (from deepaugment)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/d8/feeb346d41f181e83fba45224ab14a8d8af019b48af742e047f3845d8cff/pandas-0.23.4-cp36-cp36m-manylinux1_x86_64.whl (8.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 8.9MB 5.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scikit-optimize==0.5.2 (from deepaugment)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/44/60f82c97d1caa98752c7da2c1681cab5c7a390a0fdd3a55fac672b321cac/scikit_optimize-0.5.2-py2.py3-none-any.whl (74kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 34.7MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting click==7.0 (from deepaugment)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fa/37/45185cb5abbc30d7257104c434fe0b07e5a195a6847506c074527aa599ec/Click-7.0-py2.py3-none-any.whl (81kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 33.4MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting numpy==1.15.4 (from deepaugment)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/7f/9d804d2348471c67a7d8b5f84f9bc59fd1cefa148986f2b74552f8573555/numpy-1.15.4-cp36-cp36m-manylinux1_x86_64.whl (13.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 13.9MB 3.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opencv-contrib-python (from deepaugment)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c6/db/790dbc6bcfea87fc6f790c6306509c2691ce31c96d82e5b826545d90ea52/opencv_contrib_python-4.2.0.34-cp36-cp36m-manylinux1_x86_64.whl (34.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 34.2MB 1.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting imgaug==0.2.7 (from deepaugment)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/fc/c56a7da8c23122b7c5325b941850013880a7a93c21dc95e2b1ecd4750108/imgaug-0.2.7-py3-none-any.whl (644kB)\n",
      "\u001b[K    100% |████████████████████████████████| 645kB 34.7MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow==1.12.0 (from deepaugment)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/cc/ca70b78087015d21c5f3f93694107f34ebccb3be9624385a911d4b52ecef/tensorflow-1.12.0-cp36-cp36m-manylinux1_x86_64.whl (83.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 83.1MB 574kB/s eta 0:00:01    62% |███████████████████▉            | 51.6MB 52.9MB/s eta 0:00:01    76% |████████████████████████▋       | 63.8MB 62.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting matplotlib==3.0.2 (from deepaugment)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/07/16d781df15be30df4acfd536c479268f1208b2dfbc91e9ca5d92c9caf673/matplotlib-3.0.2-cp36-cp36m-manylinux1_x86_64.whl (12.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 12.9MB 4.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting setuptools==40.6.3 (from deepaugment)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/37/06/754589caf971b0d2d48f151c2586f62902d93dc908e2fd9b9b9f6aa3c9dd/setuptools-40.6.3-py2.py3-none-any.whl (573kB)\n",
      "\u001b[K    100% |████████████████████████████████| 573kB 31.6MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting keras==2.2.4 (from deepaugment)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl (312kB)\n",
      "\u001b[K    100% |████████████████████████████████| 317kB 37.5MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: h5py in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from keras-applications==1.0.6->deepaugment) (2.8.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from pandas==0.23.4->deepaugment) (2.7.3)\n",
      "Requirement already satisfied: pytz>=2011k in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from pandas==0.23.4->deepaugment) (2018.4)\n",
      "Requirement already satisfied: scipy>=0.14.0 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from scikit-optimize==0.5.2->deepaugment) (1.1.0)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from scikit-optimize==0.5.2->deepaugment) (0.20.3)\n",
      "Requirement already satisfied: scikit-image>=0.11.0 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from imgaug==0.2.7->deepaugment) (0.13.1)\n",
      "Collecting Shapely (from imgaug==0.2.7->deepaugment)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/fa/c96d3461fda99ed8e82ff0b219ac2c8384694b4e640a611a1a8390ecd415/Shapely-1.7.0-cp36-cp36m-manylinux1_x86_64.whl (1.8MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.8MB 18.5MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: Pillow in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from imgaug==0.2.7->deepaugment) (5.1.0)\n",
      "Requirement already satisfied: imageio in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from imgaug==0.2.7->deepaugment) (2.3.0)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from imgaug==0.2.7->deepaugment) (1.11.0)\n",
      "Collecting absl-py>=0.1.6 (from tensorflow==1.12.0->deepaugment)\n",
      "Collecting keras-preprocessing>=1.0.5 (from tensorflow==1.12.0->deepaugment)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/4c/7c3275a01e12ef9368a892926ab932b33bb13d55794881e3573482b378a7/Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 16.3MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting protobuf>=3.6.1 (from tensorflow==1.12.0->deepaugment)\n",
      "  Using cached https://files.pythonhosted.org/packages/28/05/9867ef8eafd12265267bee138fa2c46ebf34a276ea4cbe184cba4c606e8b/protobuf-3.12.2-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting grpcio>=1.8.6 (from tensorflow==1.12.0->deepaugment)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow==1.12.0->deepaugment)\n",
      "Collecting astor>=0.6.0 (from tensorflow==1.12.0->deepaugment)\n",
      "  Downloading https://files.pythonhosted.org/packages/c3/88/97eef84f48fa04fbd6750e62dcceafba6c63c81b7ac1420856c8dcc0a3f9/astor-0.8.1-py2.py3-none-any.whl\n",
      "Collecting tensorboard<1.13.0,>=1.12.0 (from tensorflow==1.12.0->deepaugment)\n",
      "\u001b[33m  Cache entry deserialization failed, entry ignored\u001b[0m\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/53/8d32ce9471c18f8d99028b7cef2e5b39ea8765bd7ef250ca05b490880971/tensorboard-1.12.2-py3-none-any.whl (3.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.1MB 20.0MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from tensorflow==1.12.0->deepaugment) (0.31.1)\n",
      "Collecting gast>=0.2.0 (from tensorflow==1.12.0->deepaugment)\n",
      "  Downloading https://files.pythonhosted.org/packages/d6/84/759f5dd23fec8ba71952d97bcc7e2c9d7d63bdc582421f3cd4be845f0c98/gast-0.3.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from matplotlib==3.0.2->deepaugment) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from matplotlib==3.0.2->deepaugment) (2.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from matplotlib==3.0.2->deepaugment) (1.0.1)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from keras==2.2.4->deepaugment) (5.3.1)\n",
      "Requirement already satisfied: networkx>=1.8 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from scikit-image>=0.11.0->imgaug==0.2.7->deepaugment) (2.1)\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from scikit-image>=0.11.0->imgaug==0.2.7->deepaugment) (0.5.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.10 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.12.0->deepaugment) (0.14.1)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.12.0->deepaugment)\n",
      "  Using cached https://files.pythonhosted.org/packages/a4/63/eaec2bd025ab48c754b55e8819af0f6a69e2b1e187611dd40cbbe101ee7f/Markdown-3.2.2-py3-none-any.whl\n",
      "Requirement already satisfied: decorator>=4.1.0 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from networkx>=1.8->scikit-image>=0.11.0->imgaug==0.2.7->deepaugment) (4.3.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard<1.13.0,>=1.12.0->tensorflow==1.12.0->deepaugment) (1.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.13.0,>=1.12.0->tensorflow==1.12.0->deepaugment) (3.0.0)\n",
      "\u001b[31mamazonei-mxnet 1.5.1 has requirement numpy<1.15.0,>=1.8.2, but you'll have numpy 1.15.4 which is incompatible.\u001b[0m\n",
      "Installing collected packages: numpy, keras-applications, pandas, scikit-optimize, click, opencv-contrib-python, Shapely, matplotlib, imgaug, absl-py, keras-preprocessing, setuptools, protobuf, grpcio, termcolor, astor, markdown, tensorboard, gast, tensorflow, keras, deepaugment\n",
      "  Found existing installation: numpy 1.14.6\n",
      "    Uninstalling numpy-1.14.6:\n",
      "      Successfully uninstalled numpy-1.14.6\n",
      "  Found existing installation: pandas 0.24.2\n",
      "    Uninstalling pandas-0.24.2:\n",
      "      Successfully uninstalled pandas-0.24.2\n",
      "  Found existing installation: click 6.7\n",
      "    Uninstalling click-6.7:\n",
      "      Successfully uninstalled click-6.7\n",
      "  Found existing installation: matplotlib 3.0.3\n",
      "    Uninstalling matplotlib-3.0.3:\n",
      "      Successfully uninstalled matplotlib-3.0.3\n",
      "  Found existing installation: setuptools 39.1.0\n",
      "    Uninstalling setuptools-39.1.0:\n",
      "      Successfully uninstalled setuptools-39.1.0\n",
      "  Found existing installation: protobuf 3.5.2\n",
      "    Uninstalling protobuf-3.5.2:\n",
      "      Successfully uninstalled protobuf-3.5.2\n",
      "Successfully installed Shapely-1.7.0 absl-py-0.9.0 astor-0.8.1 click-7.0 deepaugment-1.1.2 gast-0.3.3 grpcio-1.29.0 imgaug-0.2.7 keras-2.2.4 keras-applications-1.0.6 keras-preprocessing-1.1.2 markdown-3.2.2 matplotlib-3.0.2 numpy-1.15.4 opencv-contrib-python-4.2.0.34 pandas-0.23.4 protobuf-3.12.2 scikit-optimize-0.5.2 setuptools-40.6.3 tensorboard-1.12.2 tensorflow-1.12.0 termcolor-1.1.0\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.2b1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install deepaugment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__new__() got an unexpected keyword argument 'serialized_options'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-952c53067624>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdeepaugment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepaugment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDeepAugment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m deepaug = DeepAugment('/home/ec2-user/SageMaker/w210-capstone/data/cifar10/subset300', \n\u001b[1;32m      4\u001b[0m                       '/home/ec2-user/SageMaker/w210-capstone/data/cifar10/trainLabels.csv')\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/deepaugment/deepaugment.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# (C) 2019 Baris Ozmen <hbaristr@gmail.com>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# pylint: disable=g-bad-import-order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m  \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;31m# Protocol buffers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_pb2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_def_pb2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary_pb2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/tensorflow/core/framework/graph_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnode_def_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_node__def__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunction_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_function__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mversions_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_versions__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/tensorflow/core/framework/node_def_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mattr_value_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_attr__value__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/tensorflow/core/framework/attr_value_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_tensor__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_shape_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypes_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_types__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/tensorflow/core/framework/tensor_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mresource_handle_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_resource__handle__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_shape_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypes_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_types__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/tensorflow/core/framework/resource_handle_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0msyntax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'proto3'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0mserialized_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\030org.tensorflow.frameworkB\\016ResourceHandleP\\001Z=github.com/tensorflow/tensorflow/tensorflow/go/core/framework\\370\\001\\001'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m   \u001b[0mserialized_pb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n/tensorflow/core/framework/resource_handle.proto\\x12\\ntensorflow\\\"r\\n\\x13ResourceHandleProto\\x12\\x0e\\n\\x06\\x64\\x65vice\\x18\\x01 \\x01(\\t\\x12\\x11\\n\\tcontainer\\x18\\x02 \\x01(\\t\\x12\\x0c\\n\\x04name\\x18\\x03 \\x01(\\t\\x12\\x11\\n\\thash_code\\x18\\x04 \\x01(\\x04\\x12\\x17\\n\\x0fmaybe_type_name\\x18\\x05 \\x01(\\tBn\\n\\x18org.tensorflow.frameworkB\\x0eResourceHandleP\\x01Z=github.com/tensorflow/tensorflow/tensorflow/go/core/framework\\xf8\\x01\\x01\\x62\\x06proto3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __new__() got an unexpected keyword argument 'serialized_options'"
     ]
    }
   ],
   "source": [
    "from deepaugment.deepaugment import DeepAugment\n",
    "\n",
    "deepaug = DeepAugment('/home/ec2-user/SageMaker/w210-capstone/data/cifar10/subset300', \n",
    "                      '/home/ec2-user/SageMaker/w210-capstone/data/cifar10/trainLabels.csv')\n",
    "\n",
    "best_policies = deepaug.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker\n"
     ]
    }
   ],
   "source": [
    "!ls /home/ec2-user/SageMaker/w210-capstone/data/cifar10/subset300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must be conda_pytroch_p36 notebook\n",
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import sagemaker\n",
    "import torch\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.pytorch import PyTorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r ./w210-capstone/models/pytorch_imageclass/requirements.txt (line 1)) (1.15.4)\n",
      "Requirement already satisfied: torch>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r ./w210-capstone/models/pytorch_imageclass/requirements.txt (line 2)) (1.4.0)\n",
      "Requirement already satisfied: torchvision in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r ./w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (0.5.0)\n",
      "Requirement already satisfied: fvcore in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r ./w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (0.1.1.post20200603)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r ./w210-capstone/models/pytorch_imageclass/requirements.txt (line 5)) (4.42.1)\n",
      "Requirement already satisfied: yacs in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r ./w210-capstone/models/pytorch_imageclass/requirements.txt (line 6)) (0.1.7)\n",
      "Requirement already satisfied: apex from git+https://github.com/NVIDIA/apex.git#egg=apex in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r ./w210-capstone/models/pytorch_imageclass/requirements.txt (line 7)) (0.1)\n",
      "Requirement already satisfied: termcolor in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r ./w210-capstone/models/pytorch_imageclass/requirements.txt (line 8)) (1.1.0)\n",
      "Requirement already satisfied: thop<0.0.31.post2004070130 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r ./w210-capstone/models/pytorch_imageclass/requirements.txt (line 9)) (0.0.31.post2001170342)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision->-r ./w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (1.11.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision->-r ./w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (5.4.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r ./w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (5.3.1)\n",
      "Requirement already satisfied: tabulate in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r ./w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (0.8.7)\n",
      "Requirement already satisfied: portalocker in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r ./w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (1.7.0)\n",
      "\u001b[31mfastai 1.0.60 requires nvidia-ml-py3, which is not installed.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.2b1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r ./w210-capstone/models/pytorch_imageclass/requirements.txt\n",
    "\n",
    "# Do we need NVIDIA installed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-04 00:41:31] __main__ INFO: \u001b[0mdevice: cpu\n",
      "cudnn:\n",
      "  benchmark: True\n",
      "  deterministic: False\n",
      "dataset:\n",
      "  name: CIFAR10\n",
      "  dataset_dir: ~/.torch/datasets/CIFAR10\n",
      "  image_size: 32\n",
      "  n_channels: 3\n",
      "  n_classes: 10\n",
      "model:\n",
      "  type: cifar\n",
      "  name: resnext\n",
      "  init_mode: kaiming_fan_out\n",
      "  vgg:\n",
      "    n_channels: [64, 128, 256, 512, 512]\n",
      "    n_layers: [2, 2, 3, 3, 3]\n",
      "    use_bn: True\n",
      "  resnet:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "  resnet_preact:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "  wrn:\n",
      "    depth: 28\n",
      "    initial_channels: 16\n",
      "    widening_factor: 10\n",
      "    drop_rate: 0.0\n",
      "  densenet:\n",
      "    depth: 100\n",
      "    n_blocks: [6, 12, 24, 16]\n",
      "    block_type: bottleneck\n",
      "    growth_rate: 12\n",
      "    drop_rate: 0.0\n",
      "    compression_rate: 0.5\n",
      "  pyramidnet:\n",
      "    depth: 272\n",
      "    n_blocks: [3, 24, 36, 3]\n",
      "    initial_channels: 16\n",
      "    block_type: bottleneck\n",
      "    alpha: 200\n",
      "  resnext:\n",
      "    depth: 29\n",
      "    n_blocks: [3, 4, 6, 3]\n",
      "    initial_channels: 64\n",
      "    cardinality: 4\n",
      "    base_channels: 64\n",
      "  shake_shake:\n",
      "    depth: 26\n",
      "    initial_channels: 96\n",
      "    shake_forward: True\n",
      "    shake_backward: True\n",
      "    shake_image: True\n",
      "  se_resnet_preact:\n",
      "    depth: 110\n",
      "    initial_channels: 16\n",
      "    se_reduction: 16\n",
      "    block_type: basic\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "train:\n",
      "  checkpoint: ''\n",
      "  resume: False\n",
      "  precision: O0\n",
      "  batch_size: 128\n",
      "  subdivision: 1\n",
      "  optimizer: sgd\n",
      "  base_lr: 0.1\n",
      "  momentum: 0.9\n",
      "  nesterov: True\n",
      "  weight_decay: 0.0005\n",
      "  no_weight_decay_on_bn: False\n",
      "  gradient_clip: 0\n",
      "  start_epoch: 0\n",
      "  seed: 0\n",
      "  val_first: True\n",
      "  val_period: 1\n",
      "  val_ratio: 0.1\n",
      "  use_test_as_val: False\n",
      "  output_dir: experiments/resnext_29_4x64d/exp00\n",
      "  log_period: 100\n",
      "  checkpoint_period: 100\n",
      "  use_tensorboard: True\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: True\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "  distributed: False\n",
      "  dist:\n",
      "    backend: nccl\n",
      "    init_method: env://\n",
      "    world_size: -1\n",
      "    node_rank: -1\n",
      "    local_rank: 0\n",
      "    use_sync_bn: False\n",
      "tensorboard:\n",
      "  train_images: False\n",
      "  val_images: False\n",
      "  model_params: False\n",
      "optim:\n",
      "  adam:\n",
      "    betas: (0.9, 0.999)\n",
      "  lars:\n",
      "    eps: 1e-09\n",
      "    threshold: 0.01\n",
      "  adabound:\n",
      "    betas: (0.9, 0.999)\n",
      "    final_lr: 0.1\n",
      "    gamma: 0.001\n",
      "scheduler:\n",
      "  epochs: 300\n",
      "  warmup:\n",
      "    type: none\n",
      "    epochs: 0\n",
      "    start_factor: 0.001\n",
      "    exponent: 4\n",
      "  type: multistep\n",
      "  milestones: [150, 225]\n",
      "  lr_decay: 0.1\n",
      "  lr_min_factor: 0.001\n",
      "  T0: 10\n",
      "  T_mul: 1.0\n",
      "validation:\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: False\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "augmentation:\n",
      "  use_random_crop: True\n",
      "  use_random_horizontal_flip: True\n",
      "  use_cutout: False\n",
      "  use_random_erasing: False\n",
      "  use_dual_cutout: False\n",
      "  use_mixup: False\n",
      "  use_ricap: False\n",
      "  use_cutmix: False\n",
      "  use_label_smoothing: False\n",
      "  random_crop:\n",
      "    padding: 4\n",
      "    fill: 0\n",
      "    padding_mode: constant\n",
      "  random_horizontal_flip:\n",
      "    prob: 0.5\n",
      "  cutout:\n",
      "    prob: 1.0\n",
      "    mask_size: 16\n",
      "    cut_inside: False\n",
      "    mask_color: 0\n",
      "    dual_cutout_alpha: 0.1\n",
      "  random_erasing:\n",
      "    prob: 0.5\n",
      "    area_ratio_range: [0.02, 0.4]\n",
      "    min_aspect_ratio: 0.3\n",
      "    max_attempt: 20\n",
      "  mixup:\n",
      "    alpha: 1.0\n",
      "  ricap:\n",
      "    beta: 0.3\n",
      "  cutmix:\n",
      "    alpha: 1.0\n",
      "  label_smoothing:\n",
      "    epsilon: 0.1\n",
      "tta:\n",
      "  use_resize: False\n",
      "  use_center_crop: False\n",
      "  resize: 256\n",
      "test:\n",
      "  checkpoint: ''\n",
      "  output_dir: ''\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    pin_memory: False\n",
      "\u001b[32m[2020-06-04 00:41:31] __main__ INFO: \u001b[0menv_info:\n",
      "  pytorch_version: 1.4.0\n",
      "  cuda_version: 10.1\n",
      "  cudnn_version: 7603\n",
      "Files already downloaded and verified\n",
      "\u001b[32m[2020-06-04 00:41:33] __main__ INFO: \u001b[0mMACs  : 2.75G\n",
      "\u001b[32m[2020-06-04 00:41:33] __main__ INFO: \u001b[0m#params: 17.56M\n",
      "Selected optimization level O0:  Pure FP32 training.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Traceback (most recent call last):\n",
      "  File \"w210-capstone/models/pytorch_imageclass/train.py\", line 436, in <module>\n",
      "    main()\n",
      "  File \"w210-capstone/models/pytorch_imageclass/train.py\", line 362, in main\n",
      "    opt_level=config.train.precision)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/apex/amp/frontend.py\", line 358, in initialize\n",
      "    return _initialize(models, optimizers, _amp_state.opt_properties, num_losses, cast_model_outputs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/apex/amp/_initialize.py\", line 171, in _initialize\n",
      "    check_params_fp32(models)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/apex/amp/_initialize.py\", line 93, in check_params_fp32\n",
      "    name, param.type()))\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/apex/amp/_amp_state.py\", line 32, in warn_or_err\n",
      "    raise RuntimeError(msg)\n",
      "RuntimeError: Found param conv.weight with type torch.FloatTensor, expected torch.cuda.FloatTensor.\n",
      "When using amp.initialize, you need to provide a model with parameters\n",
      "located on a CUDA device before passing it no matter what optimization level\n",
      "you chose. Use model.to('cuda') to use the default device.\n"
     ]
    }
   ],
   "source": [
    "# Must be conda_pytroch_p36 notebook\n",
    "!python w210-capstone/models/pytorch_imageclass/train.py --config w210-capstone/models/pytorch_imageclass/configs/cifar/resnext.yaml \\\n",
    "   model.resnext.depth 29 \\\n",
    "   model.resnext.cardinality 4 \\\n",
    "   train.batch_size 128 \\\n",
    "   train.base_lr 0.1 \\\n",
    "   train.output_dir experiments/resnext_29_4x64d/exp00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCRATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Data and model checkpoints directories\n",
    "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                        help='input batch size for training (default: 64)')\n",
    "    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                        help='input batch size for testing (default: 1000)')\n",
    "    parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
    "                        help='number of epochs to train (default: 10)')\n",
    "    parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n",
    "                        help='learning rate (default: 0.01)')\n",
    "    parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n",
    "                        help='SGD momentum (default: 0.5)')\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    parser.add_argument('--log-interval', type=int, default=100, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "    parser.add_argument('--backend', type=str, default=None,\n",
    "                        help='backend for distributed training (tcp, gloo on cpu and gloo, nccl on gpu)')\n",
    "\n",
    "    \n",
    "    parser.add_argument('--epochs', type=int, default=10)\n",
    "    parser.add_argument('--batch-size', type=int, default=128)\n",
    "    parser.add_argument('--learning-rate', type=float, default=0.1)\n",
    "    \n",
    "    parser.add_argument('--model_name', dest='model_name', type=str, help='model to train')\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'],\n",
    "                        help='directory to save trained model to')\n",
    "    parser.add_argument('--train_data', dest='train_data', type=str, help='dataset for model training') \n",
    "    parser.add_argument('--workers', dest='workers', type=int, help='number of V100 worker instances; 1 indicates non-distributed training')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Container environment\n",
    "    env = sagemaker_containers.training_env()\n",
    "    parser.add_argument('--hosts', type=list, default=env.hosts)\n",
    "    parser.add_argument('--current-host', type=str, default=env.current_host)\n",
    "    parser.add_argument('--model-dir', type=str, default=env.model_dir)\n",
    "    parser.add_argument('--data-dir', type=str,\n",
    "                        default=env.channel_input_dirs['training'])\n",
    "    parser.add_argument('--num-gpus', type=int, default=env.num_gpus)\n",
    "\n",
    "    train(parser.parse_args())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-3319f1f978a5>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-3319f1f978a5>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    echo $CUDA_PATH\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#torch.cuda.get_device_name(0)\n",
    "echo $CUDA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mCould not open requirements file: [Errno 2] No such file or directory: './w210-capstone/models/pytorch/requirements.txt'\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.2b1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r ./w210-capstone/models/pytorch_/requirements.txt\n",
    "\n",
    "# Do we need NVIDIA installed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance type = local\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "instance_type = 'local'\n",
    "\n",
    "if subprocess.call('nvidia-smi') == 0:\n",
    "    ## Set type to GPU if one is present\n",
    "    instance_type = 'local_gpu'\n",
    "    \n",
    "print(\"Instance type = \" + instance_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-03 20:54:56] __main__ INFO: \u001b[0mdevice: cpu\n",
      "cudnn:\n",
      "  benchmark: True\n",
      "  deterministic: False\n",
      "dataset:\n",
      "  name: CIFAR10\n",
      "  dataset_dir: ~/.torch/datasets/CIFAR10\n",
      "  image_size: 32\n",
      "  n_channels: 3\n",
      "  n_classes: 10\n",
      "model:\n",
      "  type: cifar\n",
      "  name: resnext\n",
      "  init_mode: kaiming_fan_out\n",
      "  vgg:\n",
      "    n_channels: [64, 128, 256, 512, 512]\n",
      "    n_layers: [2, 2, 3, 3, 3]\n",
      "    use_bn: True\n",
      "  resnet:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "  resnet_preact:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "  wrn:\n",
      "    depth: 28\n",
      "    initial_channels: 16\n",
      "    widening_factor: 10\n",
      "    drop_rate: 0.0\n",
      "  densenet:\n",
      "    depth: 100\n",
      "    n_blocks: [6, 12, 24, 16]\n",
      "    block_type: bottleneck\n",
      "    growth_rate: 12\n",
      "    drop_rate: 0.0\n",
      "    compression_rate: 0.5\n",
      "  pyramidnet:\n",
      "    depth: 272\n",
      "    n_blocks: [3, 24, 36, 3]\n",
      "    initial_channels: 16\n",
      "    block_type: bottleneck\n",
      "    alpha: 200\n",
      "  resnext:\n",
      "    depth: 29\n",
      "    n_blocks: [3, 4, 6, 3]\n",
      "    initial_channels: 64\n",
      "    cardinality: 4\n",
      "    base_channels: 64\n",
      "  shake_shake:\n",
      "    depth: 26\n",
      "    initial_channels: 96\n",
      "    shake_forward: True\n",
      "    shake_backward: True\n",
      "    shake_image: True\n",
      "  se_resnet_preact:\n",
      "    depth: 110\n",
      "    initial_channels: 16\n",
      "    se_reduction: 16\n",
      "    block_type: basic\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "train:\n",
      "  checkpoint: ''\n",
      "  resume: False\n",
      "  precision: O0\n",
      "  batch_size: 128\n",
      "  subdivision: 1\n",
      "  optimizer: sgd\n",
      "  base_lr: 0.1\n",
      "  momentum: 0.9\n",
      "  nesterov: True\n",
      "  weight_decay: 0.0005\n",
      "  no_weight_decay_on_bn: False\n",
      "  gradient_clip: 0\n",
      "  start_epoch: 0\n",
      "  seed: 0\n",
      "  val_first: True\n",
      "  val_period: 1\n",
      "  val_ratio: 0.1\n",
      "  use_test_as_val: False\n",
      "  output_dir: experiments/resnext_29_4x64d/exp00\n",
      "  log_period: 100\n",
      "  checkpoint_period: 100\n",
      "  use_tensorboard: True\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: True\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "  distributed: False\n",
      "  dist:\n",
      "    backend: nccl\n",
      "    init_method: env://\n",
      "    world_size: -1\n",
      "    node_rank: -1\n",
      "    local_rank: 0\n",
      "    use_sync_bn: False\n",
      "tensorboard:\n",
      "  train_images: False\n",
      "  val_images: False\n",
      "  model_params: False\n",
      "optim:\n",
      "  adam:\n",
      "    betas: (0.9, 0.999)\n",
      "  lars:\n",
      "    eps: 1e-09\n",
      "    threshold: 0.01\n",
      "  adabound:\n",
      "    betas: (0.9, 0.999)\n",
      "    final_lr: 0.1\n",
      "    gamma: 0.001\n",
      "scheduler:\n",
      "  epochs: 300\n",
      "  warmup:\n",
      "    type: none\n",
      "    epochs: 0\n",
      "    start_factor: 0.001\n",
      "    exponent: 4\n",
      "  type: multistep\n",
      "  milestones: [150, 225]\n",
      "  lr_decay: 0.1\n",
      "  lr_min_factor: 0.001\n",
      "  T0: 10\n",
      "  T_mul: 1.0\n",
      "validation:\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: False\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "augmentation:\n",
      "  use_random_crop: True\n",
      "  use_random_horizontal_flip: True\n",
      "  use_cutout: False\n",
      "  use_random_erasing: False\n",
      "  use_dual_cutout: False\n",
      "  use_mixup: False\n",
      "  use_ricap: False\n",
      "  use_cutmix: False\n",
      "  use_label_smoothing: False\n",
      "  random_crop:\n",
      "    padding: 4\n",
      "    fill: 0\n",
      "    padding_mode: constant\n",
      "  random_horizontal_flip:\n",
      "    prob: 0.5\n",
      "  cutout:\n",
      "    prob: 1.0\n",
      "    mask_size: 16\n",
      "    cut_inside: False\n",
      "    mask_color: 0\n",
      "    dual_cutout_alpha: 0.1\n",
      "  random_erasing:\n",
      "    prob: 0.5\n",
      "    area_ratio_range: [0.02, 0.4]\n",
      "    min_aspect_ratio: 0.3\n",
      "    max_attempt: 20\n",
      "  mixup:\n",
      "    alpha: 1.0\n",
      "  ricap:\n",
      "    beta: 0.3\n",
      "  cutmix:\n",
      "    alpha: 1.0\n",
      "  label_smoothing:\n",
      "    epsilon: 0.1\n",
      "tta:\n",
      "  use_resize: False\n",
      "  use_center_crop: False\n",
      "  resize: 256\n",
      "test:\n",
      "  checkpoint: ''\n",
      "  output_dir: ''\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    pin_memory: False\n",
      "\u001b[32m[2020-06-03 20:54:56] __main__ INFO: \u001b[0menv_info:\n",
      "  pytorch_version: 1.4.0\n",
      "  cuda_version: 10.1\n",
      "  cudnn_version: 7603\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /home/ec2-user/.torch/datasets/CIFAR10/cifar-10-python.tar.gz\n",
      "170500096it [00:02, 67641451.46it/s]                                            \n",
      "Extracting /home/ec2-user/.torch/datasets/CIFAR10/cifar-10-python.tar.gz to /home/ec2-user/.torch/datasets/CIFAR10\n",
      "\u001b[32m[2020-06-03 20:55:02] __main__ INFO: \u001b[0mMACs  : 2.75G\n",
      "\u001b[32m[2020-06-03 20:55:02] __main__ INFO: \u001b[0m#params: 17.56M\n",
      "Selected optimization level O0:  Pure FP32 training.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Traceback (most recent call last):\n",
      "  File \"w210-capstone/models/pytorch/train.py\", line 436, in <module>\n",
      "    main()\n",
      "  File \"w210-capstone/models/pytorch/train.py\", line 362, in main\n",
      "    opt_level=config.train.precision)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/apex/amp/frontend.py\", line 358, in initialize\n",
      "    return _initialize(models, optimizers, _amp_state.opt_properties, num_losses, cast_model_outputs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/apex/amp/_initialize.py\", line 171, in _initialize\n",
      "    check_params_fp32(models)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/apex/amp/_initialize.py\", line 93, in check_params_fp32\n",
      "    name, param.type()))\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/apex/amp/_amp_state.py\", line 32, in warn_or_err\n",
      "    raise RuntimeError(msg)\n",
      "RuntimeError: Found param conv.weight with type torch.FloatTensor, expected torch.cuda.FloatTensor.\n",
      "When using amp.initialize, you need to provide a model with parameters\n",
      "located on a CUDA device before passing it no matter what optimization level\n",
      "you chose. Use model.to('cuda') to use the default device.\n"
     ]
    }
   ],
   "source": [
    "# ResNext-29_4x64\n",
    "!python w210-capstone/models/pytorch/train.py --config w210-capstone/models/pytorch/configs/cifar/resnext.yaml \\\n",
    "   model.resnext.depth 29 \\\n",
    "   model.resnext.cardinality 4 \\\n",
    "   train.batch_size 128 \\\n",
    "   train.base_lr 0.1 \\\n",
    "   train.output_dir experiments/resnext_29_4x64d/exp00\n",
    "\n",
    "# ResNeXt-29 4x64d with a single GPU, batch size 32 and initial learning rate 0.025 \n",
    "# (8 GPUs, batch size 128 and initial learning rate 0.1 in paper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# initialize command line argument parsing\n",
    "if __name__=='__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--epochs', type=int, default=10)\n",
    "    parser.add_argument('--batch-size', type=int, default=128)\n",
    "    parser.add_argument('--learning-rate', type=float, default=0.1)\n",
    "    \n",
    "    parser.add_argument('--model_name', dest='model_name', type=str, help='model to train')\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'],\n",
    "                        help='directory to save trained model to')\n",
    "    parser.add_argument('--train_data', dest='train_data', type=str, help='dataset for model training') \n",
    "    parser.add_argument('--workers', dest='workers', type=int, help='number of V100 worker instances; 1 indicates non-distributed training')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Check quality of arguments\n",
    "    valid_args = {'datasets': ['cifar10', 'cifar10_10k', 'cifar10_30k', 'cifar102', 'cifar102_30k'],\n",
    "                  'model_names': ['wrn', 'shake_shake_32', 'shake_shake_96', 'shake_shake_112', 'pyramid_net']}\n",
    "\n",
    "    if args.train_data not in valid_args['datasets']:\n",
    "        parser.error('Invalid train_data parameter')\n",
    "\n",
    "    if args.model_name not in valid_args['model_names']:\n",
    "        parser.error('Invalid model_name parameter')\n",
    "    \n",
    "    if args.workers < 1:\n",
    "        parser.error('Invalid number of workers')\n",
    "\n",
    "    if not args.model_name:\n",
    "        parser.error('--model_name parameter is required')\n",
    "    elif not args.train_data:\n",
    "        parser.error('--train_data parameter is required')\n",
    "    elif not args.workers:\n",
    "        parser.error('--workers parameter is required')\n",
    "\n",
    "    # Set SageMaker session & execution role\n",
    "    bucket='sagemaker-may29'\n",
    "    sagemaker_session = sagemaker.Session(default_bucket=bucket)\n",
    "    role = get_execution_role()\n",
    "\n",
    "\n",
    "    # Set S3 path for data batches\n",
    "    inputs = 's3://' + bucket + '/sagemaker/{}'.format(args.train_data)\n",
    "\n",
    "    # Create the sagemaker estimator\n",
    "    pytorch_estimator = PyTorch('./w210-capstone/models/pytorch/train.py',\n",
    "                                train_instance_type='ml.p3.2xlarge',\n",
    "                                train_instance_count=1,\n",
    "                                framework_version='1.0.0',\n",
    "                                hyperparameters = {'epochs': 20, 'batch-size': 64, 'learning-rate': 0.1})\n",
    "\n",
    "    --config w210-capstone/models/pytorch/configs/cifar/resnext.yaml \\\n",
    "   model.resnext.depth 29 \\\n",
    "   model.resnext.cardinality 4 \\\n",
    "   train.batch_size 128 \\\n",
    "   train.base_lr 0.1 \\\n",
    "   train.output_dir experiments/resnext_29_4x64d/exp00\n",
    "    \n",
    "    \n",
    "    # Train the Model\n",
    "    pytorch_estimator.fit({'train': 's3://my-data-bucket/path/to/my/training/data',\n",
    "                           'test': 's3://my-data-bucket/path/to/my/test/data'})\n",
    "\n",
    "\n",
    "    # After training, save the model to `model_dir`\n",
    "    with open(os.path.join(args.model_dir, 'model.pth'), 'wb') as f:\n",
    "        torch.save(model.state_dict(), f)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
