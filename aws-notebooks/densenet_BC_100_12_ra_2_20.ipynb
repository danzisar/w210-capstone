{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DENSENET 211\n",
    "- Training Dataset: RandAugment, N=2, M=20\n",
    "- Sagemaker Notebook must be of type, conda_pytorch_p36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import numpy \n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 1)) (1.15.4)\n",
      "Requirement already satisfied: torch>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 2)) (1.4.0)\n",
      "Requirement already satisfied: torchvision in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (0.5.0)\n",
      "Requirement already satisfied: fvcore in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (0.1.1.post20200623)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 5)) (4.42.1)\n",
      "Requirement already satisfied: yacs in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 6)) (0.1.7)\n",
      "Requirement already satisfied: apex from git+https://github.com/NVIDIA/apex.git#egg=apex in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 7)) (0.1)\n",
      "Requirement already satisfied: termcolor in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 8)) (1.1.0)\n",
      "Requirement already satisfied: thop<0.0.31.post2004070130 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 9)) (0.0.31.post2001170342)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (1.11.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (5.4.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (5.3.1)\n",
      "Requirement already satisfied: tabulate in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (0.8.7)\n",
      "Requirement already satisfied: portalocker in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (1.7.0)\n",
      "\u001b[31mfastai 1.0.60 requires nvidia-ml-py3, which is not installed.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.2b1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Sagemaker Notebook must be of type, conda_pytorch_p36\n",
    "\n",
    "!pip install -r '/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorboard in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (2.2.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (0.14.1)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (3.12.2)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.15.4)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (0.31.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (2.24.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.6.0.post3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (3.2.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (0.9.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.11.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (0.4.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.18.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (47.3.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.30.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (1.23)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (2.6)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (2019.11.28)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard) (1.5.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (4.1.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (3.4.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (0.2.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard) (3.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.1.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard) (0.4.8)\n",
      "\u001b[31mfastai 1.0.60 requires nvidia-ml-py3, which is not installed.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.2b1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Need to add this to requirements.txt\n",
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-26 23:57:36] __main__ INFO: \u001b[0mdevice: cuda\n",
      "cudnn:\n",
      "  benchmark: True\n",
      "  deterministic: False\n",
      "dataset:\n",
      "  name: CIFAR10_RA_2_20\n",
      "  dataset_dir: ''\n",
      "  image_size: 32\n",
      "  n_channels: 3\n",
      "  n_classes: 10\n",
      "model:\n",
      "  type: cifar\n",
      "  name: densenet\n",
      "  init_mode: kaiming_fan_out\n",
      "  vgg:\n",
      "    n_channels: [64, 128, 256, 512, 512]\n",
      "    n_layers: [2, 2, 3, 3, 3]\n",
      "    use_bn: True\n",
      "  resnet:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "  resnet_preact:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "  wrn:\n",
      "    depth: 28\n",
      "    initial_channels: 16\n",
      "    widening_factor: 10\n",
      "    drop_rate: 0.0\n",
      "  densenet:\n",
      "    depth: 100\n",
      "    n_blocks: [6, 12, 24, 16]\n",
      "    block_type: bottleneck\n",
      "    growth_rate: 12\n",
      "    drop_rate: 0.0\n",
      "    compression_rate: 0.5\n",
      "  pyramidnet:\n",
      "    depth: 272\n",
      "    n_blocks: [3, 24, 36, 3]\n",
      "    initial_channels: 16\n",
      "    block_type: bottleneck\n",
      "    alpha: 200\n",
      "  resnext:\n",
      "    depth: 29\n",
      "    n_blocks: [3, 4, 6, 3]\n",
      "    initial_channels: 64\n",
      "    cardinality: 8\n",
      "    base_channels: 4\n",
      "  shake_shake:\n",
      "    depth: 26\n",
      "    initial_channels: 96\n",
      "    shake_forward: True\n",
      "    shake_backward: True\n",
      "    shake_image: True\n",
      "  se_resnet_preact:\n",
      "    depth: 110\n",
      "    initial_channels: 16\n",
      "    se_reduction: 16\n",
      "    block_type: basic\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "train:\n",
      "  checkpoint: ''\n",
      "  resume: False\n",
      "  precision: O0\n",
      "  batch_size: 64\n",
      "  subdivision: 1\n",
      "  optimizer: sgd\n",
      "  base_lr: 0.1\n",
      "  momentum: 0.9\n",
      "  nesterov: True\n",
      "  weight_decay: 0.0001\n",
      "  no_weight_decay_on_bn: False\n",
      "  gradient_clip: 0\n",
      "  start_epoch: 0\n",
      "  seed: 0\n",
      "  val_first: True\n",
      "  val_period: 1\n",
      "  val_ratio: 0.1\n",
      "  use_test_as_val: False\n",
      "  output_dir: /home/ec2-user/SageMaker/experiments/densenet_BC_100_12_ra_2_20/exp00\n",
      "  log_period: 100\n",
      "  checkpoint_period: 100\n",
      "  use_tensorboard: True\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: True\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "  distributed: False\n",
      "  dist:\n",
      "    backend: nccl\n",
      "    init_method: env://\n",
      "    world_size: -1\n",
      "    node_rank: -1\n",
      "    local_rank: 0\n",
      "    use_sync_bn: False\n",
      "tensorboard:\n",
      "  train_images: False\n",
      "  val_images: False\n",
      "  model_params: False\n",
      "optim:\n",
      "  adam:\n",
      "    betas: (0.9, 0.999)\n",
      "  lars:\n",
      "    eps: 1e-09\n",
      "    threshold: 0.01\n",
      "  adabound:\n",
      "    betas: (0.9, 0.999)\n",
      "    final_lr: 0.1\n",
      "    gamma: 0.001\n",
      "scheduler:\n",
      "  epochs: 400\n",
      "  warmup:\n",
      "    type: none\n",
      "    epochs: 0\n",
      "    start_factor: 0.001\n",
      "    exponent: 4\n",
      "  type: multistep\n",
      "  milestones: [150, 225]\n",
      "  lr_decay: 0.1\n",
      "  lr_min_factor: 0.001\n",
      "  T0: 10\n",
      "  T_mul: 1.0\n",
      "validation:\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: False\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "augmentation:\n",
      "  use_random_crop: True\n",
      "  use_random_horizontal_flip: True\n",
      "  use_cutout: False\n",
      "  use_random_erasing: False\n",
      "  use_dual_cutout: False\n",
      "  use_mixup: False\n",
      "  use_ricap: False\n",
      "  use_cutmix: False\n",
      "  use_label_smoothing: False\n",
      "  random_crop:\n",
      "    padding: 4\n",
      "    fill: 0\n",
      "    padding_mode: constant\n",
      "  random_horizontal_flip:\n",
      "    prob: 0.5\n",
      "  cutout:\n",
      "    prob: 1.0\n",
      "    mask_size: 16\n",
      "    cut_inside: False\n",
      "    mask_color: 0\n",
      "    dual_cutout_alpha: 0.1\n",
      "  random_erasing:\n",
      "    prob: 0.5\n",
      "    area_ratio_range: [0.02, 0.4]\n",
      "    min_aspect_ratio: 0.3\n",
      "    max_attempt: 20\n",
      "  mixup:\n",
      "    alpha: 1.0\n",
      "  ricap:\n",
      "    beta: 0.3\n",
      "  cutmix:\n",
      "    alpha: 1.0\n",
      "  label_smoothing:\n",
      "    epsilon: 0.1\n",
      "tta:\n",
      "  use_resize: False\n",
      "  use_center_crop: False\n",
      "  resize: 256\n",
      "test:\n",
      "  checkpoint: ''\n",
      "  output_dir: ''\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    pin_memory: False\n",
      "\u001b[32m[2020-06-26 23:57:36] __main__ INFO: \u001b[0menv_info:\n",
      "  pytorch_version: 1.4.0\n",
      "  cuda_version: 10.1\n",
      "  cudnn_version: 7603\n",
      "  num_gpus: 1\n",
      "  gpu_name: Tesla K80\n",
      "  gpu_capability: 3.7\n",
      "(50000, 32, 32, 3)\n",
      "\u001b[32m[2020-06-26 23:58:11] __main__ INFO: \u001b[0mMACs  : 296.49M\n",
      "\u001b[32m[2020-06-26 23:58:11] __main__ INFO: \u001b[0m#params: 769.16K\n",
      "Selected optimization level O0:  Pure FP32 training.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\",)\n",
      "\u001b[32m[2020-06-26 23:58:11] __main__ INFO: \u001b[0mVal 0\n",
      "\u001b[32m[2020-06-26 23:58:22] __main__ INFO: \u001b[0mEpoch 0 loss 80328776.6016 acc@1 0.1008 acc@5 0.5028\n",
      "\u001b[32m[2020-06-26 23:58:22] __main__ INFO: \u001b[0mElapsed 11.58\n",
      "\u001b[32m[2020-06-26 23:58:22] __main__ INFO: \u001b[0mTrain 1 0\n",
      "\u001b[32m[2020-06-26 23:58:56] __main__ INFO: \u001b[0mEpoch 1 Step 100/703 lr 0.100000 loss 2.5227 (2.9214) acc@1 0.0938 (0.1000) acc@5 0.3594 (0.5034)\n",
      "\u001b[32m[2020-06-26 23:59:27] __main__ INFO: \u001b[0mEpoch 1 Step 200/703 lr 0.100000 loss 2.2756 (2.6681) acc@1 0.1875 (0.1016) acc@5 0.5625 (0.5093)\n",
      "\u001b[32m[2020-06-26 23:59:59] __main__ INFO: \u001b[0mEpoch 1 Step 300/703 lr 0.100000 loss 2.3459 (2.5594) acc@1 0.1250 (0.1056) acc@5 0.6094 (0.5157)\n",
      "\u001b[32m[2020-06-27 00:00:30] __main__ INFO: \u001b[0mEpoch 1 Step 400/703 lr 0.100000 loss 2.2771 (2.5054) acc@1 0.2344 (0.1084) acc@5 0.6094 (0.5206)\n",
      "\u001b[32m[2020-06-27 00:01:02] __main__ INFO: \u001b[0mEpoch 1 Step 500/703 lr 0.100000 loss 2.4250 (2.4687) acc@1 0.0156 (0.1091) acc@5 0.4062 (0.5248)\n",
      "\u001b[32m[2020-06-27 00:01:33] __main__ INFO: \u001b[0mEpoch 1 Step 600/703 lr 0.100000 loss 2.2579 (2.4421) acc@1 0.2031 (0.1112) acc@5 0.6094 (0.5288)\n",
      "\u001b[32m[2020-06-27 00:02:05] __main__ INFO: \u001b[0mEpoch 1 Step 700/703 lr 0.100000 loss 2.3316 (2.4225) acc@1 0.0781 (0.1132) acc@5 0.4844 (0.5326)\n",
      "\u001b[32m[2020-06-27 00:02:06] __main__ INFO: \u001b[0mEpoch 1 Step 703/703 lr 0.100000 loss 2.2963 (2.4219) acc@1 0.0938 (0.1133) acc@5 0.5469 (0.5328)\n",
      "\u001b[32m[2020-06-27 00:02:06] __main__ INFO: \u001b[0mElapsed 223.87\n",
      "\u001b[32m[2020-06-27 00:02:06] __main__ INFO: \u001b[0mVal 1\n",
      "\u001b[32m[2020-06-27 00:02:14] __main__ INFO: \u001b[0mEpoch 1 loss 2.2731 acc@1 0.1254 acc@5 0.5720\n",
      "\u001b[32m[2020-06-27 00:02:14] __main__ INFO: \u001b[0mElapsed 7.77\n",
      "\u001b[32m[2020-06-27 00:02:14] __main__ INFO: \u001b[0mTrain 2 703\n",
      "\u001b[32m[2020-06-27 00:02:46] __main__ INFO: \u001b[0mEpoch 2 Step 100/703 lr 0.100000 loss 2.2764 (2.2857) acc@1 0.1250 (0.1300) acc@5 0.5312 (0.5664)\n",
      "\u001b[32m[2020-06-27 00:03:17] __main__ INFO: \u001b[0mEpoch 2 Step 200/703 lr 0.100000 loss 2.2376 (2.2804) acc@1 0.1406 (0.1336) acc@5 0.5938 (0.5784)\n",
      "\u001b[32m[2020-06-27 00:03:49] __main__ INFO: \u001b[0mEpoch 2 Step 300/703 lr 0.100000 loss 2.1562 (2.2769) acc@1 0.2188 (0.1342) acc@5 0.6094 (0.5817)\n",
      "\u001b[32m[2020-06-27 00:04:21] __main__ INFO: \u001b[0mEpoch 2 Step 400/703 lr 0.100000 loss 2.2654 (2.2716) acc@1 0.0781 (0.1357) acc@5 0.6094 (0.5852)\n",
      "\u001b[32m[2020-06-27 00:04:52] __main__ INFO: \u001b[0mEpoch 2 Step 500/703 lr 0.100000 loss 2.1717 (2.2657) acc@1 0.2031 (0.1376) acc@5 0.5938 (0.5873)\n",
      "\u001b[32m[2020-06-27 00:05:24] __main__ INFO: \u001b[0mEpoch 2 Step 600/703 lr 0.100000 loss 2.1383 (2.2594) acc@1 0.1406 (0.1404) acc@5 0.7031 (0.5926)\n",
      "\u001b[32m[2020-06-27 00:05:56] __main__ INFO: \u001b[0mEpoch 2 Step 700/703 lr 0.100000 loss 2.1988 (2.2556) acc@1 0.1875 (0.1424) acc@5 0.6094 (0.5948)\n",
      "\u001b[32m[2020-06-27 00:05:57] __main__ INFO: \u001b[0mEpoch 2 Step 703/703 lr 0.100000 loss 2.2239 (2.2554) acc@1 0.1250 (0.1425) acc@5 0.6406 (0.5950)\n",
      "\u001b[32m[2020-06-27 00:05:57] __main__ INFO: \u001b[0mElapsed 222.86\n",
      "\u001b[32m[2020-06-27 00:05:57] __main__ INFO: \u001b[0mVal 2\n",
      "\u001b[32m[2020-06-27 00:06:04] __main__ INFO: \u001b[0mEpoch 2 loss 2.2566 acc@1 0.1484 acc@5 0.6152\n",
      "\u001b[32m[2020-06-27 00:06:04] __main__ INFO: \u001b[0mElapsed 7.75\n",
      "\u001b[32m[2020-06-27 00:06:04] __main__ INFO: \u001b[0mTrain 3 1406\n",
      "\u001b[32m[2020-06-27 00:06:36] __main__ INFO: \u001b[0mEpoch 3 Step 100/703 lr 0.100000 loss 2.2293 (2.2142) acc@1 0.2344 (0.1613) acc@5 0.5469 (0.6295)\n",
      "\u001b[32m[2020-06-27 00:07:08] __main__ INFO: \u001b[0mEpoch 3 Step 200/703 lr 0.100000 loss 2.2048 (2.2118) acc@1 0.1562 (0.1599) acc@5 0.6250 (0.6321)\n",
      "\u001b[32m[2020-06-27 00:07:40] __main__ INFO: \u001b[0mEpoch 3 Step 300/703 lr 0.100000 loss 2.1826 (2.2066) acc@1 0.2500 (0.1608) acc@5 0.6250 (0.6371)\n",
      "\u001b[32m[2020-06-27 00:08:11] __main__ INFO: \u001b[0mEpoch 3 Step 400/703 lr 0.100000 loss 2.2435 (2.2011) acc@1 0.1719 (0.1617) acc@5 0.5781 (0.6392)\n",
      "\u001b[32m[2020-06-27 00:08:43] __main__ INFO: \u001b[0mEpoch 3 Step 500/703 lr 0.100000 loss 2.1608 (2.2013) acc@1 0.2031 (0.1609) acc@5 0.6406 (0.6378)\n",
      "\u001b[32m[2020-06-27 00:09:15] __main__ INFO: \u001b[0mEpoch 3 Step 600/703 lr 0.100000 loss 2.0782 (2.1979) acc@1 0.2188 (0.1620) acc@5 0.6406 (0.6385)\n",
      "\u001b[32m[2020-06-27 00:09:46] __main__ INFO: \u001b[0mEpoch 3 Step 700/703 lr 0.100000 loss 2.2082 (2.1950) acc@1 0.2031 (0.1637) acc@5 0.6719 (0.6383)\n",
      "\u001b[32m[2020-06-27 00:09:47] __main__ INFO: \u001b[0mEpoch 3 Step 703/703 lr 0.100000 loss 2.1733 (2.1949) acc@1 0.1719 (0.1638) acc@5 0.6094 (0.6381)\n",
      "\u001b[32m[2020-06-27 00:09:47] __main__ INFO: \u001b[0mElapsed 222.84\n",
      "\u001b[32m[2020-06-27 00:09:47] __main__ INFO: \u001b[0mVal 3\n",
      "\u001b[32m[2020-06-27 00:09:55] __main__ INFO: \u001b[0mEpoch 3 loss 2.3401 acc@1 0.1594 acc@5 0.6344\n",
      "\u001b[32m[2020-06-27 00:09:55] __main__ INFO: \u001b[0mElapsed 7.78\n",
      "\u001b[32m[2020-06-27 00:09:55] __main__ INFO: \u001b[0mTrain 4 2109\n",
      "\u001b[32m[2020-06-27 00:10:27] __main__ INFO: \u001b[0mEpoch 4 Step 100/703 lr 0.100000 loss 2.1126 (2.1581) acc@1 0.2656 (0.1698) acc@5 0.6406 (0.6561)\n",
      "\u001b[32m[2020-06-27 00:10:59] __main__ INFO: \u001b[0mEpoch 4 Step 200/703 lr 0.100000 loss 2.0820 (2.1596) acc@1 0.1875 (0.1741) acc@5 0.7344 (0.6541)\n",
      "\u001b[32m[2020-06-27 00:11:30] __main__ INFO: \u001b[0mEpoch 4 Step 300/703 lr 0.100000 loss 2.2014 (2.1593) acc@1 0.0938 (0.1757) acc@5 0.6875 (0.6571)\n",
      "\u001b[32m[2020-06-27 00:12:02] __main__ INFO: \u001b[0mEpoch 4 Step 400/703 lr 0.100000 loss 2.1183 (2.1580) acc@1 0.1406 (0.1772) acc@5 0.6719 (0.6575)\n",
      "\u001b[32m[2020-06-27 00:12:34] __main__ INFO: \u001b[0mEpoch 4 Step 500/703 lr 0.100000 loss 2.1195 (2.1576) acc@1 0.1719 (0.1764) acc@5 0.7031 (0.6580)\n",
      "\u001b[32m[2020-06-27 00:13:06] __main__ INFO: \u001b[0mEpoch 4 Step 600/703 lr 0.100000 loss 2.1881 (2.1549) acc@1 0.1875 (0.1783) acc@5 0.6094 (0.6586)\n",
      "\u001b[32m[2020-06-27 00:13:37] __main__ INFO: \u001b[0mEpoch 4 Step 700/703 lr 0.100000 loss 2.0736 (2.1521) acc@1 0.1875 (0.1813) acc@5 0.6719 (0.6578)\n",
      "\u001b[32m[2020-06-27 00:13:38] __main__ INFO: \u001b[0mEpoch 4 Step 703/703 lr 0.100000 loss 2.2075 (2.1522) acc@1 0.2344 (0.1814) acc@5 0.5938 (0.6576)\n",
      "\u001b[32m[2020-06-27 00:13:38] __main__ INFO: \u001b[0mElapsed 223.03\n",
      "\u001b[32m[2020-06-27 00:13:38] __main__ INFO: \u001b[0mVal 4\n",
      "\u001b[32m[2020-06-27 00:13:46] __main__ INFO: \u001b[0mEpoch 4 loss 2.1161 acc@1 0.1986 acc@5 0.6742\n",
      "\u001b[32m[2020-06-27 00:13:46] __main__ INFO: \u001b[0mElapsed 7.77\n",
      "\u001b[32m[2020-06-27 00:13:46] __main__ INFO: \u001b[0mTrain 5 2812\n",
      "\u001b[32m[2020-06-27 00:14:18] __main__ INFO: \u001b[0mEpoch 5 Step 100/703 lr 0.100000 loss 1.9763 (2.1182) acc@1 0.2656 (0.2027) acc@5 0.7969 (0.6761)\n",
      "\u001b[32m[2020-06-27 00:14:49] __main__ INFO: \u001b[0mEpoch 5 Step 200/703 lr 0.100000 loss 2.1083 (2.1142) acc@1 0.2500 (0.2048) acc@5 0.6875 (0.6776)\n",
      "\u001b[32m[2020-06-27 00:15:21] __main__ INFO: \u001b[0mEpoch 5 Step 300/703 lr 0.100000 loss 2.1560 (2.1187) acc@1 0.1719 (0.2035) acc@5 0.6406 (0.6760)\n",
      "\u001b[32m[2020-06-27 00:15:53] __main__ INFO: \u001b[0mEpoch 5 Step 400/703 lr 0.100000 loss 2.1953 (2.1174) acc@1 0.1406 (0.2042) acc@5 0.6875 (0.6738)\n",
      "\u001b[32m[2020-06-27 00:16:25] __main__ INFO: \u001b[0mEpoch 5 Step 500/703 lr 0.100000 loss 1.9209 (2.1130) acc@1 0.3125 (0.2052) acc@5 0.7500 (0.6753)\n",
      "\u001b[32m[2020-06-27 00:16:56] __main__ INFO: \u001b[0mEpoch 5 Step 600/703 lr 0.100000 loss 2.0961 (2.1127) acc@1 0.1875 (0.2063) acc@5 0.5938 (0.6757)\n",
      "\u001b[32m[2020-06-27 00:17:28] __main__ INFO: \u001b[0mEpoch 5 Step 700/703 lr 0.100000 loss 2.0608 (2.1110) acc@1 0.2500 (0.2067) acc@5 0.7188 (0.6783)\n",
      "\u001b[32m[2020-06-27 00:17:29] __main__ INFO: \u001b[0mEpoch 5 Step 703/703 lr 0.100000 loss 2.0538 (2.1112) acc@1 0.2031 (0.2066) acc@5 0.7344 (0.6784)\n",
      "\u001b[32m[2020-06-27 00:17:29] __main__ INFO: \u001b[0mElapsed 223.15\n",
      "\u001b[32m[2020-06-27 00:17:29] __main__ INFO: \u001b[0mVal 5\n",
      "\u001b[32m[2020-06-27 00:17:37] __main__ INFO: \u001b[0mEpoch 5 loss 2.1127 acc@1 0.2104 acc@5 0.6716\n",
      "\u001b[32m[2020-06-27 00:17:37] __main__ INFO: \u001b[0mElapsed 7.81\n",
      "\u001b[32m[2020-06-27 00:17:37] __main__ INFO: \u001b[0mTrain 6 3515\n",
      "\u001b[32m[2020-06-27 00:18:09] __main__ INFO: \u001b[0mEpoch 6 Step 100/703 lr 0.100000 loss 2.1304 (2.0934) acc@1 0.1250 (0.2136) acc@5 0.6719 (0.6725)\n",
      "\u001b[32m[2020-06-27 00:18:40] __main__ INFO: \u001b[0mEpoch 6 Step 200/703 lr 0.100000 loss 2.0583 (2.0892) acc@1 0.2500 (0.2147) acc@5 0.7344 (0.6788)\n",
      "\u001b[32m[2020-06-27 00:19:12] __main__ INFO: \u001b[0mEpoch 6 Step 300/703 lr 0.100000 loss 2.0254 (2.0832) acc@1 0.1875 (0.2174) acc@5 0.6875 (0.6801)\n",
      "\u001b[32m[2020-06-27 00:19:44] __main__ INFO: \u001b[0mEpoch 6 Step 400/703 lr 0.100000 loss 2.0861 (2.0816) acc@1 0.1719 (0.2182) acc@5 0.6875 (0.6827)\n",
      "\u001b[32m[2020-06-27 00:20:16] __main__ INFO: \u001b[0mEpoch 6 Step 500/703 lr 0.100000 loss 2.0253 (2.0825) acc@1 0.2188 (0.2171) acc@5 0.7188 (0.6835)\n",
      "\u001b[32m[2020-06-27 00:20:47] __main__ INFO: \u001b[0mEpoch 6 Step 600/703 lr 0.100000 loss 1.9921 (2.0778) acc@1 0.2500 (0.2196) acc@5 0.6562 (0.6858)\n",
      "\u001b[32m[2020-06-27 00:21:19] __main__ INFO: \u001b[0mEpoch 6 Step 700/703 lr 0.100000 loss 2.1144 (2.0747) acc@1 0.2188 (0.2213) acc@5 0.6875 (0.6861)\n",
      "\u001b[32m[2020-06-27 00:21:20] __main__ INFO: \u001b[0mEpoch 6 Step 703/703 lr 0.100000 loss 2.0916 (2.0747) acc@1 0.2188 (0.2213) acc@5 0.7031 (0.6862)\n",
      "\u001b[32m[2020-06-27 00:21:20] __main__ INFO: \u001b[0mElapsed 223.11\n",
      "\u001b[32m[2020-06-27 00:21:20] __main__ INFO: \u001b[0mVal 6\n",
      "\u001b[32m[2020-06-27 00:21:28] __main__ INFO: \u001b[0mEpoch 6 loss 2.0844 acc@1 0.2270 acc@5 0.6894\n",
      "\u001b[32m[2020-06-27 00:21:28] __main__ INFO: \u001b[0mElapsed 7.74\n",
      "\u001b[32m[2020-06-27 00:21:28] __main__ INFO: \u001b[0mTrain 7 4218\n",
      "\u001b[32m[2020-06-27 00:22:00] __main__ INFO: \u001b[0mEpoch 7 Step 100/703 lr 0.100000 loss 1.9496 (2.0374) acc@1 0.2969 (0.2455) acc@5 0.7031 (0.7044)\n",
      "\u001b[32m[2020-06-27 00:22:31] __main__ INFO: \u001b[0mEpoch 7 Step 200/703 lr 0.100000 loss 2.0726 (2.0390) acc@1 0.2188 (0.2408) acc@5 0.5625 (0.7037)\n",
      "\u001b[32m[2020-06-27 00:23:03] __main__ INFO: \u001b[0mEpoch 7 Step 300/703 lr 0.100000 loss 1.9344 (2.0353) acc@1 0.2031 (0.2419) acc@5 0.7188 (0.7034)\n",
      "\u001b[32m[2020-06-27 00:23:35] __main__ INFO: \u001b[0mEpoch 7 Step 400/703 lr 0.100000 loss 1.8744 (2.0330) acc@1 0.3281 (0.2431) acc@5 0.7344 (0.7037)\n",
      "\u001b[32m[2020-06-27 00:24:06] __main__ INFO: \u001b[0mEpoch 7 Step 500/703 lr 0.100000 loss 2.1017 (2.0310) acc@1 0.1719 (0.2433) acc@5 0.6875 (0.7032)\n",
      "\u001b[32m[2020-06-27 00:24:38] __main__ INFO: \u001b[0mEpoch 7 Step 600/703 lr 0.100000 loss 1.9497 (2.0254) acc@1 0.2344 (0.2456) acc@5 0.7031 (0.7047)\n",
      "\u001b[32m[2020-06-27 00:25:10] __main__ INFO: \u001b[0mEpoch 7 Step 700/703 lr 0.100000 loss 2.0156 (2.0239) acc@1 0.2500 (0.2456) acc@5 0.7812 (0.7058)\n",
      "\u001b[32m[2020-06-27 00:25:11] __main__ INFO: \u001b[0mEpoch 7 Step 703/703 lr 0.100000 loss 2.0762 (2.0237) acc@1 0.2344 (0.2457) acc@5 0.7031 (0.7059)\n",
      "\u001b[32m[2020-06-27 00:25:11] __main__ INFO: \u001b[0mElapsed 223.01\n",
      "\u001b[32m[2020-06-27 00:25:11] __main__ INFO: \u001b[0mVal 7\n",
      "\u001b[32m[2020-06-27 00:25:19] __main__ INFO: \u001b[0mEpoch 7 loss 1.9862 acc@1 0.2616 acc@5 0.7168\n",
      "\u001b[32m[2020-06-27 00:25:19] __main__ INFO: \u001b[0mElapsed 7.77\n",
      "\u001b[32m[2020-06-27 00:25:19] __main__ INFO: \u001b[0mTrain 8 4921\n",
      "\u001b[32m[2020-06-27 00:25:50] __main__ INFO: \u001b[0mEpoch 8 Step 100/703 lr 0.100000 loss 2.2551 (1.9864) acc@1 0.1406 (0.2523) acc@5 0.7188 (0.7164)\n",
      "\u001b[32m[2020-06-27 00:26:22] __main__ INFO: \u001b[0mEpoch 8 Step 200/703 lr 0.100000 loss 1.8598 (1.9836) acc@1 0.2031 (0.2526) acc@5 0.7812 (0.7136)\n",
      "\u001b[32m[2020-06-27 00:26:54] __main__ INFO: \u001b[0mEpoch 8 Step 300/703 lr 0.100000 loss 1.9274 (1.9815) acc@1 0.2344 (0.2552) acc@5 0.7656 (0.7125)\n",
      "\u001b[32m[2020-06-27 00:27:26] __main__ INFO: \u001b[0mEpoch 8 Step 400/703 lr 0.100000 loss 1.8206 (1.9767) acc@1 0.3281 (0.2593) acc@5 0.8281 (0.7143)\n",
      "\u001b[32m[2020-06-27 00:27:57] __main__ INFO: \u001b[0mEpoch 8 Step 500/703 lr 0.100000 loss 1.9051 (1.9732) acc@1 0.3281 (0.2620) acc@5 0.7812 (0.7150)\n",
      "\u001b[32m[2020-06-27 00:28:29] __main__ INFO: \u001b[0mEpoch 8 Step 600/703 lr 0.100000 loss 1.9987 (1.9696) acc@1 0.2500 (0.2646) acc@5 0.7344 (0.7159)\n",
      "\u001b[32m[2020-06-27 00:29:01] __main__ INFO: \u001b[0mEpoch 8 Step 700/703 lr 0.100000 loss 1.9105 (1.9655) acc@1 0.2812 (0.2665) acc@5 0.7031 (0.7169)\n",
      "\u001b[32m[2020-06-27 00:29:02] __main__ INFO: \u001b[0mEpoch 8 Step 703/703 lr 0.100000 loss 1.9504 (1.9655) acc@1 0.2969 (0.2665) acc@5 0.7656 (0.7169)\n",
      "\u001b[32m[2020-06-27 00:29:02] __main__ INFO: \u001b[0mElapsed 223.28\n",
      "\u001b[32m[2020-06-27 00:29:02] __main__ INFO: \u001b[0mVal 8\n",
      "\u001b[32m[2020-06-27 00:29:10] __main__ INFO: \u001b[0mEpoch 8 loss 1.9777 acc@1 0.2672 acc@5 0.7288\n",
      "\u001b[32m[2020-06-27 00:29:10] __main__ INFO: \u001b[0mElapsed 7.82\n",
      "\u001b[32m[2020-06-27 00:29:10] __main__ INFO: \u001b[0mTrain 9 5624\n",
      "\u001b[32m[2020-06-27 00:29:42] __main__ INFO: \u001b[0mEpoch 9 Step 100/703 lr 0.100000 loss 1.8283 (1.9271) acc@1 0.3125 (0.2842) acc@5 0.8438 (0.7273)\n",
      "\u001b[32m[2020-06-27 00:30:13] __main__ INFO: \u001b[0mEpoch 9 Step 200/703 lr 0.100000 loss 1.8439 (1.9232) acc@1 0.2969 (0.2829) acc@5 0.6875 (0.7288)\n",
      "\u001b[32m[2020-06-27 00:30:45] __main__ INFO: \u001b[0mEpoch 9 Step 300/703 lr 0.100000 loss 1.8647 (1.9205) acc@1 0.2656 (0.2860) acc@5 0.7812 (0.7302)\n",
      "\u001b[32m[2020-06-27 00:31:17] __main__ INFO: \u001b[0mEpoch 9 Step 400/703 lr 0.100000 loss 1.8849 (1.9144) acc@1 0.3438 (0.2866) acc@5 0.7344 (0.7318)\n",
      "\u001b[32m[2020-06-27 00:31:48] __main__ INFO: \u001b[0mEpoch 9 Step 500/703 lr 0.100000 loss 1.8207 (1.9112) acc@1 0.3750 (0.2881) acc@5 0.6719 (0.7316)\n",
      "\u001b[32m[2020-06-27 00:32:20] __main__ INFO: \u001b[0mEpoch 9 Step 600/703 lr 0.100000 loss 1.6852 (1.9091) acc@1 0.3125 (0.2871) acc@5 0.8281 (0.7322)\n",
      "\u001b[32m[2020-06-27 00:32:52] __main__ INFO: \u001b[0mEpoch 9 Step 700/703 lr 0.100000 loss 1.7920 (1.9077) acc@1 0.3438 (0.2880) acc@5 0.7656 (0.7315)\n",
      "\u001b[32m[2020-06-27 00:32:53] __main__ INFO: \u001b[0mEpoch 9 Step 703/703 lr 0.100000 loss 1.8168 (1.9076) acc@1 0.3125 (0.2880) acc@5 0.7344 (0.7314)\n",
      "\u001b[32m[2020-06-27 00:32:53] __main__ INFO: \u001b[0mElapsed 223.36\n",
      "\u001b[32m[2020-06-27 00:32:53] __main__ INFO: \u001b[0mVal 9\n",
      "\u001b[32m[2020-06-27 00:33:01] __main__ INFO: \u001b[0mEpoch 9 loss 1.8883 acc@1 0.2950 acc@5 0.7380\n",
      "\u001b[32m[2020-06-27 00:33:01] __main__ INFO: \u001b[0mElapsed 7.81\n",
      "\u001b[32m[2020-06-27 00:33:01] __main__ INFO: \u001b[0mTrain 10 6327\n",
      "\u001b[32m[2020-06-27 00:33:33] __main__ INFO: \u001b[0mEpoch 10 Step 100/703 lr 0.100000 loss 1.7500 (1.8730) acc@1 0.3750 (0.3064) acc@5 0.7188 (0.7353)\n",
      "\u001b[32m[2020-06-27 00:34:04] __main__ INFO: \u001b[0mEpoch 10 Step 200/703 lr 0.100000 loss 1.8772 (1.8742) acc@1 0.3125 (0.3042) acc@5 0.7188 (0.7351)\n",
      "\u001b[32m[2020-06-27 00:34:36] __main__ INFO: \u001b[0mEpoch 10 Step 300/703 lr 0.100000 loss 1.9692 (1.8671) acc@1 0.2500 (0.3055) acc@5 0.7500 (0.7384)\n",
      "\u001b[32m[2020-06-27 00:35:08] __main__ INFO: \u001b[0mEpoch 10 Step 400/703 lr 0.100000 loss 1.7909 (1.8667) acc@1 0.3750 (0.3061) acc@5 0.8906 (0.7364)\n",
      "\u001b[32m[2020-06-27 00:35:40] __main__ INFO: \u001b[0mEpoch 10 Step 500/703 lr 0.100000 loss 1.7554 (1.8633) acc@1 0.3438 (0.3068) acc@5 0.8594 (0.7370)\n",
      "\u001b[32m[2020-06-27 00:36:12] __main__ INFO: \u001b[0mEpoch 10 Step 600/703 lr 0.100000 loss 1.8362 (1.8588) acc@1 0.2500 (0.3085) acc@5 0.7500 (0.7388)\n",
      "\u001b[32m[2020-06-27 00:36:43] __main__ INFO: \u001b[0mEpoch 10 Step 700/703 lr 0.100000 loss 1.6871 (1.8540) acc@1 0.3438 (0.3117) acc@5 0.7969 (0.7411)\n",
      "\u001b[32m[2020-06-27 00:36:44] __main__ INFO: \u001b[0mEpoch 10 Step 703/703 lr 0.100000 loss 1.5729 (1.8537) acc@1 0.4844 (0.3119) acc@5 0.8281 (0.7412)\n",
      "\u001b[32m[2020-06-27 00:36:44] __main__ INFO: \u001b[0mElapsed 223.59\n",
      "\u001b[32m[2020-06-27 00:36:44] __main__ INFO: \u001b[0mVal 10\n",
      "\u001b[32m[2020-06-27 00:36:52] __main__ INFO: \u001b[0mEpoch 10 loss 1.9549 acc@1 0.3046 acc@5 0.7356\n",
      "\u001b[32m[2020-06-27 00:36:52] __main__ INFO: \u001b[0mElapsed 7.75\n",
      "\u001b[32m[2020-06-27 00:36:52] __main__ INFO: \u001b[0mTrain 11 7030\n",
      "\u001b[32m[2020-06-27 00:37:24] __main__ INFO: \u001b[0mEpoch 11 Step 100/703 lr 0.100000 loss 1.8807 (1.8193) acc@1 0.2969 (0.3275) acc@5 0.7812 (0.7472)\n",
      "\u001b[32m[2020-06-27 00:37:56] __main__ INFO: \u001b[0mEpoch 11 Step 200/703 lr 0.100000 loss 1.9873 (1.8197) acc@1 0.2969 (0.3259) acc@5 0.7031 (0.7510)\n",
      "\u001b[32m[2020-06-27 00:38:27] __main__ INFO: \u001b[0mEpoch 11 Step 300/703 lr 0.100000 loss 1.7450 (1.8198) acc@1 0.2656 (0.3260) acc@5 0.6875 (0.7500)\n",
      "\u001b[32m[2020-06-27 00:38:59] __main__ INFO: \u001b[0mEpoch 11 Step 400/703 lr 0.100000 loss 1.7364 (1.8202) acc@1 0.3281 (0.3252) acc@5 0.7500 (0.7489)\n",
      "\u001b[32m[2020-06-27 00:39:31] __main__ INFO: \u001b[0mEpoch 11 Step 500/703 lr 0.100000 loss 1.7758 (1.8155) acc@1 0.3750 (0.3283) acc@5 0.7031 (0.7521)\n",
      "\u001b[32m[2020-06-27 00:40:03] __main__ INFO: \u001b[0mEpoch 11 Step 600/703 lr 0.100000 loss 1.9719 (1.8133) acc@1 0.2031 (0.3292) acc@5 0.7500 (0.7528)\n",
      "\u001b[32m[2020-06-27 00:40:34] __main__ INFO: \u001b[0mEpoch 11 Step 700/703 lr 0.100000 loss 1.5551 (1.8137) acc@1 0.4688 (0.3291) acc@5 0.8594 (0.7514)\n",
      "\u001b[32m[2020-06-27 00:40:35] __main__ INFO: \u001b[0mEpoch 11 Step 703/703 lr 0.100000 loss 1.6415 (1.8132) acc@1 0.4062 (0.3291) acc@5 0.7188 (0.7515)\n",
      "\u001b[32m[2020-06-27 00:40:35] __main__ INFO: \u001b[0mElapsed 223.30\n",
      "\u001b[32m[2020-06-27 00:40:35] __main__ INFO: \u001b[0mVal 11\n",
      "\u001b[32m[2020-06-27 00:40:43] __main__ INFO: \u001b[0mEpoch 11 loss 1.8045 acc@1 0.3302 acc@5 0.7420\n",
      "\u001b[32m[2020-06-27 00:40:43] __main__ INFO: \u001b[0mElapsed 7.78\n",
      "\u001b[32m[2020-06-27 00:40:43] __main__ INFO: \u001b[0mTrain 12 7733\n",
      "\u001b[32m[2020-06-27 00:41:15] __main__ INFO: \u001b[0mEpoch 12 Step 100/703 lr 0.100000 loss 2.0551 (1.7723) acc@1 0.1875 (0.3408) acc@5 0.5625 (0.7570)\n",
      "\u001b[32m[2020-06-27 00:41:47] __main__ INFO: \u001b[0mEpoch 12 Step 200/703 lr 0.100000 loss 1.8060 (1.7811) acc@1 0.2969 (0.3383) acc@5 0.8281 (0.7534)\n",
      "\u001b[32m[2020-06-27 00:42:19] __main__ INFO: \u001b[0mEpoch 12 Step 300/703 lr 0.100000 loss 1.8160 (1.7815) acc@1 0.3750 (0.3383) acc@5 0.7344 (0.7545)\n",
      "\u001b[32m[2020-06-27 00:42:50] __main__ INFO: \u001b[0mEpoch 12 Step 400/703 lr 0.100000 loss 1.6382 (1.7835) acc@1 0.4219 (0.3364) acc@5 0.8281 (0.7550)\n",
      "\u001b[32m[2020-06-27 00:43:22] __main__ INFO: \u001b[0mEpoch 12 Step 500/703 lr 0.100000 loss 1.8557 (1.7829) acc@1 0.2812 (0.3369) acc@5 0.7656 (0.7549)\n",
      "\u001b[32m[2020-06-27 00:43:54] __main__ INFO: \u001b[0mEpoch 12 Step 600/703 lr 0.100000 loss 1.5041 (1.7787) acc@1 0.4375 (0.3399) acc@5 0.7969 (0.7560)\n",
      "\u001b[32m[2020-06-27 00:44:26] __main__ INFO: \u001b[0mEpoch 12 Step 700/703 lr 0.100000 loss 1.7592 (1.7772) acc@1 0.3125 (0.3412) acc@5 0.7500 (0.7560)\n",
      "\u001b[32m[2020-06-27 00:44:27] __main__ INFO: \u001b[0mEpoch 12 Step 703/703 lr 0.100000 loss 1.7374 (1.7771) acc@1 0.3594 (0.3412) acc@5 0.7031 (0.7561)\n",
      "\u001b[32m[2020-06-27 00:44:27] __main__ INFO: \u001b[0mElapsed 223.52\n",
      "\u001b[32m[2020-06-27 00:44:27] __main__ INFO: \u001b[0mVal 12\n",
      "\u001b[32m[2020-06-27 00:44:35] __main__ INFO: \u001b[0mEpoch 12 loss 1.7793 acc@1 0.3406 acc@5 0.7638\n",
      "\u001b[32m[2020-06-27 00:44:35] __main__ INFO: \u001b[0mElapsed 7.78\n",
      "\u001b[32m[2020-06-27 00:44:35] __main__ INFO: \u001b[0mTrain 13 8436\n",
      "\u001b[32m[2020-06-27 00:45:06] __main__ INFO: \u001b[0mEpoch 13 Step 100/703 lr 0.100000 loss 1.6206 (1.7348) acc@1 0.4062 (0.3536) acc@5 0.7656 (0.7588)\n",
      "\u001b[32m[2020-06-27 00:45:38] __main__ INFO: \u001b[0mEpoch 13 Step 200/703 lr 0.100000 loss 1.7845 (1.7424) acc@1 0.3281 (0.3513) acc@5 0.7656 (0.7605)\n",
      "\u001b[32m[2020-06-27 00:46:10] __main__ INFO: \u001b[0mEpoch 13 Step 300/703 lr 0.100000 loss 1.7759 (1.7470) acc@1 0.3438 (0.3495) acc@5 0.7812 (0.7588)\n",
      "\u001b[32m[2020-06-27 00:46:42] __main__ INFO: \u001b[0mEpoch 13 Step 400/703 lr 0.100000 loss 1.5599 (1.7490) acc@1 0.4844 (0.3495) acc@5 0.8125 (0.7577)\n",
      "\u001b[32m[2020-06-27 00:47:14] __main__ INFO: \u001b[0mEpoch 13 Step 500/703 lr 0.100000 loss 1.6649 (1.7498) acc@1 0.4062 (0.3487) acc@5 0.7812 (0.7577)\n",
      "\u001b[32m[2020-06-27 00:47:45] __main__ INFO: \u001b[0mEpoch 13 Step 600/703 lr 0.100000 loss 1.5418 (1.7523) acc@1 0.4375 (0.3479) acc@5 0.7812 (0.7571)\n",
      "\u001b[32m[2020-06-27 00:48:17] __main__ INFO: \u001b[0mEpoch 13 Step 700/703 lr 0.100000 loss 1.7988 (1.7486) acc@1 0.3438 (0.3503) acc@5 0.7812 (0.7587)\n",
      "\u001b[32m[2020-06-27 00:48:18] __main__ INFO: \u001b[0mEpoch 13 Step 703/703 lr 0.100000 loss 1.6846 (1.7487) acc@1 0.3750 (0.3504) acc@5 0.7969 (0.7588)\n",
      "\u001b[32m[2020-06-27 00:48:18] __main__ INFO: \u001b[0mElapsed 223.47\n",
      "\u001b[32m[2020-06-27 00:48:18] __main__ INFO: \u001b[0mVal 13\n",
      "\u001b[32m[2020-06-27 00:48:26] __main__ INFO: \u001b[0mEpoch 13 loss 1.7741 acc@1 0.3438 acc@5 0.7646\n",
      "\u001b[32m[2020-06-27 00:48:26] __main__ INFO: \u001b[0mElapsed 7.79\n",
      "\u001b[32m[2020-06-27 00:48:26] __main__ INFO: \u001b[0mTrain 14 9139\n",
      "\u001b[32m[2020-06-27 00:48:58] __main__ INFO: \u001b[0mEpoch 14 Step 100/703 lr 0.100000 loss 1.7571 (1.7242) acc@1 0.3281 (0.3591) acc@5 0.7812 (0.7619)\n",
      "\u001b[32m[2020-06-27 00:49:30] __main__ INFO: \u001b[0mEpoch 14 Step 200/703 lr 0.100000 loss 1.8247 (1.7227) acc@1 0.2656 (0.3587) acc@5 0.7500 (0.7673)\n",
      "\u001b[32m[2020-06-27 00:50:01] __main__ INFO: \u001b[0mEpoch 14 Step 300/703 lr 0.100000 loss 1.7442 (1.7254) acc@1 0.3438 (0.3598) acc@5 0.7812 (0.7673)\n",
      "\u001b[32m[2020-06-27 00:50:33] __main__ INFO: \u001b[0mEpoch 14 Step 400/703 lr 0.100000 loss 1.7778 (1.7250) acc@1 0.2969 (0.3604) acc@5 0.6562 (0.7650)\n",
      "\u001b[32m[2020-06-27 00:51:05] __main__ INFO: \u001b[0mEpoch 14 Step 500/703 lr 0.100000 loss 1.7722 (1.7275) acc@1 0.3281 (0.3589) acc@5 0.7031 (0.7640)\n",
      "\u001b[32m[2020-06-27 00:51:36] __main__ INFO: \u001b[0mEpoch 14 Step 600/703 lr 0.100000 loss 1.8177 (1.7254) acc@1 0.3125 (0.3592) acc@5 0.7500 (0.7637)\n",
      "\u001b[32m[2020-06-27 00:52:08] __main__ INFO: \u001b[0mEpoch 14 Step 700/703 lr 0.100000 loss 1.9783 (1.7231) acc@1 0.2188 (0.3588) acc@5 0.8125 (0.7650)\n",
      "\u001b[32m[2020-06-27 00:52:09] __main__ INFO: \u001b[0mEpoch 14 Step 703/703 lr 0.100000 loss 1.5607 (1.7228) acc@1 0.4219 (0.3589) acc@5 0.8438 (0.7650)\n",
      "\u001b[32m[2020-06-27 00:52:09] __main__ INFO: \u001b[0mElapsed 223.28\n",
      "\u001b[32m[2020-06-27 00:52:09] __main__ INFO: \u001b[0mVal 14\n",
      "\u001b[32m[2020-06-27 00:52:17] __main__ INFO: \u001b[0mEpoch 14 loss 1.7955 acc@1 0.3508 acc@5 0.7590\n",
      "\u001b[32m[2020-06-27 00:52:17] __main__ INFO: \u001b[0mElapsed 7.77\n",
      "\u001b[32m[2020-06-27 00:52:17] __main__ INFO: \u001b[0mTrain 15 9842\n",
      "\u001b[32m[2020-06-27 00:52:49] __main__ INFO: \u001b[0mEpoch 15 Step 100/703 lr 0.100000 loss 1.6022 (1.6984) acc@1 0.3750 (0.3653) acc@5 0.7500 (0.7714)\n",
      "\u001b[32m[2020-06-27 00:53:20] __main__ INFO: \u001b[0mEpoch 15 Step 200/703 lr 0.100000 loss 1.8452 (1.7039) acc@1 0.2969 (0.3678) acc@5 0.7969 (0.7670)\n",
      "\u001b[32m[2020-06-27 00:53:52] __main__ INFO: \u001b[0mEpoch 15 Step 300/703 lr 0.100000 loss 1.8800 (1.7029) acc@1 0.3594 (0.3686) acc@5 0.7344 (0.7672)\n",
      "\u001b[32m[2020-06-27 00:54:24] __main__ INFO: \u001b[0mEpoch 15 Step 400/703 lr 0.100000 loss 1.4719 (1.7012) acc@1 0.4844 (0.3696) acc@5 0.8438 (0.7689)\n",
      "\u001b[32m[2020-06-27 00:54:56] __main__ INFO: \u001b[0mEpoch 15 Step 500/703 lr 0.100000 loss 1.7413 (1.7066) acc@1 0.3906 (0.3693) acc@5 0.7500 (0.7664)\n",
      "\u001b[32m[2020-06-27 00:55:28] __main__ INFO: \u001b[0mEpoch 15 Step 600/703 lr 0.100000 loss 1.7909 (1.7088) acc@1 0.4062 (0.3682) acc@5 0.6875 (0.7658)\n",
      "\u001b[32m[2020-06-27 00:55:59] __main__ INFO: \u001b[0mEpoch 15 Step 700/703 lr 0.100000 loss 1.8214 (1.7069) acc@1 0.3438 (0.3688) acc@5 0.7812 (0.7651)\n",
      "\u001b[32m[2020-06-27 00:56:00] __main__ INFO: \u001b[0mEpoch 15 Step 703/703 lr 0.100000 loss 1.4945 (1.7066) acc@1 0.4375 (0.3688) acc@5 0.8281 (0.7651)\n",
      "\u001b[32m[2020-06-27 00:56:00] __main__ INFO: \u001b[0mElapsed 223.59\n",
      "\u001b[32m[2020-06-27 00:56:00] __main__ INFO: \u001b[0mVal 15\n",
      "\u001b[32m[2020-06-27 00:56:08] __main__ INFO: \u001b[0mEpoch 15 loss 1.7397 acc@1 0.3644 acc@5 0.7562\n",
      "\u001b[32m[2020-06-27 00:56:08] __main__ INFO: \u001b[0mElapsed 7.74\n",
      "\u001b[32m[2020-06-27 00:56:08] __main__ INFO: \u001b[0mTrain 16 10545\n",
      "\u001b[32m[2020-06-27 00:56:40] __main__ INFO: \u001b[0mEpoch 16 Step 100/703 lr 0.100000 loss 1.5843 (1.6562) acc@1 0.4688 (0.3916) acc@5 0.8594 (0.7775)\n",
      "\u001b[32m[2020-06-27 00:57:12] __main__ INFO: \u001b[0mEpoch 16 Step 200/703 lr 0.100000 loss 1.7693 (1.6623) acc@1 0.3750 (0.3864) acc@5 0.7031 (0.7726)\n",
      "\u001b[32m[2020-06-27 00:57:43] __main__ INFO: \u001b[0mEpoch 16 Step 300/703 lr 0.100000 loss 1.5929 (1.6710) acc@1 0.4219 (0.3829) acc@5 0.7969 (0.7709)\n",
      "\u001b[32m[2020-06-27 00:58:15] __main__ INFO: \u001b[0mEpoch 16 Step 400/703 lr 0.100000 loss 1.7582 (1.6814) acc@1 0.3594 (0.3782) acc@5 0.7500 (0.7702)\n",
      "\u001b[32m[2020-06-27 00:58:47] __main__ INFO: \u001b[0mEpoch 16 Step 500/703 lr 0.100000 loss 1.7070 (1.6811) acc@1 0.4219 (0.3776) acc@5 0.7656 (0.7695)\n",
      "\u001b[32m[2020-06-27 00:59:18] __main__ INFO: \u001b[0mEpoch 16 Step 600/703 lr 0.100000 loss 1.7422 (1.6823) acc@1 0.3438 (0.3769) acc@5 0.7656 (0.7695)\n",
      "\u001b[32m[2020-06-27 00:59:50] __main__ INFO: \u001b[0mEpoch 16 Step 700/703 lr 0.100000 loss 1.9514 (1.6863) acc@1 0.2656 (0.3752) acc@5 0.7500 (0.7693)\n",
      "\u001b[32m[2020-06-27 00:59:51] __main__ INFO: \u001b[0mEpoch 16 Step 703/703 lr 0.100000 loss 1.8628 (1.6864) acc@1 0.3438 (0.3752) acc@5 0.7812 (0.7694)\n",
      "\u001b[32m[2020-06-27 00:59:51] __main__ INFO: \u001b[0mElapsed 222.90\n",
      "\u001b[32m[2020-06-27 00:59:51] __main__ INFO: \u001b[0mVal 16\n",
      "\u001b[32m[2020-06-27 00:59:59] __main__ INFO: \u001b[0mEpoch 16 loss 1.6894 acc@1 0.3818 acc@5 0.7722\n",
      "\u001b[32m[2020-06-27 00:59:59] __main__ INFO: \u001b[0mElapsed 7.79\n",
      "\u001b[32m[2020-06-27 00:59:59] __main__ INFO: \u001b[0mTrain 17 11248\n",
      "\u001b[32m[2020-06-27 01:00:31] __main__ INFO: \u001b[0mEpoch 17 Step 100/703 lr 0.100000 loss 1.6518 (1.6735) acc@1 0.3906 (0.3816) acc@5 0.7656 (0.7628)\n",
      "\u001b[32m[2020-06-27 01:01:02] __main__ INFO: \u001b[0mEpoch 17 Step 200/703 lr 0.100000 loss 1.8664 (1.6631) acc@1 0.3594 (0.3856) acc@5 0.7500 (0.7674)\n",
      "\u001b[32m[2020-06-27 01:01:34] __main__ INFO: \u001b[0mEpoch 17 Step 300/703 lr 0.100000 loss 1.5481 (1.6686) acc@1 0.4375 (0.3833) acc@5 0.7656 (0.7683)\n",
      "\u001b[32m[2020-06-27 01:02:06] __main__ INFO: \u001b[0mEpoch 17 Step 400/703 lr 0.100000 loss 1.6002 (1.6629) acc@1 0.4062 (0.3849) acc@5 0.8281 (0.7720)\n",
      "\u001b[32m[2020-06-27 01:02:38] __main__ INFO: \u001b[0mEpoch 17 Step 500/703 lr 0.100000 loss 1.5067 (1.6619) acc@1 0.4688 (0.3856) acc@5 0.8281 (0.7708)\n",
      "\u001b[32m[2020-06-27 01:03:09] __main__ INFO: \u001b[0mEpoch 17 Step 600/703 lr 0.100000 loss 1.5133 (1.6656) acc@1 0.4219 (0.3836) acc@5 0.8438 (0.7705)\n",
      "\u001b[32m[2020-06-27 01:03:41] __main__ INFO: \u001b[0mEpoch 17 Step 700/703 lr 0.100000 loss 1.7775 (1.6676) acc@1 0.3750 (0.3828) acc@5 0.7188 (0.7702)\n",
      "\u001b[32m[2020-06-27 01:03:42] __main__ INFO: \u001b[0mEpoch 17 Step 703/703 lr 0.100000 loss 1.8652 (1.6676) acc@1 0.2969 (0.3828) acc@5 0.7656 (0.7703)\n",
      "\u001b[32m[2020-06-27 01:03:42] __main__ INFO: \u001b[0mElapsed 223.29\n",
      "\u001b[32m[2020-06-27 01:03:42] __main__ INFO: \u001b[0mVal 17\n",
      "\u001b[32m[2020-06-27 01:03:50] __main__ INFO: \u001b[0mEpoch 17 loss 1.6804 acc@1 0.3832 acc@5 0.7788\n",
      "\u001b[32m[2020-06-27 01:03:50] __main__ INFO: \u001b[0mElapsed 7.80\n",
      "\u001b[32m[2020-06-27 01:03:50] __main__ INFO: \u001b[0mTrain 18 11951\n",
      "\u001b[32m[2020-06-27 01:04:22] __main__ INFO: \u001b[0mEpoch 18 Step 100/703 lr 0.100000 loss 1.3713 (1.6677) acc@1 0.4688 (0.3887) acc@5 0.8125 (0.7697)\n",
      "\u001b[32m[2020-06-27 01:04:54] __main__ INFO: \u001b[0mEpoch 18 Step 200/703 lr 0.100000 loss 1.5702 (1.6637) acc@1 0.3906 (0.3901) acc@5 0.7656 (0.7680)\n",
      "\u001b[32m[2020-06-27 01:05:25] __main__ INFO: \u001b[0mEpoch 18 Step 300/703 lr 0.100000 loss 1.7351 (1.6637) acc@1 0.4062 (0.3895) acc@5 0.7656 (0.7709)\n",
      "\u001b[32m[2020-06-27 01:05:57] __main__ INFO: \u001b[0mEpoch 18 Step 400/703 lr 0.100000 loss 1.7207 (1.6658) acc@1 0.3438 (0.3886) acc@5 0.7500 (0.7711)\n",
      "\u001b[32m[2020-06-27 01:06:29] __main__ INFO: \u001b[0mEpoch 18 Step 500/703 lr 0.100000 loss 1.5842 (1.6613) acc@1 0.3438 (0.3887) acc@5 0.7969 (0.7730)\n",
      "\u001b[32m[2020-06-27 01:07:00] __main__ INFO: \u001b[0mEpoch 18 Step 600/703 lr 0.100000 loss 1.7348 (1.6591) acc@1 0.3750 (0.3882) acc@5 0.7188 (0.7721)\n",
      "\u001b[32m[2020-06-27 01:07:32] __main__ INFO: \u001b[0mEpoch 18 Step 700/703 lr 0.100000 loss 1.8822 (1.6562) acc@1 0.2812 (0.3888) acc@5 0.7812 (0.7730)\n",
      "\u001b[32m[2020-06-27 01:07:33] __main__ INFO: \u001b[0mEpoch 18 Step 703/703 lr 0.100000 loss 1.9013 (1.6566) acc@1 0.3438 (0.3887) acc@5 0.7656 (0.7730)\n",
      "\u001b[32m[2020-06-27 01:07:33] __main__ INFO: \u001b[0mElapsed 223.10\n",
      "\u001b[32m[2020-06-27 01:07:33] __main__ INFO: \u001b[0mVal 18\n",
      "\u001b[32m[2020-06-27 01:07:41] __main__ INFO: \u001b[0mEpoch 18 loss 1.6722 acc@1 0.3844 acc@5 0.7714\n",
      "\u001b[32m[2020-06-27 01:07:41] __main__ INFO: \u001b[0mElapsed 7.75\n",
      "\u001b[32m[2020-06-27 01:07:41] __main__ INFO: \u001b[0mTrain 19 12654\n",
      "\u001b[32m[2020-06-27 01:08:13] __main__ INFO: \u001b[0mEpoch 19 Step 100/703 lr 0.100000 loss 1.5536 (1.6206) acc@1 0.4688 (0.3927) acc@5 0.7969 (0.7842)\n",
      "\u001b[32m[2020-06-27 01:08:45] __main__ INFO: \u001b[0mEpoch 19 Step 200/703 lr 0.100000 loss 1.6803 (1.6297) acc@1 0.3125 (0.3929) acc@5 0.7812 (0.7773)\n",
      "\u001b[32m[2020-06-27 01:09:16] __main__ INFO: \u001b[0mEpoch 19 Step 300/703 lr 0.100000 loss 1.6447 (1.6291) acc@1 0.3594 (0.3923) acc@5 0.7500 (0.7771)\n",
      "\u001b[32m[2020-06-27 01:09:48] __main__ INFO: \u001b[0mEpoch 19 Step 400/703 lr 0.100000 loss 1.8236 (1.6322) acc@1 0.2812 (0.3923) acc@5 0.7188 (0.7784)\n",
      "\u001b[32m[2020-06-27 01:10:20] __main__ INFO: \u001b[0mEpoch 19 Step 500/703 lr 0.100000 loss 1.5413 (1.6335) acc@1 0.3906 (0.3929) acc@5 0.7656 (0.7780)\n",
      "\u001b[32m[2020-06-27 01:10:52] __main__ INFO: \u001b[0mEpoch 19 Step 600/703 lr 0.100000 loss 1.6365 (1.6354) acc@1 0.3906 (0.3935) acc@5 0.7656 (0.7778)\n",
      "\u001b[32m[2020-06-27 01:11:23] __main__ INFO: \u001b[0mEpoch 19 Step 700/703 lr 0.100000 loss 1.5570 (1.6378) acc@1 0.4844 (0.3935) acc@5 0.7344 (0.7771)\n",
      "\u001b[32m[2020-06-27 01:11:24] __main__ INFO: \u001b[0mEpoch 19 Step 703/703 lr 0.100000 loss 1.5642 (1.6376) acc@1 0.4375 (0.3936) acc@5 0.7500 (0.7770)\n",
      "\u001b[32m[2020-06-27 01:11:24] __main__ INFO: \u001b[0mElapsed 223.35\n",
      "\u001b[32m[2020-06-27 01:11:24] __main__ INFO: \u001b[0mVal 19\n",
      "\u001b[32m[2020-06-27 01:11:32] __main__ INFO: \u001b[0mEpoch 19 loss 1.6221 acc@1 0.4000 acc@5 0.7678\n",
      "\u001b[32m[2020-06-27 01:11:32] __main__ INFO: \u001b[0mElapsed 7.81\n",
      "\u001b[32m[2020-06-27 01:11:32] __main__ INFO: \u001b[0mTrain 20 13357\n",
      "\u001b[32m[2020-06-27 01:12:04] __main__ INFO: \u001b[0mEpoch 20 Step 100/703 lr 0.100000 loss 1.4978 (1.6242) acc@1 0.4375 (0.3958) acc@5 0.8750 (0.7789)\n",
      "\u001b[32m[2020-06-27 01:12:36] __main__ INFO: \u001b[0mEpoch 20 Step 200/703 lr 0.100000 loss 1.6092 (1.6235) acc@1 0.3438 (0.3980) acc@5 0.8438 (0.7768)\n",
      "\u001b[32m[2020-06-27 01:13:07] __main__ INFO: \u001b[0mEpoch 20 Step 300/703 lr 0.100000 loss 1.6196 (1.6203) acc@1 0.4375 (0.3980) acc@5 0.7500 (0.7765)\n",
      "\u001b[32m[2020-06-27 01:13:39] __main__ INFO: \u001b[0mEpoch 20 Step 400/703 lr 0.100000 loss 1.5159 (1.6226) acc@1 0.3906 (0.3986) acc@5 0.8281 (0.7779)\n",
      "\u001b[32m[2020-06-27 01:14:11] __main__ INFO: \u001b[0mEpoch 20 Step 500/703 lr 0.100000 loss 1.5284 (1.6222) acc@1 0.4219 (0.3982) acc@5 0.7344 (0.7781)\n",
      "\u001b[32m[2020-06-27 01:14:43] __main__ INFO: \u001b[0mEpoch 20 Step 600/703 lr 0.100000 loss 1.7254 (1.6277) acc@1 0.3906 (0.3957) acc@5 0.8281 (0.7774)\n",
      "\u001b[32m[2020-06-27 01:15:14] __main__ INFO: \u001b[0mEpoch 20 Step 700/703 lr 0.100000 loss 1.6112 (1.6304) acc@1 0.3594 (0.3954) acc@5 0.7500 (0.7761)\n",
      "\u001b[32m[2020-06-27 01:15:15] __main__ INFO: \u001b[0mEpoch 20 Step 703/703 lr 0.100000 loss 1.6205 (1.6302) acc@1 0.3750 (0.3955) acc@5 0.8281 (0.7762)\n",
      "\u001b[32m[2020-06-27 01:15:15] __main__ INFO: \u001b[0mElapsed 223.37\n",
      "\u001b[32m[2020-06-27 01:15:15] __main__ INFO: \u001b[0mVal 20\n",
      "\u001b[32m[2020-06-27 01:15:23] __main__ INFO: \u001b[0mEpoch 20 loss 1.8217 acc@1 0.3508 acc@5 0.7610\n",
      "\u001b[32m[2020-06-27 01:15:23] __main__ INFO: \u001b[0mElapsed 7.79\n",
      "\u001b[32m[2020-06-27 01:15:23] __main__ INFO: \u001b[0mTrain 21 14060\n",
      "\u001b[32m[2020-06-27 01:15:55] __main__ INFO: \u001b[0mEpoch 21 Step 100/703 lr 0.100000 loss 1.5948 (1.6216) acc@1 0.3906 (0.3973) acc@5 0.8125 (0.7806)\n",
      "\u001b[32m[2020-06-27 01:16:27] __main__ INFO: \u001b[0mEpoch 21 Step 200/703 lr 0.100000 loss 1.6074 (1.6129) acc@1 0.3906 (0.4005) acc@5 0.7812 (0.7814)\n",
      "\u001b[32m[2020-06-27 01:16:58] __main__ INFO: \u001b[0mEpoch 21 Step 300/703 lr 0.100000 loss 1.5235 (1.6187) acc@1 0.4688 (0.3958) acc@5 0.7344 (0.7794)\n",
      "\u001b[32m[2020-06-27 01:17:30] __main__ INFO: \u001b[0mEpoch 21 Step 400/703 lr 0.100000 loss 1.5213 (1.6142) acc@1 0.4219 (0.3971) acc@5 0.8125 (0.7820)\n",
      "\u001b[32m[2020-06-27 01:18:02] __main__ INFO: \u001b[0mEpoch 21 Step 500/703 lr 0.100000 loss 1.6108 (1.6162) acc@1 0.3594 (0.3964) acc@5 0.8438 (0.7812)\n",
      "\u001b[32m[2020-06-27 01:18:34] __main__ INFO: \u001b[0mEpoch 21 Step 600/703 lr 0.100000 loss 1.6032 (1.6173) acc@1 0.3906 (0.3961) acc@5 0.7656 (0.7802)\n",
      "\u001b[32m[2020-06-27 01:19:05] __main__ INFO: \u001b[0mEpoch 21 Step 700/703 lr 0.100000 loss 1.5575 (1.6185) acc@1 0.4062 (0.3965) acc@5 0.7656 (0.7793)\n",
      "\u001b[32m[2020-06-27 01:19:06] __main__ INFO: \u001b[0mEpoch 21 Step 703/703 lr 0.100000 loss 1.6649 (1.6185) acc@1 0.3438 (0.3965) acc@5 0.7812 (0.7794)\n",
      "\u001b[32m[2020-06-27 01:19:06] __main__ INFO: \u001b[0mElapsed 223.16\n",
      "\u001b[32m[2020-06-27 01:19:06] __main__ INFO: \u001b[0mVal 21\n",
      "\u001b[32m[2020-06-27 01:19:14] __main__ INFO: \u001b[0mEpoch 21 loss 1.7453 acc@1 0.3762 acc@5 0.7600\n",
      "\u001b[32m[2020-06-27 01:19:14] __main__ INFO: \u001b[0mElapsed 7.77\n",
      "\u001b[32m[2020-06-27 01:19:14] __main__ INFO: \u001b[0mTrain 22 14763\n",
      "\u001b[32m[2020-06-27 01:19:46] __main__ INFO: \u001b[0mEpoch 22 Step 100/703 lr 0.100000 loss 1.6710 (1.5967) acc@1 0.3906 (0.4098) acc@5 0.7344 (0.7775)\n",
      "\u001b[32m[2020-06-27 01:20:18] __main__ INFO: \u001b[0mEpoch 22 Step 200/703 lr 0.100000 loss 1.6464 (1.6024) acc@1 0.3594 (0.4065) acc@5 0.7969 (0.7748)\n",
      "\u001b[32m[2020-06-27 01:20:50] __main__ INFO: \u001b[0mEpoch 22 Step 300/703 lr 0.100000 loss 1.6723 (1.6011) acc@1 0.3750 (0.4065) acc@5 0.7656 (0.7767)\n",
      "\u001b[32m[2020-06-27 01:21:21] __main__ INFO: \u001b[0mEpoch 22 Step 400/703 lr 0.100000 loss 1.5339 (1.6033) acc@1 0.3750 (0.4064) acc@5 0.8281 (0.7778)\n",
      "\u001b[32m[2020-06-27 01:21:53] __main__ INFO: \u001b[0mEpoch 22 Step 500/703 lr 0.100000 loss 1.7212 (1.6063) acc@1 0.3906 (0.4042) acc@5 0.7500 (0.7786)\n",
      "\u001b[32m[2020-06-27 01:22:25] __main__ INFO: \u001b[0mEpoch 22 Step 600/703 lr 0.100000 loss 1.6145 (1.6053) acc@1 0.3594 (0.4052) acc@5 0.7812 (0.7781)\n",
      "\u001b[32m[2020-06-27 01:22:56] __main__ INFO: \u001b[0mEpoch 22 Step 700/703 lr 0.100000 loss 1.6585 (1.6051) acc@1 0.4219 (0.4054) acc@5 0.7969 (0.7784)\n",
      "\u001b[32m[2020-06-27 01:22:57] __main__ INFO: \u001b[0mEpoch 22 Step 703/703 lr 0.100000 loss 1.5147 (1.6056) acc@1 0.4062 (0.4051) acc@5 0.7344 (0.7782)\n",
      "\u001b[32m[2020-06-27 01:22:57] __main__ INFO: \u001b[0mElapsed 223.15\n",
      "\u001b[32m[2020-06-27 01:22:57] __main__ INFO: \u001b[0mVal 22\n",
      "\u001b[32m[2020-06-27 01:23:05] __main__ INFO: \u001b[0mEpoch 22 loss 1.7634 acc@1 0.3714 acc@5 0.7704\n",
      "\u001b[32m[2020-06-27 01:23:05] __main__ INFO: \u001b[0mElapsed 7.81\n",
      "\u001b[32m[2020-06-27 01:23:05] __main__ INFO: \u001b[0mTrain 23 15466\n",
      "\u001b[32m[2020-06-27 01:23:37] __main__ INFO: \u001b[0mEpoch 23 Step 100/703 lr 0.100000 loss 1.7046 (1.5846) acc@1 0.3125 (0.4144) acc@5 0.7969 (0.7820)\n",
      "\u001b[32m[2020-06-27 01:24:09] __main__ INFO: \u001b[0mEpoch 23 Step 200/703 lr 0.100000 loss 1.6286 (1.6047) acc@1 0.4219 (0.4050) acc@5 0.8125 (0.7805)\n",
      "\u001b[32m[2020-06-27 01:24:41] __main__ INFO: \u001b[0mEpoch 23 Step 300/703 lr 0.100000 loss 1.5941 (1.6009) acc@1 0.3906 (0.4057) acc@5 0.7969 (0.7812)\n",
      "\u001b[32m[2020-06-27 01:25:12] __main__ INFO: \u001b[0mEpoch 23 Step 400/703 lr 0.100000 loss 1.8711 (1.5992) acc@1 0.2500 (0.4058) acc@5 0.6875 (0.7829)\n",
      "\u001b[32m[2020-06-27 01:25:44] __main__ INFO: \u001b[0mEpoch 23 Step 500/703 lr 0.100000 loss 1.6934 (1.6009) acc@1 0.3125 (0.4057) acc@5 0.7188 (0.7820)\n",
      "\u001b[32m[2020-06-27 01:26:16] __main__ INFO: \u001b[0mEpoch 23 Step 600/703 lr 0.100000 loss 1.4165 (1.6014) acc@1 0.4531 (0.4062) acc@5 0.8281 (0.7814)\n",
      "\u001b[32m[2020-06-27 01:26:48] __main__ INFO: \u001b[0mEpoch 23 Step 700/703 lr 0.100000 loss 1.5079 (1.6001) acc@1 0.4062 (0.4064) acc@5 0.7344 (0.7810)\n",
      "\u001b[32m[2020-06-27 01:26:48] __main__ INFO: \u001b[0mEpoch 23 Step 703/703 lr 0.100000 loss 1.3203 (1.5999) acc@1 0.5625 (0.4065) acc@5 0.8125 (0.7811)\n",
      "\u001b[32m[2020-06-27 01:26:48] __main__ INFO: \u001b[0mElapsed 223.40\n",
      "\u001b[32m[2020-06-27 01:26:48] __main__ INFO: \u001b[0mVal 23\n",
      "\u001b[32m[2020-06-27 01:26:56] __main__ INFO: \u001b[0mEpoch 23 loss 1.6502 acc@1 0.3978 acc@5 0.7670\n",
      "\u001b[32m[2020-06-27 01:26:56] __main__ INFO: \u001b[0mElapsed 7.77\n",
      "\u001b[32m[2020-06-27 01:26:56] __main__ INFO: \u001b[0mTrain 24 16169\n",
      "\u001b[32m[2020-06-27 01:27:28] __main__ INFO: \u001b[0mEpoch 24 Step 100/703 lr 0.100000 loss 1.6394 (1.5822) acc@1 0.3906 (0.4089) acc@5 0.7500 (0.7808)\n",
      "\u001b[32m[2020-06-27 01:28:00] __main__ INFO: \u001b[0mEpoch 24 Step 200/703 lr 0.100000 loss 1.5665 (1.5872) acc@1 0.3906 (0.4053) acc@5 0.7969 (0.7759)\n",
      "\u001b[32m[2020-06-27 01:28:31] __main__ INFO: \u001b[0mEpoch 24 Step 300/703 lr 0.100000 loss 1.6210 (1.5906) acc@1 0.3906 (0.4057) acc@5 0.7812 (0.7757)\n",
      "\u001b[32m[2020-06-27 01:29:03] __main__ INFO: \u001b[0mEpoch 24 Step 400/703 lr 0.100000 loss 1.5408 (1.5893) acc@1 0.4219 (0.4082) acc@5 0.8125 (0.7775)\n",
      "\u001b[32m[2020-06-27 01:29:35] __main__ INFO: \u001b[0mEpoch 24 Step 500/703 lr 0.100000 loss 1.4692 (1.5927) acc@1 0.4375 (0.4080) acc@5 0.7969 (0.7773)\n",
      "\u001b[32m[2020-06-27 01:30:07] __main__ INFO: \u001b[0mEpoch 24 Step 600/703 lr 0.100000 loss 1.5638 (1.5947) acc@1 0.4219 (0.4082) acc@5 0.8281 (0.7775)\n",
      "\u001b[32m[2020-06-27 01:30:38] __main__ INFO: \u001b[0mEpoch 24 Step 700/703 lr 0.100000 loss 1.4123 (1.5917) acc@1 0.4844 (0.4088) acc@5 0.7969 (0.7788)\n",
      "\u001b[32m[2020-06-27 01:30:39] __main__ INFO: \u001b[0mEpoch 24 Step 703/703 lr 0.100000 loss 1.5683 (1.5915) acc@1 0.4062 (0.4088) acc@5 0.7031 (0.7788)\n",
      "\u001b[32m[2020-06-27 01:30:39] __main__ INFO: \u001b[0mElapsed 222.92\n",
      "\u001b[32m[2020-06-27 01:30:39] __main__ INFO: \u001b[0mVal 24\n",
      "\u001b[32m[2020-06-27 01:30:47] __main__ INFO: \u001b[0mEpoch 24 loss 1.6034 acc@1 0.4108 acc@5 0.7730\n",
      "\u001b[32m[2020-06-27 01:30:47] __main__ INFO: \u001b[0mElapsed 7.81\n",
      "\u001b[32m[2020-06-27 01:30:47] __main__ INFO: \u001b[0mTrain 25 16872\n",
      "\u001b[32m[2020-06-27 01:31:19] __main__ INFO: \u001b[0mEpoch 25 Step 100/703 lr 0.100000 loss 1.5900 (1.5470) acc@1 0.4844 (0.4275) acc@5 0.7500 (0.7842)\n"
     ]
    }
   ],
   "source": [
    "# Train the model per the settings specified in the original paper\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "!python train.py --config configs/cifar/densenet.yaml \\\n",
    "    model.densenet.depth 100 \\\n",
    "    model.densenet.growth_rate 12 \\\n",
    "    train.batch_size 64 \\\n",
    "    train.base_lr 0.1 \\\n",
    "    dataset.name CIFAR10_RA_2_20 \\\n",
    "    train.output_dir /home/ec2-user/SageMaker/experiments/densenet_BC_100_12_ra_2_20/exp00 \\\n",
    "    scheduler.epochs 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-28 01:43:16] __main__ INFO: \u001b[0mdevice: cuda\n",
      "cudnn:\n",
      "  benchmark: True\n",
      "  deterministic: False\n",
      "dataset:\n",
      "  name: CIFAR10\n",
      "  dataset_dir: ~/.torch/datasets/CIFAR10\n",
      "  image_size: 32\n",
      "  n_channels: 3\n",
      "  n_classes: 10\n",
      "model:\n",
      "  type: cifar\n",
      "  name: densenet\n",
      "  init_mode: kaiming_fan_out\n",
      "  vgg:\n",
      "    n_channels: [64, 128, 256, 512, 512]\n",
      "    n_layers: [2, 2, 3, 3, 3]\n",
      "    use_bn: True\n",
      "  resnet:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "  resnet_preact:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "  wrn:\n",
      "    depth: 28\n",
      "    initial_channels: 16\n",
      "    widening_factor: 10\n",
      "    drop_rate: 0.0\n",
      "  densenet:\n",
      "    depth: 100\n",
      "    n_blocks: [6, 12, 24, 16]\n",
      "    block_type: bottleneck\n",
      "    growth_rate: 12\n",
      "    drop_rate: 0.0\n",
      "    compression_rate: 0.5\n",
      "  pyramidnet:\n",
      "    depth: 272\n",
      "    n_blocks: [3, 24, 36, 3]\n",
      "    initial_channels: 16\n",
      "    block_type: bottleneck\n",
      "    alpha: 200\n",
      "  resnext:\n",
      "    depth: 29\n",
      "    n_blocks: [3, 4, 6, 3]\n",
      "    initial_channels: 64\n",
      "    cardinality: 8\n",
      "    base_channels: 4\n",
      "  shake_shake:\n",
      "    depth: 26\n",
      "    initial_channels: 96\n",
      "    shake_forward: True\n",
      "    shake_backward: True\n",
      "    shake_image: True\n",
      "  se_resnet_preact:\n",
      "    depth: 110\n",
      "    initial_channels: 16\n",
      "    se_reduction: 16\n",
      "    block_type: basic\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "train:\n",
      "  checkpoint: /home/ec2-user/SageMaker/experiments/densenet_BC_100_12_ra_2_20/exp00/checkpoint_00400.pth\n",
      "  resume: False\n",
      "  precision: O0\n",
      "  batch_size: 64\n",
      "  subdivision: 1\n",
      "  optimizer: sgd\n",
      "  base_lr: 0.001\n",
      "  momentum: 0.9\n",
      "  nesterov: True\n",
      "  weight_decay: 0.0001\n",
      "  no_weight_decay_on_bn: False\n",
      "  gradient_clip: 0\n",
      "  start_epoch: 0\n",
      "  seed: 0\n",
      "  val_first: True\n",
      "  val_period: 1\n",
      "  val_ratio: 0.1\n",
      "  use_test_as_val: False\n",
      "  output_dir: /home/ec2-user/SageMaker/experiments/densenet_BC_100_12_ra_2_20/exp00_resume400_50\n",
      "  log_period: 100\n",
      "  checkpoint_period: 100\n",
      "  use_tensorboard: True\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: True\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "  distributed: False\n",
      "  dist:\n",
      "    backend: nccl\n",
      "    init_method: env://\n",
      "    world_size: -1\n",
      "    node_rank: -1\n",
      "    local_rank: 0\n",
      "    use_sync_bn: False\n",
      "tensorboard:\n",
      "  train_images: False\n",
      "  val_images: False\n",
      "  model_params: False\n",
      "optim:\n",
      "  adam:\n",
      "    betas: (0.9, 0.999)\n",
      "  lars:\n",
      "    eps: 1e-09\n",
      "    threshold: 0.01\n",
      "  adabound:\n",
      "    betas: (0.9, 0.999)\n",
      "    final_lr: 0.1\n",
      "    gamma: 0.001\n",
      "scheduler:\n",
      "  epochs: 50\n",
      "  warmup:\n",
      "    type: none\n",
      "    epochs: 0\n",
      "    start_factor: 0.001\n",
      "    exponent: 4\n",
      "  type: multistep\n",
      "  milestones: [150, 225]\n",
      "  lr_decay: 0.1\n",
      "  lr_min_factor: 0.001\n",
      "  T0: 10\n",
      "  T_mul: 1.0\n",
      "validation:\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: False\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "augmentation:\n",
      "  use_random_crop: True\n",
      "  use_random_horizontal_flip: True\n",
      "  use_cutout: False\n",
      "  use_random_erasing: False\n",
      "  use_dual_cutout: False\n",
      "  use_mixup: False\n",
      "  use_ricap: False\n",
      "  use_cutmix: False\n",
      "  use_label_smoothing: False\n",
      "  random_crop:\n",
      "    padding: 4\n",
      "    fill: 0\n",
      "    padding_mode: constant\n",
      "  random_horizontal_flip:\n",
      "    prob: 0.5\n",
      "  cutout:\n",
      "    prob: 1.0\n",
      "    mask_size: 16\n",
      "    cut_inside: False\n",
      "    mask_color: 0\n",
      "    dual_cutout_alpha: 0.1\n",
      "  random_erasing:\n",
      "    prob: 0.5\n",
      "    area_ratio_range: [0.02, 0.4]\n",
      "    min_aspect_ratio: 0.3\n",
      "    max_attempt: 20\n",
      "  mixup:\n",
      "    alpha: 1.0\n",
      "  ricap:\n",
      "    beta: 0.3\n",
      "  cutmix:\n",
      "    alpha: 1.0\n",
      "  label_smoothing:\n",
      "    epsilon: 0.1\n",
      "tta:\n",
      "  use_resize: False\n",
      "  use_center_crop: False\n",
      "  resize: 256\n",
      "test:\n",
      "  checkpoint: ''\n",
      "  output_dir: ''\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    pin_memory: False\n",
      "\u001b[32m[2020-06-28 01:43:16] __main__ INFO: \u001b[0menv_info:\n",
      "  pytorch_version: 1.4.0\n",
      "  cuda_version: 10.1\n",
      "  cudnn_version: 7603\n",
      "  num_gpus: 1\n",
      "  gpu_name: Tesla K80\n",
      "  gpu_capability: 3.7\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /home/ec2-user/.torch/datasets/CIFAR10/cifar-10-python.tar.gz\n",
      "170500096it [00:02, 59473060.34it/s]                                            \n",
      "Extracting /home/ec2-user/.torch/datasets/CIFAR10/cifar-10-python.tar.gz to /home/ec2-user/.torch/datasets/CIFAR10\n",
      "\u001b[32m[2020-06-28 01:43:24] __main__ INFO: \u001b[0mMACs  : 296.49M\n",
      "\u001b[32m[2020-06-28 01:43:24] __main__ INFO: \u001b[0m#params: 769.16K\n",
      "Selected optimization level O0:  Pure FP32 training.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\",)\n",
      "\u001b[32m[2020-06-28 01:43:24] __main__ INFO: \u001b[0mVal 0\n",
      "\u001b[32m[2020-06-28 01:43:35] __main__ INFO: \u001b[0mEpoch 0 loss 1.1141 acc@1 0.7904 acc@5 0.9814\n",
      "\u001b[32m[2020-06-28 01:43:35] __main__ INFO: \u001b[0mElapsed 11.38\n",
      "\u001b[32m[2020-06-28 01:43:35] __main__ INFO: \u001b[0mTrain 1 0\n",
      "\u001b[32m[2020-06-28 01:44:08] __main__ INFO: \u001b[0mEpoch 1 Step 100/703 lr 0.001000 loss 0.3519 (0.4738) acc@1 0.9219 (0.8702) acc@5 1.0000 (0.9920)\n",
      "\u001b[32m[2020-06-28 01:44:40] __main__ INFO: \u001b[0mEpoch 1 Step 200/703 lr 0.001000 loss 0.3249 (0.4269) acc@1 0.9375 (0.8808) acc@5 1.0000 (0.9938)\n",
      "\u001b[32m[2020-06-28 01:45:12] __main__ INFO: \u001b[0mEpoch 1 Step 300/703 lr 0.001000 loss 0.8058 (0.4000) acc@1 0.8594 (0.8867) acc@5 1.0000 (0.9945)\n",
      "\u001b[32m[2020-06-28 01:45:43] __main__ INFO: \u001b[0mEpoch 1 Step 400/703 lr 0.001000 loss 0.4172 (0.3871) acc@1 0.8906 (0.8896) acc@5 1.0000 (0.9948)\n",
      "\u001b[32m[2020-06-28 01:46:15] __main__ INFO: \u001b[0mEpoch 1 Step 500/703 lr 0.001000 loss 0.0211 (0.3736) acc@1 1.0000 (0.8914) acc@5 1.0000 (0.9952)\n",
      "\u001b[32m[2020-06-28 01:46:46] __main__ INFO: \u001b[0mEpoch 1 Step 600/703 lr 0.001000 loss 0.2738 (0.3642) acc@1 0.8750 (0.8931) acc@5 0.9844 (0.9951)\n",
      "\u001b[32m[2020-06-28 01:47:18] __main__ INFO: \u001b[0mEpoch 1 Step 700/703 lr 0.001000 loss 0.1723 (0.3566) acc@1 0.9531 (0.8937) acc@5 1.0000 (0.9953)\n",
      "\u001b[32m[2020-06-28 01:47:19] __main__ INFO: \u001b[0mEpoch 1 Step 703/703 lr 0.001000 loss 0.3195 (0.3563) acc@1 0.8438 (0.8936) acc@5 1.0000 (0.9953)\n",
      "\u001b[32m[2020-06-28 01:47:19] __main__ INFO: \u001b[0mElapsed 224.09\n",
      "\u001b[32m[2020-06-28 01:47:19] __main__ INFO: \u001b[0mVal 1\n",
      "\u001b[32m[2020-06-28 01:47:27] __main__ INFO: \u001b[0mEpoch 1 loss 0.3820 acc@1 0.8836 acc@5 0.9952\n",
      "\u001b[32m[2020-06-28 01:47:27] __main__ INFO: \u001b[0mElapsed 7.76\n",
      "\u001b[32m[2020-06-28 01:47:27] __main__ INFO: \u001b[0mTrain 2 703\n",
      "\u001b[32m[2020-06-28 01:47:59] __main__ INFO: \u001b[0mEpoch 2 Step 100/703 lr 0.001000 loss 0.3457 (0.2675) acc@1 0.9062 (0.9130) acc@5 0.9844 (0.9970)\n",
      "\u001b[32m[2020-06-28 01:48:30] __main__ INFO: \u001b[0mEpoch 2 Step 200/703 lr 0.001000 loss 0.2636 (0.2641) acc@1 0.9219 (0.9125) acc@5 1.0000 (0.9973)\n",
      "\u001b[32m[2020-06-28 01:49:02] __main__ INFO: \u001b[0mEpoch 2 Step 300/703 lr 0.001000 loss 0.0845 (0.2634) acc@1 0.9844 (0.9133) acc@5 1.0000 (0.9973)\n",
      "\u001b[32m[2020-06-28 01:49:34] __main__ INFO: \u001b[0mEpoch 2 Step 400/703 lr 0.001000 loss 0.3011 (0.2672) acc@1 0.9062 (0.9122) acc@5 1.0000 (0.9973)\n",
      "\u001b[32m[2020-06-28 01:50:05] __main__ INFO: \u001b[0mEpoch 2 Step 500/703 lr 0.001000 loss 0.2558 (0.2696) acc@1 0.9062 (0.9111) acc@5 1.0000 (0.9972)\n",
      "\u001b[32m[2020-06-28 01:50:37] __main__ INFO: \u001b[0mEpoch 2 Step 600/703 lr 0.001000 loss 0.3344 (0.2672) acc@1 0.8906 (0.9116) acc@5 1.0000 (0.9973)\n",
      "\u001b[32m[2020-06-28 01:51:09] __main__ INFO: \u001b[0mEpoch 2 Step 700/703 lr 0.001000 loss 0.2185 (0.2646) acc@1 0.9062 (0.9125) acc@5 1.0000 (0.9972)\n",
      "\u001b[32m[2020-06-28 01:51:10] __main__ INFO: \u001b[0mEpoch 2 Step 703/703 lr 0.001000 loss 0.2071 (0.2645) acc@1 0.9219 (0.9125) acc@5 1.0000 (0.9972)\n",
      "\u001b[32m[2020-06-28 01:51:10] __main__ INFO: \u001b[0mElapsed 222.84\n",
      "\u001b[32m[2020-06-28 01:51:10] __main__ INFO: \u001b[0mVal 2\n",
      "\u001b[32m[2020-06-28 01:51:17] __main__ INFO: \u001b[0mEpoch 2 loss 0.3397 acc@1 0.8934 acc@5 0.9956\n",
      "\u001b[32m[2020-06-28 01:51:17] __main__ INFO: \u001b[0mElapsed 7.76\n",
      "\u001b[32m[2020-06-28 01:51:17] __main__ INFO: \u001b[0mTrain 3 1406\n",
      "\u001b[32m[2020-06-28 01:51:49] __main__ INFO: \u001b[0mEpoch 3 Step 100/703 lr 0.001000 loss 0.3767 (0.2253) acc@1 0.8438 (0.9230) acc@5 1.0000 (0.9988)\n",
      "\u001b[32m[2020-06-28 01:52:21] __main__ INFO: \u001b[0mEpoch 3 Step 200/703 lr 0.001000 loss 0.0658 (0.2302) acc@1 0.9844 (0.9226) acc@5 1.0000 (0.9986)\n",
      "\u001b[32m[2020-06-28 01:52:53] __main__ INFO: \u001b[0mEpoch 3 Step 300/703 lr 0.001000 loss 0.3463 (0.2274) acc@1 0.9219 (0.9230) acc@5 0.9844 (0.9985)\n",
      "\u001b[32m[2020-06-28 01:53:24] __main__ INFO: \u001b[0mEpoch 3 Step 400/703 lr 0.001000 loss 0.1515 (0.2261) acc@1 0.9375 (0.9240) acc@5 1.0000 (0.9982)\n",
      "\u001b[32m[2020-06-28 01:53:56] __main__ INFO: \u001b[0mEpoch 3 Step 500/703 lr 0.001000 loss 0.2719 (0.2300) acc@1 0.9219 (0.9218) acc@5 1.0000 (0.9982)\n",
      "\u001b[32m[2020-06-28 01:54:28] __main__ INFO: \u001b[0mEpoch 3 Step 600/703 lr 0.001000 loss 0.3372 (0.2300) acc@1 0.9062 (0.9226) acc@5 1.0000 (0.9981)\n",
      "\u001b[32m[2020-06-28 01:55:00] __main__ INFO: \u001b[0mEpoch 3 Step 700/703 lr 0.001000 loss 0.2452 (0.2293) acc@1 0.9062 (0.9229) acc@5 1.0000 (0.9980)\n",
      "\u001b[32m[2020-06-28 01:55:01] __main__ INFO: \u001b[0mEpoch 3 Step 703/703 lr 0.001000 loss 0.3297 (0.2293) acc@1 0.8594 (0.9229) acc@5 1.0000 (0.9980)\n",
      "\u001b[32m[2020-06-28 01:55:01] __main__ INFO: \u001b[0mElapsed 223.19\n",
      "\u001b[32m[2020-06-28 01:55:01] __main__ INFO: \u001b[0mVal 3\n",
      "\u001b[32m[2020-06-28 01:55:08] __main__ INFO: \u001b[0mEpoch 3 loss 0.3239 acc@1 0.8968 acc@5 0.9960\n",
      "\u001b[32m[2020-06-28 01:55:08] __main__ INFO: \u001b[0mElapsed 7.70\n",
      "\u001b[32m[2020-06-28 01:55:08] __main__ INFO: \u001b[0mTrain 4 2109\n",
      "\u001b[32m[2020-06-28 01:55:40] __main__ INFO: \u001b[0mEpoch 4 Step 100/703 lr 0.001000 loss 0.2833 (0.2191) acc@1 0.9375 (0.9248) acc@5 0.9844 (0.9972)\n",
      "\u001b[32m[2020-06-28 01:56:12] __main__ INFO: \u001b[0mEpoch 4 Step 200/703 lr 0.001000 loss 0.1811 (0.2153) acc@1 0.9375 (0.9264) acc@5 1.0000 (0.9981)\n",
      "\u001b[32m[2020-06-28 01:56:44] __main__ INFO: \u001b[0mEpoch 4 Step 300/703 lr 0.001000 loss 0.2237 (0.2106) acc@1 0.9375 (0.9274) acc@5 1.0000 (0.9984)\n",
      "\u001b[32m[2020-06-28 01:57:15] __main__ INFO: \u001b[0mEpoch 4 Step 400/703 lr 0.001000 loss 0.2209 (0.2105) acc@1 0.9375 (0.9275) acc@5 1.0000 (0.9984)\n",
      "\u001b[32m[2020-06-28 01:57:47] __main__ INFO: \u001b[0mEpoch 4 Step 500/703 lr 0.001000 loss 0.1983 (0.2094) acc@1 0.9062 (0.9275) acc@5 1.0000 (0.9983)\n",
      "\u001b[32m[2020-06-28 01:58:19] __main__ INFO: \u001b[0mEpoch 4 Step 600/703 lr 0.001000 loss 0.2522 (0.2092) acc@1 0.8750 (0.9278) acc@5 1.0000 (0.9984)\n",
      "\u001b[32m[2020-06-28 01:58:51] __main__ INFO: \u001b[0mEpoch 4 Step 700/703 lr 0.001000 loss 0.1164 (0.2077) acc@1 0.9531 (0.9285) acc@5 1.0000 (0.9983)\n",
      "\u001b[32m[2020-06-28 01:58:51] __main__ INFO: \u001b[0mEpoch 4 Step 703/703 lr 0.001000 loss 0.2621 (0.2077) acc@1 0.9375 (0.9285) acc@5 1.0000 (0.9983)\n",
      "\u001b[32m[2020-06-28 01:58:52] __main__ INFO: \u001b[0mElapsed 223.16\n",
      "\u001b[32m[2020-06-28 01:58:52] __main__ INFO: \u001b[0mVal 4\n",
      "\u001b[32m[2020-06-28 01:58:59] __main__ INFO: \u001b[0mEpoch 4 loss 0.3162 acc@1 0.9012 acc@5 0.9956\n",
      "\u001b[32m[2020-06-28 01:58:59] __main__ INFO: \u001b[0mElapsed 7.72\n",
      "\u001b[32m[2020-06-28 01:58:59] __main__ INFO: \u001b[0mTrain 5 2812\n",
      "\u001b[32m[2020-06-28 01:59:31] __main__ INFO: \u001b[0mEpoch 5 Step 100/703 lr 0.001000 loss 0.0715 (0.1881) acc@1 0.9844 (0.9348) acc@5 1.0000 (0.9981)\n",
      "\u001b[32m[2020-06-28 02:00:03] __main__ INFO: \u001b[0mEpoch 5 Step 200/703 lr 0.001000 loss 0.1853 (0.1905) acc@1 0.9375 (0.9327) acc@5 1.0000 (0.9988)\n",
      "\u001b[32m[2020-06-28 02:00:35] __main__ INFO: \u001b[0mEpoch 5 Step 300/703 lr 0.001000 loss 0.2143 (0.1910) acc@1 0.8750 (0.9316) acc@5 1.0000 (0.9987)\n",
      "\u001b[32m[2020-06-28 02:01:06] __main__ INFO: \u001b[0mEpoch 5 Step 400/703 lr 0.001000 loss 0.2540 (0.1929) acc@1 0.9062 (0.9314) acc@5 1.0000 (0.9986)\n",
      "\u001b[32m[2020-06-28 02:01:38] __main__ INFO: \u001b[0mEpoch 5 Step 500/703 lr 0.001000 loss 0.3280 (0.1910) acc@1 0.9375 (0.9323) acc@5 0.9844 (0.9987)\n",
      "\u001b[32m[2020-06-28 02:02:10] __main__ INFO: \u001b[0mEpoch 5 Step 600/703 lr 0.001000 loss 0.1892 (0.1931) acc@1 0.9219 (0.9319) acc@5 1.0000 (0.9986)\n",
      "\u001b[32m[2020-06-28 02:02:42] __main__ INFO: \u001b[0mEpoch 5 Step 700/703 lr 0.001000 loss 0.2028 (0.1926) acc@1 0.9219 (0.9322) acc@5 1.0000 (0.9986)\n",
      "\u001b[32m[2020-06-28 02:02:43] __main__ INFO: \u001b[0mEpoch 5 Step 703/703 lr 0.001000 loss 0.4674 (0.1930) acc@1 0.8594 (0.9321) acc@5 1.0000 (0.9986)\n",
      "\u001b[32m[2020-06-28 02:02:43] __main__ INFO: \u001b[0mElapsed 223.29\n",
      "\u001b[32m[2020-06-28 02:02:43] __main__ INFO: \u001b[0mVal 5\n",
      "\u001b[32m[2020-06-28 02:02:50] __main__ INFO: \u001b[0mEpoch 5 loss 0.3042 acc@1 0.9038 acc@5 0.9956\n",
      "\u001b[32m[2020-06-28 02:02:50] __main__ INFO: \u001b[0mElapsed 7.72\n",
      "\u001b[32m[2020-06-28 02:02:50] __main__ INFO: \u001b[0mTrain 6 3515\n",
      "\u001b[32m[2020-06-28 02:03:22] __main__ INFO: \u001b[0mEpoch 6 Step 100/703 lr 0.001000 loss 0.2023 (0.1769) acc@1 0.9531 (0.9397) acc@5 1.0000 (0.9988)\n",
      "\u001b[32m[2020-06-28 02:03:54] __main__ INFO: \u001b[0mEpoch 6 Step 200/703 lr 0.001000 loss 0.1215 (0.1756) acc@1 0.9375 (0.9405) acc@5 1.0000 (0.9989)\n",
      "\u001b[32m[2020-06-28 02:04:26] __main__ INFO: \u001b[0mEpoch 6 Step 300/703 lr 0.001000 loss 0.1320 (0.1785) acc@1 0.9688 (0.9391) acc@5 1.0000 (0.9988)\n",
      "\u001b[32m[2020-06-28 02:04:57] __main__ INFO: \u001b[0mEpoch 6 Step 400/703 lr 0.001000 loss 0.1864 (0.1799) acc@1 0.9375 (0.9394) acc@5 1.0000 (0.9987)\n",
      "\u001b[32m[2020-06-28 02:05:29] __main__ INFO: \u001b[0mEpoch 6 Step 500/703 lr 0.001000 loss 0.2576 (0.1800) acc@1 0.8906 (0.9387) acc@5 1.0000 (0.9988)\n",
      "\u001b[32m[2020-06-28 02:06:00] __main__ INFO: \u001b[0mEpoch 6 Step 600/703 lr 0.001000 loss 0.0834 (0.1786) acc@1 0.9844 (0.9384) acc@5 1.0000 (0.9989)\n",
      "\u001b[32m[2020-06-28 02:06:32] __main__ INFO: \u001b[0mEpoch 6 Step 700/703 lr 0.001000 loss 0.2714 (0.1811) acc@1 0.8750 (0.9373) acc@5 0.9844 (0.9989)\n",
      "\u001b[32m[2020-06-28 02:06:33] __main__ INFO: \u001b[0mEpoch 6 Step 703/703 lr 0.001000 loss 0.1605 (0.1814) acc@1 0.9062 (0.9372) acc@5 1.0000 (0.9989)\n",
      "\u001b[32m[2020-06-28 02:06:33] __main__ INFO: \u001b[0mElapsed 222.80\n",
      "\u001b[32m[2020-06-28 02:06:33] __main__ INFO: \u001b[0mVal 6\n",
      "\u001b[32m[2020-06-28 02:06:41] __main__ INFO: \u001b[0mEpoch 6 loss 0.3007 acc@1 0.9044 acc@5 0.9958\n",
      "\u001b[32m[2020-06-28 02:06:41] __main__ INFO: \u001b[0mElapsed 7.70\n",
      "\u001b[32m[2020-06-28 02:06:41] __main__ INFO: \u001b[0mTrain 7 4218\n",
      "\u001b[32m[2020-06-28 02:07:13] __main__ INFO: \u001b[0mEpoch 7 Step 100/703 lr 0.001000 loss 0.1363 (0.1625) acc@1 0.9688 (0.9445) acc@5 1.0000 (0.9988)\n",
      "\u001b[32m[2020-06-28 02:07:44] __main__ INFO: \u001b[0mEpoch 7 Step 200/703 lr 0.001000 loss 0.0794 (0.1644) acc@1 0.9688 (0.9429) acc@5 1.0000 (0.9989)\n",
      "\u001b[32m[2020-06-28 02:08:16] __main__ INFO: \u001b[0mEpoch 7 Step 300/703 lr 0.001000 loss 0.2601 (0.1653) acc@1 0.9688 (0.9424) acc@5 1.0000 (0.9987)\n",
      "\u001b[32m[2020-06-28 02:08:48] __main__ INFO: \u001b[0mEpoch 7 Step 400/703 lr 0.001000 loss 0.2473 (0.1672) acc@1 0.9375 (0.9415) acc@5 1.0000 (0.9987)\n",
      "\u001b[32m[2020-06-28 02:09:19] __main__ INFO: \u001b[0mEpoch 7 Step 500/703 lr 0.001000 loss 0.2033 (0.1685) acc@1 0.9375 (0.9405) acc@5 1.0000 (0.9988)\n",
      "\u001b[32m[2020-06-28 02:09:51] __main__ INFO: \u001b[0mEpoch 7 Step 600/703 lr 0.001000 loss 0.0517 (0.1678) acc@1 1.0000 (0.9410) acc@5 1.0000 (0.9988)\n",
      "\u001b[32m[2020-06-28 02:10:23] __main__ INFO: \u001b[0mEpoch 7 Step 700/703 lr 0.001000 loss 0.3827 (0.1692) acc@1 0.8906 (0.9407) acc@5 0.9844 (0.9988)\n",
      "\u001b[32m[2020-06-28 02:10:24] __main__ INFO: \u001b[0mEpoch 7 Step 703/703 lr 0.001000 loss 0.1838 (0.1692) acc@1 0.9062 (0.9407) acc@5 1.0000 (0.9988)\n",
      "\u001b[32m[2020-06-28 02:10:24] __main__ INFO: \u001b[0mElapsed 222.94\n",
      "\u001b[32m[2020-06-28 02:10:24] __main__ INFO: \u001b[0mVal 7\n",
      "\u001b[32m[2020-06-28 02:10:31] __main__ INFO: \u001b[0mEpoch 7 loss 0.3010 acc@1 0.9056 acc@5 0.9954\n",
      "\u001b[32m[2020-06-28 02:10:31] __main__ INFO: \u001b[0mElapsed 7.71\n",
      "\u001b[32m[2020-06-28 02:10:31] __main__ INFO: \u001b[0mTrain 8 4921\n",
      "\u001b[32m[2020-06-28 02:11:03] __main__ INFO: \u001b[0mEpoch 8 Step 100/703 lr 0.001000 loss 0.0954 (0.1547) acc@1 0.9688 (0.9461) acc@5 1.0000 (0.9989)\n",
      "\u001b[32m[2020-06-28 02:11:35] __main__ INFO: \u001b[0mEpoch 8 Step 200/703 lr 0.001000 loss 0.1285 (0.1558) acc@1 0.9375 (0.9463) acc@5 1.0000 (0.9988)\n",
      "\u001b[32m[2020-06-28 02:12:07] __main__ INFO: \u001b[0mEpoch 8 Step 300/703 lr 0.001000 loss 0.1258 (0.1555) acc@1 0.9531 (0.9454) acc@5 1.0000 (0.9990)\n",
      "\u001b[32m[2020-06-28 02:12:38] __main__ INFO: \u001b[0mEpoch 8 Step 400/703 lr 0.001000 loss 0.2328 (0.1589) acc@1 0.9219 (0.9441) acc@5 1.0000 (0.9989)\n",
      "\u001b[32m[2020-06-28 02:13:10] __main__ INFO: \u001b[0mEpoch 8 Step 500/703 lr 0.001000 loss 0.1555 (0.1579) acc@1 0.9531 (0.9444) acc@5 1.0000 (0.9989)\n",
      "\u001b[32m[2020-06-28 02:13:42] __main__ INFO: \u001b[0mEpoch 8 Step 600/703 lr 0.001000 loss 0.1329 (0.1575) acc@1 0.9531 (0.9445) acc@5 1.0000 (0.9990)\n",
      "\u001b[32m[2020-06-28 02:14:13] __main__ INFO: \u001b[0mEpoch 8 Step 700/703 lr 0.001000 loss 0.1873 (0.1573) acc@1 0.9219 (0.9446) acc@5 1.0000 (0.9990)\n",
      "\u001b[32m[2020-06-28 02:14:14] __main__ INFO: \u001b[0mEpoch 8 Step 703/703 lr 0.001000 loss 0.2287 (0.1578) acc@1 0.9219 (0.9445) acc@5 1.0000 (0.9990)\n",
      "\u001b[32m[2020-06-28 02:14:14] __main__ INFO: \u001b[0mElapsed 222.84\n",
      "\u001b[32m[2020-06-28 02:14:14] __main__ INFO: \u001b[0mVal 8\n",
      "\u001b[32m[2020-06-28 02:14:22] __main__ INFO: \u001b[0mEpoch 8 loss 0.3008 acc@1 0.9074 acc@5 0.9960\n",
      "\u001b[32m[2020-06-28 02:14:22] __main__ INFO: \u001b[0mElapsed 7.71\n",
      "\u001b[32m[2020-06-28 02:14:22] __main__ INFO: \u001b[0mTrain 9 5624\n",
      "\u001b[32m[2020-06-28 02:14:54] __main__ INFO: \u001b[0mEpoch 9 Step 100/703 lr 0.001000 loss 0.1624 (0.1425) acc@1 0.9375 (0.9503) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-06-28 02:15:26] __main__ INFO: \u001b[0mEpoch 9 Step 200/703 lr 0.001000 loss 0.2782 (0.1430) acc@1 0.9062 (0.9509) acc@5 1.0000 (0.9994)\n",
      "\u001b[32m[2020-06-28 02:15:57] __main__ INFO: \u001b[0mEpoch 9 Step 300/703 lr 0.001000 loss 0.1082 (0.1475) acc@1 0.9531 (0.9499) acc@5 1.0000 (0.9992)\n",
      "\u001b[32m[2020-06-28 02:16:29] __main__ INFO: \u001b[0mEpoch 9 Step 400/703 lr 0.001000 loss 0.1185 (0.1454) acc@1 0.9531 (0.9502) acc@5 1.0000 (0.9993)\n",
      "\u001b[32m[2020-06-28 02:17:01] __main__ INFO: \u001b[0mEpoch 9 Step 500/703 lr 0.001000 loss 0.0777 (0.1468) acc@1 0.9844 (0.9493) acc@5 1.0000 (0.9992)\n",
      "\u001b[32m[2020-06-28 02:17:32] __main__ INFO: \u001b[0mEpoch 9 Step 600/703 lr 0.001000 loss 0.0894 (0.1488) acc@1 0.9688 (0.9484) acc@5 1.0000 (0.9992)\n",
      "\u001b[32m[2020-06-28 02:18:04] __main__ INFO: \u001b[0mEpoch 9 Step 700/703 lr 0.001000 loss 0.1521 (0.1500) acc@1 0.9375 (0.9477) acc@5 1.0000 (0.9991)\n",
      "\u001b[32m[2020-06-28 02:18:05] __main__ INFO: \u001b[0mEpoch 9 Step 703/703 lr 0.001000 loss 0.1388 (0.1498) acc@1 0.9531 (0.9477) acc@5 1.0000 (0.9991)\n",
      "\u001b[32m[2020-06-28 02:18:05] __main__ INFO: \u001b[0mElapsed 223.09\n",
      "\u001b[32m[2020-06-28 02:18:05] __main__ INFO: \u001b[0mVal 9\n",
      "\u001b[32m[2020-06-28 02:18:13] __main__ INFO: \u001b[0mEpoch 9 loss 0.2982 acc@1 0.9056 acc@5 0.9950\n",
      "\u001b[32m[2020-06-28 02:18:13] __main__ INFO: \u001b[0mElapsed 7.72\n",
      "\u001b[32m[2020-06-28 02:18:13] __main__ INFO: \u001b[0mTrain 10 6327\n",
      "\u001b[32m[2020-06-28 02:18:45] __main__ INFO: \u001b[0mEpoch 10 Step 100/703 lr 0.001000 loss 0.0851 (0.1356) acc@1 0.9688 (0.9547) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-06-28 02:19:16] __main__ INFO: \u001b[0mEpoch 10 Step 200/703 lr 0.001000 loss 0.2117 (0.1368) acc@1 0.9531 (0.9537) acc@5 0.9844 (0.9993)\n",
      "\u001b[32m[2020-06-28 02:19:48] __main__ INFO: \u001b[0mEpoch 10 Step 300/703 lr 0.001000 loss 0.1392 (0.1382) acc@1 0.9375 (0.9518) acc@5 1.0000 (0.9993)\n",
      "\u001b[32m[2020-06-28 02:20:20] __main__ INFO: \u001b[0mEpoch 10 Step 400/703 lr 0.001000 loss 0.1142 (0.1372) acc@1 0.9375 (0.9521) acc@5 1.0000 (0.9993)\n",
      "\u001b[32m[2020-06-28 02:20:51] __main__ INFO: \u001b[0mEpoch 10 Step 500/703 lr 0.001000 loss 0.1139 (0.1382) acc@1 0.9219 (0.9519) acc@5 1.0000 (0.9992)\n",
      "\u001b[32m[2020-06-28 02:21:23] __main__ INFO: \u001b[0mEpoch 10 Step 600/703 lr 0.001000 loss 0.2460 (0.1402) acc@1 0.9219 (0.9509) acc@5 1.0000 (0.9992)\n",
      "\u001b[32m[2020-06-28 02:21:55] __main__ INFO: \u001b[0mEpoch 10 Step 700/703 lr 0.001000 loss 0.1623 (0.1407) acc@1 0.9531 (0.9509) acc@5 1.0000 (0.9992)\n",
      "\u001b[32m[2020-06-28 02:21:56] __main__ INFO: \u001b[0mEpoch 10 Step 703/703 lr 0.001000 loss 0.1933 (0.1409) acc@1 0.9219 (0.9508) acc@5 1.0000 (0.9992)\n",
      "\u001b[32m[2020-06-28 02:21:56] __main__ INFO: \u001b[0mElapsed 222.95\n",
      "\u001b[32m[2020-06-28 02:21:56] __main__ INFO: \u001b[0mVal 10\n",
      "\u001b[32m[2020-06-28 02:22:03] __main__ INFO: \u001b[0mEpoch 10 loss 0.2969 acc@1 0.9072 acc@5 0.9958\n",
      "\u001b[32m[2020-06-28 02:22:03] __main__ INFO: \u001b[0mElapsed 7.70\n",
      "\u001b[32m[2020-06-28 02:22:03] __main__ INFO: \u001b[0mTrain 11 7030\n",
      "\u001b[32m[2020-06-28 02:22:35] __main__ INFO: \u001b[0mEpoch 11 Step 100/703 lr 0.001000 loss 0.1396 (0.1392) acc@1 0.9219 (0.9527) acc@5 1.0000 (0.9984)\n",
      "\u001b[32m[2020-06-28 02:23:07] __main__ INFO: \u001b[0mEpoch 11 Step 200/703 lr 0.001000 loss 0.1265 (0.1395) acc@1 0.9844 (0.9534) acc@5 1.0000 (0.9990)\n",
      "\u001b[32m[2020-06-28 02:23:39] __main__ INFO: \u001b[0mEpoch 11 Step 300/703 lr 0.001000 loss 0.1879 (0.1394) acc@1 0.9531 (0.9521) acc@5 1.0000 (0.9992)\n",
      "\u001b[32m[2020-06-28 02:24:10] __main__ INFO: \u001b[0mEpoch 11 Step 400/703 lr 0.001000 loss 0.1194 (0.1379) acc@1 0.9375 (0.9529) acc@5 1.0000 (0.9992)\n",
      "\u001b[32m[2020-06-28 02:24:42] __main__ INFO: \u001b[0mEpoch 11 Step 500/703 lr 0.001000 loss 0.0888 (0.1372) acc@1 0.9688 (0.9535) acc@5 1.0000 (0.9992)\n",
      "\u001b[32m[2020-06-28 02:25:14] __main__ INFO: \u001b[0mEpoch 11 Step 600/703 lr 0.001000 loss 0.1385 (0.1353) acc@1 0.9375 (0.9542) acc@5 1.0000 (0.9993)\n",
      "\u001b[32m[2020-06-28 02:25:46] __main__ INFO: \u001b[0mEpoch 11 Step 700/703 lr 0.001000 loss 0.1511 (0.1362) acc@1 0.9531 (0.9540) acc@5 1.0000 (0.9993)\n",
      "\u001b[32m[2020-06-28 02:25:47] __main__ INFO: \u001b[0mEpoch 11 Step 703/703 lr 0.001000 loss 0.0839 (0.1361) acc@1 0.9844 (0.9540) acc@5 1.0000 (0.9993)\n",
      "\u001b[32m[2020-06-28 02:25:47] __main__ INFO: \u001b[0mElapsed 223.09\n",
      "\u001b[32m[2020-06-28 02:25:47] __main__ INFO: \u001b[0mVal 11\n",
      "\u001b[32m[2020-06-28 02:25:54] __main__ INFO: \u001b[0mEpoch 11 loss 0.2955 acc@1 0.9096 acc@5 0.9954\n",
      "\u001b[32m[2020-06-28 02:25:54] __main__ INFO: \u001b[0mElapsed 7.73\n",
      "\u001b[32m[2020-06-28 02:25:54] __main__ INFO: \u001b[0mTrain 12 7733\n",
      "\u001b[32m[2020-06-28 02:26:26] __main__ INFO: \u001b[0mEpoch 12 Step 100/703 lr 0.001000 loss 0.1716 (0.1265) acc@1 0.9375 (0.9561) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-06-28 02:26:58] __main__ INFO: \u001b[0mEpoch 12 Step 200/703 lr 0.001000 loss 0.1551 (0.1273) acc@1 0.9375 (0.9553) acc@5 1.0000 (0.9995)\n",
      "\u001b[32m[2020-06-28 02:27:30] __main__ INFO: \u001b[0mEpoch 12 Step 300/703 lr 0.001000 loss 0.1864 (0.1307) acc@1 0.9219 (0.9544) acc@5 1.0000 (0.9994)\n",
      "\u001b[32m[2020-06-28 02:28:01] __main__ INFO: \u001b[0mEpoch 12 Step 400/703 lr 0.001000 loss 0.1075 (0.1279) acc@1 0.9688 (0.9554) acc@5 1.0000 (0.9994)\n",
      "\u001b[32m[2020-06-28 02:28:33] __main__ INFO: \u001b[0mEpoch 12 Step 500/703 lr 0.001000 loss 0.1395 (0.1284) acc@1 0.9375 (0.9547) acc@5 1.0000 (0.9995)\n",
      "\u001b[32m[2020-06-28 02:29:05] __main__ INFO: \u001b[0mEpoch 12 Step 600/703 lr 0.001000 loss 0.0434 (0.1279) acc@1 1.0000 (0.9554) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-06-28 02:29:36] __main__ INFO: \u001b[0mEpoch 12 Step 700/703 lr 0.001000 loss 0.1876 (0.1270) acc@1 0.9375 (0.9554) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-06-28 02:29:37] __main__ INFO: \u001b[0mEpoch 12 Step 703/703 lr 0.001000 loss 0.0439 (0.1269) acc@1 1.0000 (0.9555) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-06-28 02:29:37] __main__ INFO: \u001b[0mElapsed 223.05\n",
      "\u001b[32m[2020-06-28 02:29:37] __main__ INFO: \u001b[0mVal 12\n",
      "\u001b[32m[2020-06-28 02:29:45] __main__ INFO: \u001b[0mEpoch 12 loss 0.3020 acc@1 0.9072 acc@5 0.9958\n",
      "\u001b[32m[2020-06-28 02:29:45] __main__ INFO: \u001b[0mElapsed 7.70\n",
      "\u001b[32m[2020-06-28 02:29:45] __main__ INFO: \u001b[0mTrain 13 8436\n",
      "\u001b[32m[2020-06-28 02:30:17] __main__ INFO: \u001b[0mEpoch 13 Step 100/703 lr 0.001000 loss 0.0629 (0.1200) acc@1 1.0000 (0.9577) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-28 02:30:49] __main__ INFO: \u001b[0mEpoch 13 Step 200/703 lr 0.001000 loss 0.1006 (0.1209) acc@1 0.9688 (0.9569) acc@5 1.0000 (0.9995)\n",
      "\u001b[32m[2020-06-28 02:31:20] __main__ INFO: \u001b[0mEpoch 13 Step 300/703 lr 0.001000 loss 0.0854 (0.1222) acc@1 0.9844 (0.9566) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-06-28 02:31:52] __main__ INFO: \u001b[0mEpoch 13 Step 400/703 lr 0.001000 loss 0.1388 (0.1210) acc@1 0.9219 (0.9567) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-06-28 02:32:24] __main__ INFO: \u001b[0mEpoch 13 Step 500/703 lr 0.001000 loss 0.1499 (0.1233) acc@1 0.9375 (0.9565) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-06-28 02:32:55] __main__ INFO: \u001b[0mEpoch 13 Step 600/703 lr 0.001000 loss 0.0403 (0.1209) acc@1 1.0000 (0.9577) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-06-28 02:33:27] __main__ INFO: \u001b[0mEpoch 13 Step 700/703 lr 0.001000 loss 0.1395 (0.1219) acc@1 0.9688 (0.9575) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-06-28 02:33:28] __main__ INFO: \u001b[0mEpoch 13 Step 703/703 lr 0.001000 loss 0.1197 (0.1220) acc@1 0.9531 (0.9575) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-06-28 02:33:28] __main__ INFO: \u001b[0mElapsed 222.94\n",
      "\u001b[32m[2020-06-28 02:33:28] __main__ INFO: \u001b[0mVal 13\n",
      "\u001b[32m[2020-06-28 02:33:36] __main__ INFO: \u001b[0mEpoch 13 loss 0.2951 acc@1 0.9082 acc@5 0.9954\n",
      "\u001b[32m[2020-06-28 02:33:36] __main__ INFO: \u001b[0mElapsed 7.71\n",
      "\u001b[32m[2020-06-28 02:33:36] __main__ INFO: \u001b[0mTrain 14 9139\n",
      "\u001b[32m[2020-06-28 02:34:07] __main__ INFO: \u001b[0mEpoch 14 Step 100/703 lr 0.001000 loss 0.2230 (0.1110) acc@1 0.9219 (0.9639) acc@5 0.9844 (0.9995)\n",
      "\u001b[32m[2020-06-28 02:34:39] __main__ INFO: \u001b[0mEpoch 14 Step 200/703 lr 0.001000 loss 0.1780 (0.1102) acc@1 0.9375 (0.9632) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-06-28 02:35:11] __main__ INFO: \u001b[0mEpoch 14 Step 300/703 lr 0.001000 loss 0.0606 (0.1139) acc@1 0.9844 (0.9614) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-06-28 02:35:43] __main__ INFO: \u001b[0mEpoch 14 Step 400/703 lr 0.001000 loss 0.0891 (0.1131) acc@1 0.9688 (0.9611) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-06-28 02:36:14] __main__ INFO: \u001b[0mEpoch 14 Step 500/703 lr 0.001000 loss 0.1149 (0.1143) acc@1 0.9375 (0.9603) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-06-28 02:36:46] __main__ INFO: \u001b[0mEpoch 14 Step 600/703 lr 0.001000 loss 0.0157 (0.1155) acc@1 1.0000 (0.9599) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-06-28 02:37:18] __main__ INFO: \u001b[0mEpoch 14 Step 700/703 lr 0.001000 loss 0.0772 (0.1152) acc@1 0.9688 (0.9599) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-06-28 02:37:19] __main__ INFO: \u001b[0mEpoch 14 Step 703/703 lr 0.001000 loss 0.1163 (0.1153) acc@1 0.9688 (0.9599) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-06-28 02:37:19] __main__ INFO: \u001b[0mElapsed 222.98\n",
      "\u001b[32m[2020-06-28 02:37:19] __main__ INFO: \u001b[0mVal 14\n",
      "\u001b[32m[2020-06-28 02:37:26] __main__ INFO: \u001b[0mEpoch 14 loss 0.2991 acc@1 0.9068 acc@5 0.9958\n",
      "\u001b[32m[2020-06-28 02:37:26] __main__ INFO: \u001b[0mElapsed 7.74\n",
      "\u001b[32m[2020-06-28 02:37:26] __main__ INFO: \u001b[0mTrain 15 9842\n",
      "\u001b[32m[2020-06-28 02:37:58] __main__ INFO: \u001b[0mEpoch 15 Step 100/703 lr 0.001000 loss 0.1460 (0.1037) acc@1 0.9531 (0.9645) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 02:38:30] __main__ INFO: \u001b[0mEpoch 15 Step 200/703 lr 0.001000 loss 0.2264 (0.1101) acc@1 0.9062 (0.9626) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-28 02:39:01] __main__ INFO: \u001b[0mEpoch 15 Step 300/703 lr 0.001000 loss 0.0551 (0.1099) acc@1 0.9844 (0.9639) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-06-28 02:39:33] __main__ INFO: \u001b[0mEpoch 15 Step 400/703 lr 0.001000 loss 0.0438 (0.1122) acc@1 1.0000 (0.9624) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-06-28 02:40:05] __main__ INFO: \u001b[0mEpoch 15 Step 500/703 lr 0.001000 loss 0.1713 (0.1128) acc@1 0.9531 (0.9625) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-28 02:40:36] __main__ INFO: \u001b[0mEpoch 15 Step 600/703 lr 0.001000 loss 0.0995 (0.1128) acc@1 0.9844 (0.9615) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-06-28 02:41:08] __main__ INFO: \u001b[0mEpoch 15 Step 700/703 lr 0.001000 loss 0.1305 (0.1139) acc@1 0.9531 (0.9609) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-06-28 02:41:09] __main__ INFO: \u001b[0mEpoch 15 Step 703/703 lr 0.001000 loss 0.0842 (0.1139) acc@1 0.9688 (0.9609) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-06-28 02:41:09] __main__ INFO: \u001b[0mElapsed 222.66\n",
      "\u001b[32m[2020-06-28 02:41:09] __main__ INFO: \u001b[0mVal 15\n",
      "\u001b[32m[2020-06-28 02:41:17] __main__ INFO: \u001b[0mEpoch 15 loss 0.2961 acc@1 0.9084 acc@5 0.9964\n",
      "\u001b[32m[2020-06-28 02:41:17] __main__ INFO: \u001b[0mElapsed 7.69\n",
      "\u001b[32m[2020-06-28 02:41:17] __main__ INFO: \u001b[0mTrain 16 10545\n",
      "\u001b[32m[2020-06-28 02:41:49] __main__ INFO: \u001b[0mEpoch 16 Step 100/703 lr 0.001000 loss 0.0745 (0.0994) acc@1 0.9688 (0.9678) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-06-28 02:42:20] __main__ INFO: \u001b[0mEpoch 16 Step 200/703 lr 0.001000 loss 0.1430 (0.1000) acc@1 0.9531 (0.9676) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-06-28 02:42:52] __main__ INFO: \u001b[0mEpoch 16 Step 300/703 lr 0.001000 loss 0.0795 (0.1009) acc@1 0.9531 (0.9666) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-06-28 02:43:23] __main__ INFO: \u001b[0mEpoch 16 Step 400/703 lr 0.001000 loss 0.1024 (0.1009) acc@1 0.9844 (0.9662) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-28 02:43:55] __main__ INFO: \u001b[0mEpoch 16 Step 500/703 lr 0.001000 loss 0.0302 (0.1033) acc@1 1.0000 (0.9649) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-28 02:44:27] __main__ INFO: \u001b[0mEpoch 16 Step 600/703 lr 0.001000 loss 0.0882 (0.1047) acc@1 0.9688 (0.9647) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-06-28 02:44:59] __main__ INFO: \u001b[0mEpoch 16 Step 700/703 lr 0.001000 loss 0.0401 (0.1039) acc@1 0.9844 (0.9651) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-06-28 02:45:00] __main__ INFO: \u001b[0mEpoch 16 Step 703/703 lr 0.001000 loss 0.1185 (0.1040) acc@1 0.9688 (0.9651) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-06-28 02:45:00] __main__ INFO: \u001b[0mElapsed 222.85\n",
      "\u001b[32m[2020-06-28 02:45:00] __main__ INFO: \u001b[0mVal 16\n",
      "\u001b[32m[2020-06-28 02:45:07] __main__ INFO: \u001b[0mEpoch 16 loss 0.3000 acc@1 0.9050 acc@5 0.9960\n",
      "\u001b[32m[2020-06-28 02:45:07] __main__ INFO: \u001b[0mElapsed 7.76\n",
      "\u001b[32m[2020-06-28 02:45:07] __main__ INFO: \u001b[0mTrain 17 11248\n",
      "\u001b[32m[2020-06-28 02:45:39] __main__ INFO: \u001b[0mEpoch 17 Step 100/703 lr 0.001000 loss 0.0855 (0.0989) acc@1 0.9844 (0.9653) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-06-28 02:46:11] __main__ INFO: \u001b[0mEpoch 17 Step 200/703 lr 0.001000 loss 0.0796 (0.1016) acc@1 0.9688 (0.9669) acc@5 1.0000 (0.9995)\n",
      "\u001b[32m[2020-06-28 02:46:42] __main__ INFO: \u001b[0mEpoch 17 Step 300/703 lr 0.001000 loss 0.0691 (0.0987) acc@1 0.9844 (0.9673) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-06-28 02:47:14] __main__ INFO: \u001b[0mEpoch 17 Step 400/703 lr 0.001000 loss 0.0556 (0.0999) acc@1 0.9844 (0.9660) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-06-28 02:47:46] __main__ INFO: \u001b[0mEpoch 17 Step 500/703 lr 0.001000 loss 0.1307 (0.0989) acc@1 0.9688 (0.9663) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-06-28 02:48:17] __main__ INFO: \u001b[0mEpoch 17 Step 600/703 lr 0.001000 loss 0.1212 (0.0993) acc@1 0.9375 (0.9664) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-06-28 02:48:49] __main__ INFO: \u001b[0mEpoch 17 Step 700/703 lr 0.001000 loss 0.1039 (0.1004) acc@1 0.9844 (0.9661) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-06-28 02:48:50] __main__ INFO: \u001b[0mEpoch 17 Step 703/703 lr 0.001000 loss 0.1491 (0.1005) acc@1 0.9531 (0.9661) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-06-28 02:48:50] __main__ INFO: \u001b[0mElapsed 222.67\n",
      "\u001b[32m[2020-06-28 02:48:50] __main__ INFO: \u001b[0mVal 17\n",
      "\u001b[32m[2020-06-28 02:48:58] __main__ INFO: \u001b[0mEpoch 17 loss 0.2991 acc@1 0.9080 acc@5 0.9956\n",
      "\u001b[32m[2020-06-28 02:48:58] __main__ INFO: \u001b[0mElapsed 7.70\n",
      "\u001b[32m[2020-06-28 02:48:58] __main__ INFO: \u001b[0mTrain 18 11951\n",
      "\u001b[32m[2020-06-28 02:49:30] __main__ INFO: \u001b[0mEpoch 18 Step 100/703 lr 0.001000 loss 0.1226 (0.0921) acc@1 0.9531 (0.9705) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-06-28 02:50:01] __main__ INFO: \u001b[0mEpoch 18 Step 200/703 lr 0.001000 loss 0.1117 (0.0957) acc@1 0.9375 (0.9688) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-28 02:50:33] __main__ INFO: \u001b[0mEpoch 18 Step 300/703 lr 0.001000 loss 0.0803 (0.0955) acc@1 0.9844 (0.9695) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-28 02:51:05] __main__ INFO: \u001b[0mEpoch 18 Step 400/703 lr 0.001000 loss 0.1958 (0.0968) acc@1 0.9375 (0.9685) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-28 02:51:36] __main__ INFO: \u001b[0mEpoch 18 Step 500/703 lr 0.001000 loss 0.0903 (0.0959) acc@1 0.9688 (0.9687) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-28 02:52:08] __main__ INFO: \u001b[0mEpoch 18 Step 600/703 lr 0.001000 loss 0.0622 (0.0958) acc@1 0.9688 (0.9688) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-28 02:52:40] __main__ INFO: \u001b[0mEpoch 18 Step 700/703 lr 0.001000 loss 0.1055 (0.0962) acc@1 0.9688 (0.9683) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 02:52:41] __main__ INFO: \u001b[0mEpoch 18 Step 703/703 lr 0.001000 loss 0.2033 (0.0962) acc@1 0.9375 (0.9683) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 02:52:41] __main__ INFO: \u001b[0mElapsed 222.98\n",
      "\u001b[32m[2020-06-28 02:52:41] __main__ INFO: \u001b[0mVal 18\n",
      "\u001b[32m[2020-06-28 02:52:48] __main__ INFO: \u001b[0mEpoch 18 loss 0.2985 acc@1 0.9104 acc@5 0.9958\n",
      "\u001b[32m[2020-06-28 02:52:48] __main__ INFO: \u001b[0mElapsed 7.71\n",
      "\u001b[32m[2020-06-28 02:52:48] __main__ INFO: \u001b[0mTrain 19 12654\n",
      "\u001b[32m[2020-06-28 02:53:20] __main__ INFO: \u001b[0mEpoch 19 Step 100/703 lr 0.001000 loss 0.0572 (0.0981) acc@1 0.9844 (0.9670) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 02:53:52] __main__ INFO: \u001b[0mEpoch 19 Step 200/703 lr 0.001000 loss 0.1088 (0.0931) acc@1 0.9688 (0.9698) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-28 02:54:23] __main__ INFO: \u001b[0mEpoch 19 Step 300/703 lr 0.001000 loss 0.0913 (0.0916) acc@1 0.9531 (0.9694) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-06-28 02:54:55] __main__ INFO: \u001b[0mEpoch 19 Step 400/703 lr 0.001000 loss 0.0797 (0.0936) acc@1 0.9688 (0.9683) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-06-28 02:55:27] __main__ INFO: \u001b[0mEpoch 19 Step 500/703 lr 0.001000 loss 0.0422 (0.0940) acc@1 0.9844 (0.9678) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-06-28 02:55:58] __main__ INFO: \u001b[0mEpoch 19 Step 600/703 lr 0.001000 loss 0.0415 (0.0944) acc@1 0.9844 (0.9679) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-06-28 02:56:30] __main__ INFO: \u001b[0mEpoch 19 Step 700/703 lr 0.001000 loss 0.0788 (0.0927) acc@1 0.9688 (0.9686) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-06-28 02:56:31] __main__ INFO: \u001b[0mEpoch 19 Step 703/703 lr 0.001000 loss 0.0174 (0.0926) acc@1 1.0000 (0.9686) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-06-28 02:56:31] __main__ INFO: \u001b[0mElapsed 222.55\n",
      "\u001b[32m[2020-06-28 02:56:31] __main__ INFO: \u001b[0mVal 19\n",
      "\u001b[32m[2020-06-28 02:56:39] __main__ INFO: \u001b[0mEpoch 19 loss 0.2976 acc@1 0.9092 acc@5 0.9960\n",
      "\u001b[32m[2020-06-28 02:56:39] __main__ INFO: \u001b[0mElapsed 7.73\n",
      "\u001b[32m[2020-06-28 02:56:39] __main__ INFO: \u001b[0mTrain 20 13357\n",
      "\u001b[32m[2020-06-28 02:57:10] __main__ INFO: \u001b[0mEpoch 20 Step 100/703 lr 0.001000 loss 0.1846 (0.0847) acc@1 0.9375 (0.9712) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-06-28 02:57:42] __main__ INFO: \u001b[0mEpoch 20 Step 200/703 lr 0.001000 loss 0.0620 (0.0861) acc@1 0.9688 (0.9712) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-28 02:58:14] __main__ INFO: \u001b[0mEpoch 20 Step 300/703 lr 0.001000 loss 0.0410 (0.0863) acc@1 0.9844 (0.9703) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-06-28 02:58:45] __main__ INFO: \u001b[0mEpoch 20 Step 400/703 lr 0.001000 loss 0.0150 (0.0873) acc@1 1.0000 (0.9698) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-06-28 02:59:17] __main__ INFO: \u001b[0mEpoch 20 Step 500/703 lr 0.001000 loss 0.1077 (0.0877) acc@1 0.9844 (0.9699) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-06-28 02:59:49] __main__ INFO: \u001b[0mEpoch 20 Step 600/703 lr 0.001000 loss 0.2526 (0.0881) acc@1 0.9219 (0.9699) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-06-28 03:00:20] __main__ INFO: \u001b[0mEpoch 20 Step 700/703 lr 0.001000 loss 0.0866 (0.0878) acc@1 0.9688 (0.9700) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-06-28 03:00:21] __main__ INFO: \u001b[0mEpoch 20 Step 703/703 lr 0.001000 loss 0.1136 (0.0878) acc@1 0.9688 (0.9701) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-06-28 03:00:21] __main__ INFO: \u001b[0mElapsed 222.62\n",
      "\u001b[32m[2020-06-28 03:00:21] __main__ INFO: \u001b[0mVal 20\n",
      "\u001b[32m[2020-06-28 03:00:29] __main__ INFO: \u001b[0mEpoch 20 loss 0.2976 acc@1 0.9122 acc@5 0.9954\n",
      "\u001b[32m[2020-06-28 03:00:29] __main__ INFO: \u001b[0mElapsed 7.73\n",
      "\u001b[32m[2020-06-28 03:00:29] __main__ INFO: \u001b[0mTrain 21 14060\n",
      "\u001b[32m[2020-06-28 03:01:01] __main__ INFO: \u001b[0mEpoch 21 Step 100/703 lr 0.001000 loss 0.1072 (0.0821) acc@1 0.9688 (0.9745) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-06-28 03:01:32] __main__ INFO: \u001b[0mEpoch 21 Step 200/703 lr 0.001000 loss 0.0492 (0.0821) acc@1 0.9844 (0.9722) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-28 03:02:04] __main__ INFO: \u001b[0mEpoch 21 Step 300/703 lr 0.001000 loss 0.0526 (0.0827) acc@1 0.9844 (0.9719) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-28 03:02:36] __main__ INFO: \u001b[0mEpoch 21 Step 400/703 lr 0.001000 loss 0.0421 (0.0861) acc@1 0.9844 (0.9704) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-28 03:03:07] __main__ INFO: \u001b[0mEpoch 21 Step 500/703 lr 0.001000 loss 0.0818 (0.0848) acc@1 0.9688 (0.9710) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-28 03:03:39] __main__ INFO: \u001b[0mEpoch 21 Step 600/703 lr 0.001000 loss 0.0290 (0.0848) acc@1 1.0000 (0.9710) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-28 03:04:11] __main__ INFO: \u001b[0mEpoch 21 Step 700/703 lr 0.001000 loss 0.0787 (0.0852) acc@1 0.9844 (0.9708) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-28 03:04:12] __main__ INFO: \u001b[0mEpoch 21 Step 703/703 lr 0.001000 loss 0.0173 (0.0851) acc@1 1.0000 (0.9708) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-28 03:04:12] __main__ INFO: \u001b[0mElapsed 222.79\n",
      "\u001b[32m[2020-06-28 03:04:12] __main__ INFO: \u001b[0mVal 21\n",
      "\u001b[32m[2020-06-28 03:04:20] __main__ INFO: \u001b[0mEpoch 21 loss 0.3052 acc@1 0.9096 acc@5 0.9956\n",
      "\u001b[32m[2020-06-28 03:04:20] __main__ INFO: \u001b[0mElapsed 7.75\n",
      "\u001b[32m[2020-06-28 03:04:20] __main__ INFO: \u001b[0mTrain 22 14763\n",
      "\u001b[32m[2020-06-28 03:04:51] __main__ INFO: \u001b[0mEpoch 22 Step 100/703 lr 0.001000 loss 0.0473 (0.0781) acc@1 0.9844 (0.9753) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 03:05:23] __main__ INFO: \u001b[0mEpoch 22 Step 200/703 lr 0.001000 loss 0.0330 (0.0788) acc@1 0.9844 (0.9733) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-28 03:05:55] __main__ INFO: \u001b[0mEpoch 22 Step 300/703 lr 0.001000 loss 0.0530 (0.0777) acc@1 0.9844 (0.9736) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-28 03:06:26] __main__ INFO: \u001b[0mEpoch 22 Step 400/703 lr 0.001000 loss 0.1215 (0.0789) acc@1 0.9375 (0.9730) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-28 03:06:58] __main__ INFO: \u001b[0mEpoch 22 Step 500/703 lr 0.001000 loss 0.1305 (0.0793) acc@1 0.9688 (0.9726) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-28 03:07:30] __main__ INFO: \u001b[0mEpoch 22 Step 600/703 lr 0.001000 loss 0.0790 (0.0788) acc@1 0.9688 (0.9731) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-28 03:08:01] __main__ INFO: \u001b[0mEpoch 22 Step 700/703 lr 0.001000 loss 0.0745 (0.0805) acc@1 0.9844 (0.9727) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-28 03:08:02] __main__ INFO: \u001b[0mEpoch 22 Step 703/703 lr 0.001000 loss 0.1156 (0.0806) acc@1 0.9531 (0.9726) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-28 03:08:02] __main__ INFO: \u001b[0mElapsed 222.75\n",
      "\u001b[32m[2020-06-28 03:08:02] __main__ INFO: \u001b[0mVal 22\n",
      "\u001b[32m[2020-06-28 03:08:10] __main__ INFO: \u001b[0mEpoch 22 loss 0.3031 acc@1 0.9114 acc@5 0.9954\n",
      "\u001b[32m[2020-06-28 03:08:10] __main__ INFO: \u001b[0mElapsed 7.73\n",
      "\u001b[32m[2020-06-28 03:08:10] __main__ INFO: \u001b[0mTrain 23 15466\n",
      "\u001b[32m[2020-06-28 03:08:42] __main__ INFO: \u001b[0mEpoch 23 Step 100/703 lr 0.001000 loss 0.0777 (0.0761) acc@1 0.9844 (0.9770) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-06-28 03:09:14] __main__ INFO: \u001b[0mEpoch 23 Step 200/703 lr 0.001000 loss 0.0932 (0.0779) acc@1 0.9688 (0.9755) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-28 03:09:45] __main__ INFO: \u001b[0mEpoch 23 Step 300/703 lr 0.001000 loss 0.0538 (0.0766) acc@1 0.9844 (0.9747) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-28 03:10:17] __main__ INFO: \u001b[0mEpoch 23 Step 400/703 lr 0.001000 loss 0.0847 (0.0787) acc@1 0.9688 (0.9740) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 03:10:48] __main__ INFO: \u001b[0mEpoch 23 Step 500/703 lr 0.001000 loss 0.1294 (0.0781) acc@1 0.9688 (0.9742) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 03:11:20] __main__ INFO: \u001b[0mEpoch 23 Step 600/703 lr 0.001000 loss 0.0846 (0.0775) acc@1 0.9688 (0.9743) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-28 03:11:52] __main__ INFO: \u001b[0mEpoch 23 Step 700/703 lr 0.001000 loss 0.0436 (0.0786) acc@1 0.9844 (0.9739) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-28 03:11:53] __main__ INFO: \u001b[0mEpoch 23 Step 703/703 lr 0.001000 loss 0.0516 (0.0787) acc@1 0.9844 (0.9739) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-28 03:11:53] __main__ INFO: \u001b[0mElapsed 222.83\n",
      "\u001b[32m[2020-06-28 03:11:53] __main__ INFO: \u001b[0mVal 23\n",
      "\u001b[32m[2020-06-28 03:12:01] __main__ INFO: \u001b[0mEpoch 23 loss 0.3048 acc@1 0.9108 acc@5 0.9962\n",
      "\u001b[32m[2020-06-28 03:12:01] __main__ INFO: \u001b[0mElapsed 7.72\n",
      "\u001b[32m[2020-06-28 03:12:01] __main__ INFO: \u001b[0mTrain 24 16169\n",
      "\u001b[32m[2020-06-28 03:12:32] __main__ INFO: \u001b[0mEpoch 24 Step 100/703 lr 0.001000 loss 0.0771 (0.0713) acc@1 0.9688 (0.9756) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 03:13:04] __main__ INFO: \u001b[0mEpoch 24 Step 200/703 lr 0.001000 loss 0.0633 (0.0722) acc@1 0.9688 (0.9771) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-28 03:13:36] __main__ INFO: \u001b[0mEpoch 24 Step 300/703 lr 0.001000 loss 0.0993 (0.0711) acc@1 0.9531 (0.9773) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-28 03:14:07] __main__ INFO: \u001b[0mEpoch 24 Step 400/703 lr 0.001000 loss 0.0925 (0.0735) acc@1 0.9531 (0.9759) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-28 03:14:39] __main__ INFO: \u001b[0mEpoch 24 Step 500/703 lr 0.001000 loss 0.0353 (0.0739) acc@1 1.0000 (0.9762) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-28 03:15:11] __main__ INFO: \u001b[0mEpoch 24 Step 600/703 lr 0.001000 loss 0.0837 (0.0747) acc@1 0.9531 (0.9759) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-28 03:15:42] __main__ INFO: \u001b[0mEpoch 24 Step 700/703 lr 0.001000 loss 0.0697 (0.0741) acc@1 0.9844 (0.9760) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 03:15:43] __main__ INFO: \u001b[0mEpoch 24 Step 703/703 lr 0.001000 loss 0.0459 (0.0742) acc@1 0.9688 (0.9759) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 03:15:43] __main__ INFO: \u001b[0mElapsed 222.82\n",
      "\u001b[32m[2020-06-28 03:15:43] __main__ INFO: \u001b[0mVal 24\n",
      "\u001b[32m[2020-06-28 03:15:51] __main__ INFO: \u001b[0mEpoch 24 loss 0.3024 acc@1 0.9136 acc@5 0.9964\n",
      "\u001b[32m[2020-06-28 03:15:51] __main__ INFO: \u001b[0mElapsed 7.70\n",
      "\u001b[32m[2020-06-28 03:15:51] __main__ INFO: \u001b[0mTrain 25 16872\n",
      "\u001b[32m[2020-06-28 03:16:23] __main__ INFO: \u001b[0mEpoch 25 Step 100/703 lr 0.001000 loss 0.1029 (0.0688) acc@1 0.9531 (0.9766) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-28 03:16:55] __main__ INFO: \u001b[0mEpoch 25 Step 200/703 lr 0.001000 loss 0.1116 (0.0680) acc@1 0.9531 (0.9775) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 03:17:26] __main__ INFO: \u001b[0mEpoch 25 Step 300/703 lr 0.001000 loss 0.0494 (0.0708) acc@1 1.0000 (0.9760) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 03:17:58] __main__ INFO: \u001b[0mEpoch 25 Step 400/703 lr 0.001000 loss 0.0874 (0.0702) acc@1 0.9844 (0.9766) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 03:18:30] __main__ INFO: \u001b[0mEpoch 25 Step 500/703 lr 0.001000 loss 0.0627 (0.0704) acc@1 0.9688 (0.9763) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 03:19:01] __main__ INFO: \u001b[0mEpoch 25 Step 600/703 lr 0.001000 loss 0.0580 (0.0713) acc@1 0.9844 (0.9759) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 03:19:33] __main__ INFO: \u001b[0mEpoch 25 Step 700/703 lr 0.001000 loss 0.0787 (0.0720) acc@1 0.9531 (0.9757) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 03:19:34] __main__ INFO: \u001b[0mEpoch 25 Step 703/703 lr 0.001000 loss 0.1483 (0.0723) acc@1 0.9375 (0.9756) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 03:19:34] __main__ INFO: \u001b[0mElapsed 222.61\n",
      "\u001b[32m[2020-06-28 03:19:34] __main__ INFO: \u001b[0mVal 25\n",
      "\u001b[32m[2020-06-28 03:19:42] __main__ INFO: \u001b[0mEpoch 25 loss 0.3027 acc@1 0.9140 acc@5 0.9952\n",
      "\u001b[32m[2020-06-28 03:19:42] __main__ INFO: \u001b[0mElapsed 7.72\n",
      "\u001b[32m[2020-06-28 03:19:42] __main__ INFO: \u001b[0mTrain 26 17575\n",
      "\u001b[32m[2020-06-28 03:20:13] __main__ INFO: \u001b[0mEpoch 26 Step 100/703 lr 0.001000 loss 0.0403 (0.0626) acc@1 0.9844 (0.9778) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-28 03:20:45] __main__ INFO: \u001b[0mEpoch 26 Step 200/703 lr 0.001000 loss 0.0733 (0.0646) acc@1 0.9688 (0.9777) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 03:21:17] __main__ INFO: \u001b[0mEpoch 26 Step 300/703 lr 0.001000 loss 0.1111 (0.0666) acc@1 0.9531 (0.9778) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 03:21:48] __main__ INFO: \u001b[0mEpoch 26 Step 400/703 lr 0.001000 loss 0.0546 (0.0672) acc@1 0.9844 (0.9780) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 03:22:20] __main__ INFO: \u001b[0mEpoch 26 Step 500/703 lr 0.001000 loss 0.0938 (0.0679) acc@1 0.9844 (0.9780) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 03:22:52] __main__ INFO: \u001b[0mEpoch 26 Step 600/703 lr 0.001000 loss 0.0702 (0.0692) acc@1 0.9688 (0.9769) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 03:23:23] __main__ INFO: \u001b[0mEpoch 26 Step 700/703 lr 0.001000 loss 0.0541 (0.0695) acc@1 0.9844 (0.9766) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 03:23:24] __main__ INFO: \u001b[0mEpoch 26 Step 703/703 lr 0.001000 loss 0.1185 (0.0696) acc@1 0.9688 (0.9767) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 03:23:24] __main__ INFO: \u001b[0mElapsed 222.78\n",
      "\u001b[32m[2020-06-28 03:23:24] __main__ INFO: \u001b[0mVal 26\n",
      "\u001b[32m[2020-06-28 03:23:32] __main__ INFO: \u001b[0mEpoch 26 loss 0.3053 acc@1 0.9112 acc@5 0.9960\n",
      "\u001b[32m[2020-06-28 03:23:32] __main__ INFO: \u001b[0mElapsed 7.70\n",
      "\u001b[32m[2020-06-28 03:23:32] __main__ INFO: \u001b[0mTrain 27 18278\n",
      "\u001b[32m[2020-06-28 03:24:04] __main__ INFO: \u001b[0mEpoch 27 Step 100/703 lr 0.001000 loss 0.0275 (0.0580) acc@1 1.0000 (0.9834) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 03:24:35] __main__ INFO: \u001b[0mEpoch 27 Step 200/703 lr 0.001000 loss 0.0723 (0.0643) acc@1 0.9844 (0.9805) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-28 03:25:07] __main__ INFO: \u001b[0mEpoch 27 Step 300/703 lr 0.001000 loss 0.1154 (0.0648) acc@1 0.9844 (0.9796) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-28 03:25:39] __main__ INFO: \u001b[0mEpoch 27 Step 400/703 lr 0.001000 loss 0.0864 (0.0652) acc@1 0.9688 (0.9789) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 03:26:11] __main__ INFO: \u001b[0mEpoch 27 Step 500/703 lr 0.001000 loss 0.0357 (0.0659) acc@1 0.9844 (0.9788) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 03:26:42] __main__ INFO: \u001b[0mEpoch 27 Step 600/703 lr 0.001000 loss 0.1485 (0.0667) acc@1 0.9531 (0.9783) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 03:27:14] __main__ INFO: \u001b[0mEpoch 27 Step 700/703 lr 0.001000 loss 0.0263 (0.0678) acc@1 1.0000 (0.9777) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 03:27:15] __main__ INFO: \u001b[0mEpoch 27 Step 703/703 lr 0.001000 loss 0.0454 (0.0678) acc@1 1.0000 (0.9777) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 03:27:15] __main__ INFO: \u001b[0mElapsed 222.95\n",
      "\u001b[32m[2020-06-28 03:27:15] __main__ INFO: \u001b[0mVal 27\n",
      "\u001b[32m[2020-06-28 03:27:23] __main__ INFO: \u001b[0mEpoch 27 loss 0.3079 acc@1 0.9130 acc@5 0.9962\n",
      "\u001b[32m[2020-06-28 03:27:23] __main__ INFO: \u001b[0mElapsed 7.75\n",
      "\u001b[32m[2020-06-28 03:27:23] __main__ INFO: \u001b[0mTrain 28 18981\n",
      "\u001b[32m[2020-06-28 03:27:54] __main__ INFO: \u001b[0mEpoch 28 Step 100/703 lr 0.001000 loss 0.0661 (0.0664) acc@1 0.9844 (0.9783) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-06-28 03:28:26] __main__ INFO: \u001b[0mEpoch 28 Step 200/703 lr 0.001000 loss 0.0580 (0.0646) acc@1 0.9844 (0.9787) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-28 03:28:58] __main__ INFO: \u001b[0mEpoch 28 Step 300/703 lr 0.001000 loss 0.0621 (0.0622) acc@1 0.9844 (0.9803) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 03:29:29] __main__ INFO: \u001b[0mEpoch 28 Step 400/703 lr 0.001000 loss 0.0428 (0.0625) acc@1 0.9844 (0.9799) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 03:30:01] __main__ INFO: \u001b[0mEpoch 28 Step 500/703 lr 0.001000 loss 0.0453 (0.0631) acc@1 1.0000 (0.9798) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 03:30:33] __main__ INFO: \u001b[0mEpoch 28 Step 600/703 lr 0.001000 loss 0.1361 (0.0635) acc@1 0.9688 (0.9795) acc@5 0.9844 (0.9999)\n",
      "\u001b[32m[2020-06-28 03:31:05] __main__ INFO: \u001b[0mEpoch 28 Step 700/703 lr 0.001000 loss 0.0740 (0.0638) acc@1 0.9531 (0.9794) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 03:31:06] __main__ INFO: \u001b[0mEpoch 28 Step 703/703 lr 0.001000 loss 0.0565 (0.0639) acc@1 0.9844 (0.9793) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 03:31:06] __main__ INFO: \u001b[0mElapsed 222.93\n",
      "\u001b[32m[2020-06-28 03:31:06] __main__ INFO: \u001b[0mVal 28\n",
      "\u001b[32m[2020-06-28 03:31:13] __main__ INFO: \u001b[0mEpoch 28 loss 0.3016 acc@1 0.9124 acc@5 0.9962\n",
      "\u001b[32m[2020-06-28 03:31:13] __main__ INFO: \u001b[0mElapsed 7.72\n",
      "\u001b[32m[2020-06-28 03:31:13] __main__ INFO: \u001b[0mTrain 29 19684\n",
      "\u001b[32m[2020-06-28 03:31:45] __main__ INFO: \u001b[0mEpoch 29 Step 100/703 lr 0.001000 loss 0.0659 (0.0562) acc@1 0.9844 (0.9831) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 03:32:17] __main__ INFO: \u001b[0mEpoch 29 Step 200/703 lr 0.001000 loss 0.1204 (0.0590) acc@1 0.9375 (0.9813) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 03:32:48] __main__ INFO: \u001b[0mEpoch 29 Step 300/703 lr 0.001000 loss 0.1454 (0.0594) acc@1 0.9375 (0.9807) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 03:33:20] __main__ INFO: \u001b[0mEpoch 29 Step 400/703 lr 0.001000 loss 0.0908 (0.0594) acc@1 0.9688 (0.9812) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 03:33:52] __main__ INFO: \u001b[0mEpoch 29 Step 500/703 lr 0.001000 loss 0.0536 (0.0595) acc@1 0.9844 (0.9812) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-28 03:34:23] __main__ INFO: \u001b[0mEpoch 29 Step 600/703 lr 0.001000 loss 0.1520 (0.0606) acc@1 0.9375 (0.9809) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-28 03:34:55] __main__ INFO: \u001b[0mEpoch 29 Step 700/703 lr 0.001000 loss 0.0237 (0.0607) acc@1 1.0000 (0.9808) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-28 03:34:56] __main__ INFO: \u001b[0mEpoch 29 Step 703/703 lr 0.001000 loss 0.1001 (0.0608) acc@1 0.9688 (0.9808) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-28 03:34:56] __main__ INFO: \u001b[0mElapsed 222.82\n",
      "\u001b[32m[2020-06-28 03:34:56] __main__ INFO: \u001b[0mVal 29\n",
      "\u001b[32m[2020-06-28 03:35:04] __main__ INFO: \u001b[0mEpoch 29 loss 0.3098 acc@1 0.9116 acc@5 0.9958\n",
      "\u001b[32m[2020-06-28 03:35:04] __main__ INFO: \u001b[0mElapsed 7.71\n",
      "\u001b[32m[2020-06-28 03:35:04] __main__ INFO: \u001b[0mTrain 30 20387\n",
      "\u001b[32m[2020-06-28 03:35:36] __main__ INFO: \u001b[0mEpoch 30 Step 100/703 lr 0.001000 loss 0.1097 (0.0579) acc@1 0.9531 (0.9820) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-28 03:36:07] __main__ INFO: \u001b[0mEpoch 30 Step 200/703 lr 0.001000 loss 0.0981 (0.0580) acc@1 0.9531 (0.9815) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-28 03:36:39] __main__ INFO: \u001b[0mEpoch 30 Step 300/703 lr 0.001000 loss 0.0613 (0.0567) acc@1 0.9844 (0.9821) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-28 03:37:11] __main__ INFO: \u001b[0mEpoch 30 Step 400/703 lr 0.001000 loss 0.0420 (0.0569) acc@1 1.0000 (0.9820) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 03:37:42] __main__ INFO: \u001b[0mEpoch 30 Step 500/703 lr 0.001000 loss 0.1045 (0.0575) acc@1 0.9688 (0.9819) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 03:38:14] __main__ INFO: \u001b[0mEpoch 30 Step 600/703 lr 0.001000 loss 0.0246 (0.0578) acc@1 0.9844 (0.9814) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 03:38:46] __main__ INFO: \u001b[0mEpoch 30 Step 700/703 lr 0.001000 loss 0.0832 (0.0592) acc@1 0.9844 (0.9810) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 03:38:47] __main__ INFO: \u001b[0mEpoch 30 Step 703/703 lr 0.001000 loss 0.1182 (0.0593) acc@1 0.9531 (0.9810) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 03:38:47] __main__ INFO: \u001b[0mElapsed 223.01\n",
      "\u001b[32m[2020-06-28 03:38:47] __main__ INFO: \u001b[0mVal 30\n",
      "\u001b[32m[2020-06-28 03:38:55] __main__ INFO: \u001b[0mEpoch 30 loss 0.3119 acc@1 0.9116 acc@5 0.9956\n",
      "\u001b[32m[2020-06-28 03:38:55] __main__ INFO: \u001b[0mElapsed 7.77\n",
      "\u001b[32m[2020-06-28 03:38:55] __main__ INFO: \u001b[0mTrain 31 21090\n",
      "\u001b[32m[2020-06-28 03:39:26] __main__ INFO: \u001b[0mEpoch 31 Step 100/703 lr 0.001000 loss 0.0426 (0.0516) acc@1 0.9688 (0.9858) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 03:39:58] __main__ INFO: \u001b[0mEpoch 31 Step 200/703 lr 0.001000 loss 0.0761 (0.0530) acc@1 0.9688 (0.9852) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 03:40:30] __main__ INFO: \u001b[0mEpoch 31 Step 300/703 lr 0.001000 loss 0.0550 (0.0551) acc@1 0.9844 (0.9835) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 03:41:01] __main__ INFO: \u001b[0mEpoch 31 Step 400/703 lr 0.001000 loss 0.0437 (0.0546) acc@1 1.0000 (0.9834) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 03:41:33] __main__ INFO: \u001b[0mEpoch 31 Step 500/703 lr 0.001000 loss 0.0804 (0.0547) acc@1 0.9844 (0.9832) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 03:42:05] __main__ INFO: \u001b[0mEpoch 31 Step 600/703 lr 0.001000 loss 0.0767 (0.0557) acc@1 0.9688 (0.9828) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 03:42:36] __main__ INFO: \u001b[0mEpoch 31 Step 700/703 lr 0.001000 loss 0.0402 (0.0552) acc@1 1.0000 (0.9830) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 03:42:37] __main__ INFO: \u001b[0mEpoch 31 Step 703/703 lr 0.001000 loss 0.0169 (0.0551) acc@1 1.0000 (0.9830) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 03:42:37] __main__ INFO: \u001b[0mElapsed 222.60\n",
      "\u001b[32m[2020-06-28 03:42:37] __main__ INFO: \u001b[0mVal 31\n",
      "\u001b[32m[2020-06-28 03:42:45] __main__ INFO: \u001b[0mEpoch 31 loss 0.3115 acc@1 0.9116 acc@5 0.9960\n",
      "\u001b[32m[2020-06-28 03:42:45] __main__ INFO: \u001b[0mElapsed 7.72\n",
      "\u001b[32m[2020-06-28 03:42:45] __main__ INFO: \u001b[0mTrain 32 21793\n",
      "\u001b[32m[2020-06-28 03:43:17] __main__ INFO: \u001b[0mEpoch 32 Step 100/703 lr 0.001000 loss 0.0716 (0.0501) acc@1 0.9531 (0.9853) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-28 03:43:49] __main__ INFO: \u001b[0mEpoch 32 Step 200/703 lr 0.001000 loss 0.0852 (0.0510) acc@1 0.9688 (0.9848) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 03:44:20] __main__ INFO: \u001b[0mEpoch 32 Step 300/703 lr 0.001000 loss 0.0785 (0.0532) acc@1 0.9688 (0.9837) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 03:44:52] __main__ INFO: \u001b[0mEpoch 32 Step 400/703 lr 0.001000 loss 0.0245 (0.0523) acc@1 1.0000 (0.9838) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 03:45:23] __main__ INFO: \u001b[0mEpoch 32 Step 500/703 lr 0.001000 loss 0.0588 (0.0534) acc@1 0.9844 (0.9832) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 03:45:55] __main__ INFO: \u001b[0mEpoch 32 Step 600/703 lr 0.001000 loss 0.0503 (0.0528) acc@1 0.9688 (0.9834) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 03:46:27] __main__ INFO: \u001b[0mEpoch 32 Step 700/703 lr 0.001000 loss 0.0328 (0.0534) acc@1 1.0000 (0.9831) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 03:46:28] __main__ INFO: \u001b[0mEpoch 32 Step 703/703 lr 0.001000 loss 0.0343 (0.0533) acc@1 1.0000 (0.9832) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 03:46:28] __main__ INFO: \u001b[0mElapsed 222.80\n",
      "\u001b[32m[2020-06-28 03:46:28] __main__ INFO: \u001b[0mVal 32\n",
      "\u001b[32m[2020-06-28 03:46:36] __main__ INFO: \u001b[0mEpoch 32 loss 0.3141 acc@1 0.9116 acc@5 0.9956\n",
      "\u001b[32m[2020-06-28 03:46:36] __main__ INFO: \u001b[0mElapsed 7.75\n",
      "\u001b[32m[2020-06-28 03:46:36] __main__ INFO: \u001b[0mTrain 33 22496\n",
      "\u001b[32m[2020-06-28 03:47:07] __main__ INFO: \u001b[0mEpoch 33 Step 100/703 lr 0.001000 loss 0.0285 (0.0507) acc@1 1.0000 (0.9853) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 03:47:39] __main__ INFO: \u001b[0mEpoch 33 Step 200/703 lr 0.001000 loss 0.0233 (0.0511) acc@1 1.0000 (0.9846) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 03:48:11] __main__ INFO: \u001b[0mEpoch 33 Step 300/703 lr 0.001000 loss 0.0797 (0.0530) acc@1 0.9688 (0.9833) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 03:48:42] __main__ INFO: \u001b[0mEpoch 33 Step 400/703 lr 0.001000 loss 0.0955 (0.0533) acc@1 0.9531 (0.9832) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 03:49:14] __main__ INFO: \u001b[0mEpoch 33 Step 500/703 lr 0.001000 loss 0.0847 (0.0539) acc@1 0.9688 (0.9828) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 03:49:46] __main__ INFO: \u001b[0mEpoch 33 Step 600/703 lr 0.001000 loss 0.0524 (0.0548) acc@1 0.9844 (0.9824) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 03:50:17] __main__ INFO: \u001b[0mEpoch 33 Step 700/703 lr 0.001000 loss 0.0484 (0.0551) acc@1 0.9844 (0.9822) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 03:50:18] __main__ INFO: \u001b[0mEpoch 33 Step 703/703 lr 0.001000 loss 0.0492 (0.0550) acc@1 0.9844 (0.9822) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 03:50:18] __main__ INFO: \u001b[0mElapsed 222.77\n",
      "\u001b[32m[2020-06-28 03:50:18] __main__ INFO: \u001b[0mVal 33\n",
      "\u001b[32m[2020-06-28 03:50:26] __main__ INFO: \u001b[0mEpoch 33 loss 0.3130 acc@1 0.9120 acc@5 0.9956\n",
      "\u001b[32m[2020-06-28 03:50:26] __main__ INFO: \u001b[0mElapsed 7.68\n",
      "\u001b[32m[2020-06-28 03:50:26] __main__ INFO: \u001b[0mTrain 34 23199\n",
      "\u001b[32m[2020-06-28 03:50:58] __main__ INFO: \u001b[0mEpoch 34 Step 100/703 lr 0.001000 loss 0.1365 (0.0536) acc@1 0.9375 (0.9830) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 03:51:30] __main__ INFO: \u001b[0mEpoch 34 Step 200/703 lr 0.001000 loss 0.0373 (0.0519) acc@1 0.9844 (0.9833) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 03:52:01] __main__ INFO: \u001b[0mEpoch 34 Step 300/703 lr 0.001000 loss 0.0466 (0.0507) acc@1 0.9688 (0.9836) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 03:52:33] __main__ INFO: \u001b[0mEpoch 34 Step 400/703 lr 0.001000 loss 0.0735 (0.0508) acc@1 0.9688 (0.9838) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 03:53:05] __main__ INFO: \u001b[0mEpoch 34 Step 500/703 lr 0.001000 loss 0.0740 (0.0510) acc@1 0.9688 (0.9838) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 03:53:37] __main__ INFO: \u001b[0mEpoch 34 Step 600/703 lr 0.001000 loss 0.0199 (0.0519) acc@1 1.0000 (0.9835) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 03:54:08] __main__ INFO: \u001b[0mEpoch 34 Step 700/703 lr 0.001000 loss 0.0738 (0.0518) acc@1 0.9688 (0.9838) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 03:54:09] __main__ INFO: \u001b[0mEpoch 34 Step 703/703 lr 0.001000 loss 0.1674 (0.0520) acc@1 0.9375 (0.9837) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 03:54:09] __main__ INFO: \u001b[0mElapsed 223.30\n",
      "\u001b[32m[2020-06-28 03:54:09] __main__ INFO: \u001b[0mVal 34\n",
      "\u001b[32m[2020-06-28 03:54:17] __main__ INFO: \u001b[0mEpoch 34 loss 0.3169 acc@1 0.9150 acc@5 0.9958\n",
      "\u001b[32m[2020-06-28 03:54:17] __main__ INFO: \u001b[0mElapsed 7.74\n",
      "\u001b[32m[2020-06-28 03:54:17] __main__ INFO: \u001b[0mTrain 35 23902\n",
      "\u001b[32m[2020-06-28 03:54:49] __main__ INFO: \u001b[0mEpoch 35 Step 100/703 lr 0.001000 loss 0.0182 (0.0498) acc@1 1.0000 (0.9844) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 03:55:21] __main__ INFO: \u001b[0mEpoch 35 Step 200/703 lr 0.001000 loss 0.0243 (0.0509) acc@1 1.0000 (0.9841) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 03:55:52] __main__ INFO: \u001b[0mEpoch 35 Step 300/703 lr 0.001000 loss 0.0229 (0.0511) acc@1 1.0000 (0.9839) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 03:56:24] __main__ INFO: \u001b[0mEpoch 35 Step 400/703 lr 0.001000 loss 0.0200 (0.0503) acc@1 1.0000 (0.9842) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 03:56:56] __main__ INFO: \u001b[0mEpoch 35 Step 500/703 lr 0.001000 loss 0.1274 (0.0499) acc@1 0.9688 (0.9843) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 03:57:27] __main__ INFO: \u001b[0mEpoch 35 Step 600/703 lr 0.001000 loss 0.0484 (0.0493) acc@1 0.9844 (0.9848) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 03:57:59] __main__ INFO: \u001b[0mEpoch 35 Step 700/703 lr 0.001000 loss 0.0450 (0.0501) acc@1 0.9844 (0.9846) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 03:58:00] __main__ INFO: \u001b[0mEpoch 35 Step 703/703 lr 0.001000 loss 0.0534 (0.0501) acc@1 1.0000 (0.9846) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 03:58:00] __main__ INFO: \u001b[0mElapsed 222.98\n",
      "\u001b[32m[2020-06-28 03:58:00] __main__ INFO: \u001b[0mVal 35\n",
      "\u001b[32m[2020-06-28 03:58:08] __main__ INFO: \u001b[0mEpoch 35 loss 0.3181 acc@1 0.9108 acc@5 0.9958\n",
      "\u001b[32m[2020-06-28 03:58:08] __main__ INFO: \u001b[0mElapsed 7.71\n",
      "\u001b[32m[2020-06-28 03:58:08] __main__ INFO: \u001b[0mTrain 36 24605\n",
      "\u001b[32m[2020-06-28 03:58:39] __main__ INFO: \u001b[0mEpoch 36 Step 100/703 lr 0.001000 loss 0.0263 (0.0440) acc@1 0.9844 (0.9858) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-28 03:59:11] __main__ INFO: \u001b[0mEpoch 36 Step 200/703 lr 0.001000 loss 0.0152 (0.0451) acc@1 1.0000 (0.9860) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 03:59:43] __main__ INFO: \u001b[0mEpoch 36 Step 300/703 lr 0.001000 loss 0.0590 (0.0453) acc@1 0.9688 (0.9853) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 04:00:14] __main__ INFO: \u001b[0mEpoch 36 Step 400/703 lr 0.001000 loss 0.0477 (0.0471) acc@1 0.9688 (0.9848) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 04:00:46] __main__ INFO: \u001b[0mEpoch 36 Step 500/703 lr 0.001000 loss 0.0316 (0.0473) acc@1 1.0000 (0.9848) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 04:01:18] __main__ INFO: \u001b[0mEpoch 36 Step 600/703 lr 0.001000 loss 0.0342 (0.0470) acc@1 1.0000 (0.9851) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 04:01:50] __main__ INFO: \u001b[0mEpoch 36 Step 700/703 lr 0.001000 loss 0.0304 (0.0479) acc@1 0.9844 (0.9848) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 04:01:51] __main__ INFO: \u001b[0mEpoch 36 Step 703/703 lr 0.001000 loss 0.0309 (0.0478) acc@1 0.9844 (0.9848) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 04:01:51] __main__ INFO: \u001b[0mElapsed 222.86\n",
      "\u001b[32m[2020-06-28 04:01:51] __main__ INFO: \u001b[0mVal 36\n",
      "\u001b[32m[2020-06-28 04:01:58] __main__ INFO: \u001b[0mEpoch 36 loss 0.3186 acc@1 0.9126 acc@5 0.9954\n",
      "\u001b[32m[2020-06-28 04:01:58] __main__ INFO: \u001b[0mElapsed 7.73\n",
      "\u001b[32m[2020-06-28 04:01:58] __main__ INFO: \u001b[0mTrain 37 25308\n",
      "\u001b[32m[2020-06-28 04:02:30] __main__ INFO: \u001b[0mEpoch 37 Step 100/703 lr 0.001000 loss 0.0096 (0.0431) acc@1 1.0000 (0.9878) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 04:03:02] __main__ INFO: \u001b[0mEpoch 37 Step 200/703 lr 0.001000 loss 0.1100 (0.0451) acc@1 0.9531 (0.9870) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 04:03:33] __main__ INFO: \u001b[0mEpoch 37 Step 300/703 lr 0.001000 loss 0.0274 (0.0462) acc@1 1.0000 (0.9860) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 04:04:05] __main__ INFO: \u001b[0mEpoch 37 Step 400/703 lr 0.001000 loss 0.1129 (0.0455) acc@1 0.9531 (0.9864) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 04:04:37] __main__ INFO: \u001b[0mEpoch 37 Step 500/703 lr 0.001000 loss 0.0190 (0.0454) acc@1 0.9844 (0.9865) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 04:05:08] __main__ INFO: \u001b[0mEpoch 37 Step 600/703 lr 0.001000 loss 0.0895 (0.0457) acc@1 0.9531 (0.9865) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 04:05:40] __main__ INFO: \u001b[0mEpoch 37 Step 700/703 lr 0.001000 loss 0.1168 (0.0461) acc@1 0.9688 (0.9864) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 04:05:41] __main__ INFO: \u001b[0mEpoch 37 Step 703/703 lr 0.001000 loss 0.0892 (0.0461) acc@1 0.9531 (0.9864) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 04:05:41] __main__ INFO: \u001b[0mElapsed 222.81\n",
      "\u001b[32m[2020-06-28 04:05:41] __main__ INFO: \u001b[0mVal 37\n",
      "\u001b[32m[2020-06-28 04:05:49] __main__ INFO: \u001b[0mEpoch 37 loss 0.3201 acc@1 0.9158 acc@5 0.9958\n",
      "\u001b[32m[2020-06-28 04:05:49] __main__ INFO: \u001b[0mElapsed 7.70\n",
      "\u001b[32m[2020-06-28 04:05:49] __main__ INFO: \u001b[0mTrain 38 26011\n",
      "\u001b[32m[2020-06-28 04:06:21] __main__ INFO: \u001b[0mEpoch 38 Step 100/703 lr 0.001000 loss 0.0284 (0.0450) acc@1 0.9844 (0.9872) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 04:06:52] __main__ INFO: \u001b[0mEpoch 38 Step 200/703 lr 0.001000 loss 0.0235 (0.0443) acc@1 1.0000 (0.9870) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 04:07:24] __main__ INFO: \u001b[0mEpoch 38 Step 300/703 lr 0.001000 loss 0.0538 (0.0448) acc@1 1.0000 (0.9867) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 04:07:56] __main__ INFO: \u001b[0mEpoch 38 Step 400/703 lr 0.001000 loss 0.0279 (0.0457) acc@1 1.0000 (0.9862) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 04:08:27] __main__ INFO: \u001b[0mEpoch 38 Step 500/703 lr 0.001000 loss 0.0579 (0.0452) acc@1 0.9688 (0.9862) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 04:08:59] __main__ INFO: \u001b[0mEpoch 38 Step 600/703 lr 0.001000 loss 0.0468 (0.0459) acc@1 0.9844 (0.9860) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 04:09:31] __main__ INFO: \u001b[0mEpoch 38 Step 700/703 lr 0.001000 loss 0.0543 (0.0459) acc@1 0.9844 (0.9862) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 04:09:32] __main__ INFO: \u001b[0mEpoch 38 Step 703/703 lr 0.001000 loss 0.0721 (0.0459) acc@1 0.9844 (0.9862) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 04:09:32] __main__ INFO: \u001b[0mElapsed 222.84\n",
      "\u001b[32m[2020-06-28 04:09:32] __main__ INFO: \u001b[0mVal 38\n",
      "\u001b[32m[2020-06-28 04:09:39] __main__ INFO: \u001b[0mEpoch 38 loss 0.3180 acc@1 0.9152 acc@5 0.9958\n",
      "\u001b[32m[2020-06-28 04:09:39] __main__ INFO: \u001b[0mElapsed 7.70\n",
      "\u001b[32m[2020-06-28 04:09:39] __main__ INFO: \u001b[0mTrain 39 26714\n",
      "\u001b[32m[2020-06-28 04:10:11] __main__ INFO: \u001b[0mEpoch 39 Step 100/703 lr 0.001000 loss 0.0123 (0.0383) acc@1 1.0000 (0.9886) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 04:10:43] __main__ INFO: \u001b[0mEpoch 39 Step 200/703 lr 0.001000 loss 0.0541 (0.0379) acc@1 0.9688 (0.9891) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 04:11:15] __main__ INFO: \u001b[0mEpoch 39 Step 300/703 lr 0.001000 loss 0.0169 (0.0392) acc@1 1.0000 (0.9884) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 04:11:46] __main__ INFO: \u001b[0mEpoch 39 Step 400/703 lr 0.001000 loss 0.0453 (0.0402) acc@1 0.9844 (0.9882) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 04:12:18] __main__ INFO: \u001b[0mEpoch 39 Step 500/703 lr 0.001000 loss 0.0769 (0.0417) acc@1 0.9844 (0.9877) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 04:12:49] __main__ INFO: \u001b[0mEpoch 39 Step 600/703 lr 0.001000 loss 0.0251 (0.0418) acc@1 0.9844 (0.9878) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 04:13:21] __main__ INFO: \u001b[0mEpoch 39 Step 700/703 lr 0.001000 loss 0.0144 (0.0419) acc@1 1.0000 (0.9879) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 04:13:22] __main__ INFO: \u001b[0mEpoch 39 Step 703/703 lr 0.001000 loss 0.0265 (0.0419) acc@1 1.0000 (0.9880) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-28 04:13:22] __main__ INFO: \u001b[0mElapsed 222.61\n",
      "\u001b[32m[2020-06-28 04:13:22] __main__ INFO: \u001b[0mVal 39\n",
      "\u001b[32m[2020-06-28 04:13:30] __main__ INFO: \u001b[0mEpoch 39 loss 0.3205 acc@1 0.9128 acc@5 0.9958\n",
      "\u001b[32m[2020-06-28 04:13:30] __main__ INFO: \u001b[0mElapsed 7.68\n",
      "\u001b[32m[2020-06-28 04:13:30] __main__ INFO: \u001b[0mTrain 40 27417\n",
      "\u001b[32m[2020-06-28 04:14:01] __main__ INFO: \u001b[0mEpoch 40 Step 100/703 lr 0.001000 loss 0.0303 (0.0399) acc@1 1.0000 (0.9862) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 04:14:33] __main__ INFO: \u001b[0mEpoch 40 Step 200/703 lr 0.001000 loss 0.0243 (0.0404) acc@1 0.9844 (0.9868) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 04:15:05] __main__ INFO: \u001b[0mEpoch 40 Step 300/703 lr 0.001000 loss 0.0307 (0.0415) acc@1 0.9844 (0.9864) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 04:15:36] __main__ INFO: \u001b[0mEpoch 40 Step 400/703 lr 0.001000 loss 0.0634 (0.0429) acc@1 0.9844 (0.9861) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 04:16:08] __main__ INFO: \u001b[0mEpoch 40 Step 500/703 lr 0.001000 loss 0.0385 (0.0432) acc@1 0.9844 (0.9861) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 04:16:40] __main__ INFO: \u001b[0mEpoch 40 Step 600/703 lr 0.001000 loss 0.0672 (0.0433) acc@1 0.9844 (0.9866) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 04:17:12] __main__ INFO: \u001b[0mEpoch 40 Step 700/703 lr 0.001000 loss 0.0269 (0.0432) acc@1 0.9844 (0.9866) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 04:17:12] __main__ INFO: \u001b[0mEpoch 40 Step 703/703 lr 0.001000 loss 0.0318 (0.0432) acc@1 0.9844 (0.9866) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 04:17:12] __main__ INFO: \u001b[0mElapsed 222.78\n",
      "\u001b[32m[2020-06-28 04:17:12] __main__ INFO: \u001b[0mVal 40\n",
      "\u001b[32m[2020-06-28 04:17:20] __main__ INFO: \u001b[0mEpoch 40 loss 0.3247 acc@1 0.9162 acc@5 0.9962\n",
      "\u001b[32m[2020-06-28 04:17:20] __main__ INFO: \u001b[0mElapsed 7.69\n",
      "\u001b[32m[2020-06-28 04:17:20] __main__ INFO: \u001b[0mTrain 41 28120\n",
      "\u001b[32m[2020-06-28 04:17:52] __main__ INFO: \u001b[0mEpoch 41 Step 100/703 lr 0.001000 loss 0.0865 (0.0361) acc@1 0.9844 (0.9898) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 04:18:24] __main__ INFO: \u001b[0mEpoch 41 Step 200/703 lr 0.001000 loss 0.0739 (0.0362) acc@1 0.9844 (0.9903) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 04:18:55] __main__ INFO: \u001b[0mEpoch 41 Step 300/703 lr 0.001000 loss 0.0375 (0.0380) acc@1 0.9688 (0.9895) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 04:19:27] __main__ INFO: \u001b[0mEpoch 41 Step 400/703 lr 0.001000 loss 0.0144 (0.0387) acc@1 1.0000 (0.9891) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 04:19:59] __main__ INFO: \u001b[0mEpoch 41 Step 500/703 lr 0.001000 loss 0.0697 (0.0385) acc@1 0.9688 (0.9892) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 04:20:30] __main__ INFO: \u001b[0mEpoch 41 Step 600/703 lr 0.001000 loss 0.0646 (0.0391) acc@1 0.9844 (0.9889) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 04:21:02] __main__ INFO: \u001b[0mEpoch 41 Step 700/703 lr 0.001000 loss 0.0190 (0.0388) acc@1 1.0000 (0.9892) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 04:21:03] __main__ INFO: \u001b[0mEpoch 41 Step 703/703 lr 0.001000 loss 0.0475 (0.0389) acc@1 0.9844 (0.9892) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 04:21:03] __main__ INFO: \u001b[0mElapsed 222.87\n",
      "\u001b[32m[2020-06-28 04:21:03] __main__ INFO: \u001b[0mVal 41\n",
      "\u001b[32m[2020-06-28 04:21:11] __main__ INFO: \u001b[0mEpoch 41 loss 0.3249 acc@1 0.9122 acc@5 0.9956\n",
      "\u001b[32m[2020-06-28 04:21:11] __main__ INFO: \u001b[0mElapsed 7.70\n",
      "\u001b[32m[2020-06-28 04:21:11] __main__ INFO: \u001b[0mTrain 42 28823\n",
      "\u001b[32m[2020-06-28 04:21:43] __main__ INFO: \u001b[0mEpoch 42 Step 100/703 lr 0.001000 loss 0.0203 (0.0398) acc@1 1.0000 (0.9881) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 04:22:14] __main__ INFO: \u001b[0mEpoch 42 Step 200/703 lr 0.001000 loss 0.0211 (0.0404) acc@1 1.0000 (0.9875) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 04:22:46] __main__ INFO: \u001b[0mEpoch 42 Step 300/703 lr 0.001000 loss 0.0651 (0.0397) acc@1 0.9688 (0.9877) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 04:23:18] __main__ INFO: \u001b[0mEpoch 42 Step 400/703 lr 0.001000 loss 0.0160 (0.0392) acc@1 1.0000 (0.9883) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 04:23:49] __main__ INFO: \u001b[0mEpoch 42 Step 500/703 lr 0.001000 loss 0.0173 (0.0402) acc@1 1.0000 (0.9880) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 04:24:21] __main__ INFO: \u001b[0mEpoch 42 Step 600/703 lr 0.001000 loss 0.0103 (0.0404) acc@1 1.0000 (0.9877) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 04:24:53] __main__ INFO: \u001b[0mEpoch 42 Step 700/703 lr 0.001000 loss 0.0268 (0.0404) acc@1 1.0000 (0.9877) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 04:24:54] __main__ INFO: \u001b[0mEpoch 42 Step 703/703 lr 0.001000 loss 0.0364 (0.0404) acc@1 0.9844 (0.9877) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 04:24:54] __main__ INFO: \u001b[0mElapsed 222.83\n",
      "\u001b[32m[2020-06-28 04:24:54] __main__ INFO: \u001b[0mVal 42\n",
      "\u001b[32m[2020-06-28 04:25:01] __main__ INFO: \u001b[0mEpoch 42 loss 0.3260 acc@1 0.9112 acc@5 0.9954\n",
      "\u001b[32m[2020-06-28 04:25:01] __main__ INFO: \u001b[0mElapsed 7.72\n",
      "\u001b[32m[2020-06-28 04:25:01] __main__ INFO: \u001b[0mTrain 43 29526\n",
      "\u001b[32m[2020-06-28 04:25:33] __main__ INFO: \u001b[0mEpoch 43 Step 100/703 lr 0.001000 loss 0.0184 (0.0361) acc@1 1.0000 (0.9905) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 04:26:05] __main__ INFO: \u001b[0mEpoch 43 Step 200/703 lr 0.001000 loss 0.0525 (0.0368) acc@1 0.9844 (0.9897) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 04:26:36] __main__ INFO: \u001b[0mEpoch 43 Step 300/703 lr 0.001000 loss 0.0537 (0.0393) acc@1 0.9844 (0.9887) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 04:27:08] __main__ INFO: \u001b[0mEpoch 43 Step 400/703 lr 0.001000 loss 0.0553 (0.0395) acc@1 0.9844 (0.9886) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 04:27:40] __main__ INFO: \u001b[0mEpoch 43 Step 500/703 lr 0.001000 loss 0.0196 (0.0397) acc@1 1.0000 (0.9886) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 04:28:11] __main__ INFO: \u001b[0mEpoch 43 Step 600/703 lr 0.001000 loss 0.0655 (0.0401) acc@1 0.9844 (0.9880) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 04:28:43] __main__ INFO: \u001b[0mEpoch 43 Step 700/703 lr 0.001000 loss 0.0646 (0.0400) acc@1 0.9688 (0.9882) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 04:28:44] __main__ INFO: \u001b[0mEpoch 43 Step 703/703 lr 0.001000 loss 0.0128 (0.0399) acc@1 1.0000 (0.9882) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 04:28:44] __main__ INFO: \u001b[0mElapsed 222.59\n",
      "\u001b[32m[2020-06-28 04:28:44] __main__ INFO: \u001b[0mVal 43\n",
      "\u001b[32m[2020-06-28 04:28:52] __main__ INFO: \u001b[0mEpoch 43 loss 0.3249 acc@1 0.9160 acc@5 0.9958\n",
      "\u001b[32m[2020-06-28 04:28:52] __main__ INFO: \u001b[0mElapsed 7.70\n",
      "\u001b[32m[2020-06-28 04:28:52] __main__ INFO: \u001b[0mTrain 44 30229\n",
      "\u001b[32m[2020-06-28 04:29:23] __main__ INFO: \u001b[0mEpoch 44 Step 100/703 lr 0.001000 loss 0.0493 (0.0361) acc@1 0.9844 (0.9889) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 04:29:55] __main__ INFO: \u001b[0mEpoch 44 Step 200/703 lr 0.001000 loss 0.0118 (0.0377) acc@1 1.0000 (0.9887) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 04:30:27] __main__ INFO: \u001b[0mEpoch 44 Step 300/703 lr 0.001000 loss 0.0176 (0.0370) acc@1 1.0000 (0.9890) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-28 04:30:58] __main__ INFO: \u001b[0mEpoch 44 Step 400/703 lr 0.001000 loss 0.0363 (0.0373) acc@1 1.0000 (0.9891) acc@5 1.0000 (1.0000)\n"
     ]
    }
   ],
   "source": [
    "# Resume training with the un-augmented data\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "!python train.py --config /home/ec2-user/SageMaker/experiments/densenet_BC_100_12_ra_2_20/exp00/config.yaml \\\n",
    "    train.checkpoint /home/ec2-user/SageMaker/experiments/densenet_BC_100_12_ra_2_20/exp00/checkpoint_00400.pth \\\n",
    "    dataset.name CIFAR10 \\\n",
    "    train.base_lr .001 \\\n",
    "    train.output_dir /home/ec2-user/SageMaker/experiments/densenet_BC_100_12_ra_2_20/exp00_resume400_50 \\\n",
    "    scheduler.epochs 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-28 13:09:27] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/densenet_BC_100_12_ra_2_20/exp00_resume400_50/checkpoint_00050.pth\n",
      "Files already downloaded and verified\n",
      "100%|| 157/157 [00:19<00:00,  8.09it/s]\n",
      "\u001b[32m[2020-06-28 13:09:47] __main__ INFO: \u001b[0mElapsed 19.41\n",
      "\u001b[32m[2020-06-28 13:09:47] __main__ INFO: \u001b[0mLoss 0.3217 Accuracy 0.9168\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10 test dataset \n",
    "# Write the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/densenet.yaml \\\n",
    "   model.densenet.depth 100 \\\n",
    "   model.densenet.growth_rate 12 \\\n",
    "   test.batch_size 64 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/densenet_BC_100_12_ra_2_20/exp00_resume400_50/checkpoint_00050.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/densenet_BC_100_12_ra_2_20/exp00_resume400_50/test_results_0050_cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-28 13:10:47] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/densenet_BC_100_12_ra_2_20/exp00/checkpoint_00400.pth\n",
      "Files already downloaded and verified\n",
      "100%|| 157/157 [00:19<00:00,  8.06it/s]\n",
      "\u001b[32m[2020-06-28 13:11:08] __main__ INFO: \u001b[0mElapsed 19.48\n",
      "\u001b[32m[2020-06-28 13:11:08] __main__ INFO: \u001b[0mLoss 1.1176 Accuracy 0.7900\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10 test dataset \n",
    "# Write the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/densenet.yaml \\\n",
    "   model.densenet.depth 100 \\\n",
    "   model.densenet.growth_rate 12 \\\n",
    "   test.batch_size 64 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/densenet_BC_100_12_ra_2_20/exp00/checkpoint_00400.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/densenet_BC_100_12_ra_2_20/exp00/test_results_0400_cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-28 13:11:58] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/densenet_BC_100_12_ra_2_20/exp00_resume400_50/checkpoint_00050.pth\n",
      "CIFAR 10.1\n",
      "100%|| 32/32 [00:04<00:00,  7.57it/s]\n",
      "\u001b[32m[2020-06-28 13:12:02] __main__ INFO: \u001b[0mElapsed 4.23\n",
      "\u001b[32m[2020-06-28 13:12:02] __main__ INFO: \u001b[0mLoss 0.6475 Accuracy 0.8290\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10 test dataset \n",
    "# Write the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/densenet.yaml \\\n",
    "   model.densenet.depth 100 \\\n",
    "   model.densenet.growth_rate 12 \\\n",
    "   dataset.name CIFAR101 \\\n",
    "   test.batch_size 64 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/densenet_BC_100_12_ra_2_20/exp00_resume400_50/checkpoint_00050.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/densenet_BC_100_12_ra_2_20/exp00_resume400_50/test_results_0050_cifar101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-28 13:12:56] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/densenet_BC_100_12_ra_2_20/exp00/checkpoint_00400.pth\n",
      "CIFAR 10.1\n",
      "100%|| 32/32 [00:04<00:00,  7.67it/s]\n",
      "\u001b[32m[2020-06-28 13:13:01] __main__ INFO: \u001b[0mElapsed 4.18\n",
      "\u001b[32m[2020-06-28 13:13:01] __main__ INFO: \u001b[0mLoss 2.0759 Accuracy 0.6390\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10.1 test dataset \n",
    "# Write the results to the test output directory specified.\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "!python evaluate.py --config configs/cifar/densenet.yaml \\\n",
    "    model.densenet.depth 100 \\\n",
    "    model.densenet.growth_rate 12 \\\n",
    "    test.batch_size 64 \\\n",
    "    dataset.name CIFAR101 \\\n",
    "    test.checkpoint /home/ec2-user/SageMaker/experiments/densenet_BC_100_12_ra_2_20/exp00/checkpoint_00400.pth \\\n",
    "    test.output_dir /home/ec2-user/SageMaker/experiments/densenet_BC_100_12_ra_2_20/exp00/test_results_0400_cifar101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-28 13:13:46] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/densenet_BC_100_12_ra_2_20/exp00/checkpoint_00300.pth\n",
      "Files already downloaded and verified\n",
      "100%|| 157/157 [00:19<00:00,  8.03it/s]\n",
      "\u001b[32m[2020-06-28 13:14:06] __main__ INFO: \u001b[0mElapsed 19.54\n",
      "\u001b[32m[2020-06-28 13:14:06] __main__ INFO: \u001b[0mLoss 1.0739 Accuracy 0.7854\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10 test dataset \n",
    "# Write the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/densenet.yaml \\\n",
    "   model.densenet.depth 100 \\\n",
    "   model.densenet.growth_rate 12 \\\n",
    "   test.batch_size 64 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/densenet_BC_100_12_ra_2_20/exp00/checkpoint_00300.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/densenet_BC_100_12_ra_2_20/exp00/test_results_0300_cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-28 13:14:22] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/densenet_BC_100_12_ra_2_20/exp00/checkpoint_00300.pth\n",
      "CIFAR 10.1\n",
      "100%|| 32/32 [00:04<00:00,  7.62it/s]\n",
      "\u001b[32m[2020-06-28 13:14:27] __main__ INFO: \u001b[0mElapsed 4.20\n",
      "\u001b[32m[2020-06-28 13:14:27] __main__ INFO: \u001b[0mLoss 1.9997 Accuracy 0.6275\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10 test dataset \n",
    "# Write the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/densenet.yaml \\\n",
    "   model.densenet.depth 100 \\\n",
    "   model.densenet.growth_rate 12 \\\n",
    "   test.batch_size 64 \\\n",
    "   dataset.name CIFAR101 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/densenet_BC_100_12_ra_2_20/exp00/checkpoint_00300.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/densenet_BC_100_12_ra_2_20/exp00/test_results_0300_cifar101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the results to a CSV file so that we can analyze later.\n",
    "import pandas as pd\n",
    "\n",
    "results = {'Model': ['densenet_BC_100_12', 'densenet_BC_100_12', 'densenet_BC_100_12', 'densenet_BC_100_12'],\n",
    "           'Testset': ['cifar10', 'cifar10', 'cifar10', 'cifar10.1'],\n",
    "           'Epoch': [100, 200, 300, 300],\n",
    "           'Loss': [0.3681, 0.2279, 0.2223, 0.5098],\n",
    "           'Accuracy': [0.8875, 0.9456, 0.9484, 0.8830],\n",
    "           'Original_Accuracy': [95.5, 95.5, 95.5, 87.6],\n",
    "           'Original_CI': [(95.1, 95.9), (95.1, 95.9), (95.1, 95.9), (86.1, 89.0)]\n",
    "           }\n",
    "\n",
    "df = pd.DataFrame(results, columns = ['Model', 'Testset', 'Epoch', 'Loss', 'Accuracy', \n",
    "                                      'Original_Accuracy', 'Original_CI'])\n",
    "\n",
    "\n",
    "df.to_csv('/home/ec2-user/SageMaker/experiments/densenet_BC_100_12/exp00/results.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Testset</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Original_Accuracy</th>\n",
       "      <th>Original_CI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>densenet_BC_100_12_ra_2_20</td>\n",
       "      <td>400</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>1.1176</td>\n",
       "      <td>0.79</td>\n",
       "      <td>95.5</td>\n",
       "      <td>(95.1, 95.9)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>densenet_BC_100_12_ra_2_20</td>\n",
       "      <td>300</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>1.0739</td>\n",
       "      <td>0.7854</td>\n",
       "      <td>95.5</td>\n",
       "      <td>(95.1, 95.9)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>densenet_BC_100_12_ra_2_20</td>\n",
       "      <td>400</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>2.0759</td>\n",
       "      <td>0.639</td>\n",
       "      <td>87.6</td>\n",
       "      <td>(86.1, 89.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>densenet_BC_100_12_ra_2_20</td>\n",
       "      <td>300</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>1.9997</td>\n",
       "      <td>0.6275</td>\n",
       "      <td>87.6</td>\n",
       "      <td>(86.1, 89.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>densenet_BC_100_12_ra_2_20_refined400</td>\n",
       "      <td>50</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>0.6475</td>\n",
       "      <td>0.829</td>\n",
       "      <td>87.6</td>\n",
       "      <td>(86.1, 89.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>densenet_BC_100_12_ra_2_20_refined400</td>\n",
       "      <td>50</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.3217</td>\n",
       "      <td>0.9168</td>\n",
       "      <td>95.5</td>\n",
       "      <td>(95.1, 95.9)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Model Epoch    Testset    Loss Accuracy  \\\n",
       "0             densenet_BC_100_12_ra_2_20   400    cifar10  1.1176     0.79   \n",
       "1             densenet_BC_100_12_ra_2_20   300    cifar10  1.0739   0.7854   \n",
       "2             densenet_BC_100_12_ra_2_20   400  cifar10.1  2.0759    0.639   \n",
       "3             densenet_BC_100_12_ra_2_20   300  cifar10.1  1.9997   0.6275   \n",
       "4  densenet_BC_100_12_ra_2_20_refined400    50  cifar10.1  0.6475    0.829   \n",
       "5  densenet_BC_100_12_ra_2_20_refined400    50    cifar10  0.3217   0.9168   \n",
       "\n",
       "   Original_Accuracy   Original_CI  \n",
       "0               95.5  (95.1, 95.9)  \n",
       "1               95.5  (95.1, 95.9)  \n",
       "2               87.6  (86.1, 89.0)  \n",
       "3               87.6  (86.1, 89.0)  \n",
       "4               87.6  (86.1, 89.0)  \n",
       "5               95.5  (95.1, 95.9)  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "a = pd.Series(['densenet_BC_100_12_ra_2_20', 400, 'cifar10', 1.1176, 0.7900])\n",
    "b = pd.Series(['densenet_BC_100_12_ra_2_20', 300, 'cifar10', 1.0739, 0.7854])\n",
    "c = pd.Series(['densenet_BC_100_12_ra_2_20', 400, 'cifar10.1', 2.0759, 0.6390])\n",
    "d = pd.Series(['densenet_BC_100_12_ra_2_20', 300, 'cifar10.1', 1.9997, 0.6275])\n",
    "\n",
    "    \n",
    "e = pd.Series(['densenet_BC_100_12_ra_2_20_refined400', 50, 'cifar10.1', 0.6475, 0.8290])\n",
    "f = pd.Series(['densenet_BC_100_12_ra_2_20_refined400', 50, 'cifar10', 0.3217, 0.9168])\n",
    "# g = pd.Series(['resnet_basic_32_ra_2_5_refined300', 150, 'cifar10', 0.4499, 0.8795])\n",
    "# h = pd.Series(['resnet_basic_32_ra_2_5_refined300', 150, 'cifar10.1', 0.8206, 0.7710])\n",
    "               \n",
    "df_results = pd.concat([a,b,c,d,e,f], axis=1).T\n",
    "df_results.columns = ['Model', 'Epoch', 'Testset', 'Loss', 'Accuracy']\n",
    "\n",
    "df_results['Original_Accuracy'] = df_results.apply((lambda row: 95.5 if row[2] == 'cifar10' else 87.6), axis=1)\n",
    "df_results['Original_CI'] = df_results.apply((lambda row: (95.1, 95.9) if row[2] == 'cifar10' else (86.1, 89.0)), axis=1)\n",
    "\n",
    "df_results.to_csv('/home/ec2-user/SageMaker/experiments/densenet_BC_100_12_ra_2_20/results.csv')\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peak inside the output file for predictions\n",
    "import numpy as np\n",
    "output = '/home/ec2-user/SageMaker/experiments/densenet_BC_100_12_ra_2_20/exp00/test_results_0400/predictions.npz'\n",
    "npzfile = np.load(output)\n",
    "print(npzfile.files)\n",
    "npzfile['preds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the model checkpoints, configs, and results to S3 \n",
    "bucket='sagemaker-june29'\n",
    "prefix = 'sagemaker/results/original-models/densenet_BC_100_12_ra_2_20'\n",
    "path = '/home/ec2-user/SageMaker/experiments/densenet_BC_100_12_ra_2_20'\n",
    "\n",
    "s3_resource = boto3.resource(\"s3\", region_name=\"us-east-2\")\n",
    "\n",
    "def uploadDirectory(local_path,bucket_name,s3_prefix):\n",
    "\n",
    "    my_bucket = s3_resource.Bucket(bucket_name)\n",
    "    \n",
    "    for path, subdirs, files in os.walk(local_path):\n",
    "        path = path.replace(\"\\\\\",\"/\")\n",
    "        directory_name = path.replace(local_path,\"\")\n",
    "        for file in files:\n",
    "            #print(\"Local File:\", os.path.join(path, file))\n",
    "            #print(\"      Dest:\", s3_prefix+directory_name+'/'+file)\n",
    "            my_bucket.upload_file(os.path.join(path, file), s3_prefix+directory_name+'/'+file)\n",
    "    \n",
    "uploadDirectory(path,bucket,prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
