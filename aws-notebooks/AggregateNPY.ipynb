{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import IPython.display as display\n",
    "import os\n",
    "import glob\n",
    "import datetime\n",
    "import logging\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cifar10_ra_2_5_500.npy',\n",
       " 'cifar10_ra_2_5_1000.npy',\n",
       " 'cifar10_ra_2_5_1500.npy',\n",
       " 'cifar10_ra_2_5_2000.npy',\n",
       " 'cifar10_ra_2_5_2500.npy',\n",
       " 'cifar10_ra_2_5_3000.npy',\n",
       " 'cifar10_ra_2_5_3500.npy',\n",
       " 'cifar10_ra_2_5_4000.npy',\n",
       " 'cifar10_ra_2_5_4500.npy',\n",
       " 'cifar10_ra_2_5_5000.npy',\n",
       " 'cifar10_ra_2_5_5500.npy',\n",
       " 'cifar10_ra_2_5_6000.npy',\n",
       " 'cifar10_ra_2_5_6500.npy',\n",
       " 'cifar10_ra_2_5_7000.npy',\n",
       " 'cifar10_ra_2_5_7500.npy',\n",
       " 'cifar10_ra_2_5_8000.npy',\n",
       " 'cifar10_ra_2_5_8500.npy',\n",
       " 'cifar10_ra_2_5_9000.npy',\n",
       " 'cifar10_ra_2_5_9500.npy',\n",
       " 'cifar10_ra_2_5_10000.npy',\n",
       " 'cifar10_ra_2_5_10500.npy',\n",
       " 'cifar10_ra_2_5_11000.npy',\n",
       " 'cifar10_ra_2_5_11500.npy',\n",
       " 'cifar10_ra_2_5_12000.npy',\n",
       " 'cifar10_ra_2_5_12500.npy',\n",
       " 'cifar10_ra_2_5_13000.npy',\n",
       " 'cifar10_ra_2_5_13500.npy',\n",
       " 'cifar10_ra_2_5_14000.npy',\n",
       " 'cifar10_ra_2_5_14500.npy',\n",
       " 'cifar10_ra_2_5_15000.npy',\n",
       " 'cifar10_ra_2_5_15500.npy',\n",
       " 'cifar10_ra_2_5_16000.npy',\n",
       " 'cifar10_ra_2_5_16500.npy',\n",
       " 'cifar10_ra_2_5_17000.npy',\n",
       " 'cifar10_ra_2_5_17500.npy',\n",
       " 'cifar10_ra_2_5_18000.npy',\n",
       " 'cifar10_ra_2_5_18500.npy',\n",
       " 'cifar10_ra_2_5_19000.npy',\n",
       " 'cifar10_ra_2_5_19500.npy',\n",
       " 'cifar10_ra_2_5_20000.npy',\n",
       " 'cifar10_ra_2_5_20500.npy',\n",
       " 'cifar10_ra_2_5_21000.npy',\n",
       " 'cifar10_ra_2_5_21500.npy',\n",
       " 'cifar10_ra_2_5_22000.npy',\n",
       " 'cifar10_ra_2_5_22500.npy',\n",
       " 'cifar10_ra_2_5_23000.npy',\n",
       " 'cifar10_ra_2_5_23500.npy',\n",
       " 'cifar10_ra_2_5_24000.npy',\n",
       " 'cifar10_ra_2_5_24500.npy',\n",
       " 'cifar10_ra_2_5_25000.npy',\n",
       " 'cifar10_ra_2_5_25500.npy',\n",
       " 'cifar10_ra_2_5_26000.npy',\n",
       " 'cifar10_ra_2_5_26500.npy',\n",
       " 'cifar10_ra_2_5_27000.npy',\n",
       " 'cifar10_ra_2_5_27500.npy',\n",
       " 'cifar10_ra_2_5_28000.npy',\n",
       " 'cifar10_ra_2_5_28500.npy',\n",
       " 'cifar10_ra_2_5_29000.npy',\n",
       " 'cifar10_ra_2_5_29500.npy',\n",
       " 'cifar10_ra_2_5_30000.npy',\n",
       " 'cifar10_ra_2_5_30500.npy',\n",
       " 'cifar10_ra_2_5_31000.npy',\n",
       " 'cifar10_ra_2_5_31500.npy',\n",
       " 'cifar10_ra_2_5_32000.npy',\n",
       " 'cifar10_ra_2_5_32500.npy',\n",
       " 'cifar10_ra_2_5_33000.npy',\n",
       " 'cifar10_ra_2_5_33500.npy',\n",
       " 'cifar10_ra_2_5_34000.npy',\n",
       " 'cifar10_ra_2_5_34500.npy',\n",
       " 'cifar10_ra_2_5_35000.npy',\n",
       " 'cifar10_ra_2_5_35500.npy',\n",
       " 'cifar10_ra_2_5_36000.npy',\n",
       " 'cifar10_ra_2_5_36500.npy',\n",
       " 'cifar10_ra_2_5_37000.npy',\n",
       " 'cifar10_ra_2_5_37500.npy',\n",
       " 'cifar10_ra_2_5_38000.npy',\n",
       " 'cifar10_ra_2_5_38500.npy',\n",
       " 'cifar10_ra_2_5_39000.npy',\n",
       " 'cifar10_ra_2_5_39500.npy',\n",
       " 'cifar10_ra_2_5_40000.npy',\n",
       " 'cifar10_ra_2_5_40500.npy',\n",
       " 'cifar10_ra_2_5_41000.npy',\n",
       " 'cifar10_ra_2_5_41500.npy',\n",
       " 'cifar10_ra_2_5_42000.npy',\n",
       " 'cifar10_ra_2_5_42500.npy',\n",
       " 'cifar10_ra_2_5_43000.npy',\n",
       " 'cifar10_ra_2_5_43500.npy',\n",
       " 'cifar10_ra_2_5_44000.npy',\n",
       " 'cifar10_ra_2_5_44500.npy',\n",
       " 'cifar10_ra_2_5_45000.npy',\n",
       " 'cifar10_ra_2_5_45500.npy',\n",
       " 'cifar10_ra_2_5_46000.npy',\n",
       " 'cifar10_ra_2_5_46500.npy',\n",
       " 'cifar10_ra_2_5_47000.npy',\n",
       " 'cifar10_ra_2_5_47500.npy',\n",
       " 'cifar10_ra_2_5_48000.npy',\n",
       " 'cifar10_ra_2_5_48500.npy',\n",
       " 'cifar10_ra_2_5_49000.npy',\n",
       " 'cifar10_ra_2_5_49500.npy',\n",
       " 'cifar10_ra_2_5_50000.npy']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the filepath where the mini-NPY files are stored\n",
    "filepath = \"/home/ec2-user/SageMaker\"\n",
    "\n",
    "# Create a list of all mini-NPY files\n",
    "os.chdir(filepath)\n",
    "filelist = []\n",
    "for file in glob.glob(\"*.npy\"):\n",
    "    filelist.append(file)\n",
    "    \n",
    "# Sort the filelist based on the numeric extension\n",
    "filelist = sorted(filelist,key=lambda x: int(os.path.splitext(x)[0].split('_')[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will write aggregated data to: /home/ec2-user/SageMaker/cifar10_ra_2_5.npy\n"
     ]
    }
   ],
   "source": [
    "# Determine filename for concatenated data based on smaller filename\n",
    "s = \"_\"\n",
    "all_data_filename = s.join(os.path.splitext(filelist[0])[0].split(s)[:-1]) + \".npy\"\n",
    "all_data_filepath = filepath + \"/\" + all_data_filename\n",
    "print(\"Will write aggregated data to:\", all_data_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack all of the smaller NPY files\n",
    "os.chdir(filepath)\n",
    "all_arrays = np.load(os.path.join(filepath, filelist[0]))\n",
    "\n",
    "for npfile in filelist[1:]:\n",
    "    all_arrays = np.vstack((all_arrays, (np.load(os.path.join(filepath, npfile)))))\n",
    "\n",
    "# Save the stacked NPY file \n",
    "np.save(all_data_filepath, all_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the NPY file to S3\n",
    "bucket='sagemaker-may29'\n",
    "prefix = 'sagemaker/RandAugmentation/'\n",
    "path = '/home/ec2-user/SageMaker/'\n",
    "\n",
    "s3_resource = boto3.resource(\"s3\", region_name=\"us-east-2\")\n",
    "my_bucket = s3_resource.Bucket(bucket)\n",
    "my_bucket.upload_file(all_data_filepath, prefix + all_data_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_filename = \"cifar10_ra_2_5.npy\"\n",
    "all_data_filepath = \"/home/ec2-user/SageMaker/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Computing mean and std..\n",
      "Dataset Mean: [122.83165607 122.33019602 119.30480225]\n",
      "  -> Normalized Dataset Mean: [0.58368352 0.58130063 0.56692427]\n",
      "Dataset STD [36.46719942 36.15130414 36.85062984]\n",
      "  -> Normalized Dataset STD: [0.57697637 0.57197834 0.58304292]\n",
      "time elapsed:  7.365251779556274\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from tqdm.notebook import tqdm\n",
    "from time import time\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "N_CHANNELS = 3\n",
    "\n",
    "new_dataset = np.load(os.path.join(all_data_filepath, all_data_filename))\n",
    "#np.load(\"/home/ec2-user/SageMaker/w210-capstone/data/cifar101/cifar10.1_v6_data.npy\")\n",
    "\n",
    "before = time()\n",
    "mean = np.zeros(N_CHANNELS)\n",
    "std = np.zeros(N_CHANNELS)\n",
    "\n",
    "print('==> Computing mean and std..')\n",
    "for inputs in new_dataset:\n",
    "    for i in range(N_CHANNELS):\n",
    "        mean[i] += inputs[:,:,i].mean()\n",
    "        std[i] += inputs[:,:,i].std()\n",
    "\n",
    "dataset_mean = np.true_divide(mean, len(new_dataset))\n",
    "dataset_std = np.true_divide(std, len(new_dataset))\n",
    "\n",
    "print(\"Dataset Mean:\", dataset_mean)\n",
    "print(\"  -> Normalized Dataset Mean:\", dataset_mean / np.linalg.norm(dataset_mean))\n",
    "print(\"Dataset STD\", dataset_std)\n",
    "print(\"  -> Normalized Dataset STD:\", dataset_std / np.linalg.norm(dataset_std))\n",
    "\n",
    "print(\"time elapsed: \", time()-before)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_arrays.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data shape: (50000, 32, 32, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[255, 255, 255],\n",
       "        [245, 241, 246],\n",
       "        [243, 241, 238],\n",
       "        ...,\n",
       "        [163, 162, 189],\n",
       "        [163, 162, 189],\n",
       "        [163, 162, 189]],\n",
       "\n",
       "       [[232, 235, 237],\n",
       "        [203, 205, 208],\n",
       "        [196, 202, 178],\n",
       "        ...,\n",
       "        [163, 162, 189],\n",
       "        [163, 162, 189],\n",
       "        [163, 162, 189]],\n",
       "\n",
       "       [[226, 230, 227],\n",
       "        [152, 162, 162],\n",
       "        [134, 145, 127],\n",
       "        ...,\n",
       "        [163, 162, 189],\n",
       "        [163, 162, 189],\n",
       "        [163, 162, 189]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[227, 230, 225],\n",
       "        [145, 150, 159],\n",
       "        [121, 133, 123],\n",
       "        ...,\n",
       "        [  0,   0,   0],\n",
       "        [ 73,  88, 114],\n",
       "        [240, 239, 252]],\n",
       "\n",
       "       [[230, 234, 229],\n",
       "        [189, 195, 176],\n",
       "        [137, 143, 150],\n",
       "        ...,\n",
       "        [  0,   0,   0],\n",
       "        [ 73,  88, 114],\n",
       "        [240, 240, 252]],\n",
       "\n",
       "       [[253, 247, 254],\n",
       "        [230, 234, 232],\n",
       "        [226, 228, 224],\n",
       "        ...,\n",
       "        [  2,   2,   2],\n",
       "        [ 75,  90, 118],\n",
       "        [240, 240, 252]]], dtype=uint8)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.load(os.path.join(filepath, all_data_filename))\n",
    "print(\"All data shape:\", t.shape)\n",
    "\n",
    "k = np.load(os.path.join(filepath, filelist[1]))\n",
    "k[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[255, 255, 255],\n",
       "        [245, 241, 246],\n",
       "        [243, 241, 238],\n",
       "        ...,\n",
       "        [163, 162, 189],\n",
       "        [163, 162, 189],\n",
       "        [163, 162, 189]],\n",
       "\n",
       "       [[232, 235, 237],\n",
       "        [203, 205, 208],\n",
       "        [196, 202, 178],\n",
       "        ...,\n",
       "        [163, 162, 189],\n",
       "        [163, 162, 189],\n",
       "        [163, 162, 189]],\n",
       "\n",
       "       [[226, 230, 227],\n",
       "        [152, 162, 162],\n",
       "        [134, 145, 127],\n",
       "        ...,\n",
       "        [163, 162, 189],\n",
       "        [163, 162, 189],\n",
       "        [163, 162, 189]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[227, 230, 225],\n",
       "        [145, 150, 159],\n",
       "        [121, 133, 123],\n",
       "        ...,\n",
       "        [  0,   0,   0],\n",
       "        [ 73,  88, 114],\n",
       "        [240, 239, 252]],\n",
       "\n",
       "       [[230, 234, 229],\n",
       "        [189, 195, 176],\n",
       "        [137, 143, 150],\n",
       "        ...,\n",
       "        [  0,   0,   0],\n",
       "        [ 73,  88, 114],\n",
       "        [240, 240, 252]],\n",
       "\n",
       "       [[253, 247, 254],\n",
       "        [230, 234, 232],\n",
       "        [226, 228, 224],\n",
       "        ...,\n",
       "        [  2,   2,   2],\n",
       "        [ 75,  90, 118],\n",
       "        [240, 240, 252]]], dtype=uint8)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCRATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a09ea91e14474fcc858ff92059090b11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/cifar-10-python.tar.gz to data\n",
      "5000\n",
      "<class 'torch.utils.data.dataloader.DataLoader'>\n",
      "5000\n",
      "==> Computing mean and std..\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8701f5708096460491b71fcb0d85fbd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tensor([0.0490, 0.0482, 0.0447]) tensor([0.0201, 0.0199, 0.0201])\n",
      "time elapsed:  2.4803249835968018\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from tqdm.notebook import tqdm\n",
    "from time import time\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "N_CHANNELS = 3\n",
    "\n",
    "dataset = datasets.CIFAR10(\"data\", download=True,\n",
    "                 train=True, transform=transforms.ToTensor())\n",
    "#full_loader = torch.utils.data.DataLoader(dataset, shuffle=False, num_workers=os.cpu_count())\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(.9 * dataset_size))\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "print(len(train_indices))\n",
    "subset_loader = torch.utils.data.DataLoader(dataset, batch_size=1, #shuffle=True, \n",
    "                                            sampler=SubsetRandomSampler(train_indices))\n",
    "\n",
    "print(type(subset_loader))\n",
    "print(len(subset_loader))\n",
    "\n",
    "before = time()\n",
    "mean = torch.zeros(N_CHANNELS)\n",
    "std = torch.zeros(N_CHANNELS)\n",
    "example = None\n",
    "print('==> Computing mean and std..')\n",
    "for inputs, _labels in tqdm(subset_loader):\n",
    "    #print(inputs.shape)\n",
    "    example = inputs\n",
    "    for i in range(N_CHANNELS):\n",
    "        #print(inputs[:,i,:,:])\n",
    "        mean[i] += inputs[:,i,:,:].mean()\n",
    "        std[i] += inputs[:,i,:,:].std()\n",
    "mean.div_(len(dataset))\n",
    "std.div_(len(dataset))\n",
    "print(mean, std)\n",
    "\n",
    "print(\"time elapsed: \", time()-before)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
