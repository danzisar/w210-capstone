{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNext 29 4x64\n",
    "\n",
    " - Training Dataset:  RandAugment, N=3, M=20\n",
    "   Validation with Unaugmented Data\n",
    " - Sagemaker Notebook must be of type, conda_pytorch_p36\n",
    " \n",
    "#### Install Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import numpy \n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 1)) (1.18.1)\n",
      "Requirement already satisfied: torch>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 2)) (1.4.0)\n",
      "Requirement already satisfied: torchvision in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (0.5.0)\n",
      "Requirement already satisfied: fvcore in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (0.1.1.post20200711)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 5)) (4.44.1)\n",
      "Requirement already satisfied: yacs in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 6)) (0.1.7)\n",
      "Requirement already satisfied: apex from git+https://github.com/NVIDIA/apex.git#egg=apex in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 7)) (0.1)\n",
      "Requirement already satisfied: termcolor in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 8)) (1.1.0)\n",
      "Requirement already satisfied: thop<0.0.31.post2004070130 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 9)) (0.0.31.post2001170342)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (1.14.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (7.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (5.3.1)\n",
      "Requirement already satisfied: portalocker in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (1.7.0)\n",
      "Requirement already satisfied: tabulate in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (0.8.7)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: tensorboard in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (2.2.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (46.1.3.post20200330)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (2.23.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (0.4.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (0.9.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (3.2.2)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.14.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.7.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.18.1)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (0.34.2)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (3.12.2)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.18.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.0.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.30.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (2020.4.5.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (1.25.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard) (1.5.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (3.4.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (4.1.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (0.2.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard) (2.2.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard) (0.4.8)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r '/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt'\n",
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-17 12:10:28] __main__ INFO: \u001b[0mdevice: cuda\n",
      "cudnn:\n",
      "  benchmark: True\n",
      "  deterministic: False\n",
      "dataset:\n",
      "  name: CIFAR10_RA_3_20\n",
      "  dataset_dir: ''\n",
      "  image_size: 32\n",
      "  n_channels: 3\n",
      "  n_classes: 10\n",
      "model:\n",
      "  type: cifar\n",
      "  name: resnext\n",
      "  init_mode: kaiming_fan_out\n",
      "  vgg:\n",
      "    n_channels: [64, 128, 256, 512, 512]\n",
      "    n_layers: [2, 2, 3, 3, 3]\n",
      "    use_bn: True\n",
      "  resnet:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "  resnet_preact:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "  wrn:\n",
      "    depth: 28\n",
      "    initial_channels: 16\n",
      "    widening_factor: 10\n",
      "    drop_rate: 0.0\n",
      "  densenet:\n",
      "    depth: 100\n",
      "    n_blocks: [6, 12, 24, 16]\n",
      "    block_type: bottleneck\n",
      "    growth_rate: 12\n",
      "    drop_rate: 0.0\n",
      "    compression_rate: 0.5\n",
      "  pyramidnet:\n",
      "    depth: 272\n",
      "    n_blocks: [3, 24, 36, 3]\n",
      "    initial_channels: 16\n",
      "    block_type: bottleneck\n",
      "    alpha: 200\n",
      "  resnext:\n",
      "    depth: 29\n",
      "    n_blocks: [3, 4, 6, 3]\n",
      "    initial_channels: 64\n",
      "    cardinality: 4\n",
      "    base_channels: 64\n",
      "  shake_shake:\n",
      "    depth: 26\n",
      "    initial_channels: 96\n",
      "    shake_forward: True\n",
      "    shake_backward: True\n",
      "    shake_image: True\n",
      "  se_resnet_preact:\n",
      "    depth: 110\n",
      "    initial_channels: 16\n",
      "    se_reduction: 16\n",
      "    block_type: basic\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "train:\n",
      "  checkpoint: ''\n",
      "  resume: False\n",
      "  precision: O0\n",
      "  batch_size: 128\n",
      "  subdivision: 1\n",
      "  optimizer: sgd\n",
      "  base_lr: 0.1\n",
      "  momentum: 0.9\n",
      "  nesterov: True\n",
      "  weight_decay: 0.0005\n",
      "  no_weight_decay_on_bn: False\n",
      "  gradient_clip: 0\n",
      "  start_epoch: 0\n",
      "  seed: 0\n",
      "  val_first: True\n",
      "  val_period: 1\n",
      "  val_ratio: 0.1\n",
      "  use_test_as_val: False\n",
      "  output_dir: /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_3_20_c10val/exp00\n",
      "  log_period: 100\n",
      "  checkpoint_period: 100\n",
      "  use_tensorboard: True\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: True\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "  distributed: False\n",
      "  dist:\n",
      "    backend: nccl\n",
      "    init_method: env://\n",
      "    world_size: -1\n",
      "    node_rank: -1\n",
      "    local_rank: 0\n",
      "    use_sync_bn: False\n",
      "tensorboard:\n",
      "  train_images: False\n",
      "  val_images: False\n",
      "  model_params: False\n",
      "optim:\n",
      "  adam:\n",
      "    betas: (0.9, 0.999)\n",
      "  lars:\n",
      "    eps: 1e-09\n",
      "    threshold: 0.01\n",
      "  adabound:\n",
      "    betas: (0.9, 0.999)\n",
      "    final_lr: 0.1\n",
      "    gamma: 0.001\n",
      "scheduler:\n",
      "  epochs: 400\n",
      "  warmup:\n",
      "    type: none\n",
      "    epochs: 0\n",
      "    start_factor: 0.001\n",
      "    exponent: 4\n",
      "  type: multistep\n",
      "  milestones: [150, 225]\n",
      "  lr_decay: 0.1\n",
      "  lr_min_factor: 0.001\n",
      "  T0: 10\n",
      "  T_mul: 1.0\n",
      "validation:\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: False\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "augmentation:\n",
      "  use_random_crop: True\n",
      "  use_random_horizontal_flip: True\n",
      "  use_cutout: False\n",
      "  use_random_erasing: False\n",
      "  use_dual_cutout: False\n",
      "  use_mixup: False\n",
      "  use_ricap: False\n",
      "  use_cutmix: False\n",
      "  use_label_smoothing: False\n",
      "  random_crop:\n",
      "    padding: 4\n",
      "    fill: 0\n",
      "    padding_mode: constant\n",
      "  random_horizontal_flip:\n",
      "    prob: 0.5\n",
      "  cutout:\n",
      "    prob: 1.0\n",
      "    mask_size: 16\n",
      "    cut_inside: False\n",
      "    mask_color: 0\n",
      "    dual_cutout_alpha: 0.1\n",
      "  random_erasing:\n",
      "    prob: 0.5\n",
      "    area_ratio_range: [0.02, 0.4]\n",
      "    min_aspect_ratio: 0.3\n",
      "    max_attempt: 20\n",
      "  mixup:\n",
      "    alpha: 1.0\n",
      "  ricap:\n",
      "    beta: 0.3\n",
      "  cutmix:\n",
      "    alpha: 1.0\n",
      "  label_smoothing:\n",
      "    epsilon: 0.1\n",
      "tta:\n",
      "  use_resize: False\n",
      "  use_center_crop: False\n",
      "  resize: 256\n",
      "test:\n",
      "  checkpoint: ''\n",
      "  output_dir: ''\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    pin_memory: False\n",
      "\u001b[32m[2020-07-17 12:10:28] __main__ INFO: \u001b[0menv_info:\n",
      "  pytorch_version: 1.4.0\n",
      "  cuda_version: 10.1\n",
      "  cudnn_version: 7603\n",
      "  num_gpus: 1\n",
      "  gpu_name: Tesla K80\n",
      "  gpu_capability: 3.7\n",
      "Files already downloaded and verified\n",
      "\u001b[32m[2020-07-17 12:10:35] __main__ INFO: \u001b[0mMACs  : 2.75G\n",
      "\u001b[32m[2020-07-17 12:10:35] __main__ INFO: \u001b[0m#params: 17.56M\n",
      "Selected optimization level O0:  Pure FP32 training.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\",)\n",
      "\u001b[32m[2020-07-17 12:10:35] __main__ INFO: \u001b[0mVal 0\n",
      "\u001b[32m[2020-07-17 12:11:01] __main__ INFO: \u001b[0mEpoch 0 loss 14.9023 acc@1 0.1032 acc@5 0.4996\n",
      "\u001b[32m[2020-07-17 12:11:01] __main__ INFO: \u001b[0mElapsed 26.18\n",
      "\u001b[32m[2020-07-17 12:11:01] __main__ INFO: \u001b[0mTrain 1 0\n",
      "\u001b[32m[2020-07-17 12:13:32] __main__ INFO: \u001b[0mEpoch 1 Step 100/351 lr 0.100000 loss 2.2743 (9.7335) acc@1 0.1562 (0.0993) acc@5 0.5156 (0.5038)\n",
      "\u001b[32m[2020-07-17 12:15:56] __main__ INFO: \u001b[0mEpoch 1 Step 200/351 lr 0.100000 loss 2.4019 (6.2024) acc@1 0.0781 (0.0989) acc@5 0.4375 (0.5035)\n",
      "\u001b[32m[2020-07-17 12:18:19] __main__ INFO: \u001b[0mEpoch 1 Step 300/351 lr 0.100000 loss 2.7928 (4.9654) acc@1 0.0859 (0.1007) acc@5 0.4531 (0.5029)\n",
      "\u001b[32m[2020-07-17 12:19:32] __main__ INFO: \u001b[0mEpoch 1 Step 351/351 lr 0.100000 loss 2.3967 (4.6069) acc@1 0.1094 (0.1017) acc@5 0.5156 (0.5015)\n",
      "\u001b[32m[2020-07-17 12:19:32] __main__ INFO: \u001b[0mElapsed 511.41\n",
      "\u001b[32m[2020-07-17 12:19:32] __main__ INFO: \u001b[0mVal 1\n",
      "\u001b[32m[2020-07-17 12:19:50] __main__ INFO: \u001b[0mEpoch 1 loss 2.5290 acc@1 0.0976 acc@5 0.5092\n",
      "\u001b[32m[2020-07-17 12:19:50] __main__ INFO: \u001b[0mElapsed 18.04\n",
      "\u001b[32m[2020-07-17 12:19:50] __main__ INFO: \u001b[0mTrain 2 351\n",
      "\u001b[32m[2020-07-17 12:22:14] __main__ INFO: \u001b[0mEpoch 2 Step 100/351 lr 0.100000 loss 2.4421 (2.4527) acc@1 0.0938 (0.0957) acc@5 0.4219 (0.4980)\n",
      "\u001b[32m[2020-07-17 12:24:37] __main__ INFO: \u001b[0mEpoch 2 Step 200/351 lr 0.100000 loss 2.4079 (2.4374) acc@1 0.1016 (0.0995) acc@5 0.4688 (0.4993)\n",
      "\u001b[32m[2020-07-17 12:27:01] __main__ INFO: \u001b[0mEpoch 2 Step 300/351 lr 0.100000 loss 2.3937 (2.4152) acc@1 0.1328 (0.0985) acc@5 0.4609 (0.4997)\n",
      "\u001b[32m[2020-07-17 12:28:14] __main__ INFO: \u001b[0mEpoch 2 Step 351/351 lr 0.100000 loss 2.3627 (2.4072) acc@1 0.0781 (0.0996) acc@5 0.5078 (0.5003)\n",
      "\u001b[32m[2020-07-17 12:28:14] __main__ INFO: \u001b[0mElapsed 503.96\n",
      "\u001b[32m[2020-07-17 12:28:14] __main__ INFO: \u001b[0mVal 2\n",
      "\u001b[32m[2020-07-17 12:28:32] __main__ INFO: \u001b[0mEpoch 2 loss 2.3087 acc@1 0.0952 acc@5 0.5052\n",
      "\u001b[32m[2020-07-17 12:28:32] __main__ INFO: \u001b[0mElapsed 18.04\n",
      "\u001b[32m[2020-07-17 12:28:32] __main__ INFO: \u001b[0mTrain 3 702\n",
      "\u001b[32m[2020-07-17 12:30:56] __main__ INFO: \u001b[0mEpoch 3 Step 100/351 lr 0.100000 loss 2.3405 (2.3355) acc@1 0.1016 (0.0980) acc@5 0.4453 (0.4957)\n",
      "\u001b[32m[2020-07-17 12:33:20] __main__ INFO: \u001b[0mEpoch 3 Step 200/351 lr 0.100000 loss 2.3209 (2.3327) acc@1 0.1250 (0.0991) acc@5 0.5547 (0.4958)\n",
      "\u001b[32m[2020-07-17 12:35:44] __main__ INFO: \u001b[0mEpoch 3 Step 300/351 lr 0.100000 loss 2.3213 (2.3291) acc@1 0.0938 (0.1016) acc@5 0.4531 (0.4974)\n",
      "\u001b[32m[2020-07-17 12:36:57] __main__ INFO: \u001b[0mEpoch 3 Step 351/351 lr 0.100000 loss 2.3152 (2.3278) acc@1 0.0781 (0.1017) acc@5 0.4688 (0.4983)\n",
      "\u001b[32m[2020-07-17 12:36:57] __main__ INFO: \u001b[0mElapsed 504.70\n",
      "\u001b[32m[2020-07-17 12:36:57] __main__ INFO: \u001b[0mVal 3\n",
      "\u001b[32m[2020-07-17 12:37:15] __main__ INFO: \u001b[0mEpoch 3 loss 2.4215 acc@1 0.0972 acc@5 0.5230\n",
      "\u001b[32m[2020-07-17 12:37:15] __main__ INFO: \u001b[0mElapsed 18.17\n",
      "\u001b[32m[2020-07-17 12:37:15] __main__ INFO: \u001b[0mTrain 4 1053\n",
      "\u001b[32m[2020-07-17 12:39:40] __main__ INFO: \u001b[0mEpoch 4 Step 100/351 lr 0.100000 loss 2.3457 (2.3174) acc@1 0.1094 (0.1050) acc@5 0.4922 (0.5039)\n",
      "\u001b[32m[2020-07-17 12:42:03] __main__ INFO: \u001b[0mEpoch 4 Step 200/351 lr 0.100000 loss 2.3736 (2.3163) acc@1 0.0781 (0.1027) acc@5 0.4844 (0.5053)\n",
      "\u001b[32m[2020-07-17 12:44:27] __main__ INFO: \u001b[0mEpoch 4 Step 300/351 lr 0.100000 loss 2.3244 (2.3156) acc@1 0.0859 (0.1032) acc@5 0.4766 (0.5060)\n",
      "\u001b[32m[2020-07-17 12:45:41] __main__ INFO: \u001b[0mEpoch 4 Step 351/351 lr 0.100000 loss 2.3004 (2.3147) acc@1 0.1172 (0.1028) acc@5 0.5391 (0.5061)\n",
      "\u001b[32m[2020-07-17 12:45:41] __main__ INFO: \u001b[0mElapsed 505.56\n",
      "\u001b[32m[2020-07-17 12:45:41] __main__ INFO: \u001b[0mVal 4\n",
      "\u001b[32m[2020-07-17 12:45:59] __main__ INFO: \u001b[0mEpoch 4 loss 2.3041 acc@1 0.1072 acc@5 0.5332\n",
      "\u001b[32m[2020-07-17 12:45:59] __main__ INFO: \u001b[0mElapsed 18.16\n",
      "\u001b[32m[2020-07-17 12:45:59] __main__ INFO: \u001b[0mTrain 5 1404\n",
      "\u001b[32m[2020-07-17 12:48:23] __main__ INFO: \u001b[0mEpoch 5 Step 100/351 lr 0.100000 loss 2.3047 (2.3097) acc@1 0.1094 (0.1048) acc@5 0.5156 (0.5039)\n",
      "\u001b[32m[2020-07-17 12:50:47] __main__ INFO: \u001b[0mEpoch 5 Step 200/351 lr 0.100000 loss 2.3074 (2.3100) acc@1 0.1250 (0.1027) acc@5 0.5234 (0.5076)\n",
      "\u001b[32m[2020-07-17 12:53:11] __main__ INFO: \u001b[0mEpoch 5 Step 300/351 lr 0.100000 loss 2.2908 (2.3085) acc@1 0.0859 (0.1041) acc@5 0.5391 (0.5115)\n",
      "\u001b[32m[2020-07-17 12:54:24] __main__ INFO: \u001b[0mEpoch 5 Step 351/351 lr 0.100000 loss 2.2899 (2.3080) acc@1 0.1328 (0.1044) acc@5 0.5312 (0.5128)\n",
      "\u001b[32m[2020-07-17 12:54:24] __main__ INFO: \u001b[0mElapsed 505.49\n",
      "\u001b[32m[2020-07-17 12:54:25] __main__ INFO: \u001b[0mVal 5\n",
      "\u001b[32m[2020-07-17 12:54:43] __main__ INFO: \u001b[0mEpoch 5 loss 2.2975 acc@1 0.1236 acc@5 0.5740\n",
      "\u001b[32m[2020-07-17 12:54:43] __main__ INFO: \u001b[0mElapsed 18.19\n",
      "\u001b[32m[2020-07-17 12:54:43] __main__ INFO: \u001b[0mTrain 6 1755\n",
      "\u001b[32m[2020-07-17 12:57:07] __main__ INFO: \u001b[0mEpoch 6 Step 100/351 lr 0.100000 loss 2.2788 (2.3028) acc@1 0.1562 (0.1091) acc@5 0.5391 (0.5179)\n",
      "\u001b[32m[2020-07-17 12:59:31] __main__ INFO: \u001b[0mEpoch 6 Step 200/351 lr 0.100000 loss 2.3224 (2.3033) acc@1 0.1250 (0.1089) acc@5 0.5469 (0.5195)\n",
      "\u001b[32m[2020-07-17 13:01:55] __main__ INFO: \u001b[0mEpoch 6 Step 300/351 lr 0.100000 loss 2.3113 (2.3028) acc@1 0.1328 (0.1093) acc@5 0.5234 (0.5194)\n",
      "\u001b[32m[2020-07-17 13:03:09] __main__ INFO: \u001b[0mEpoch 6 Step 351/351 lr 0.100000 loss 2.3115 (2.3028) acc@1 0.1172 (0.1094) acc@5 0.4922 (0.5175)\n",
      "\u001b[32m[2020-07-17 13:03:09] __main__ INFO: \u001b[0mElapsed 506.02\n",
      "\u001b[32m[2020-07-17 13:03:09] __main__ INFO: \u001b[0mVal 6\n",
      "\u001b[32m[2020-07-17 13:03:27] __main__ INFO: \u001b[0mEpoch 6 loss 2.2828 acc@1 0.1446 acc@5 0.6082\n",
      "\u001b[32m[2020-07-17 13:03:27] __main__ INFO: \u001b[0mElapsed 18.11\n",
      "\u001b[32m[2020-07-17 13:03:27] __main__ INFO: \u001b[0mTrain 7 2106\n",
      "\u001b[32m[2020-07-17 13:05:51] __main__ INFO: \u001b[0mEpoch 7 Step 100/351 lr 0.100000 loss 2.3029 (2.3025) acc@1 0.1250 (0.1066) acc@5 0.5547 (0.5252)\n",
      "\u001b[32m[2020-07-17 13:08:16] __main__ INFO: \u001b[0mEpoch 7 Step 200/351 lr 0.100000 loss 2.2996 (2.3006) acc@1 0.1172 (0.1102) acc@5 0.5234 (0.5268)\n",
      "\u001b[32m[2020-07-17 13:10:39] __main__ INFO: \u001b[0mEpoch 7 Step 300/351 lr 0.100000 loss 2.3005 (2.3001) acc@1 0.1094 (0.1099) acc@5 0.4609 (0.5257)\n",
      "\u001b[32m[2020-07-17 13:11:53] __main__ INFO: \u001b[0mEpoch 7 Step 351/351 lr 0.100000 loss 2.3154 (2.2997) acc@1 0.0781 (0.1103) acc@5 0.5234 (0.5262)\n",
      "\u001b[32m[2020-07-17 13:11:53] __main__ INFO: \u001b[0mElapsed 506.20\n",
      "\u001b[32m[2020-07-17 13:11:53] __main__ INFO: \u001b[0mVal 7\n",
      "\u001b[32m[2020-07-17 13:12:11] __main__ INFO: \u001b[0mEpoch 7 loss 2.2811 acc@1 0.1396 acc@5 0.6160\n",
      "\u001b[32m[2020-07-17 13:12:11] __main__ INFO: \u001b[0mElapsed 18.17\n",
      "\u001b[32m[2020-07-17 13:12:11] __main__ INFO: \u001b[0mTrain 8 2457\n",
      "\u001b[32m[2020-07-17 13:14:35] __main__ INFO: \u001b[0mEpoch 8 Step 100/351 lr 0.100000 loss 2.2892 (2.2977) acc@1 0.1406 (0.1174) acc@5 0.5703 (0.5237)\n",
      "\u001b[32m[2020-07-17 13:16:59] __main__ INFO: \u001b[0mEpoch 8 Step 200/351 lr 0.100000 loss 2.3057 (2.2971) acc@1 0.1406 (0.1149) acc@5 0.5078 (0.5293)\n",
      "\u001b[32m[2020-07-17 13:19:23] __main__ INFO: \u001b[0mEpoch 8 Step 300/351 lr 0.100000 loss 2.3116 (2.2972) acc@1 0.0781 (0.1135) acc@5 0.5078 (0.5304)\n",
      "\u001b[32m[2020-07-17 13:20:37] __main__ INFO: \u001b[0mEpoch 8 Step 351/351 lr 0.100000 loss 2.3303 (2.2972) acc@1 0.0625 (0.1131) acc@5 0.4531 (0.5307)\n",
      "\u001b[32m[2020-07-17 13:20:37] __main__ INFO: \u001b[0mElapsed 505.37\n",
      "\u001b[32m[2020-07-17 13:20:37] __main__ INFO: \u001b[0mVal 8\n",
      "\u001b[32m[2020-07-17 13:20:55] __main__ INFO: \u001b[0mEpoch 8 loss 2.2495 acc@1 0.1376 acc@5 0.6266\n",
      "\u001b[32m[2020-07-17 13:20:55] __main__ INFO: \u001b[0mElapsed 18.13\n",
      "\u001b[32m[2020-07-17 13:20:55] __main__ INFO: \u001b[0mTrain 9 2808\n",
      "\u001b[32m[2020-07-17 13:23:19] __main__ INFO: \u001b[0mEpoch 9 Step 100/351 lr 0.100000 loss 2.2993 (2.2922) acc@1 0.0938 (0.1204) acc@5 0.4844 (0.5360)\n",
      "\u001b[32m[2020-07-17 13:25:43] __main__ INFO: \u001b[0mEpoch 9 Step 200/351 lr 0.100000 loss 2.3051 (2.2943) acc@1 0.0859 (0.1149) acc@5 0.5078 (0.5348)\n",
      "\u001b[32m[2020-07-17 13:28:07] __main__ INFO: \u001b[0mEpoch 9 Step 300/351 lr 0.100000 loss 2.2938 (2.2947) acc@1 0.1172 (0.1136) acc@5 0.5234 (0.5327)\n",
      "\u001b[32m[2020-07-17 13:29:20] __main__ INFO: \u001b[0mEpoch 9 Step 351/351 lr 0.100000 loss 2.2990 (2.2950) acc@1 0.1016 (0.1134) acc@5 0.5469 (0.5322)\n",
      "\u001b[32m[2020-07-17 13:29:20] __main__ INFO: \u001b[0mElapsed 505.45\n",
      "\u001b[32m[2020-07-17 13:29:20] __main__ INFO: \u001b[0mVal 9\n",
      "\u001b[32m[2020-07-17 13:29:38] __main__ INFO: \u001b[0mEpoch 9 loss 2.2625 acc@1 0.1492 acc@5 0.6130\n",
      "\u001b[32m[2020-07-17 13:29:38] __main__ INFO: \u001b[0mElapsed 18.15\n",
      "\u001b[32m[2020-07-17 13:29:38] __main__ INFO: \u001b[0mTrain 10 3159\n",
      "\u001b[32m[2020-07-17 13:32:02] __main__ INFO: \u001b[0mEpoch 10 Step 100/351 lr 0.100000 loss 2.2843 (2.2921) acc@1 0.1094 (0.1130) acc@5 0.5391 (0.5395)\n",
      "\u001b[32m[2020-07-17 13:34:27] __main__ INFO: \u001b[0mEpoch 10 Step 200/351 lr 0.100000 loss 2.2775 (2.2909) acc@1 0.1406 (0.1143) acc@5 0.5781 (0.5420)\n",
      "\u001b[32m[2020-07-17 13:36:50] __main__ INFO: \u001b[0mEpoch 10 Step 300/351 lr 0.100000 loss 2.2980 (2.2913) acc@1 0.1016 (0.1138) acc@5 0.5312 (0.5423)\n",
      "\u001b[32m[2020-07-17 13:38:04] __main__ INFO: \u001b[0mEpoch 10 Step 351/351 lr 0.100000 loss 2.2818 (2.2904) acc@1 0.1172 (0.1143) acc@5 0.5625 (0.5429)\n",
      "\u001b[32m[2020-07-17 13:38:04] __main__ INFO: \u001b[0mElapsed 505.42\n",
      "\u001b[32m[2020-07-17 13:38:04] __main__ INFO: \u001b[0mVal 10\n",
      "\u001b[32m[2020-07-17 13:38:22] __main__ INFO: \u001b[0mEpoch 10 loss 2.2044 acc@1 0.1688 acc@5 0.6730\n",
      "\u001b[32m[2020-07-17 13:38:22] __main__ INFO: \u001b[0mElapsed 18.16\n",
      "\u001b[32m[2020-07-17 13:38:22] __main__ INFO: \u001b[0mTrain 11 3510\n",
      "\u001b[32m[2020-07-17 13:40:46] __main__ INFO: \u001b[0mEpoch 11 Step 100/351 lr 0.100000 loss 2.2954 (2.2885) acc@1 0.1094 (0.1163) acc@5 0.4922 (0.5402)\n",
      "\u001b[32m[2020-07-17 13:43:10] __main__ INFO: \u001b[0mEpoch 11 Step 200/351 lr 0.100000 loss 2.2690 (2.2870) acc@1 0.1328 (0.1176) acc@5 0.5703 (0.5439)\n",
      "\u001b[32m[2020-07-17 13:45:34] __main__ INFO: \u001b[0mEpoch 11 Step 300/351 lr 0.100000 loss 2.2533 (2.2862) acc@1 0.1562 (0.1189) acc@5 0.5703 (0.5455)\n",
      "\u001b[32m[2020-07-17 13:46:48] __main__ INFO: \u001b[0mEpoch 11 Step 351/351 lr 0.100000 loss 2.2641 (2.2850) acc@1 0.1484 (0.1199) acc@5 0.5781 (0.5476)\n",
      "\u001b[32m[2020-07-17 13:46:48] __main__ INFO: \u001b[0mElapsed 505.85\n",
      "\u001b[32m[2020-07-17 13:46:48] __main__ INFO: \u001b[0mVal 11\n",
      "\u001b[32m[2020-07-17 13:47:06] __main__ INFO: \u001b[0mEpoch 11 loss 2.1992 acc@1 0.1500 acc@5 0.6786\n",
      "\u001b[32m[2020-07-17 13:47:06] __main__ INFO: \u001b[0mElapsed 18.18\n",
      "\u001b[32m[2020-07-17 13:47:06] __main__ INFO: \u001b[0mTrain 12 3861\n",
      "\u001b[32m[2020-07-17 13:49:30] __main__ INFO: \u001b[0mEpoch 12 Step 100/351 lr 0.100000 loss 2.2680 (2.2764) acc@1 0.1250 (0.1223) acc@5 0.5625 (0.5641)\n",
      "\u001b[32m[2020-07-17 13:51:54] __main__ INFO: \u001b[0mEpoch 12 Step 200/351 lr 0.100000 loss 2.2987 (2.2735) acc@1 0.0781 (0.1232) acc@5 0.4922 (0.5663)\n",
      "\u001b[32m[2020-07-17 13:54:18] __main__ INFO: \u001b[0mEpoch 12 Step 300/351 lr 0.100000 loss 2.2545 (2.2731) acc@1 0.1875 (0.1243) acc@5 0.6016 (0.5658)\n",
      "\u001b[32m[2020-07-17 13:55:32] __main__ INFO: \u001b[0mEpoch 12 Step 351/351 lr 0.100000 loss 2.2790 (2.2732) acc@1 0.1406 (0.1239) acc@5 0.5625 (0.5641)\n",
      "\u001b[32m[2020-07-17 13:55:32] __main__ INFO: \u001b[0mElapsed 505.67\n",
      "\u001b[32m[2020-07-17 13:55:32] __main__ INFO: \u001b[0mVal 12\n",
      "\u001b[32m[2020-07-17 13:55:50] __main__ INFO: \u001b[0mEpoch 12 loss 2.1442 acc@1 0.1882 acc@5 0.6944\n",
      "\u001b[32m[2020-07-17 13:55:50] __main__ INFO: \u001b[0mElapsed 18.14\n",
      "\u001b[32m[2020-07-17 13:55:50] __main__ INFO: \u001b[0mTrain 13 4212\n",
      "\u001b[32m[2020-07-17 13:58:13] __main__ INFO: \u001b[0mEpoch 13 Step 100/351 lr 0.100000 loss 2.2585 (2.2645) acc@1 0.2031 (0.1298) acc@5 0.5938 (0.5705)\n",
      "\u001b[32m[2020-07-17 14:00:36] __main__ INFO: \u001b[0mEpoch 13 Step 200/351 lr 0.100000 loss 2.2961 (2.2658) acc@1 0.1250 (0.1265) acc@5 0.5469 (0.5680)\n",
      "\u001b[32m[2020-07-17 14:03:01] __main__ INFO: \u001b[0mEpoch 13 Step 300/351 lr 0.100000 loss 2.2734 (2.2654) acc@1 0.1328 (0.1273) acc@5 0.5625 (0.5683)\n",
      "\u001b[32m[2020-07-17 14:04:14] __main__ INFO: \u001b[0mEpoch 13 Step 351/351 lr 0.100000 loss 2.2879 (2.2655) acc@1 0.1172 (0.1273) acc@5 0.4688 (0.5672)\n",
      "\u001b[32m[2020-07-17 14:04:14] __main__ INFO: \u001b[0mElapsed 504.13\n",
      "\u001b[32m[2020-07-17 14:04:14] __main__ INFO: \u001b[0mVal 13\n",
      "\u001b[32m[2020-07-17 14:04:32] __main__ INFO: \u001b[0mEpoch 13 loss 2.2183 acc@1 0.1796 acc@5 0.6928\n",
      "\u001b[32m[2020-07-17 14:04:32] __main__ INFO: \u001b[0mElapsed 18.03\n",
      "\u001b[32m[2020-07-17 14:04:32] __main__ INFO: \u001b[0mTrain 14 4563\n",
      "\u001b[32m[2020-07-17 14:06:55] __main__ INFO: \u001b[0mEpoch 14 Step 100/351 lr 0.100000 loss 2.2486 (2.2560) acc@1 0.1172 (0.1298) acc@5 0.5859 (0.5823)\n",
      "\u001b[32m[2020-07-17 14:09:19] __main__ INFO: \u001b[0mEpoch 14 Step 200/351 lr 0.100000 loss 2.2372 (2.2568) acc@1 0.1406 (0.1306) acc@5 0.6172 (0.5809)\n",
      "\u001b[32m[2020-07-17 14:11:43] __main__ INFO: \u001b[0mEpoch 14 Step 300/351 lr 0.100000 loss 2.2541 (2.2562) acc@1 0.1094 (0.1293) acc@5 0.6562 (0.5782)\n",
      "\u001b[32m[2020-07-17 14:12:57] __main__ INFO: \u001b[0mEpoch 14 Step 351/351 lr 0.100000 loss 2.2724 (2.2549) acc@1 0.1094 (0.1309) acc@5 0.5938 (0.5786)\n",
      "\u001b[32m[2020-07-17 14:12:57] __main__ INFO: \u001b[0mElapsed 504.93\n",
      "\u001b[32m[2020-07-17 14:12:57] __main__ INFO: \u001b[0mVal 14\n",
      "\u001b[32m[2020-07-17 14:13:15] __main__ INFO: \u001b[0mEpoch 14 loss 2.1129 acc@1 0.1942 acc@5 0.7322\n",
      "\u001b[32m[2020-07-17 14:13:15] __main__ INFO: \u001b[0mElapsed 18.06\n",
      "\u001b[32m[2020-07-17 14:13:15] __main__ INFO: \u001b[0mTrain 15 4914\n",
      "\u001b[32m[2020-07-17 14:15:38] __main__ INFO: \u001b[0mEpoch 15 Step 100/351 lr 0.100000 loss 2.3194 (2.2519) acc@1 0.0938 (0.1343) acc@5 0.4766 (0.5815)\n",
      "\u001b[32m[2020-07-17 14:18:03] __main__ INFO: \u001b[0mEpoch 15 Step 200/351 lr 0.100000 loss 2.2606 (2.2492) acc@1 0.1094 (0.1358) acc@5 0.5547 (0.5830)\n",
      "\u001b[32m[2020-07-17 14:20:27] __main__ INFO: \u001b[0mEpoch 15 Step 300/351 lr 0.100000 loss 2.2928 (2.2457) acc@1 0.1172 (0.1369) acc@5 0.5703 (0.5855)\n",
      "\u001b[32m[2020-07-17 14:21:40] __main__ INFO: \u001b[0mEpoch 15 Step 351/351 lr 0.100000 loss 2.2556 (2.2447) acc@1 0.1328 (0.1376) acc@5 0.6250 (0.5871)\n",
      "\u001b[32m[2020-07-17 14:21:40] __main__ INFO: \u001b[0mElapsed 504.92\n",
      "\u001b[32m[2020-07-17 14:21:40] __main__ INFO: \u001b[0mVal 15\n",
      "\u001b[32m[2020-07-17 14:21:58] __main__ INFO: \u001b[0mEpoch 15 loss 2.0399 acc@1 0.2266 acc@5 0.7832\n",
      "\u001b[32m[2020-07-17 14:21:58] __main__ INFO: \u001b[0mElapsed 18.05\n",
      "\u001b[32m[2020-07-17 14:21:58] __main__ INFO: \u001b[0mTrain 16 5265\n",
      "\u001b[32m[2020-07-17 14:24:22] __main__ INFO: \u001b[0mEpoch 16 Step 100/351 lr 0.100000 loss 2.2721 (2.2434) acc@1 0.1172 (0.1385) acc@5 0.5625 (0.5925)\n",
      "\u001b[32m[2020-07-17 14:26:46] __main__ INFO: \u001b[0mEpoch 16 Step 200/351 lr 0.100000 loss 2.1868 (2.2399) acc@1 0.1641 (0.1418) acc@5 0.6484 (0.5950)\n",
      "\u001b[32m[2020-07-17 14:29:11] __main__ INFO: \u001b[0mEpoch 16 Step 300/351 lr 0.100000 loss 2.2272 (2.2363) acc@1 0.2266 (0.1429) acc@5 0.5781 (0.5973)\n",
      "\u001b[32m[2020-07-17 14:30:24] __main__ INFO: \u001b[0mEpoch 16 Step 351/351 lr 0.100000 loss 2.2139 (2.2353) acc@1 0.1797 (0.1438) acc@5 0.6172 (0.5987)\n",
      "\u001b[32m[2020-07-17 14:30:24] __main__ INFO: \u001b[0mElapsed 506.19\n",
      "\u001b[32m[2020-07-17 14:30:24] __main__ INFO: \u001b[0mVal 16\n",
      "\u001b[32m[2020-07-17 14:30:42] __main__ INFO: \u001b[0mEpoch 16 loss 2.0310 acc@1 0.2388 acc@5 0.7940\n",
      "\u001b[32m[2020-07-17 14:30:42] __main__ INFO: \u001b[0mElapsed 18.14\n",
      "\u001b[32m[2020-07-17 14:30:42] __main__ INFO: \u001b[0mTrain 17 5616\n",
      "\u001b[32m[2020-07-17 14:33:06] __main__ INFO: \u001b[0mEpoch 17 Step 100/351 lr 0.100000 loss 2.2253 (2.2203) acc@1 0.1328 (0.1498) acc@5 0.6406 (0.6087)\n",
      "\u001b[32m[2020-07-17 14:35:30] __main__ INFO: \u001b[0mEpoch 17 Step 200/351 lr 0.100000 loss 2.2271 (2.2171) acc@1 0.1875 (0.1527) acc@5 0.6406 (0.6087)\n",
      "\u001b[32m[2020-07-17 14:37:54] __main__ INFO: \u001b[0mEpoch 17 Step 300/351 lr 0.100000 loss 2.2409 (2.2169) acc@1 0.1250 (0.1548) acc@5 0.5859 (0.6108)\n",
      "\u001b[32m[2020-07-17 14:39:07] __main__ INFO: \u001b[0mEpoch 17 Step 351/351 lr 0.100000 loss 2.1885 (2.2171) acc@1 0.1641 (0.1543) acc@5 0.5859 (0.6110)\n",
      "\u001b[32m[2020-07-17 14:39:07] __main__ INFO: \u001b[0mElapsed 504.97\n",
      "\u001b[32m[2020-07-17 14:39:07] __main__ INFO: \u001b[0mVal 17\n",
      "\u001b[32m[2020-07-17 14:39:25] __main__ INFO: \u001b[0mEpoch 17 loss 1.8514 acc@1 0.2730 acc@5 0.8544\n",
      "\u001b[32m[2020-07-17 14:39:25] __main__ INFO: \u001b[0mElapsed 18.14\n",
      "\u001b[32m[2020-07-17 14:39:25] __main__ INFO: \u001b[0mTrain 18 5967\n",
      "\u001b[32m[2020-07-17 14:41:49] __main__ INFO: \u001b[0mEpoch 18 Step 100/351 lr 0.100000 loss 2.1875 (2.2067) acc@1 0.2031 (0.1626) acc@5 0.6484 (0.6110)\n",
      "\u001b[32m[2020-07-17 14:44:13] __main__ INFO: \u001b[0mEpoch 18 Step 200/351 lr 0.100000 loss 2.2138 (2.2056) acc@1 0.1719 (0.1628) acc@5 0.6641 (0.6150)\n",
      "\u001b[32m[2020-07-17 14:46:36] __main__ INFO: \u001b[0mEpoch 18 Step 300/351 lr 0.100000 loss 2.1683 (2.2043) acc@1 0.1562 (0.1617) acc@5 0.6172 (0.6151)\n",
      "\u001b[32m[2020-07-17 14:47:50] __main__ INFO: \u001b[0mEpoch 18 Step 351/351 lr 0.100000 loss 2.1907 (2.2024) acc@1 0.1562 (0.1624) acc@5 0.6953 (0.6155)\n",
      "\u001b[32m[2020-07-17 14:47:50] __main__ INFO: \u001b[0mElapsed 504.39\n",
      "\u001b[32m[2020-07-17 14:47:50] __main__ INFO: \u001b[0mVal 18\n",
      "\u001b[32m[2020-07-17 14:48:08] __main__ INFO: \u001b[0mEpoch 18 loss 1.9615 acc@1 0.2742 acc@5 0.8220\n",
      "\u001b[32m[2020-07-17 14:48:08] __main__ INFO: \u001b[0mElapsed 18.00\n",
      "\u001b[32m[2020-07-17 14:48:08] __main__ INFO: \u001b[0mTrain 19 6318\n",
      "\u001b[32m[2020-07-17 14:50:31] __main__ INFO: \u001b[0mEpoch 19 Step 100/351 lr 0.100000 loss 2.1753 (2.1921) acc@1 0.1719 (0.1659) acc@5 0.5859 (0.6279)\n",
      "\u001b[32m[2020-07-17 14:52:55] __main__ INFO: \u001b[0mEpoch 19 Step 200/351 lr 0.100000 loss 2.1591 (2.1884) acc@1 0.2031 (0.1695) acc@5 0.6953 (0.6268)\n",
      "\u001b[32m[2020-07-17 14:55:19] __main__ INFO: \u001b[0mEpoch 19 Step 300/351 lr 0.100000 loss 2.2330 (2.1917) acc@1 0.1094 (0.1681) acc@5 0.6250 (0.6229)\n",
      "\u001b[32m[2020-07-17 14:56:32] __main__ INFO: \u001b[0mEpoch 19 Step 351/351 lr 0.100000 loss 2.2089 (2.1911) acc@1 0.1875 (0.1688) acc@5 0.5938 (0.6234)\n",
      "\u001b[32m[2020-07-17 14:56:32] __main__ INFO: \u001b[0mElapsed 504.18\n",
      "\u001b[32m[2020-07-17 14:56:32] __main__ INFO: \u001b[0mVal 19\n",
      "\u001b[32m[2020-07-17 14:56:50] __main__ INFO: \u001b[0mEpoch 19 loss 1.9867 acc@1 0.2688 acc@5 0.7964\n",
      "\u001b[32m[2020-07-17 14:56:50] __main__ INFO: \u001b[0mElapsed 18.06\n",
      "\u001b[32m[2020-07-17 14:56:50] __main__ INFO: \u001b[0mTrain 20 6669\n",
      "\u001b[32m[2020-07-17 14:59:13] __main__ INFO: \u001b[0mEpoch 20 Step 100/351 lr 0.100000 loss 2.1195 (2.1832) acc@1 0.1953 (0.1736) acc@5 0.6797 (0.6272)\n",
      "\u001b[32m[2020-07-17 15:01:37] __main__ INFO: \u001b[0mEpoch 20 Step 200/351 lr 0.100000 loss 2.1834 (2.1826) acc@1 0.1797 (0.1745) acc@5 0.6484 (0.6275)\n",
      "\u001b[32m[2020-07-17 15:04:00] __main__ INFO: \u001b[0mEpoch 20 Step 300/351 lr 0.100000 loss 2.2581 (2.1808) acc@1 0.0938 (0.1731) acc@5 0.5469 (0.6264)\n",
      "\u001b[32m[2020-07-17 15:05:13] __main__ INFO: \u001b[0mEpoch 20 Step 351/351 lr 0.100000 loss 2.1949 (2.1818) acc@1 0.1797 (0.1730) acc@5 0.6406 (0.6259)\n",
      "\u001b[32m[2020-07-17 15:05:13] __main__ INFO: \u001b[0mElapsed 502.91\n",
      "\u001b[32m[2020-07-17 15:05:13] __main__ INFO: \u001b[0mVal 20\n",
      "\u001b[32m[2020-07-17 15:05:31] __main__ INFO: \u001b[0mEpoch 20 loss 1.9086 acc@1 0.3096 acc@5 0.8424\n",
      "\u001b[32m[2020-07-17 15:05:31] __main__ INFO: \u001b[0mElapsed 17.96\n",
      "\u001b[32m[2020-07-17 15:05:31] __main__ INFO: \u001b[0mTrain 21 7020\n",
      "\u001b[32m[2020-07-17 15:07:54] __main__ INFO: \u001b[0mEpoch 21 Step 100/351 lr 0.100000 loss 2.1263 (2.1779) acc@1 0.2109 (0.1729) acc@5 0.6562 (0.6280)\n",
      "\u001b[32m[2020-07-17 15:10:17] __main__ INFO: \u001b[0mEpoch 21 Step 200/351 lr 0.100000 loss 2.1946 (2.1785) acc@1 0.1484 (0.1734) acc@5 0.6484 (0.6282)\n",
      "\u001b[32m[2020-07-17 15:12:41] __main__ INFO: \u001b[0mEpoch 21 Step 300/351 lr 0.100000 loss 2.1434 (2.1775) acc@1 0.2031 (0.1736) acc@5 0.6719 (0.6309)\n",
      "\u001b[32m[2020-07-17 15:13:54] __main__ INFO: \u001b[0mEpoch 21 Step 351/351 lr 0.100000 loss 2.1832 (2.1754) acc@1 0.1250 (0.1738) acc@5 0.6484 (0.6313)\n",
      "\u001b[32m[2020-07-17 15:13:54] __main__ INFO: \u001b[0mElapsed 503.27\n",
      "\u001b[32m[2020-07-17 15:13:54] __main__ INFO: \u001b[0mVal 21\n",
      "\u001b[32m[2020-07-17 15:14:12] __main__ INFO: \u001b[0mEpoch 21 loss 1.6614 acc@1 0.3916 acc@5 0.8880\n",
      "\u001b[32m[2020-07-17 15:14:12] __main__ INFO: \u001b[0mElapsed 17.99\n",
      "\u001b[32m[2020-07-17 15:14:12] __main__ INFO: \u001b[0mTrain 22 7371\n",
      "\u001b[32m[2020-07-17 15:16:35] __main__ INFO: \u001b[0mEpoch 22 Step 100/351 lr 0.100000 loss 2.1334 (2.1662) acc@1 0.1250 (0.1716) acc@5 0.6797 (0.6309)\n",
      "\u001b[32m[2020-07-17 15:18:59] __main__ INFO: \u001b[0mEpoch 22 Step 200/351 lr 0.100000 loss 2.0892 (2.1687) acc@1 0.2812 (0.1740) acc@5 0.6406 (0.6305)\n",
      "\u001b[32m[2020-07-17 15:21:21] __main__ INFO: \u001b[0mEpoch 22 Step 300/351 lr 0.100000 loss 2.2179 (2.1678) acc@1 0.1406 (0.1752) acc@5 0.6328 (0.6314)\n",
      "\u001b[32m[2020-07-17 15:22:34] __main__ INFO: \u001b[0mEpoch 22 Step 351/351 lr 0.100000 loss 2.1223 (2.1679) acc@1 0.1641 (0.1754) acc@5 0.7031 (0.6326)\n",
      "\u001b[32m[2020-07-17 15:22:34] __main__ INFO: \u001b[0mElapsed 502.04\n",
      "\u001b[32m[2020-07-17 15:22:34] __main__ INFO: \u001b[0mVal 22\n",
      "\u001b[32m[2020-07-17 15:22:52] __main__ INFO: \u001b[0mEpoch 22 loss 2.0737 acc@1 0.2750 acc@5 0.7778\n",
      "\u001b[32m[2020-07-17 15:22:52] __main__ INFO: \u001b[0mElapsed 18.06\n",
      "\u001b[32m[2020-07-17 15:22:52] __main__ INFO: \u001b[0mTrain 23 7722\n",
      "\u001b[32m[2020-07-17 15:25:16] __main__ INFO: \u001b[0mEpoch 23 Step 100/351 lr 0.100000 loss 2.1659 (2.1634) acc@1 0.2500 (0.1809) acc@5 0.6016 (0.6362)\n",
      "\u001b[32m[2020-07-17 15:27:39] __main__ INFO: \u001b[0mEpoch 23 Step 200/351 lr 0.100000 loss 2.1482 (2.1601) acc@1 0.1641 (0.1834) acc@5 0.5625 (0.6355)\n",
      "\u001b[32m[2020-07-17 15:30:02] __main__ INFO: \u001b[0mEpoch 23 Step 300/351 lr 0.100000 loss 2.2283 (2.1603) acc@1 0.1484 (0.1820) acc@5 0.5547 (0.6349)\n",
      "\u001b[32m[2020-07-17 15:31:15] __main__ INFO: \u001b[0mEpoch 23 Step 351/351 lr 0.100000 loss 2.2071 (2.1599) acc@1 0.1562 (0.1825) acc@5 0.6172 (0.6350)\n",
      "\u001b[32m[2020-07-17 15:31:15] __main__ INFO: \u001b[0mElapsed 502.64\n",
      "\u001b[32m[2020-07-17 15:31:15] __main__ INFO: \u001b[0mVal 23\n",
      "\u001b[32m[2020-07-17 15:31:33] __main__ INFO: \u001b[0mEpoch 23 loss 1.9211 acc@1 0.3104 acc@5 0.7802\n",
      "\u001b[32m[2020-07-17 15:31:33] __main__ INFO: \u001b[0mElapsed 18.00\n",
      "\u001b[32m[2020-07-17 15:31:33] __main__ INFO: \u001b[0mTrain 24 8073\n",
      "\u001b[32m[2020-07-17 15:33:56] __main__ INFO: \u001b[0mEpoch 24 Step 100/351 lr 0.100000 loss 2.1506 (2.1504) acc@1 0.1719 (0.1852) acc@5 0.6328 (0.6405)\n",
      "\u001b[32m[2020-07-17 15:36:19] __main__ INFO: \u001b[0mEpoch 24 Step 200/351 lr 0.100000 loss 2.0925 (2.1543) acc@1 0.2109 (0.1841) acc@5 0.6484 (0.6346)\n",
      "\u001b[32m[2020-07-17 15:38:42] __main__ INFO: \u001b[0mEpoch 24 Step 300/351 lr 0.100000 loss 2.1567 (2.1524) acc@1 0.1953 (0.1842) acc@5 0.6328 (0.6355)\n",
      "\u001b[32m[2020-07-17 15:39:55] __main__ INFO: \u001b[0mEpoch 24 Step 351/351 lr 0.100000 loss 2.1475 (2.1510) acc@1 0.1719 (0.1843) acc@5 0.6328 (0.6372)\n",
      "\u001b[32m[2020-07-17 15:39:55] __main__ INFO: \u001b[0mElapsed 502.11\n",
      "\u001b[32m[2020-07-17 15:39:55] __main__ INFO: \u001b[0mVal 24\n",
      "\u001b[32m[2020-07-17 15:40:13] __main__ INFO: \u001b[0mEpoch 24 loss 1.8140 acc@1 0.3262 acc@5 0.8416\n",
      "\u001b[32m[2020-07-17 15:40:13] __main__ INFO: \u001b[0mElapsed 18.07\n",
      "\u001b[32m[2020-07-17 15:40:13] __main__ INFO: \u001b[0mTrain 25 8424\n",
      "\u001b[32m[2020-07-17 15:42:36] __main__ INFO: \u001b[0mEpoch 25 Step 100/351 lr 0.100000 loss 2.1676 (2.1431) acc@1 0.1797 (0.1913) acc@5 0.6406 (0.6402)\n",
      "\u001b[32m[2020-07-17 15:45:00] __main__ INFO: \u001b[0mEpoch 25 Step 200/351 lr 0.100000 loss 2.0717 (2.1434) acc@1 0.2500 (0.1921) acc@5 0.7109 (0.6404)\n",
      "\u001b[32m[2020-07-17 15:47:23] __main__ INFO: \u001b[0mEpoch 25 Step 300/351 lr 0.100000 loss 2.1158 (2.1419) acc@1 0.1953 (0.1920) acc@5 0.6016 (0.6397)\n",
      "\u001b[32m[2020-07-17 15:48:36] __main__ INFO: \u001b[0mEpoch 25 Step 351/351 lr 0.100000 loss 2.1268 (2.1425) acc@1 0.2031 (0.1914) acc@5 0.6562 (0.6387)\n",
      "\u001b[32m[2020-07-17 15:48:36] __main__ INFO: \u001b[0mElapsed 502.75\n",
      "\u001b[32m[2020-07-17 15:48:36] __main__ INFO: \u001b[0mVal 25\n",
      "\u001b[32m[2020-07-17 15:48:54] __main__ INFO: \u001b[0mEpoch 25 loss 1.7243 acc@1 0.3806 acc@5 0.8710\n",
      "\u001b[32m[2020-07-17 15:48:54] __main__ INFO: \u001b[0mElapsed 18.00\n",
      "\u001b[32m[2020-07-17 15:48:54] __main__ INFO: \u001b[0mTrain 26 8775\n",
      "\u001b[32m[2020-07-17 15:51:17] __main__ INFO: \u001b[0mEpoch 26 Step 100/351 lr 0.100000 loss 2.1113 (2.1339) acc@1 0.1719 (0.1892) acc@5 0.6016 (0.6473)\n",
      "\u001b[32m[2020-07-17 15:53:40] __main__ INFO: \u001b[0mEpoch 26 Step 200/351 lr 0.100000 loss 2.1969 (2.1382) acc@1 0.1172 (0.1900) acc@5 0.6719 (0.6448)\n",
      "\u001b[32m[2020-07-17 15:56:04] __main__ INFO: \u001b[0mEpoch 26 Step 300/351 lr 0.100000 loss 2.1075 (2.1348) acc@1 0.1953 (0.1917) acc@5 0.6953 (0.6437)\n",
      "\u001b[32m[2020-07-17 15:57:17] __main__ INFO: \u001b[0mEpoch 26 Step 351/351 lr 0.100000 loss 2.1156 (2.1344) acc@1 0.2109 (0.1926) acc@5 0.6641 (0.6449)\n",
      "\u001b[32m[2020-07-17 15:57:17] __main__ INFO: \u001b[0mElapsed 503.18\n",
      "\u001b[32m[2020-07-17 15:57:17] __main__ INFO: \u001b[0mVal 26\n",
      "\u001b[32m[2020-07-17 15:57:35] __main__ INFO: \u001b[0mEpoch 26 loss 1.6490 acc@1 0.3998 acc@5 0.8984\n",
      "\u001b[32m[2020-07-17 15:57:35] __main__ INFO: \u001b[0mElapsed 18.11\n",
      "\u001b[32m[2020-07-17 15:57:35] __main__ INFO: \u001b[0mTrain 27 9126\n",
      "\u001b[32m[2020-07-17 15:59:59] __main__ INFO: \u001b[0mEpoch 27 Step 100/351 lr 0.100000 loss 2.1721 (2.1329) acc@1 0.1641 (0.1925) acc@5 0.5703 (0.6450)\n",
      "\u001b[32m[2020-07-17 16:02:23] __main__ INFO: \u001b[0mEpoch 27 Step 200/351 lr 0.100000 loss 2.1445 (2.1330) acc@1 0.1484 (0.1943) acc@5 0.6250 (0.6431)\n",
      "\u001b[32m[2020-07-17 16:04:46] __main__ INFO: \u001b[0mEpoch 27 Step 300/351 lr 0.100000 loss 2.1005 (2.1331) acc@1 0.2266 (0.1938) acc@5 0.6797 (0.6417)\n",
      "\u001b[32m[2020-07-17 16:05:59] __main__ INFO: \u001b[0mEpoch 27 Step 351/351 lr 0.100000 loss 2.1295 (2.1312) acc@1 0.1719 (0.1940) acc@5 0.6797 (0.6430)\n",
      "\u001b[32m[2020-07-17 16:05:59] __main__ INFO: \u001b[0mElapsed 503.99\n",
      "\u001b[32m[2020-07-17 16:05:59] __main__ INFO: \u001b[0mVal 27\n",
      "\u001b[32m[2020-07-17 16:06:17] __main__ INFO: \u001b[0mEpoch 27 loss 1.8381 acc@1 0.3576 acc@5 0.8236\n",
      "\u001b[32m[2020-07-17 16:06:17] __main__ INFO: \u001b[0mElapsed 18.10\n",
      "\u001b[32m[2020-07-17 16:06:17] __main__ INFO: \u001b[0mTrain 28 9477\n",
      "\u001b[32m[2020-07-17 16:08:41] __main__ INFO: \u001b[0mEpoch 28 Step 100/351 lr 0.100000 loss 2.0938 (2.1199) acc@1 0.1797 (0.2041) acc@5 0.6172 (0.6503)\n",
      "\u001b[32m[2020-07-17 16:11:05] __main__ INFO: \u001b[0mEpoch 28 Step 200/351 lr 0.100000 loss 2.1484 (2.1245) acc@1 0.1328 (0.1990) acc@5 0.6094 (0.6471)\n",
      "\u001b[32m[2020-07-17 16:13:28] __main__ INFO: \u001b[0mEpoch 28 Step 300/351 lr 0.100000 loss 2.1934 (2.1229) acc@1 0.1797 (0.1985) acc@5 0.6016 (0.6477)\n",
      "\u001b[32m[2020-07-17 16:14:41] __main__ INFO: \u001b[0mEpoch 28 Step 351/351 lr 0.100000 loss 2.0228 (2.1216) acc@1 0.2188 (0.1987) acc@5 0.6484 (0.6478)\n",
      "\u001b[32m[2020-07-17 16:14:41] __main__ INFO: \u001b[0mElapsed 503.57\n",
      "\u001b[32m[2020-07-17 16:14:41] __main__ INFO: \u001b[0mVal 28\n",
      "\u001b[32m[2020-07-17 16:14:59] __main__ INFO: \u001b[0mEpoch 28 loss 1.4643 acc@1 0.4694 acc@5 0.9214\n",
      "\u001b[32m[2020-07-17 16:14:59] __main__ INFO: \u001b[0mElapsed 17.97\n",
      "\u001b[32m[2020-07-17 16:14:59] __main__ INFO: \u001b[0mTrain 29 9828\n",
      "\u001b[32m[2020-07-17 16:17:22] __main__ INFO: \u001b[0mEpoch 29 Step 100/351 lr 0.100000 loss 2.1733 (2.1107) acc@1 0.1719 (0.2030) acc@5 0.6016 (0.6476)\n",
      "\u001b[32m[2020-07-17 16:19:46] __main__ INFO: \u001b[0mEpoch 29 Step 200/351 lr 0.100000 loss 2.1488 (2.1071) acc@1 0.1719 (0.2062) acc@5 0.5547 (0.6523)\n",
      "\u001b[32m[2020-07-17 16:22:09] __main__ INFO: \u001b[0mEpoch 29 Step 300/351 lr 0.100000 loss 2.0956 (2.1125) acc@1 0.1797 (0.2036) acc@5 0.6250 (0.6484)\n",
      "\u001b[32m[2020-07-17 16:23:22] __main__ INFO: \u001b[0mEpoch 29 Step 351/351 lr 0.100000 loss 2.1319 (2.1125) acc@1 0.1797 (0.2040) acc@5 0.5703 (0.6480)\n",
      "\u001b[32m[2020-07-17 16:23:22] __main__ INFO: \u001b[0mElapsed 503.50\n",
      "\u001b[32m[2020-07-17 16:23:22] __main__ INFO: \u001b[0mVal 29\n",
      "\u001b[32m[2020-07-17 16:23:40] __main__ INFO: \u001b[0mEpoch 29 loss 1.5638 acc@1 0.4382 acc@5 0.9030\n",
      "\u001b[32m[2020-07-17 16:23:40] __main__ INFO: \u001b[0mElapsed 17.99\n",
      "\u001b[32m[2020-07-17 16:23:40] __main__ INFO: \u001b[0mTrain 30 10179\n",
      "\u001b[32m[2020-07-17 16:26:03] __main__ INFO: \u001b[0mEpoch 30 Step 100/351 lr 0.100000 loss 1.9900 (2.1141) acc@1 0.1875 (0.2013) acc@5 0.6641 (0.6495)\n",
      "\u001b[32m[2020-07-17 16:28:25] __main__ INFO: \u001b[0mEpoch 30 Step 200/351 lr 0.100000 loss 2.0402 (2.1080) acc@1 0.1641 (0.2067) acc@5 0.6562 (0.6488)\n",
      "\u001b[32m[2020-07-17 16:30:48] __main__ INFO: \u001b[0mEpoch 30 Step 300/351 lr 0.100000 loss 2.1706 (2.1048) acc@1 0.1641 (0.2091) acc@5 0.6719 (0.6523)\n",
      "\u001b[32m[2020-07-17 16:32:01] __main__ INFO: \u001b[0mEpoch 30 Step 351/351 lr 0.100000 loss 2.0863 (2.1044) acc@1 0.1875 (0.2079) acc@5 0.6328 (0.6530)\n",
      "\u001b[32m[2020-07-17 16:32:01] __main__ INFO: \u001b[0mElapsed 501.10\n",
      "\u001b[32m[2020-07-17 16:32:01] __main__ INFO: \u001b[0mVal 30\n",
      "\u001b[32m[2020-07-17 16:32:19] __main__ INFO: \u001b[0mEpoch 30 loss 1.5088 acc@1 0.4434 acc@5 0.9064\n",
      "\u001b[32m[2020-07-17 16:32:19] __main__ INFO: \u001b[0mElapsed 17.98\n",
      "\u001b[32m[2020-07-17 16:32:19] __main__ INFO: \u001b[0mTrain 31 10530\n",
      "\u001b[32m[2020-07-17 16:34:42] __main__ INFO: \u001b[0mEpoch 31 Step 100/351 lr 0.100000 loss 2.2055 (2.1010) acc@1 0.2031 (0.2081) acc@5 0.6172 (0.6530)\n",
      "\u001b[32m[2020-07-17 16:37:06] __main__ INFO: \u001b[0mEpoch 31 Step 200/351 lr 0.100000 loss 2.0551 (2.0986) acc@1 0.2344 (0.2096) acc@5 0.6953 (0.6554)\n",
      "\u001b[32m[2020-07-17 16:39:29] __main__ INFO: \u001b[0mEpoch 31 Step 300/351 lr 0.100000 loss 2.0536 (2.0983) acc@1 0.2188 (0.2095) acc@5 0.6562 (0.6542)\n",
      "\u001b[32m[2020-07-17 16:40:42] __main__ INFO: \u001b[0mEpoch 31 Step 351/351 lr 0.100000 loss 2.0815 (2.0990) acc@1 0.1797 (0.2081) acc@5 0.6250 (0.6545)\n",
      "\u001b[32m[2020-07-17 16:40:42] __main__ INFO: \u001b[0mElapsed 502.34\n",
      "\u001b[32m[2020-07-17 16:40:42] __main__ INFO: \u001b[0mVal 31\n",
      "\u001b[32m[2020-07-17 16:41:00] __main__ INFO: \u001b[0mEpoch 31 loss 1.5590 acc@1 0.4214 acc@5 0.9072\n",
      "\u001b[32m[2020-07-17 16:41:00] __main__ INFO: \u001b[0mElapsed 18.05\n",
      "\u001b[32m[2020-07-17 16:41:00] __main__ INFO: \u001b[0mTrain 32 10881\n",
      "\u001b[32m[2020-07-17 16:43:23] __main__ INFO: \u001b[0mEpoch 32 Step 100/351 lr 0.100000 loss 2.0252 (2.0959) acc@1 0.2266 (0.2088) acc@5 0.6562 (0.6602)\n",
      "\u001b[32m[2020-07-17 16:45:46] __main__ INFO: \u001b[0mEpoch 32 Step 200/351 lr 0.100000 loss 2.0993 (2.0949) acc@1 0.1953 (0.2123) acc@5 0.6094 (0.6585)\n",
      "\u001b[32m[2020-07-17 16:48:09] __main__ INFO: \u001b[0mEpoch 32 Step 300/351 lr 0.100000 loss 2.0697 (2.0908) acc@1 0.2109 (0.2124) acc@5 0.6641 (0.6587)\n",
      "\u001b[32m[2020-07-17 16:49:22] __main__ INFO: \u001b[0mEpoch 32 Step 351/351 lr 0.100000 loss 2.0537 (2.0917) acc@1 0.2422 (0.2120) acc@5 0.6953 (0.6574)\n",
      "\u001b[32m[2020-07-17 16:49:22] __main__ INFO: \u001b[0mElapsed 502.14\n",
      "\u001b[32m[2020-07-17 16:49:22] __main__ INFO: \u001b[0mVal 32\n",
      "\u001b[32m[2020-07-17 16:49:40] __main__ INFO: \u001b[0mEpoch 32 loss 1.6264 acc@1 0.4040 acc@5 0.8782\n",
      "\u001b[32m[2020-07-17 16:49:40] __main__ INFO: \u001b[0mElapsed 18.04\n",
      "\u001b[32m[2020-07-17 16:49:40] __main__ INFO: \u001b[0mTrain 33 11232\n",
      "\u001b[32m[2020-07-17 16:52:03] __main__ INFO: \u001b[0mEpoch 33 Step 100/351 lr 0.100000 loss 2.0751 (2.0724) acc@1 0.2109 (0.2238) acc@5 0.6797 (0.6613)\n",
      "\u001b[32m[2020-07-17 16:54:26] __main__ INFO: \u001b[0mEpoch 33 Step 200/351 lr 0.100000 loss 2.0854 (2.0768) acc@1 0.2344 (0.2223) acc@5 0.6641 (0.6611)\n",
      "\u001b[32m[2020-07-17 16:56:49] __main__ INFO: \u001b[0mEpoch 33 Step 300/351 lr 0.100000 loss 2.0620 (2.0792) acc@1 0.2109 (0.2185) acc@5 0.5859 (0.6586)\n",
      "\u001b[32m[2020-07-17 16:58:02] __main__ INFO: \u001b[0mEpoch 33 Step 351/351 lr 0.100000 loss 2.0934 (2.0821) acc@1 0.2344 (0.2159) acc@5 0.6797 (0.6580)\n",
      "\u001b[32m[2020-07-17 16:58:02] __main__ INFO: \u001b[0mElapsed 501.69\n",
      "\u001b[32m[2020-07-17 16:58:02] __main__ INFO: \u001b[0mVal 33\n",
      "\u001b[32m[2020-07-17 16:58:20] __main__ INFO: \u001b[0mEpoch 33 loss 1.5060 acc@1 0.4604 acc@5 0.8934\n",
      "\u001b[32m[2020-07-17 16:58:20] __main__ INFO: \u001b[0mElapsed 18.04\n",
      "\u001b[32m[2020-07-17 16:58:20] __main__ INFO: \u001b[0mTrain 34 11583\n",
      "\u001b[32m[2020-07-17 17:00:42] __main__ INFO: \u001b[0mEpoch 34 Step 100/351 lr 0.100000 loss 2.0402 (2.0746) acc@1 0.2812 (0.2177) acc@5 0.6875 (0.6646)\n",
      "\u001b[32m[2020-07-17 17:03:05] __main__ INFO: \u001b[0mEpoch 34 Step 200/351 lr 0.100000 loss 2.0399 (2.0709) acc@1 0.2109 (0.2189) acc@5 0.6953 (0.6661)\n",
      "\u001b[32m[2020-07-17 17:05:28] __main__ INFO: \u001b[0mEpoch 34 Step 300/351 lr 0.100000 loss 2.0611 (2.0752) acc@1 0.1875 (0.2172) acc@5 0.6562 (0.6617)\n",
      "\u001b[32m[2020-07-17 17:06:41] __main__ INFO: \u001b[0mEpoch 34 Step 351/351 lr 0.100000 loss 2.0755 (2.0750) acc@1 0.2422 (0.2171) acc@5 0.7344 (0.6619)\n",
      "\u001b[32m[2020-07-17 17:06:41] __main__ INFO: \u001b[0mElapsed 501.21\n",
      "\u001b[32m[2020-07-17 17:06:41] __main__ INFO: \u001b[0mVal 34\n",
      "\u001b[32m[2020-07-17 17:06:59] __main__ INFO: \u001b[0mEpoch 34 loss 1.6248 acc@1 0.4248 acc@5 0.8922\n",
      "\u001b[32m[2020-07-17 17:06:59] __main__ INFO: \u001b[0mElapsed 18.01\n",
      "\u001b[32m[2020-07-17 17:06:59] __main__ INFO: \u001b[0mTrain 35 11934\n",
      "\u001b[32m[2020-07-17 17:09:21] __main__ INFO: \u001b[0mEpoch 35 Step 100/351 lr 0.100000 loss 2.1152 (2.0620) acc@1 0.2109 (0.2280) acc@5 0.6172 (0.6605)\n",
      "\u001b[32m[2020-07-17 17:11:44] __main__ INFO: \u001b[0mEpoch 35 Step 200/351 lr 0.100000 loss 2.0429 (2.0649) acc@1 0.2812 (0.2253) acc@5 0.6484 (0.6650)\n",
      "\u001b[32m[2020-07-17 17:14:07] __main__ INFO: \u001b[0mEpoch 35 Step 300/351 lr 0.100000 loss 2.0054 (2.0679) acc@1 0.2500 (0.2243) acc@5 0.7031 (0.6652)\n",
      "\u001b[32m[2020-07-17 17:15:20] __main__ INFO: \u001b[0mEpoch 35 Step 351/351 lr 0.100000 loss 2.0668 (2.0688) acc@1 0.2734 (0.2228) acc@5 0.7109 (0.6641)\n",
      "\u001b[32m[2020-07-17 17:15:20] __main__ INFO: \u001b[0mElapsed 500.92\n",
      "\u001b[32m[2020-07-17 17:15:20] __main__ INFO: \u001b[0mVal 35\n",
      "\u001b[32m[2020-07-17 17:15:38] __main__ INFO: \u001b[0mEpoch 35 loss 1.5499 acc@1 0.4576 acc@5 0.8950\n",
      "\u001b[32m[2020-07-17 17:15:38] __main__ INFO: \u001b[0mElapsed 18.06\n",
      "\u001b[32m[2020-07-17 17:15:38] __main__ INFO: \u001b[0mTrain 36 12285\n",
      "\u001b[32m[2020-07-17 17:18:01] __main__ INFO: \u001b[0mEpoch 36 Step 100/351 lr 0.100000 loss 1.9870 (2.0611) acc@1 0.2266 (0.2260) acc@5 0.7266 (0.6698)\n",
      "\u001b[32m[2020-07-17 17:20:23] __main__ INFO: \u001b[0mEpoch 36 Step 200/351 lr 0.100000 loss 2.1024 (2.0669) acc@1 0.2031 (0.2221) acc@5 0.7109 (0.6648)\n",
      "\u001b[32m[2020-07-17 17:22:46] __main__ INFO: \u001b[0mEpoch 36 Step 300/351 lr 0.100000 loss 2.0371 (2.0652) acc@1 0.2578 (0.2213) acc@5 0.7266 (0.6632)\n",
      "\u001b[32m[2020-07-17 17:23:59] __main__ INFO: \u001b[0mEpoch 36 Step 351/351 lr 0.100000 loss 2.0088 (2.0646) acc@1 0.2109 (0.2218) acc@5 0.6953 (0.6621)\n",
      "\u001b[32m[2020-07-17 17:23:59] __main__ INFO: \u001b[0mElapsed 501.67\n",
      "\u001b[32m[2020-07-17 17:23:59] __main__ INFO: \u001b[0mVal 36\n",
      "\u001b[32m[2020-07-17 17:24:18] __main__ INFO: \u001b[0mEpoch 36 loss 1.4392 acc@1 0.4812 acc@5 0.9178\n",
      "\u001b[32m[2020-07-17 17:24:18] __main__ INFO: \u001b[0mElapsed 18.08\n",
      "\u001b[32m[2020-07-17 17:24:18] __main__ INFO: \u001b[0mTrain 37 12636\n",
      "\u001b[32m[2020-07-17 17:26:40] __main__ INFO: \u001b[0mEpoch 37 Step 100/351 lr 0.100000 loss 2.0504 (2.0577) acc@1 0.2812 (0.2288) acc@5 0.7031 (0.6673)\n",
      "\u001b[32m[2020-07-17 17:29:03] __main__ INFO: \u001b[0mEpoch 37 Step 200/351 lr 0.100000 loss 2.0439 (2.0624) acc@1 0.2656 (0.2247) acc@5 0.6484 (0.6661)\n",
      "\u001b[32m[2020-07-17 17:31:27] __main__ INFO: \u001b[0mEpoch 37 Step 300/351 lr 0.100000 loss 2.0440 (2.0580) acc@1 0.2109 (0.2264) acc@5 0.6641 (0.6671)\n",
      "\u001b[32m[2020-07-17 17:32:40] __main__ INFO: \u001b[0mEpoch 37 Step 351/351 lr 0.100000 loss 2.0566 (2.0573) acc@1 0.1953 (0.2259) acc@5 0.6484 (0.6673)\n",
      "\u001b[32m[2020-07-17 17:32:40] __main__ INFO: \u001b[0mElapsed 502.04\n",
      "\u001b[32m[2020-07-17 17:32:40] __main__ INFO: \u001b[0mVal 37\n",
      "\u001b[32m[2020-07-17 17:32:58] __main__ INFO: \u001b[0mEpoch 37 loss 1.7181 acc@1 0.4290 acc@5 0.9192\n",
      "\u001b[32m[2020-07-17 17:32:58] __main__ INFO: \u001b[0mElapsed 18.12\n",
      "\u001b[32m[2020-07-17 17:32:58] __main__ INFO: \u001b[0mTrain 38 12987\n",
      "\u001b[32m[2020-07-17 17:35:21] __main__ INFO: \u001b[0mEpoch 38 Step 100/351 lr 0.100000 loss 2.0327 (2.0383) acc@1 0.2266 (0.2306) acc@5 0.6719 (0.6683)\n",
      "\u001b[32m[2020-07-17 17:37:44] __main__ INFO: \u001b[0mEpoch 38 Step 200/351 lr 0.100000 loss 2.0910 (2.0456) acc@1 0.2266 (0.2304) acc@5 0.6797 (0.6668)\n",
      "\u001b[32m[2020-07-17 17:40:07] __main__ INFO: \u001b[0mEpoch 38 Step 300/351 lr 0.100000 loss 2.1266 (2.0516) acc@1 0.2266 (0.2274) acc@5 0.6719 (0.6653)\n",
      "\u001b[32m[2020-07-17 17:41:19] __main__ INFO: \u001b[0mEpoch 38 Step 351/351 lr 0.100000 loss 1.9897 (2.0528) acc@1 0.2500 (0.2263) acc@5 0.6562 (0.6628)\n",
      "\u001b[32m[2020-07-17 17:41:19] __main__ INFO: \u001b[0mElapsed 501.73\n",
      "\u001b[32m[2020-07-17 17:41:19] __main__ INFO: \u001b[0mVal 38\n",
      "\u001b[32m[2020-07-17 17:41:37] __main__ INFO: \u001b[0mEpoch 38 loss 1.6314 acc@1 0.4266 acc@5 0.8792\n",
      "\u001b[32m[2020-07-17 17:41:37] __main__ INFO: \u001b[0mElapsed 18.02\n",
      "\u001b[32m[2020-07-17 17:41:37] __main__ INFO: \u001b[0mTrain 39 13338\n",
      "\u001b[32m[2020-07-17 17:44:00] __main__ INFO: \u001b[0mEpoch 39 Step 100/351 lr 0.100000 loss 2.0573 (2.0420) acc@1 0.2656 (0.2323) acc@5 0.6875 (0.6680)\n",
      "\u001b[32m[2020-07-17 17:46:23] __main__ INFO: \u001b[0mEpoch 39 Step 200/351 lr 0.100000 loss 2.0261 (2.0467) acc@1 0.2266 (0.2306) acc@5 0.6406 (0.6682)\n",
      "\u001b[32m[2020-07-17 17:48:46] __main__ INFO: \u001b[0mEpoch 39 Step 300/351 lr 0.100000 loss 1.9188 (2.0474) acc@1 0.3047 (0.2312) acc@5 0.7188 (0.6680)\n",
      "\u001b[32m[2020-07-17 17:49:58] __main__ INFO: \u001b[0mEpoch 39 Step 351/351 lr 0.100000 loss 2.0554 (2.0479) acc@1 0.2188 (0.2313) acc@5 0.6016 (0.6684)\n",
      "\u001b[32m[2020-07-17 17:49:58] __main__ INFO: \u001b[0mElapsed 500.99\n",
      "\u001b[32m[2020-07-17 17:49:58] __main__ INFO: \u001b[0mVal 39\n",
      "\u001b[32m[2020-07-17 17:50:16] __main__ INFO: \u001b[0mEpoch 39 loss 1.7483 acc@1 0.4152 acc@5 0.8850\n",
      "\u001b[32m[2020-07-17 17:50:16] __main__ INFO: \u001b[0mElapsed 18.01\n",
      "\u001b[32m[2020-07-17 17:50:16] __main__ INFO: \u001b[0mTrain 40 13689\n",
      "\u001b[32m[2020-07-17 17:52:39] __main__ INFO: \u001b[0mEpoch 40 Step 100/351 lr 0.100000 loss 2.0440 (2.0418) acc@1 0.1875 (0.2321) acc@5 0.6094 (0.6697)\n",
      "\u001b[32m[2020-07-17 17:55:02] __main__ INFO: \u001b[0mEpoch 40 Step 200/351 lr 0.100000 loss 2.0752 (2.0414) acc@1 0.1875 (0.2311) acc@5 0.6953 (0.6704)\n"
     ]
    }
   ],
   "source": [
    "# Train the model per the settings specified in the original paper\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "!python train.py --config configs/cifar/resnext.yaml \\\n",
    "    model.resnext.cardinality 4 \\\n",
    "    train.batch_size 128 \\\n",
    "    train.base_lr 0.1 \\\n",
    "    dataset.name CIFAR10_RA_3_20 \\\n",
    "    train.base_lr 0.1 \\\n",
    "    train.output_dir /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_3_20_c10val/exp00 \\\n",
    "    scheduler.epochs 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Refine the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-19 21:44:10] __main__ INFO: \u001b[0mdevice: cuda\n",
      "cudnn:\n",
      "  benchmark: True\n",
      "  deterministic: False\n",
      "dataset:\n",
      "  name: CIFAR10\n",
      "  dataset_dir: ~/.torch/datasets/CIFAR10\n",
      "  image_size: 32\n",
      "  n_channels: 3\n",
      "  n_classes: 10\n",
      "model:\n",
      "  type: cifar\n",
      "  name: resnext\n",
      "  init_mode: kaiming_fan_out\n",
      "  vgg:\n",
      "    n_channels: [64, 128, 256, 512, 512]\n",
      "    n_layers: [2, 2, 3, 3, 3]\n",
      "    use_bn: True\n",
      "  resnet:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "  resnet_preact:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "  wrn:\n",
      "    depth: 28\n",
      "    initial_channels: 16\n",
      "    widening_factor: 10\n",
      "    drop_rate: 0.0\n",
      "  densenet:\n",
      "    depth: 100\n",
      "    n_blocks: [6, 12, 24, 16]\n",
      "    block_type: bottleneck\n",
      "    growth_rate: 12\n",
      "    drop_rate: 0.0\n",
      "    compression_rate: 0.5\n",
      "  pyramidnet:\n",
      "    depth: 272\n",
      "    n_blocks: [3, 24, 36, 3]\n",
      "    initial_channels: 16\n",
      "    block_type: bottleneck\n",
      "    alpha: 200\n",
      "  resnext:\n",
      "    depth: 29\n",
      "    n_blocks: [3, 4, 6, 3]\n",
      "    initial_channels: 64\n",
      "    cardinality: 4\n",
      "    base_channels: 64\n",
      "  shake_shake:\n",
      "    depth: 26\n",
      "    initial_channels: 96\n",
      "    shake_forward: True\n",
      "    shake_backward: True\n",
      "    shake_image: True\n",
      "  se_resnet_preact:\n",
      "    depth: 110\n",
      "    initial_channels: 16\n",
      "    se_reduction: 16\n",
      "    block_type: basic\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "train:\n",
      "  checkpoint: /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_3_20_c10val/exp00/checkpoint_00400.pth\n",
      "  resume: False\n",
      "  precision: O0\n",
      "  batch_size: 128\n",
      "  subdivision: 1\n",
      "  optimizer: sgd\n",
      "  base_lr: 0.001\n",
      "  momentum: 0.9\n",
      "  nesterov: True\n",
      "  weight_decay: 0.0005\n",
      "  no_weight_decay_on_bn: False\n",
      "  gradient_clip: 0\n",
      "  start_epoch: 0\n",
      "  seed: 0\n",
      "  val_first: True\n",
      "  val_period: 1\n",
      "  val_ratio: 0.1\n",
      "  use_test_as_val: False\n",
      "  output_dir: /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_3_20_c10val/exp00_resume400_50\n",
      "  log_period: 100\n",
      "  checkpoint_period: 100\n",
      "  use_tensorboard: True\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: True\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "  distributed: False\n",
      "  dist:\n",
      "    backend: nccl\n",
      "    init_method: env://\n",
      "    world_size: -1\n",
      "    node_rank: -1\n",
      "    local_rank: 0\n",
      "    use_sync_bn: False\n",
      "tensorboard:\n",
      "  train_images: False\n",
      "  val_images: False\n",
      "  model_params: False\n",
      "optim:\n",
      "  adam:\n",
      "    betas: (0.9, 0.999)\n",
      "  lars:\n",
      "    eps: 1e-09\n",
      "    threshold: 0.01\n",
      "  adabound:\n",
      "    betas: (0.9, 0.999)\n",
      "    final_lr: 0.1\n",
      "    gamma: 0.001\n",
      "scheduler:\n",
      "  epochs: 50\n",
      "  warmup:\n",
      "    type: none\n",
      "    epochs: 0\n",
      "    start_factor: 0.001\n",
      "    exponent: 4\n",
      "  type: multistep\n",
      "  milestones: [150, 225]\n",
      "  lr_decay: 0.1\n",
      "  lr_min_factor: 0.001\n",
      "  T0: 10\n",
      "  T_mul: 1.0\n",
      "validation:\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: False\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "augmentation:\n",
      "  use_random_crop: True\n",
      "  use_random_horizontal_flip: True\n",
      "  use_cutout: False\n",
      "  use_random_erasing: False\n",
      "  use_dual_cutout: False\n",
      "  use_mixup: False\n",
      "  use_ricap: False\n",
      "  use_cutmix: False\n",
      "  use_label_smoothing: False\n",
      "  random_crop:\n",
      "    padding: 4\n",
      "    fill: 0\n",
      "    padding_mode: constant\n",
      "  random_horizontal_flip:\n",
      "    prob: 0.5\n",
      "  cutout:\n",
      "    prob: 1.0\n",
      "    mask_size: 16\n",
      "    cut_inside: False\n",
      "    mask_color: 0\n",
      "    dual_cutout_alpha: 0.1\n",
      "  random_erasing:\n",
      "    prob: 0.5\n",
      "    area_ratio_range: [0.02, 0.4]\n",
      "    min_aspect_ratio: 0.3\n",
      "    max_attempt: 20\n",
      "  mixup:\n",
      "    alpha: 1.0\n",
      "  ricap:\n",
      "    beta: 0.3\n",
      "  cutmix:\n",
      "    alpha: 1.0\n",
      "  label_smoothing:\n",
      "    epsilon: 0.1\n",
      "tta:\n",
      "  use_resize: False\n",
      "  use_center_crop: False\n",
      "  resize: 256\n",
      "test:\n",
      "  checkpoint: ''\n",
      "  output_dir: ''\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    pin_memory: False\n",
      "\u001b[32m[2020-07-19 21:44:10] __main__ INFO: \u001b[0menv_info:\n",
      "  pytorch_version: 1.4.0\n",
      "  cuda_version: 10.1\n",
      "  cudnn_version: 7603\n",
      "  num_gpus: 1\n",
      "  gpu_name: Tesla K80\n",
      "  gpu_capability: 3.7\n",
      "Files already downloaded and verified\n",
      "\u001b[32m[2020-07-19 21:44:14] __main__ INFO: \u001b[0mMACs  : 2.75G\n",
      "\u001b[32m[2020-07-19 21:44:14] __main__ INFO: \u001b[0m#params: 17.56M\n",
      "Selected optimization level O0:  Pure FP32 training.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\",)\n",
      "\u001b[32m[2020-07-19 21:44:14] __main__ INFO: \u001b[0mVal 0\n",
      "\u001b[32m[2020-07-19 21:44:40] __main__ INFO: \u001b[0mEpoch 0 loss 1.5521 acc@1 0.6084 acc@5 0.9502\n",
      "\u001b[32m[2020-07-19 21:44:40] __main__ INFO: \u001b[0mElapsed 25.98\n",
      "\u001b[32m[2020-07-19 21:44:40] __main__ INFO: \u001b[0mTrain 1 0\n",
      "\u001b[32m[2020-07-19 21:47:10] __main__ INFO: \u001b[0mEpoch 1 Step 100/351 lr 0.001000 loss 0.4488 (0.5642) acc@1 0.8594 (0.8148) acc@5 1.0000 (0.9866)\n",
      "\u001b[32m[2020-07-19 21:49:32] __main__ INFO: \u001b[0mEpoch 1 Step 200/351 lr 0.001000 loss 0.4457 (0.5097) acc@1 0.8438 (0.8311) acc@5 0.9922 (0.9900)\n",
      "\u001b[32m[2020-07-19 21:51:55] __main__ INFO: \u001b[0mEpoch 1 Step 300/351 lr 0.001000 loss 0.2879 (0.4760) acc@1 0.8984 (0.8409) acc@5 1.0000 (0.9915)\n",
      "\u001b[32m[2020-07-19 21:53:08] __main__ INFO: \u001b[0mEpoch 1 Step 351/351 lr 0.001000 loss 0.3471 (0.4648) acc@1 0.8828 (0.8450) acc@5 1.0000 (0.9919)\n",
      "\u001b[32m[2020-07-19 21:53:08] __main__ INFO: \u001b[0mElapsed 507.90\n",
      "\u001b[32m[2020-07-19 21:53:08] __main__ INFO: \u001b[0mVal 1\n",
      "\u001b[32m[2020-07-19 21:53:26] __main__ INFO: \u001b[0mEpoch 1 loss 0.4236 acc@1 0.8554 acc@5 0.9934\n",
      "\u001b[32m[2020-07-19 21:53:26] __main__ INFO: \u001b[0mElapsed 18.03\n",
      "\u001b[32m[2020-07-19 21:53:26] __main__ INFO: \u001b[0mTrain 2 351\n",
      "\u001b[32m[2020-07-19 21:55:49] __main__ INFO: \u001b[0mEpoch 2 Step 100/351 lr 0.001000 loss 0.2757 (0.3270) acc@1 0.8984 (0.8928) acc@5 0.9844 (0.9958)\n",
      "\u001b[32m[2020-07-19 21:58:12] __main__ INFO: \u001b[0mEpoch 2 Step 200/351 lr 0.001000 loss 0.4829 (0.3271) acc@1 0.8203 (0.8910) acc@5 0.9922 (0.9962)\n",
      "\u001b[32m[2020-07-19 22:00:35] __main__ INFO: \u001b[0mEpoch 2 Step 300/351 lr 0.001000 loss 0.2066 (0.3253) acc@1 0.9375 (0.8915) acc@5 1.0000 (0.9964)\n",
      "\u001b[32m[2020-07-19 22:01:48] __main__ INFO: \u001b[0mEpoch 2 Step 351/351 lr 0.001000 loss 0.5144 (0.3252) acc@1 0.8125 (0.8915) acc@5 0.9766 (0.9963)\n",
      "\u001b[32m[2020-07-19 22:01:48] __main__ INFO: \u001b[0mElapsed 501.77\n",
      "\u001b[32m[2020-07-19 22:01:48] __main__ INFO: \u001b[0mVal 2\n",
      "\u001b[32m[2020-07-19 22:02:06] __main__ INFO: \u001b[0mEpoch 2 loss 0.3816 acc@1 0.8706 acc@5 0.9940\n",
      "\u001b[32m[2020-07-19 22:02:06] __main__ INFO: \u001b[0mElapsed 18.05\n",
      "\u001b[32m[2020-07-19 22:02:06] __main__ INFO: \u001b[0mTrain 3 702\n",
      "\u001b[32m[2020-07-19 22:04:29] __main__ INFO: \u001b[0mEpoch 3 Step 100/351 lr 0.001000 loss 0.1706 (0.2765) acc@1 0.9531 (0.9102) acc@5 1.0000 (0.9973)\n",
      "\u001b[32m[2020-07-19 22:06:52] __main__ INFO: \u001b[0mEpoch 3 Step 200/351 lr 0.001000 loss 0.2997 (0.2731) acc@1 0.8984 (0.9105) acc@5 1.0000 (0.9974)\n",
      "\u001b[32m[2020-07-19 22:09:15] __main__ INFO: \u001b[0mEpoch 3 Step 300/351 lr 0.001000 loss 0.2753 (0.2704) acc@1 0.9297 (0.9110) acc@5 1.0000 (0.9974)\n",
      "\u001b[32m[2020-07-19 22:10:28] __main__ INFO: \u001b[0mEpoch 3 Step 351/351 lr 0.001000 loss 0.2524 (0.2703) acc@1 0.9453 (0.9116) acc@5 1.0000 (0.9975)\n",
      "\u001b[32m[2020-07-19 22:10:28] __main__ INFO: \u001b[0mElapsed 502.14\n",
      "\u001b[32m[2020-07-19 22:10:28] __main__ INFO: \u001b[0mVal 3\n",
      "\u001b[32m[2020-07-19 22:10:46] __main__ INFO: \u001b[0mEpoch 3 loss 0.3690 acc@1 0.8762 acc@5 0.9944\n",
      "\u001b[32m[2020-07-19 22:10:46] __main__ INFO: \u001b[0mElapsed 18.04\n",
      "\u001b[32m[2020-07-19 22:10:46] __main__ INFO: \u001b[0mTrain 4 1053\n",
      "\u001b[32m[2020-07-19 22:13:09] __main__ INFO: \u001b[0mEpoch 4 Step 100/351 lr 0.001000 loss 0.2296 (0.2381) acc@1 0.9141 (0.9255) acc@5 1.0000 (0.9979)\n",
      "\u001b[32m[2020-07-19 22:15:32] __main__ INFO: \u001b[0mEpoch 4 Step 200/351 lr 0.001000 loss 0.2430 (0.2318) acc@1 0.9219 (0.9268) acc@5 1.0000 (0.9981)\n",
      "\u001b[32m[2020-07-19 22:17:55] __main__ INFO: \u001b[0mEpoch 4 Step 300/351 lr 0.001000 loss 0.1980 (0.2325) acc@1 0.9453 (0.9254) acc@5 1.0000 (0.9982)\n",
      "\u001b[32m[2020-07-19 22:19:08] __main__ INFO: \u001b[0mEpoch 4 Step 351/351 lr 0.001000 loss 0.2995 (0.2309) acc@1 0.8828 (0.9260) acc@5 1.0000 (0.9982)\n",
      "\u001b[32m[2020-07-19 22:19:08] __main__ INFO: \u001b[0mElapsed 501.91\n",
      "\u001b[32m[2020-07-19 22:19:08] __main__ INFO: \u001b[0mVal 4\n",
      "\u001b[32m[2020-07-19 22:19:26] __main__ INFO: \u001b[0mEpoch 4 loss 0.3530 acc@1 0.8786 acc@5 0.9944\n",
      "\u001b[32m[2020-07-19 22:19:26] __main__ INFO: \u001b[0mElapsed 18.00\n",
      "\u001b[32m[2020-07-19 22:19:26] __main__ INFO: \u001b[0mTrain 5 1404\n",
      "\u001b[32m[2020-07-19 22:21:49] __main__ INFO: \u001b[0mEpoch 5 Step 100/351 lr 0.001000 loss 0.0924 (0.1921) acc@1 0.9844 (0.9405) acc@5 1.0000 (0.9987)\n",
      "\u001b[32m[2020-07-19 22:24:12] __main__ INFO: \u001b[0mEpoch 5 Step 200/351 lr 0.001000 loss 0.2436 (0.1996) acc@1 0.9062 (0.9372) acc@5 1.0000 (0.9988)\n",
      "\u001b[32m[2020-07-19 22:26:34] __main__ INFO: \u001b[0mEpoch 5 Step 300/351 lr 0.001000 loss 0.1759 (0.2030) acc@1 0.9453 (0.9365) acc@5 1.0000 (0.9989)\n",
      "\u001b[32m[2020-07-19 22:27:47] __main__ INFO: \u001b[0mEpoch 5 Step 351/351 lr 0.001000 loss 0.1390 (0.2045) acc@1 0.9688 (0.9361) acc@5 1.0000 (0.9988)\n",
      "\u001b[32m[2020-07-19 22:27:47] __main__ INFO: \u001b[0mElapsed 501.27\n",
      "\u001b[32m[2020-07-19 22:27:47] __main__ INFO: \u001b[0mVal 5\n",
      "\u001b[32m[2020-07-19 22:28:05] __main__ INFO: \u001b[0mEpoch 5 loss 0.3448 acc@1 0.8838 acc@5 0.9958\n",
      "\u001b[32m[2020-07-19 22:28:05] __main__ INFO: \u001b[0mElapsed 17.92\n",
      "\u001b[32m[2020-07-19 22:28:05] __main__ INFO: \u001b[0mTrain 6 1755\n",
      "\u001b[32m[2020-07-19 22:30:28] __main__ INFO: \u001b[0mEpoch 6 Step 100/351 lr 0.001000 loss 0.1402 (0.1664) acc@1 0.9688 (0.9523) acc@5 1.0000 (0.9995)\n",
      "\u001b[32m[2020-07-19 22:32:51] __main__ INFO: \u001b[0mEpoch 6 Step 200/351 lr 0.001000 loss 0.2371 (0.1729) acc@1 0.9219 (0.9498) acc@5 1.0000 (0.9992)\n",
      "\u001b[32m[2020-07-19 22:35:14] __main__ INFO: \u001b[0mEpoch 6 Step 300/351 lr 0.001000 loss 0.1824 (0.1762) acc@1 0.9297 (0.9476) acc@5 1.0000 (0.9991)\n",
      "\u001b[32m[2020-07-19 22:36:27] __main__ INFO: \u001b[0mEpoch 6 Step 351/351 lr 0.001000 loss 0.1739 (0.1763) acc@1 0.9375 (0.9475) acc@5 1.0000 (0.9991)\n",
      "\u001b[32m[2020-07-19 22:36:27] __main__ INFO: \u001b[0mElapsed 501.71\n",
      "\u001b[32m[2020-07-19 22:36:27] __main__ INFO: \u001b[0mVal 6\n"
     ]
    }
   ],
   "source": [
    "# Resume training with the un-augmented data\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "!python train.py --config /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_3_20_c10val/exp00/config.yaml \\\n",
    "    train.checkpoint /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_3_20_c10val/exp00/checkpoint_00400.pth \\\n",
    "    dataset.name CIFAR10 \\\n",
    "    model.resnext.cardinality 4 \\\n",
    "    train.batch_size 128 \\\n",
    "    train.base_lr 0.001 \\\n",
    "    train.output_dir /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_3_20_c10val/exp00_resume400_50 \\\n",
    "    scheduler.epochs 50\n",
    "\n",
    "#### Set LEARNING RATE based on ending LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-20 23:38:29] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_3_20_c10val/exp00_resume400_50/checkpoint_00050.pth\n",
      "Files already downloaded and verified\n",
      "100%|███████████████████████████████████████████| 79/79 [00:36<00:00,  2.19it/s]\n",
      "\u001b[32m[2020-07-20 23:39:06] __main__ INFO: \u001b[0mElapsed 36.11\n",
      "\u001b[32m[2020-07-20 23:39:06] __main__ INFO: \u001b[0mLoss 0.3331 Accuracy 0.9027\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/resnext.yaml \\\n",
    "   model.resnext.cardinality 4 \\\n",
    "   test.batch_size 128 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_3_20_c10val/exp00_resume400_50/checkpoint_00050.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_3_20_c10val/exp00_resume400_50/test_results_0050_cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-20 23:39:26] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_3_20_c10val/exp00_resume400_50/checkpoint_00050.pth\n",
      "CIFAR 10.1\n",
      "100%|███████████████████████████████████████████| 16/16 [00:07<00:00,  2.13it/s]\n",
      "\u001b[32m[2020-07-20 23:39:34] __main__ INFO: \u001b[0mElapsed 7.51\n",
      "\u001b[32m[2020-07-20 23:39:34] __main__ INFO: \u001b[0mLoss 0.6909 Accuracy 0.8175\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10.1 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/resnext.yaml \\\n",
    "   model.resnext.cardinality 4 \\\n",
    "   test.batch_size 128 \\\n",
    "   dataset.name CIFAR101 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_3_20_c10val/exp00_resume400_50/checkpoint_00050.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_3_20_c10val/exp00_resume400_50/test_results_0050_cifar101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-20 23:39:47] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_3_20_c10val/exp00/checkpoint_00400.pth\n",
      "Files already downloaded and verified\n",
      "100%|███████████████████████████████████████████| 79/79 [00:36<00:00,  2.18it/s]\n",
      "\u001b[32m[2020-07-20 23:40:24] __main__ INFO: \u001b[0mElapsed 36.20\n",
      "\u001b[32m[2020-07-20 23:40:24] __main__ INFO: \u001b[0mLoss 1.5333 Accuracy 0.6144\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/resnext.yaml \\\n",
    "   model.resnext.cardinality 4 \\\n",
    "   test.batch_size 128 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_3_20_c10val/exp00/checkpoint_00400.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_3_20_c10val/exp00/test_results_0400_cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-20 23:40:43] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_3_20_c10val/exp00/checkpoint_00400.pth\n",
      "CIFAR 10.1\n",
      "100%|███████████████████████████████████████████| 16/16 [00:07<00:00,  2.14it/s]\n",
      "\u001b[32m[2020-07-20 23:40:51] __main__ INFO: \u001b[0mElapsed 7.49\n",
      "\u001b[32m[2020-07-20 23:40:51] __main__ INFO: \u001b[0mLoss 1.9963 Accuracy 0.5025\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10.1 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "!python evaluate.py --config configs/cifar/resnext.yaml \\\n",
    "   model.resnext.cardinality 4 \\\n",
    "   test.batch_size 128 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_3_20_c10val/exp00/checkpoint_00400.pth \\\n",
    "   dataset.name CIFAR101 \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_3_20_c10val/exp00/test_results_0400_cifar101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Record the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Testset</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Original_Accuracy</th>\n",
       "      <th>Original_CI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>resnext_29_4x64d_ra_3_20_c10val</td>\n",
       "      <td>400</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>1.5333</td>\n",
       "      <td>0.6144</td>\n",
       "      <td>96.4</td>\n",
       "      <td>(96.0, 96.7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>resnext_29_4x64d_ra_3_20_c10val</td>\n",
       "      <td>400</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>1.9963</td>\n",
       "      <td>0.5025</td>\n",
       "      <td>89.6</td>\n",
       "      <td>(88.2, 90.9)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>resnext_29_4x64d_ra_3_20_c10val_refined400</td>\n",
       "      <td>50</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>0.6909</td>\n",
       "      <td>0.8175</td>\n",
       "      <td>89.6</td>\n",
       "      <td>(88.2, 90.9)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>resnext_29_4x64d_ra_3_20_c10val_refined400</td>\n",
       "      <td>50</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.3331</td>\n",
       "      <td>0.9027</td>\n",
       "      <td>96.4</td>\n",
       "      <td>(96.0, 96.7)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Model Epoch    Testset    Loss  \\\n",
       "0             resnext_29_4x64d_ra_3_20_c10val   400    cifar10  1.5333   \n",
       "1             resnext_29_4x64d_ra_3_20_c10val   400  cifar10.1  1.9963   \n",
       "2  resnext_29_4x64d_ra_3_20_c10val_refined400    50  cifar10.1  0.6909   \n",
       "3  resnext_29_4x64d_ra_3_20_c10val_refined400    50    cifar10  0.3331   \n",
       "\n",
       "  Accuracy  Original_Accuracy   Original_CI  \n",
       "0   0.6144               96.4  (96.0, 96.7)  \n",
       "1   0.5025               89.6  (88.2, 90.9)  \n",
       "2   0.8175               89.6  (88.2, 90.9)  \n",
       "3   0.9027               96.4  (96.0, 96.7)  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "model = 'resnext_29_4x64d_ra_3_20_c10val'\n",
    "model_refined = model + '_refined400'\n",
    "\n",
    "a = pd.Series([model, 400, 'cifar10', 1.5333, 0.6144])\n",
    "c = pd.Series([model, 400, 'cifar10.1', 1.9963, 0.5025])\n",
    "\n",
    "e = pd.Series([model_refined, 50, 'cifar10.1', 0.6909, 0.8175])\n",
    "f = pd.Series([model_refined, 50, 'cifar10', 0.3331, 0.9027])\n",
    "               \n",
    "df_results = pd.concat([a,c,e,f], axis=1).T\n",
    "df_results.columns = ['Model', 'Epoch', 'Testset', 'Loss', 'Accuracy']\n",
    "\n",
    "df_results['Original_Accuracy'] = df_results.apply((lambda row: 96.4 if row[2] == 'cifar10' else 89.6), axis=1)\n",
    "df_results['Original_CI'] = df_results.apply((lambda row: (96.0, 96.7) if row[2] == 'cifar10' else (88.2, 90.9)), axis=1)\n",
    "\n",
    "df_results.to_csv('/home/ec2-user/SageMaker/experiments/' + model + '/results.csv')\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the model checkpoints, configs, and results to S3 \n",
    "bucket='sagemaker-june29'\n",
    "prefix = 'sagemaker/results/original-models/resnext_29_4x64d_ra_3_20_c10val'\n",
    "path = '/home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_3_20_c10val'\n",
    "\n",
    "s3_resource = boto3.resource(\"s3\", region_name=\"us-east-1\")\n",
    "\n",
    "def uploadDirectory(local_path,bucket_name,s3_prefix):\n",
    "\n",
    "    my_bucket = s3_resource.Bucket(bucket_name)\n",
    "    \n",
    "    for path, subdirs, files in os.walk(local_path):\n",
    "        path = path.replace(\"\\\\\",\"/\")\n",
    "        directory_name = path.replace(local_path,\"\")\n",
    "        for file in files:\n",
    "            #print(\"Local File:\", os.path.join(path, file))\n",
    "            #print(\"      Dest:\", s3_prefix+directory_name+'/'+file)\n",
    "            my_bucket.upload_file(os.path.join(path, file), s3_prefix+directory_name+'/'+file)\n",
    "    \n",
    "uploadDirectory(path,bucket,prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
