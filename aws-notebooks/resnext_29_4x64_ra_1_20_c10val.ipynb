{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNext 29 4x64\n",
    "\n",
    " - Training Dataset:  RandAugment, N=1, M=20\n",
    "   Validation with Unaugmented Data\n",
    " - Sagemaker Notebook must be of type, conda_pytorch_p36\n",
    " \n",
    "#### Install Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import numpy \n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 1)) (1.18.1)\n",
      "Requirement already satisfied: torch>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 2)) (1.4.0)\n",
      "Requirement already satisfied: torchvision in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (0.5.0)\n",
      "Requirement already satisfied: fvcore in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (0.1.1.post20200716)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 5)) (4.44.1)\n",
      "Requirement already satisfied: yacs in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 6)) (0.1.7)\n",
      "Requirement already satisfied: apex from git+https://github.com/NVIDIA/apex.git#egg=apex in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 7)) (0.1)\n",
      "Requirement already satisfied: termcolor in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 8)) (1.1.0)\n",
      "Requirement already satisfied: thop<0.0.31.post2004070130 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 9)) (0.0.31.post2001170342)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (1.14.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (7.0.0)\n",
      "Requirement already satisfied: tabulate in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (0.8.7)\n",
      "Requirement already satisfied: portalocker in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (1.7.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (5.3.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: tensorboard in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (2.2.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.0.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.30.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (2.23.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (3.2.2)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (0.34.2)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.19.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (46.1.3.post20200330)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.7.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (0.4.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.14.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.18.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (0.9.0)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (3.12.2)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (2020.4.5.2)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (1.25.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (2.9)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard) (1.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (4.1.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (3.4.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard) (2.2.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r '/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt'\n",
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-17 01:42:39] __main__ INFO: \u001b[0mdevice: cuda\n",
      "cudnn:\n",
      "  benchmark: True\n",
      "  deterministic: False\n",
      "dataset:\n",
      "  name: CIFAR10_RA_1_20\n",
      "  dataset_dir: ''\n",
      "  image_size: 32\n",
      "  n_channels: 3\n",
      "  n_classes: 10\n",
      "model:\n",
      "  type: cifar\n",
      "  name: resnext\n",
      "  init_mode: kaiming_fan_out\n",
      "  vgg:\n",
      "    n_channels: [64, 128, 256, 512, 512]\n",
      "    n_layers: [2, 2, 3, 3, 3]\n",
      "    use_bn: True\n",
      "  resnet:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "  resnet_preact:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "  wrn:\n",
      "    depth: 28\n",
      "    initial_channels: 16\n",
      "    widening_factor: 10\n",
      "    drop_rate: 0.0\n",
      "  densenet:\n",
      "    depth: 100\n",
      "    n_blocks: [6, 12, 24, 16]\n",
      "    block_type: bottleneck\n",
      "    growth_rate: 12\n",
      "    drop_rate: 0.0\n",
      "    compression_rate: 0.5\n",
      "  pyramidnet:\n",
      "    depth: 272\n",
      "    n_blocks: [3, 24, 36, 3]\n",
      "    initial_channels: 16\n",
      "    block_type: bottleneck\n",
      "    alpha: 200\n",
      "  resnext:\n",
      "    depth: 29\n",
      "    n_blocks: [3, 4, 6, 3]\n",
      "    initial_channels: 64\n",
      "    cardinality: 4\n",
      "    base_channels: 64\n",
      "  shake_shake:\n",
      "    depth: 26\n",
      "    initial_channels: 96\n",
      "    shake_forward: True\n",
      "    shake_backward: True\n",
      "    shake_image: True\n",
      "  se_resnet_preact:\n",
      "    depth: 110\n",
      "    initial_channels: 16\n",
      "    se_reduction: 16\n",
      "    block_type: basic\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "train:\n",
      "  checkpoint: ''\n",
      "  resume: False\n",
      "  precision: O0\n",
      "  batch_size: 128\n",
      "  subdivision: 1\n",
      "  optimizer: sgd\n",
      "  base_lr: 0.1\n",
      "  momentum: 0.9\n",
      "  nesterov: True\n",
      "  weight_decay: 0.0005\n",
      "  no_weight_decay_on_bn: False\n",
      "  gradient_clip: 0\n",
      "  start_epoch: 0\n",
      "  seed: 0\n",
      "  val_first: True\n",
      "  val_period: 1\n",
      "  val_ratio: 0.1\n",
      "  use_test_as_val: False\n",
      "  output_dir: /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_1_20_c10val/exp00\n",
      "  log_period: 100\n",
      "  checkpoint_period: 100\n",
      "  use_tensorboard: True\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: True\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "  distributed: False\n",
      "  dist:\n",
      "    backend: nccl\n",
      "    init_method: env://\n",
      "    world_size: -1\n",
      "    node_rank: -1\n",
      "    local_rank: 0\n",
      "    use_sync_bn: False\n",
      "tensorboard:\n",
      "  train_images: False\n",
      "  val_images: False\n",
      "  model_params: False\n",
      "optim:\n",
      "  adam:\n",
      "    betas: (0.9, 0.999)\n",
      "  lars:\n",
      "    eps: 1e-09\n",
      "    threshold: 0.01\n",
      "  adabound:\n",
      "    betas: (0.9, 0.999)\n",
      "    final_lr: 0.1\n",
      "    gamma: 0.001\n",
      "scheduler:\n",
      "  epochs: 400\n",
      "  warmup:\n",
      "    type: none\n",
      "    epochs: 0\n",
      "    start_factor: 0.001\n",
      "    exponent: 4\n",
      "  type: multistep\n",
      "  milestones: [150, 225]\n",
      "  lr_decay: 0.1\n",
      "  lr_min_factor: 0.001\n",
      "  T0: 10\n",
      "  T_mul: 1.0\n",
      "validation:\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: False\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "augmentation:\n",
      "  use_random_crop: True\n",
      "  use_random_horizontal_flip: True\n",
      "  use_cutout: False\n",
      "  use_random_erasing: False\n",
      "  use_dual_cutout: False\n",
      "  use_mixup: False\n",
      "  use_ricap: False\n",
      "  use_cutmix: False\n",
      "  use_label_smoothing: False\n",
      "  random_crop:\n",
      "    padding: 4\n",
      "    fill: 0\n",
      "    padding_mode: constant\n",
      "  random_horizontal_flip:\n",
      "    prob: 0.5\n",
      "  cutout:\n",
      "    prob: 1.0\n",
      "    mask_size: 16\n",
      "    cut_inside: False\n",
      "    mask_color: 0\n",
      "    dual_cutout_alpha: 0.1\n",
      "  random_erasing:\n",
      "    prob: 0.5\n",
      "    area_ratio_range: [0.02, 0.4]\n",
      "    min_aspect_ratio: 0.3\n",
      "    max_attempt: 20\n",
      "  mixup:\n",
      "    alpha: 1.0\n",
      "  ricap:\n",
      "    beta: 0.3\n",
      "  cutmix:\n",
      "    alpha: 1.0\n",
      "  label_smoothing:\n",
      "    epsilon: 0.1\n",
      "tta:\n",
      "  use_resize: False\n",
      "  use_center_crop: False\n",
      "  resize: 256\n",
      "test:\n",
      "  checkpoint: ''\n",
      "  output_dir: ''\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    pin_memory: False\n",
      "\u001b[32m[2020-07-17 01:42:39] __main__ INFO: \u001b[0menv_info:\n",
      "  pytorch_version: 1.4.0\n",
      "  cuda_version: 10.1\n",
      "  cudnn_version: 7603\n",
      "  num_gpus: 1\n",
      "  gpu_name: Tesla K80\n",
      "  gpu_capability: 3.7\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /home/ec2-user/.torch/datasets/CIFAR10/cifar-10-python.tar.gz\n",
      "170500096it [00:03, 48017528.85it/s]                                            \n",
      "Extracting /home/ec2-user/.torch/datasets/CIFAR10/cifar-10-python.tar.gz to /home/ec2-user/.torch/datasets/CIFAR10\n",
      "\u001b[32m[2020-07-17 01:43:24] __main__ INFO: \u001b[0mMACs  : 2.75G\n",
      "\u001b[32m[2020-07-17 01:43:24] __main__ INFO: \u001b[0m#params: 17.56M\n",
      "Selected optimization level O0:  Pure FP32 training.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\",)\n",
      "\u001b[32m[2020-07-17 01:43:24] __main__ INFO: \u001b[0mVal 0\n",
      "\u001b[32m[2020-07-17 01:43:49] __main__ INFO: \u001b[0mEpoch 0 loss 9.8157 acc@1 0.1032 acc@5 0.4996\n",
      "\u001b[32m[2020-07-17 01:43:49] __main__ INFO: \u001b[0mElapsed 24.92\n",
      "\u001b[32m[2020-07-17 01:43:49] __main__ INFO: \u001b[0mTrain 1 0\n",
      "\u001b[32m[2020-07-17 01:46:13] __main__ INFO: \u001b[0mEpoch 1 Step 100/351 lr 0.100000 loss 2.3128 (8.8928) acc@1 0.1328 (0.1026) acc@5 0.5000 (0.5038)\n",
      "\u001b[32m[2020-07-17 01:48:30] __main__ INFO: \u001b[0mEpoch 1 Step 200/351 lr 0.100000 loss 2.9055 (5.7977) acc@1 0.1016 (0.1079) acc@5 0.5234 (0.5154)\n",
      "\u001b[32m[2020-07-17 01:50:45] __main__ INFO: \u001b[0mEpoch 1 Step 300/351 lr 0.100000 loss 2.4392 (4.7045) acc@1 0.1562 (0.1097) acc@5 0.6250 (0.5310)\n",
      "\u001b[32m[2020-07-17 01:51:53] __main__ INFO: \u001b[0mEpoch 1 Step 351/351 lr 0.100000 loss 2.4944 (4.3718) acc@1 0.1406 (0.1112) acc@5 0.5547 (0.5384)\n",
      "\u001b[32m[2020-07-17 01:51:53] __main__ INFO: \u001b[0mElapsed 484.79\n",
      "\u001b[32m[2020-07-17 01:51:53] __main__ INFO: \u001b[0mVal 1\n",
      "\u001b[32m[2020-07-17 01:52:10] __main__ INFO: \u001b[0mEpoch 1 loss 2.2170 acc@1 0.1548 acc@5 0.6854\n",
      "\u001b[32m[2020-07-17 01:52:10] __main__ INFO: \u001b[0mElapsed 16.91\n",
      "\u001b[32m[2020-07-17 01:52:10] __main__ INFO: \u001b[0mTrain 2 351\n",
      "\u001b[32m[2020-07-17 01:54:25] __main__ INFO: \u001b[0mEpoch 2 Step 100/351 lr 0.100000 loss 2.2952 (2.3980) acc@1 0.1484 (0.1233) acc@5 0.5625 (0.5709)\n",
      "\u001b[32m[2020-07-17 01:56:40] __main__ INFO: \u001b[0mEpoch 2 Step 200/351 lr 0.100000 loss 2.3827 (2.3689) acc@1 0.1328 (0.1260) acc@5 0.5703 (0.5774)\n",
      "\u001b[32m[2020-07-17 01:58:55] __main__ INFO: \u001b[0mEpoch 2 Step 300/351 lr 0.100000 loss 2.2871 (2.3488) acc@1 0.1250 (0.1303) acc@5 0.5391 (0.5824)\n",
      "\u001b[32m[2020-07-17 02:00:04] __main__ INFO: \u001b[0mEpoch 2 Step 351/351 lr 0.100000 loss 2.2833 (2.3399) acc@1 0.1797 (0.1314) acc@5 0.5547 (0.5854)\n",
      "\u001b[32m[2020-07-17 02:00:04] __main__ INFO: \u001b[0mElapsed 473.73\n",
      "\u001b[32m[2020-07-17 02:00:04] __main__ INFO: \u001b[0mVal 2\n",
      "\u001b[32m[2020-07-17 02:00:21] __main__ INFO: \u001b[0mEpoch 2 loss 2.1599 acc@1 0.2010 acc@5 0.7386\n",
      "\u001b[32m[2020-07-17 02:00:21] __main__ INFO: \u001b[0mElapsed 17.03\n",
      "\u001b[32m[2020-07-17 02:00:21] __main__ INFO: \u001b[0mTrain 3 702\n",
      "\u001b[32m[2020-07-17 02:02:36] __main__ INFO: \u001b[0mEpoch 3 Step 100/351 lr 0.100000 loss 2.2174 (2.2788) acc@1 0.1484 (0.1432) acc@5 0.5781 (0.6007)\n",
      "\u001b[32m[2020-07-17 02:04:51] __main__ INFO: \u001b[0mEpoch 3 Step 200/351 lr 0.100000 loss 2.3128 (2.2658) acc@1 0.1250 (0.1459) acc@5 0.6484 (0.6142)\n",
      "\u001b[32m[2020-07-17 02:07:06] __main__ INFO: \u001b[0mEpoch 3 Step 300/351 lr 0.100000 loss 2.2378 (2.2561) acc@1 0.1562 (0.1513) acc@5 0.6016 (0.6216)\n",
      "\u001b[32m[2020-07-17 02:08:15] __main__ INFO: \u001b[0mEpoch 3 Step 351/351 lr 0.100000 loss 2.2616 (2.2519) acc@1 0.1641 (0.1539) acc@5 0.6172 (0.6231)\n",
      "\u001b[32m[2020-07-17 02:08:15] __main__ INFO: \u001b[0mElapsed 474.07\n",
      "\u001b[32m[2020-07-17 02:08:15] __main__ INFO: \u001b[0mVal 3\n",
      "\u001b[32m[2020-07-17 02:08:32] __main__ INFO: \u001b[0mEpoch 3 loss 2.0061 acc@1 0.2606 acc@5 0.7832\n",
      "\u001b[32m[2020-07-17 02:08:32] __main__ INFO: \u001b[0mElapsed 16.94\n",
      "\u001b[32m[2020-07-17 02:08:32] __main__ INFO: \u001b[0mTrain 4 1053\n",
      "\u001b[32m[2020-07-17 02:10:47] __main__ INFO: \u001b[0mEpoch 4 Step 100/351 lr 0.100000 loss 2.2329 (2.2218) acc@1 0.1875 (0.1613) acc@5 0.6328 (0.6452)\n",
      "\u001b[32m[2020-07-17 02:13:03] __main__ INFO: \u001b[0mEpoch 4 Step 200/351 lr 0.100000 loss 2.2121 (2.2096) acc@1 0.1875 (0.1684) acc@5 0.6406 (0.6509)\n",
      "\u001b[32m[2020-07-17 02:15:18] __main__ INFO: \u001b[0mEpoch 4 Step 300/351 lr 0.100000 loss 2.0896 (2.1981) acc@1 0.2266 (0.1723) acc@5 0.6641 (0.6563)\n",
      "\u001b[32m[2020-07-17 02:16:27] __main__ INFO: \u001b[0mEpoch 4 Step 351/351 lr 0.100000 loss 2.1830 (2.1914) acc@1 0.2344 (0.1741) acc@5 0.6328 (0.6604)\n",
      "\u001b[32m[2020-07-17 02:16:27] __main__ INFO: \u001b[0mElapsed 474.80\n",
      "\u001b[32m[2020-07-17 02:16:27] __main__ INFO: \u001b[0mVal 4\n",
      "\u001b[32m[2020-07-17 02:16:44] __main__ INFO: \u001b[0mEpoch 4 loss 1.9496 acc@1 0.2626 acc@5 0.8200\n",
      "\u001b[32m[2020-07-17 02:16:44] __main__ INFO: \u001b[0mElapsed 16.92\n",
      "\u001b[32m[2020-07-17 02:16:44] __main__ INFO: \u001b[0mTrain 5 1404\n",
      "\u001b[32m[2020-07-17 02:18:59] __main__ INFO: \u001b[0mEpoch 5 Step 100/351 lr 0.100000 loss 2.1479 (2.1380) acc@1 0.2188 (0.1932) acc@5 0.7109 (0.6924)\n",
      "\u001b[32m[2020-07-17 02:21:14] __main__ INFO: \u001b[0mEpoch 5 Step 200/351 lr 0.100000 loss 2.1702 (2.1262) acc@1 0.2109 (0.1993) acc@5 0.6719 (0.6996)\n",
      "\u001b[32m[2020-07-17 02:23:30] __main__ INFO: \u001b[0mEpoch 5 Step 300/351 lr 0.100000 loss 2.0107 (2.1169) acc@1 0.2656 (0.2048) acc@5 0.7656 (0.7040)\n",
      "\u001b[32m[2020-07-17 02:24:38] __main__ INFO: \u001b[0mEpoch 5 Step 351/351 lr 0.100000 loss 2.0486 (2.1117) acc@1 0.2578 (0.2064) acc@5 0.7188 (0.7052)\n",
      "\u001b[32m[2020-07-17 02:24:38] __main__ INFO: \u001b[0mElapsed 474.55\n",
      "\u001b[32m[2020-07-17 02:24:38] __main__ INFO: \u001b[0mVal 5\n",
      "\u001b[32m[2020-07-17 02:24:55] __main__ INFO: \u001b[0mEpoch 5 loss 1.8633 acc@1 0.3022 acc@5 0.8330\n",
      "\u001b[32m[2020-07-17 02:24:55] __main__ INFO: \u001b[0mElapsed 16.84\n",
      "\u001b[32m[2020-07-17 02:24:55] __main__ INFO: \u001b[0mTrain 6 1755\n",
      "\u001b[32m[2020-07-17 02:27:10] __main__ INFO: \u001b[0mEpoch 6 Step 100/351 lr 0.100000 loss 2.0376 (2.0709) acc@1 0.1719 (0.2246) acc@5 0.7422 (0.7230)\n",
      "\u001b[32m[2020-07-17 02:29:25] __main__ INFO: \u001b[0mEpoch 6 Step 200/351 lr 0.100000 loss 2.0729 (2.0623) acc@1 0.2578 (0.2287) acc@5 0.6953 (0.7257)\n",
      "\u001b[32m[2020-07-17 02:31:40] __main__ INFO: \u001b[0mEpoch 6 Step 300/351 lr 0.100000 loss 2.0759 (2.0511) acc@1 0.2422 (0.2333) acc@5 0.7188 (0.7301)\n",
      "\u001b[32m[2020-07-17 02:32:49] __main__ INFO: \u001b[0mEpoch 6 Step 351/351 lr 0.100000 loss 1.9998 (2.0489) acc@1 0.2266 (0.2351) acc@5 0.7344 (0.7314)\n",
      "\u001b[32m[2020-07-17 02:32:49] __main__ INFO: \u001b[0mElapsed 473.97\n",
      "\u001b[32m[2020-07-17 02:32:49] __main__ INFO: \u001b[0mVal 6\n",
      "\u001b[32m[2020-07-17 02:33:06] __main__ INFO: \u001b[0mEpoch 6 loss 1.7620 acc@1 0.3584 acc@5 0.8662\n",
      "\u001b[32m[2020-07-17 02:33:06] __main__ INFO: \u001b[0mElapsed 16.90\n",
      "\u001b[32m[2020-07-17 02:33:06] __main__ INFO: \u001b[0mTrain 7 2106\n",
      "\u001b[32m[2020-07-17 02:35:21] __main__ INFO: \u001b[0mEpoch 7 Step 100/351 lr 0.100000 loss 2.0056 (2.0144) acc@1 0.2891 (0.2510) acc@5 0.7500 (0.7449)\n",
      "\u001b[32m[2020-07-17 02:37:36] __main__ INFO: \u001b[0mEpoch 7 Step 200/351 lr 0.100000 loss 2.0611 (1.9975) acc@1 0.2188 (0.2547) acc@5 0.8203 (0.7483)\n",
      "\u001b[32m[2020-07-17 02:39:51] __main__ INFO: \u001b[0mEpoch 7 Step 300/351 lr 0.100000 loss 2.0805 (1.9949) acc@1 0.2031 (0.2559) acc@5 0.7031 (0.7508)\n",
      "\u001b[32m[2020-07-17 02:41:00] __main__ INFO: \u001b[0mEpoch 7 Step 351/351 lr 0.100000 loss 1.9882 (1.9918) acc@1 0.2422 (0.2586) acc@5 0.7891 (0.7522)\n",
      "\u001b[32m[2020-07-17 02:41:00] __main__ INFO: \u001b[0mElapsed 473.82\n",
      "\u001b[32m[2020-07-17 02:41:00] __main__ INFO: \u001b[0mVal 7\n",
      "\u001b[32m[2020-07-17 02:41:17] __main__ INFO: \u001b[0mEpoch 7 loss 1.6673 acc@1 0.3928 acc@5 0.8796\n",
      "\u001b[32m[2020-07-17 02:41:17] __main__ INFO: \u001b[0mElapsed 16.90\n",
      "\u001b[32m[2020-07-17 02:41:17] __main__ INFO: \u001b[0mTrain 8 2457\n",
      "\u001b[32m[2020-07-17 02:43:32] __main__ INFO: \u001b[0mEpoch 8 Step 100/351 lr 0.100000 loss 1.9995 (1.9540) acc@1 0.2969 (0.2716) acc@5 0.7578 (0.7710)\n",
      "\u001b[32m[2020-07-17 02:45:47] __main__ INFO: \u001b[0mEpoch 8 Step 200/351 lr 0.100000 loss 1.8328 (1.9439) acc@1 0.2969 (0.2748) acc@5 0.8359 (0.7705)\n",
      "\u001b[32m[2020-07-17 02:48:02] __main__ INFO: \u001b[0mEpoch 8 Step 300/351 lr 0.100000 loss 1.9742 (1.9393) acc@1 0.3047 (0.2799) acc@5 0.7734 (0.7701)\n",
      "\u001b[32m[2020-07-17 02:49:11] __main__ INFO: \u001b[0mEpoch 8 Step 351/351 lr 0.100000 loss 1.7946 (1.9331) acc@1 0.3125 (0.2830) acc@5 0.8125 (0.7720)\n",
      "\u001b[32m[2020-07-17 02:49:11] __main__ INFO: \u001b[0mElapsed 474.27\n",
      "\u001b[32m[2020-07-17 02:49:11] __main__ INFO: \u001b[0mVal 8\n",
      "\u001b[32m[2020-07-17 02:49:28] __main__ INFO: \u001b[0mEpoch 8 loss 1.5838 acc@1 0.4282 acc@5 0.8996\n",
      "\u001b[32m[2020-07-17 02:49:28] __main__ INFO: \u001b[0mElapsed 16.93\n",
      "\u001b[32m[2020-07-17 02:49:28] __main__ INFO: \u001b[0mTrain 9 2808\n",
      "\u001b[32m[2020-07-17 02:51:43] __main__ INFO: \u001b[0mEpoch 9 Step 100/351 lr 0.100000 loss 1.9241 (1.8844) acc@1 0.2578 (0.3015) acc@5 0.7344 (0.7780)\n",
      "\u001b[32m[2020-07-17 02:53:58] __main__ INFO: \u001b[0mEpoch 9 Step 200/351 lr 0.100000 loss 1.8309 (1.8733) acc@1 0.3516 (0.3066) acc@5 0.8203 (0.7842)\n",
      "\u001b[32m[2020-07-17 02:56:13] __main__ INFO: \u001b[0mEpoch 9 Step 300/351 lr 0.100000 loss 1.9251 (1.8685) acc@1 0.2656 (0.3105) acc@5 0.7891 (0.7860)\n",
      "\u001b[32m[2020-07-17 02:57:21] __main__ INFO: \u001b[0mEpoch 9 Step 351/351 lr 0.100000 loss 1.8100 (1.8629) acc@1 0.3594 (0.3147) acc@5 0.7969 (0.7864)\n",
      "\u001b[32m[2020-07-17 02:57:21] __main__ INFO: \u001b[0mElapsed 473.32\n",
      "\u001b[32m[2020-07-17 02:57:21] __main__ INFO: \u001b[0mVal 9\n",
      "\u001b[32m[2020-07-17 02:57:38] __main__ INFO: \u001b[0mEpoch 9 loss 1.4914 acc@1 0.4714 acc@5 0.9140\n",
      "\u001b[32m[2020-07-17 02:57:38] __main__ INFO: \u001b[0mElapsed 16.86\n",
      "\u001b[32m[2020-07-17 02:57:38] __main__ INFO: \u001b[0mTrain 10 3159\n",
      "\u001b[32m[2020-07-17 02:59:54] __main__ INFO: \u001b[0mEpoch 10 Step 100/351 lr 0.100000 loss 1.7982 (1.8063) acc@1 0.3594 (0.3355) acc@5 0.8047 (0.8023)\n",
      "\u001b[32m[2020-07-17 03:02:09] __main__ INFO: \u001b[0mEpoch 10 Step 200/351 lr 0.100000 loss 1.6915 (1.7961) acc@1 0.4062 (0.3392) acc@5 0.8047 (0.8024)\n",
      "\u001b[32m[2020-07-17 03:04:24] __main__ INFO: \u001b[0mEpoch 10 Step 300/351 lr 0.100000 loss 1.7449 (1.7833) acc@1 0.3672 (0.3433) acc@5 0.8125 (0.8063)\n",
      "\u001b[32m[2020-07-17 03:05:33] __main__ INFO: \u001b[0mEpoch 10 Step 351/351 lr 0.100000 loss 1.8076 (1.7762) acc@1 0.3359 (0.3466) acc@5 0.8281 (0.8080)\n",
      "\u001b[32m[2020-07-17 03:05:33] __main__ INFO: \u001b[0mElapsed 475.19\n",
      "\u001b[32m[2020-07-17 03:05:33] __main__ INFO: \u001b[0mVal 10\n",
      "\u001b[32m[2020-07-17 03:05:50] __main__ INFO: \u001b[0mEpoch 10 loss 1.3956 acc@1 0.5080 acc@5 0.9304\n",
      "\u001b[32m[2020-07-17 03:05:50] __main__ INFO: \u001b[0mElapsed 16.92\n",
      "\u001b[32m[2020-07-17 03:05:50] __main__ INFO: \u001b[0mTrain 11 3510\n",
      "\u001b[32m[2020-07-17 03:08:05] __main__ INFO: \u001b[0mEpoch 11 Step 100/351 lr 0.100000 loss 1.7297 (1.7132) acc@1 0.3906 (0.3704) acc@5 0.7969 (0.8137)\n",
      "\u001b[32m[2020-07-17 03:10:20] __main__ INFO: \u001b[0mEpoch 11 Step 200/351 lr 0.100000 loss 1.6291 (1.7076) acc@1 0.3828 (0.3729) acc@5 0.8438 (0.8146)\n",
      "\u001b[32m[2020-07-17 03:12:35] __main__ INFO: \u001b[0mEpoch 11 Step 300/351 lr 0.100000 loss 1.4688 (1.6953) acc@1 0.4688 (0.3781) acc@5 0.8984 (0.8176)\n",
      "\u001b[32m[2020-07-17 03:13:44] __main__ INFO: \u001b[0mEpoch 11 Step 351/351 lr 0.100000 loss 1.7265 (1.6903) acc@1 0.4297 (0.3804) acc@5 0.8203 (0.8192)\n",
      "\u001b[32m[2020-07-17 03:13:44] __main__ INFO: \u001b[0mElapsed 473.79\n",
      "\u001b[32m[2020-07-17 03:13:44] __main__ INFO: \u001b[0mVal 11\n",
      "\u001b[32m[2020-07-17 03:14:01] __main__ INFO: \u001b[0mEpoch 11 loss 1.2291 acc@1 0.5694 acc@5 0.9350\n",
      "\u001b[32m[2020-07-17 03:14:01] __main__ INFO: \u001b[0mElapsed 16.95\n",
      "\u001b[32m[2020-07-17 03:14:01] __main__ INFO: \u001b[0mTrain 12 3861\n",
      "\u001b[32m[2020-07-17 03:16:17] __main__ INFO: \u001b[0mEpoch 12 Step 100/351 lr 0.100000 loss 1.6046 (1.6358) acc@1 0.4375 (0.4062) acc@5 0.8750 (0.8234)\n",
      "\u001b[32m[2020-07-17 03:18:32] __main__ INFO: \u001b[0mEpoch 12 Step 200/351 lr 0.100000 loss 1.4368 (1.6156) acc@1 0.4922 (0.4116) acc@5 0.8438 (0.8292)\n",
      "\u001b[32m[2020-07-17 03:20:47] __main__ INFO: \u001b[0mEpoch 12 Step 300/351 lr 0.100000 loss 1.6921 (1.6095) acc@1 0.3594 (0.4122) acc@5 0.8125 (0.8324)\n",
      "\u001b[32m[2020-07-17 03:21:55] __main__ INFO: \u001b[0mEpoch 12 Step 351/351 lr 0.100000 loss 1.5241 (1.6070) acc@1 0.4688 (0.4128) acc@5 0.8516 (0.8321)\n",
      "\u001b[32m[2020-07-17 03:21:55] __main__ INFO: \u001b[0mElapsed 474.25\n",
      "\u001b[32m[2020-07-17 03:21:55] __main__ INFO: \u001b[0mVal 12\n",
      "\u001b[32m[2020-07-17 03:22:12] __main__ INFO: \u001b[0mEpoch 12 loss 1.2507 acc@1 0.5578 acc@5 0.9434\n",
      "\u001b[32m[2020-07-17 03:22:12] __main__ INFO: \u001b[0mElapsed 16.90\n",
      "\u001b[32m[2020-07-17 03:22:12] __main__ INFO: \u001b[0mTrain 13 4212\n",
      "\u001b[32m[2020-07-17 03:24:27] __main__ INFO: \u001b[0mEpoch 13 Step 100/351 lr 0.100000 loss 1.7050 (1.5427) acc@1 0.3516 (0.4376) acc@5 0.8281 (0.8415)\n",
      "\u001b[32m[2020-07-17 03:26:42] __main__ INFO: \u001b[0mEpoch 13 Step 200/351 lr 0.100000 loss 1.7416 (1.5461) acc@1 0.3828 (0.4377) acc@5 0.8047 (0.8405)\n",
      "\u001b[32m[2020-07-17 03:28:57] __main__ INFO: \u001b[0mEpoch 13 Step 300/351 lr 0.100000 loss 1.6285 (1.5439) acc@1 0.4531 (0.4375) acc@5 0.7812 (0.8418)\n",
      "\u001b[32m[2020-07-17 03:30:06] __main__ INFO: \u001b[0mEpoch 13 Step 351/351 lr 0.100000 loss 1.5916 (1.5404) acc@1 0.3906 (0.4382) acc@5 0.8438 (0.8424)\n",
      "\u001b[32m[2020-07-17 03:30:06] __main__ INFO: \u001b[0mElapsed 473.61\n",
      "\u001b[32m[2020-07-17 03:30:06] __main__ INFO: \u001b[0mVal 13\n",
      "\u001b[32m[2020-07-17 03:30:23] __main__ INFO: \u001b[0mEpoch 13 loss 1.1521 acc@1 0.5914 acc@5 0.9552\n",
      "\u001b[32m[2020-07-17 03:30:23] __main__ INFO: \u001b[0mElapsed 16.93\n",
      "\u001b[32m[2020-07-17 03:30:23] __main__ INFO: \u001b[0mTrain 14 4563\n",
      "\u001b[32m[2020-07-17 03:32:38] __main__ INFO: \u001b[0mEpoch 14 Step 100/351 lr 0.100000 loss 1.4812 (1.4815) acc@1 0.4297 (0.4651) acc@5 0.8828 (0.8530)\n",
      "\u001b[32m[2020-07-17 03:34:53] __main__ INFO: \u001b[0mEpoch 14 Step 200/351 lr 0.100000 loss 1.5513 (1.4895) acc@1 0.4141 (0.4581) acc@5 0.8750 (0.8521)\n",
      "\u001b[32m[2020-07-17 03:37:08] __main__ INFO: \u001b[0mEpoch 14 Step 300/351 lr 0.100000 loss 1.3713 (1.4814) acc@1 0.4766 (0.4589) acc@5 0.8594 (0.8531)\n",
      "\u001b[32m[2020-07-17 03:38:17] __main__ INFO: \u001b[0mEpoch 14 Step 351/351 lr 0.100000 loss 1.5993 (1.4824) acc@1 0.4062 (0.4583) acc@5 0.8672 (0.8529)\n",
      "\u001b[32m[2020-07-17 03:38:17] __main__ INFO: \u001b[0mElapsed 473.96\n",
      "\u001b[32m[2020-07-17 03:38:17] __main__ INFO: \u001b[0mVal 14\n",
      "\u001b[32m[2020-07-17 03:38:34] __main__ INFO: \u001b[0mEpoch 14 loss 1.1545 acc@1 0.5948 acc@5 0.9544\n",
      "\u001b[32m[2020-07-17 03:38:34] __main__ INFO: \u001b[0mElapsed 16.95\n",
      "\u001b[32m[2020-07-17 03:38:34] __main__ INFO: \u001b[0mTrain 15 4914\n",
      "\u001b[32m[2020-07-17 03:40:49] __main__ INFO: \u001b[0mEpoch 15 Step 100/351 lr 0.100000 loss 1.3298 (1.4598) acc@1 0.5000 (0.4694) acc@5 0.9062 (0.8523)\n",
      "\u001b[32m[2020-07-17 03:43:04] __main__ INFO: \u001b[0mEpoch 15 Step 200/351 lr 0.100000 loss 1.5154 (1.4427) acc@1 0.4688 (0.4746) acc@5 0.8281 (0.8557)\n",
      "\u001b[32m[2020-07-17 03:45:19] __main__ INFO: \u001b[0mEpoch 15 Step 300/351 lr 0.100000 loss 1.3824 (1.4388) acc@1 0.4766 (0.4753) acc@5 0.8828 (0.8562)\n",
      "\u001b[32m[2020-07-17 03:46:28] __main__ INFO: \u001b[0mEpoch 15 Step 351/351 lr 0.100000 loss 1.4529 (1.4335) acc@1 0.5547 (0.4776) acc@5 0.8984 (0.8573)\n",
      "\u001b[32m[2020-07-17 03:46:28] __main__ INFO: \u001b[0mElapsed 474.18\n",
      "\u001b[32m[2020-07-17 03:46:28] __main__ INFO: \u001b[0mVal 15\n",
      "\u001b[32m[2020-07-17 03:46:45] __main__ INFO: \u001b[0mEpoch 15 loss 0.9382 acc@1 0.6666 acc@5 0.9712\n",
      "\u001b[32m[2020-07-17 03:46:45] __main__ INFO: \u001b[0mElapsed 16.90\n",
      "\u001b[32m[2020-07-17 03:46:45] __main__ INFO: \u001b[0mTrain 16 5265\n",
      "\u001b[32m[2020-07-17 03:49:00] __main__ INFO: \u001b[0mEpoch 16 Step 100/351 lr 0.100000 loss 1.3910 (1.3997) acc@1 0.4922 (0.4876) acc@5 0.8438 (0.8655)\n",
      "\u001b[32m[2020-07-17 03:51:15] __main__ INFO: \u001b[0mEpoch 16 Step 200/351 lr 0.100000 loss 1.2800 (1.4027) acc@1 0.5391 (0.4839) acc@5 0.8828 (0.8638)\n",
      "\u001b[32m[2020-07-17 03:53:30] __main__ INFO: \u001b[0mEpoch 16 Step 300/351 lr 0.100000 loss 1.4122 (1.3977) acc@1 0.4453 (0.4875) acc@5 0.8672 (0.8623)\n",
      "\u001b[32m[2020-07-17 03:54:39] __main__ INFO: \u001b[0mEpoch 16 Step 351/351 lr 0.100000 loss 1.2929 (1.3960) acc@1 0.5469 (0.4882) acc@5 0.8594 (0.8624)\n",
      "\u001b[32m[2020-07-17 03:54:39] __main__ INFO: \u001b[0mElapsed 473.82\n",
      "\u001b[32m[2020-07-17 03:54:39] __main__ INFO: \u001b[0mVal 16\n",
      "\u001b[32m[2020-07-17 03:54:56] __main__ INFO: \u001b[0mEpoch 16 loss 1.0223 acc@1 0.6408 acc@5 0.9704\n",
      "\u001b[32m[2020-07-17 03:54:56] __main__ INFO: \u001b[0mElapsed 16.89\n",
      "\u001b[32m[2020-07-17 03:54:56] __main__ INFO: \u001b[0mTrain 17 5616\n",
      "\u001b[32m[2020-07-17 03:57:11] __main__ INFO: \u001b[0mEpoch 17 Step 100/351 lr 0.100000 loss 1.2058 (1.3643) acc@1 0.5625 (0.5000) acc@5 0.8906 (0.8624)\n",
      "\u001b[32m[2020-07-17 03:59:26] __main__ INFO: \u001b[0mEpoch 17 Step 200/351 lr 0.100000 loss 1.3236 (1.3560) acc@1 0.5312 (0.5036) acc@5 0.8906 (0.8646)\n",
      "\u001b[32m[2020-07-17 04:01:42] __main__ INFO: \u001b[0mEpoch 17 Step 300/351 lr 0.100000 loss 1.2876 (1.3544) acc@1 0.5391 (0.5061) acc@5 0.8672 (0.8661)\n",
      "\u001b[32m[2020-07-17 04:02:51] __main__ INFO: \u001b[0mEpoch 17 Step 351/351 lr 0.100000 loss 1.2386 (1.3520) acc@1 0.5469 (0.5077) acc@5 0.8984 (0.8673)\n",
      "\u001b[32m[2020-07-17 04:02:51] __main__ INFO: \u001b[0mElapsed 475.38\n",
      "\u001b[32m[2020-07-17 04:02:51] __main__ INFO: \u001b[0mVal 17\n",
      "\u001b[32m[2020-07-17 04:03:08] __main__ INFO: \u001b[0mEpoch 17 loss 0.8916 acc@1 0.6840 acc@5 0.9724\n",
      "\u001b[32m[2020-07-17 04:03:08] __main__ INFO: \u001b[0mElapsed 16.90\n",
      "\u001b[32m[2020-07-17 04:03:08] __main__ INFO: \u001b[0mTrain 18 5967\n",
      "\u001b[32m[2020-07-17 04:05:23] __main__ INFO: \u001b[0mEpoch 18 Step 100/351 lr 0.100000 loss 1.4193 (1.3058) acc@1 0.4453 (0.5251) acc@5 0.8359 (0.8671)\n",
      "\u001b[32m[2020-07-17 04:07:38] __main__ INFO: \u001b[0mEpoch 18 Step 200/351 lr 0.100000 loss 1.3406 (1.3149) acc@1 0.5000 (0.5228) acc@5 0.8594 (0.8686)\n",
      "\u001b[32m[2020-07-17 04:09:53] __main__ INFO: \u001b[0mEpoch 18 Step 300/351 lr 0.100000 loss 1.1774 (1.3147) acc@1 0.6172 (0.5220) acc@5 0.9219 (0.8690)\n",
      "\u001b[32m[2020-07-17 04:11:01] __main__ INFO: \u001b[0mEpoch 18 Step 351/351 lr 0.100000 loss 1.2019 (1.3174) acc@1 0.5625 (0.5215) acc@5 0.8984 (0.8681)\n",
      "\u001b[32m[2020-07-17 04:11:01] __main__ INFO: \u001b[0mElapsed 473.58\n",
      "\u001b[32m[2020-07-17 04:11:01] __main__ INFO: \u001b[0mVal 18\n",
      "\u001b[32m[2020-07-17 04:11:18] __main__ INFO: \u001b[0mEpoch 18 loss 0.8636 acc@1 0.6980 acc@5 0.9748\n",
      "\u001b[32m[2020-07-17 04:11:18] __main__ INFO: \u001b[0mElapsed 16.88\n",
      "\u001b[32m[2020-07-17 04:11:18] __main__ INFO: \u001b[0mTrain 19 6318\n",
      "\u001b[32m[2020-07-17 04:13:33] __main__ INFO: \u001b[0mEpoch 19 Step 100/351 lr 0.100000 loss 1.2075 (1.2608) acc@1 0.5703 (0.5437) acc@5 0.8594 (0.8786)\n",
      "\u001b[32m[2020-07-17 04:15:48] __main__ INFO: \u001b[0mEpoch 19 Step 200/351 lr 0.100000 loss 1.2388 (1.2834) acc@1 0.5312 (0.5350) acc@5 0.8359 (0.8735)\n",
      "\u001b[32m[2020-07-17 04:29:54] __main__ INFO: \u001b[0mEpoch 21 Step 100/351 lr 0.100000 loss 1.1542 (1.2384) acc@1 0.5938 (0.5448) acc@5 0.8672 (0.8713)\n",
      "\u001b[32m[2020-07-17 04:32:08] __main__ INFO: \u001b[0mEpoch 21 Step 200/351 lr 0.100000 loss 1.3884 (1.2458) acc@1 0.5312 (0.5448) acc@5 0.8281 (0.8730)\n",
      "\u001b[32m[2020-07-17 04:34:23] __main__ INFO: \u001b[0mEpoch 21 Step 300/351 lr 0.100000 loss 1.2239 (1.2456) acc@1 0.5234 (0.5438) acc@5 0.8672 (0.8746)\n",
      "\u001b[32m[2020-07-17 04:35:32] __main__ INFO: \u001b[0mEpoch 21 Step 351/351 lr 0.100000 loss 1.1638 (1.2450) acc@1 0.5781 (0.5438) acc@5 0.8750 (0.8748)\n",
      "\u001b[32m[2020-07-17 04:35:32] __main__ INFO: \u001b[0mElapsed 473.06\n",
      "\u001b[32m[2020-07-17 04:35:32] __main__ INFO: \u001b[0mVal 21\n",
      "\u001b[32m[2020-07-17 04:35:49] __main__ INFO: \u001b[0mEpoch 21 loss 0.8045 acc@1 0.7188 acc@5 0.9832\n",
      "\u001b[32m[2020-07-17 04:35:49] __main__ INFO: \u001b[0mElapsed 16.91\n",
      "\u001b[32m[2020-07-17 04:35:49] __main__ INFO: \u001b[0mTrain 22 7371\n",
      "\u001b[32m[2020-07-17 04:38:03] __main__ INFO: \u001b[0mEpoch 22 Step 100/351 lr 0.100000 loss 1.0620 (1.2139) acc@1 0.6406 (0.5563) acc@5 0.8906 (0.8796)\n",
      "\u001b[32m[2020-07-17 04:40:18] __main__ INFO: \u001b[0mEpoch 22 Step 200/351 lr 0.100000 loss 1.1960 (1.2276) acc@1 0.5781 (0.5506) acc@5 0.8984 (0.8757)\n",
      "\u001b[32m[2020-07-17 04:42:33] __main__ INFO: \u001b[0mEpoch 22 Step 300/351 lr 0.100000 loss 1.2659 (1.2322) acc@1 0.5391 (0.5507) acc@5 0.8750 (0.8744)\n",
      "\u001b[32m[2020-07-17 04:43:41] __main__ INFO: \u001b[0mEpoch 22 Step 351/351 lr 0.100000 loss 1.2431 (1.2319) acc@1 0.5781 (0.5514) acc@5 0.9141 (0.8752)\n",
      "\u001b[32m[2020-07-17 04:43:41] __main__ INFO: \u001b[0mElapsed 472.73\n",
      "\u001b[32m[2020-07-17 04:43:41] __main__ INFO: \u001b[0mVal 22\n",
      "\u001b[32m[2020-07-17 04:43:58] __main__ INFO: \u001b[0mEpoch 22 loss 0.8140 acc@1 0.7120 acc@5 0.9792\n",
      "\u001b[32m[2020-07-17 04:43:58] __main__ INFO: \u001b[0mElapsed 16.87\n",
      "\u001b[32m[2020-07-17 04:43:58] __main__ INFO: \u001b[0mTrain 23 7722\n",
      "\u001b[32m[2020-07-17 04:46:13] __main__ INFO: \u001b[0mEpoch 23 Step 100/351 lr 0.100000 loss 1.1813 (1.2057) acc@1 0.5859 (0.5630) acc@5 0.8750 (0.8785)\n",
      "\u001b[32m[2020-07-17 04:48:28] __main__ INFO: \u001b[0mEpoch 23 Step 200/351 lr 0.100000 loss 1.3631 (1.2184) acc@1 0.5156 (0.5567) acc@5 0.8906 (0.8759)\n",
      "\u001b[32m[2020-07-17 04:50:43] __main__ INFO: \u001b[0mEpoch 23 Step 300/351 lr 0.100000 loss 1.3877 (1.2191) acc@1 0.4922 (0.5547) acc@5 0.8594 (0.8768)\n",
      "\u001b[32m[2020-07-17 04:51:51] __main__ INFO: \u001b[0mEpoch 23 Step 351/351 lr 0.100000 loss 1.2690 (1.2173) acc@1 0.5312 (0.5561) acc@5 0.8828 (0.8771)\n",
      "\u001b[32m[2020-07-17 04:51:51] __main__ INFO: \u001b[0mElapsed 473.27\n",
      "\u001b[32m[2020-07-17 04:51:51] __main__ INFO: \u001b[0mVal 23\n",
      "\u001b[32m[2020-07-17 04:52:08] __main__ INFO: \u001b[0mEpoch 23 loss 0.8637 acc@1 0.7092 acc@5 0.9784\n",
      "\u001b[32m[2020-07-17 04:52:08] __main__ INFO: \u001b[0mElapsed 16.91\n",
      "\u001b[32m[2020-07-17 04:52:08] __main__ INFO: \u001b[0mTrain 24 8073\n",
      "\u001b[32m[2020-07-17 04:54:23] __main__ INFO: \u001b[0mEpoch 24 Step 100/351 lr 0.100000 loss 1.1376 (1.1935) acc@1 0.5312 (0.5659) acc@5 0.9062 (0.8771)\n",
      "\u001b[32m[2020-07-17 04:56:38] __main__ INFO: \u001b[0mEpoch 24 Step 200/351 lr 0.100000 loss 1.1844 (1.1998) acc@1 0.5391 (0.5627) acc@5 0.8594 (0.8771)\n",
      "\u001b[32m[2020-07-17 04:58:52] __main__ INFO: \u001b[0mEpoch 24 Step 300/351 lr 0.100000 loss 1.2503 (1.2095) acc@1 0.5312 (0.5585) acc@5 0.8516 (0.8759)\n",
      "\u001b[32m[2020-07-17 05:00:01] __main__ INFO: \u001b[0mEpoch 24 Step 351/351 lr 0.100000 loss 1.1922 (1.2093) acc@1 0.5781 (0.5580) acc@5 0.8828 (0.8767)\n",
      "\u001b[32m[2020-07-17 05:00:01] __main__ INFO: \u001b[0mElapsed 472.99\n",
      "\u001b[32m[2020-07-17 05:00:01] __main__ INFO: \u001b[0mVal 24\n",
      "\u001b[32m[2020-07-17 05:00:18] __main__ INFO: \u001b[0mEpoch 24 loss 0.8030 acc@1 0.7252 acc@5 0.9768\n",
      "\u001b[32m[2020-07-17 05:00:18] __main__ INFO: \u001b[0mElapsed 16.92\n",
      "\u001b[32m[2020-07-17 05:00:18] __main__ INFO: \u001b[0mTrain 25 8424\n",
      "\u001b[32m[2020-07-17 05:02:33] __main__ INFO: \u001b[0mEpoch 25 Step 100/351 lr 0.100000 loss 1.3585 (1.1956) acc@1 0.5000 (0.5656) acc@5 0.8984 (0.8834)\n",
      "\u001b[32m[2020-07-17 05:04:48] __main__ INFO: \u001b[0mEpoch 25 Step 200/351 lr 0.100000 loss 0.9534 (1.1824) acc@1 0.6719 (0.5687) acc@5 0.9141 (0.8843)\n",
      "\u001b[32m[2020-07-17 05:07:02] __main__ INFO: \u001b[0mEpoch 25 Step 300/351 lr 0.100000 loss 1.2023 (1.1846) acc@1 0.5234 (0.5687) acc@5 0.8750 (0.8816)\n",
      "\u001b[32m[2020-07-17 05:08:10] __main__ INFO: \u001b[0mEpoch 25 Step 351/351 lr 0.100000 loss 1.0617 (1.1916) acc@1 0.6172 (0.5662) acc@5 0.9141 (0.8808)\n",
      "\u001b[32m[2020-07-17 05:08:10] __main__ INFO: \u001b[0mElapsed 472.26\n",
      "\u001b[32m[2020-07-17 05:08:10] __main__ INFO: \u001b[0mVal 25\n",
      "\u001b[32m[2020-07-17 05:08:27] __main__ INFO: \u001b[0mEpoch 25 loss 0.9735 acc@1 0.6672 acc@5 0.9808\n",
      "\u001b[32m[2020-07-17 05:08:27] __main__ INFO: \u001b[0mElapsed 16.86\n",
      "\u001b[32m[2020-07-17 05:08:27] __main__ INFO: \u001b[0mTrain 26 8775\n",
      "\u001b[32m[2020-07-17 05:10:42] __main__ INFO: \u001b[0mEpoch 26 Step 100/351 lr 0.100000 loss 1.2376 (1.1707) acc@1 0.5547 (0.5757) acc@5 0.8594 (0.8816)\n",
      "\u001b[32m[2020-07-17 05:12:57] __main__ INFO: \u001b[0mEpoch 26 Step 200/351 lr 0.100000 loss 1.2094 (1.1814) acc@1 0.5469 (0.5718) acc@5 0.8906 (0.8817)\n",
      "\u001b[32m[2020-07-17 05:15:11] __main__ INFO: \u001b[0mEpoch 26 Step 300/351 lr 0.100000 loss 1.0414 (1.1824) acc@1 0.5859 (0.5701) acc@5 0.9062 (0.8810)\n",
      "\u001b[32m[2020-07-17 05:16:20] __main__ INFO: \u001b[0mEpoch 26 Step 351/351 lr 0.100000 loss 1.2342 (1.1815) acc@1 0.5859 (0.5692) acc@5 0.8828 (0.8804)\n",
      "\u001b[32m[2020-07-17 05:16:20] __main__ INFO: \u001b[0mElapsed 472.44\n",
      "\u001b[32m[2020-07-17 05:16:20] __main__ INFO: \u001b[0mVal 26\n",
      "\u001b[32m[2020-07-17 05:16:37] __main__ INFO: \u001b[0mEpoch 26 loss 0.8419 acc@1 0.7146 acc@5 0.9828\n",
      "\u001b[32m[2020-07-17 05:16:37] __main__ INFO: \u001b[0mElapsed 16.89\n",
      "\u001b[32m[2020-07-17 05:16:37] __main__ INFO: \u001b[0mTrain 27 9126\n",
      "\u001b[32m[2020-07-17 05:18:51] __main__ INFO: \u001b[0mEpoch 27 Step 100/351 lr 0.100000 loss 1.1843 (1.1627) acc@1 0.5859 (0.5753) acc@5 0.9062 (0.8813)\n",
      "\u001b[32m[2020-07-17 05:21:06] __main__ INFO: \u001b[0mEpoch 27 Step 200/351 lr 0.100000 loss 1.2669 (1.1708) acc@1 0.5312 (0.5740) acc@5 0.8359 (0.8798)\n",
      "\u001b[32m[2020-07-17 05:23:21] __main__ INFO: \u001b[0mEpoch 27 Step 300/351 lr 0.100000 loss 1.2035 (1.1737) acc@1 0.5312 (0.5723) acc@5 0.8438 (0.8803)\n",
      "\u001b[32m[2020-07-17 05:24:29] __main__ INFO: \u001b[0mEpoch 27 Step 351/351 lr 0.100000 loss 1.4576 (1.1765) acc@1 0.4375 (0.5706) acc@5 0.7891 (0.8796)\n",
      "\u001b[32m[2020-07-17 05:24:29] __main__ INFO: \u001b[0mElapsed 472.67\n",
      "\u001b[32m[2020-07-17 05:24:29] __main__ INFO: \u001b[0mVal 27\n",
      "\u001b[32m[2020-07-17 05:24:46] __main__ INFO: \u001b[0mEpoch 27 loss 0.9575 acc@1 0.6980 acc@5 0.9706\n",
      "\u001b[32m[2020-07-17 05:24:46] __main__ INFO: \u001b[0mElapsed 16.87\n",
      "\u001b[32m[2020-07-17 05:24:46] __main__ INFO: \u001b[0mTrain 28 9477\n",
      "\u001b[32m[2020-07-17 05:27:01] __main__ INFO: \u001b[0mEpoch 28 Step 100/351 lr 0.100000 loss 1.0445 (1.1710) acc@1 0.6406 (0.5767) acc@5 0.8984 (0.8754)\n",
      "\u001b[32m[2020-07-17 05:29:16] __main__ INFO: \u001b[0mEpoch 28 Step 200/351 lr 0.100000 loss 0.9774 (1.1614) acc@1 0.6484 (0.5805) acc@5 0.9375 (0.8781)\n",
      "\u001b[32m[2020-07-17 05:31:30] __main__ INFO: \u001b[0mEpoch 28 Step 300/351 lr 0.100000 loss 1.0845 (1.1615) acc@1 0.5781 (0.5786) acc@5 0.9141 (0.8798)\n",
      "\u001b[32m[2020-07-17 05:32:39] __main__ INFO: \u001b[0mEpoch 28 Step 351/351 lr 0.100000 loss 1.2231 (1.1648) acc@1 0.5156 (0.5770) acc@5 0.9219 (0.8792)\n",
      "\u001b[32m[2020-07-17 05:32:39] __main__ INFO: \u001b[0mElapsed 472.74\n",
      "\u001b[32m[2020-07-17 05:32:39] __main__ INFO: \u001b[0mVal 28\n",
      "\u001b[32m[2020-07-17 05:32:56] __main__ INFO: \u001b[0mEpoch 28 loss 0.6177 acc@1 0.7836 acc@5 0.9876\n",
      "\u001b[32m[2020-07-17 05:32:56] __main__ INFO: \u001b[0mElapsed 16.87\n",
      "\u001b[32m[2020-07-17 05:32:56] __main__ INFO: \u001b[0mTrain 29 9828\n",
      "\u001b[32m[2020-07-17 05:35:11] __main__ INFO: \u001b[0mEpoch 29 Step 100/351 lr 0.100000 loss 1.0259 (1.1322) acc@1 0.5938 (0.5868) acc@5 0.8984 (0.8856)\n",
      "\u001b[32m[2020-07-17 05:37:25] __main__ INFO: \u001b[0mEpoch 29 Step 200/351 lr 0.100000 loss 1.2788 (1.1487) acc@1 0.5625 (0.5818) acc@5 0.8438 (0.8835)\n"
     ]
    }
   ],
   "source": [
    "# Train the model per the settings specified in the original paper\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "!python train.py --config configs/cifar/resnext.yaml \\\n",
    "    model.resnext.cardinality 4 \\\n",
    "    train.batch_size 128 \\\n",
    "    train.base_lr 0.1 \\\n",
    "    dataset.name CIFAR10_RA_1_20 \\\n",
    "    train.base_lr 0.1 \\\n",
    "    train.output_dir /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_1_20_c10val/exp00 \\\n",
    "    scheduler.epochs 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Refine the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-19 10:52:56] __main__ INFO: \u001b[0mdevice: cuda\n",
      "cudnn:\n",
      "  benchmark: True\n",
      "  deterministic: False\n",
      "dataset:\n",
      "  name: CIFAR10\n",
      "  dataset_dir: ~/.torch/datasets/CIFAR10\n",
      "  image_size: 32\n",
      "  n_channels: 3\n",
      "  n_classes: 10\n",
      "model:\n",
      "  type: cifar\n",
      "  name: resnext\n",
      "  init_mode: kaiming_fan_out\n",
      "  vgg:\n",
      "    n_channels: [64, 128, 256, 512, 512]\n",
      "    n_layers: [2, 2, 3, 3, 3]\n",
      "    use_bn: True\n",
      "  resnet:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "  resnet_preact:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "  wrn:\n",
      "    depth: 28\n",
      "    initial_channels: 16\n",
      "    widening_factor: 10\n",
      "    drop_rate: 0.0\n",
      "  densenet:\n",
      "    depth: 100\n",
      "    n_blocks: [6, 12, 24, 16]\n",
      "    block_type: bottleneck\n",
      "    growth_rate: 12\n",
      "    drop_rate: 0.0\n",
      "    compression_rate: 0.5\n",
      "  pyramidnet:\n",
      "    depth: 272\n",
      "    n_blocks: [3, 24, 36, 3]\n",
      "    initial_channels: 16\n",
      "    block_type: bottleneck\n",
      "    alpha: 200\n",
      "  resnext:\n",
      "    depth: 29\n",
      "    n_blocks: [3, 4, 6, 3]\n",
      "    initial_channels: 64\n",
      "    cardinality: 4\n",
      "    base_channels: 64\n",
      "  shake_shake:\n",
      "    depth: 26\n",
      "    initial_channels: 96\n",
      "    shake_forward: True\n",
      "    shake_backward: True\n",
      "    shake_image: True\n",
      "  se_resnet_preact:\n",
      "    depth: 110\n",
      "    initial_channels: 16\n",
      "    se_reduction: 16\n",
      "    block_type: basic\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "train:\n",
      "  checkpoint: /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_1_20_c10val/exp00/checkpoint_00400.pth\n",
      "  resume: False\n",
      "  precision: O0\n",
      "  batch_size: 128\n",
      "  subdivision: 1\n",
      "  optimizer: sgd\n",
      "  base_lr: 0.001\n",
      "  momentum: 0.9\n",
      "  nesterov: True\n",
      "  weight_decay: 0.0005\n",
      "  no_weight_decay_on_bn: False\n",
      "  gradient_clip: 0\n",
      "  start_epoch: 0\n",
      "  seed: 0\n",
      "  val_first: True\n",
      "  val_period: 1\n",
      "  val_ratio: 0.1\n",
      "  use_test_as_val: False\n",
      "  output_dir: /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_1_20_c10val/exp00_resume400_50\n",
      "  log_period: 100\n",
      "  checkpoint_period: 100\n",
      "  use_tensorboard: True\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: True\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "  distributed: False\n",
      "  dist:\n",
      "    backend: nccl\n",
      "    init_method: env://\n",
      "    world_size: -1\n",
      "    node_rank: -1\n",
      "    local_rank: 0\n",
      "    use_sync_bn: False\n",
      "tensorboard:\n",
      "  train_images: False\n",
      "  val_images: False\n",
      "  model_params: False\n",
      "optim:\n",
      "  adam:\n",
      "    betas: (0.9, 0.999)\n",
      "  lars:\n",
      "    eps: 1e-09\n",
      "    threshold: 0.01\n",
      "  adabound:\n",
      "    betas: (0.9, 0.999)\n",
      "    final_lr: 0.1\n",
      "    gamma: 0.001\n",
      "scheduler:\n",
      "  epochs: 50\n",
      "  warmup:\n",
      "    type: none\n",
      "    epochs: 0\n",
      "    start_factor: 0.001\n",
      "    exponent: 4\n",
      "  type: multistep\n",
      "  milestones: [150, 225]\n",
      "  lr_decay: 0.1\n",
      "  lr_min_factor: 0.001\n",
      "  T0: 10\n",
      "  T_mul: 1.0\n",
      "validation:\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: False\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "augmentation:\n",
      "  use_random_crop: True\n",
      "  use_random_horizontal_flip: True\n",
      "  use_cutout: False\n",
      "  use_random_erasing: False\n",
      "  use_dual_cutout: False\n",
      "  use_mixup: False\n",
      "  use_ricap: False\n",
      "  use_cutmix: False\n",
      "  use_label_smoothing: False\n",
      "  random_crop:\n",
      "    padding: 4\n",
      "    fill: 0\n",
      "    padding_mode: constant\n",
      "  random_horizontal_flip:\n",
      "    prob: 0.5\n",
      "  cutout:\n",
      "    prob: 1.0\n",
      "    mask_size: 16\n",
      "    cut_inside: False\n",
      "    mask_color: 0\n",
      "    dual_cutout_alpha: 0.1\n",
      "  random_erasing:\n",
      "    prob: 0.5\n",
      "    area_ratio_range: [0.02, 0.4]\n",
      "    min_aspect_ratio: 0.3\n",
      "    max_attempt: 20\n",
      "  mixup:\n",
      "    alpha: 1.0\n",
      "  ricap:\n",
      "    beta: 0.3\n",
      "  cutmix:\n",
      "    alpha: 1.0\n",
      "  label_smoothing:\n",
      "    epsilon: 0.1\n",
      "tta:\n",
      "  use_resize: False\n",
      "  use_center_crop: False\n",
      "  resize: 256\n",
      "test:\n",
      "  checkpoint: ''\n",
      "  output_dir: ''\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    pin_memory: False\n",
      "\u001b[32m[2020-07-19 10:52:56] __main__ INFO: \u001b[0menv_info:\n",
      "  pytorch_version: 1.4.0\n",
      "  cuda_version: 10.1\n",
      "  cudnn_version: 7603\n",
      "  num_gpus: 1\n",
      "  gpu_name: Tesla K80\n",
      "  gpu_capability: 3.7\n",
      "Files already downloaded and verified\n",
      "\u001b[32m[2020-07-19 10:53:00] __main__ INFO: \u001b[0mMACs  : 2.75G\n",
      "\u001b[32m[2020-07-19 10:53:00] __main__ INFO: \u001b[0m#params: 17.56M\n",
      "Selected optimization level O0:  Pure FP32 training.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\",)\n",
      "\u001b[32m[2020-07-19 10:53:00] __main__ INFO: \u001b[0mVal 0\n",
      "\u001b[32m[2020-07-19 10:53:24] __main__ INFO: \u001b[0mEpoch 0 loss 0.4388 acc@1 0.8672 acc@5 0.9930\n",
      "\u001b[32m[2020-07-19 10:53:24] __main__ INFO: \u001b[0mElapsed 24.21\n",
      "\u001b[32m[2020-07-19 10:53:24] __main__ INFO: \u001b[0mTrain 1 0\n",
      "\u001b[32m[2020-07-19 10:55:46] __main__ INFO: \u001b[0mEpoch 1 Step 100/351 lr 0.001000 loss 0.1904 (0.1490) acc@1 0.9219 (0.9531) acc@5 0.9922 (0.9978)\n",
      "\u001b[32m[2020-07-19 10:57:59] __main__ INFO: \u001b[0mEpoch 1 Step 200/351 lr 0.001000 loss 0.1657 (0.1427) acc@1 0.9453 (0.9554) acc@5 1.0000 (0.9985)\n",
      "\u001b[32m[2020-07-19 11:00:13] __main__ INFO: \u001b[0mEpoch 1 Step 300/351 lr 0.001000 loss 0.0561 (0.1389) acc@1 0.9844 (0.9557) acc@5 1.0000 (0.9986)\n",
      "\u001b[32m[2020-07-19 11:01:21] __main__ INFO: \u001b[0mEpoch 1 Step 351/351 lr 0.001000 loss 0.1282 (0.1379) acc@1 0.9609 (0.9559) acc@5 1.0000 (0.9987)\n",
      "\u001b[32m[2020-07-19 11:01:21] __main__ INFO: \u001b[0mElapsed 476.25\n",
      "\u001b[32m[2020-07-19 11:01:21] __main__ INFO: \u001b[0mVal 1\n",
      "\u001b[32m[2020-07-19 11:01:37] __main__ INFO: \u001b[0mEpoch 1 loss 0.2257 acc@1 0.9272 acc@5 0.9994\n",
      "\u001b[32m[2020-07-19 11:01:37] __main__ INFO: \u001b[0mElapsed 16.74\n",
      "\u001b[32m[2020-07-19 11:01:37] __main__ INFO: \u001b[0mTrain 2 351\n",
      "\u001b[32m[2020-07-19 11:03:51] __main__ INFO: \u001b[0mEpoch 2 Step 100/351 lr 0.001000 loss 0.1020 (0.0869) acc@1 0.9609 (0.9743) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-07-19 11:06:04] __main__ INFO: \u001b[0mEpoch 2 Step 200/351 lr 0.001000 loss 0.1233 (0.0903) acc@1 0.9609 (0.9732) acc@5 1.0000 (0.9995)\n",
      "\u001b[32m[2020-07-19 11:08:17] __main__ INFO: \u001b[0mEpoch 2 Step 300/351 lr 0.001000 loss 0.0758 (0.0915) acc@1 0.9844 (0.9728) acc@5 1.0000 (0.9995)\n",
      "\u001b[32m[2020-07-19 11:09:25] __main__ INFO: \u001b[0mEpoch 2 Step 351/351 lr 0.001000 loss 0.1815 (0.0919) acc@1 0.9219 (0.9722) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-07-19 11:09:25] __main__ INFO: \u001b[0mElapsed 467.90\n",
      "\u001b[32m[2020-07-19 11:09:25] __main__ INFO: \u001b[0mVal 2\n",
      "\u001b[32m[2020-07-19 11:09:42] __main__ INFO: \u001b[0mEpoch 2 loss 0.2104 acc@1 0.9312 acc@5 0.9992\n",
      "\u001b[32m[2020-07-19 11:09:42] __main__ INFO: \u001b[0mElapsed 16.77\n",
      "\u001b[32m[2020-07-19 11:09:42] __main__ INFO: \u001b[0mTrain 3 702\n",
      "\u001b[32m[2020-07-19 11:11:55] __main__ INFO: \u001b[0mEpoch 3 Step 100/351 lr 0.001000 loss 0.0643 (0.0744) acc@1 1.0000 (0.9794) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-07-19 11:14:08] __main__ INFO: \u001b[0mEpoch 3 Step 200/351 lr 0.001000 loss 0.1158 (0.0750) acc@1 0.9453 (0.9794) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-07-19 11:16:21] __main__ INFO: \u001b[0mEpoch 3 Step 300/351 lr 0.001000 loss 0.0311 (0.0735) acc@1 1.0000 (0.9801) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-19 11:17:29] __main__ INFO: \u001b[0mEpoch 3 Step 351/351 lr 0.001000 loss 0.0419 (0.0727) acc@1 1.0000 (0.9803) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-19 11:17:29] __main__ INFO: \u001b[0mElapsed 467.11\n",
      "\u001b[32m[2020-07-19 11:17:29] __main__ INFO: \u001b[0mVal 3\n",
      "\u001b[32m[2020-07-19 11:19:59] __main__ INFO: \u001b[0mEpoch 4 Step 100/351 lr 0.001000 loss 0.0916 (0.0562) acc@1 0.9688 (0.9869) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 11:22:12] __main__ INFO: \u001b[0mEpoch 4 Step 200/351 lr 0.001000 loss 0.0630 (0.0571) acc@1 0.9844 (0.9863) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-19 11:24:25] __main__ INFO: \u001b[0mEpoch 4 Step 300/351 lr 0.001000 loss 0.0304 (0.0583) acc@1 1.0000 (0.9855) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-19 11:25:33] __main__ INFO: \u001b[0mEpoch 4 Step 351/351 lr 0.001000 loss 0.0727 (0.0580) acc@1 0.9922 (0.9856) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-19 11:25:33] __main__ INFO: \u001b[0mElapsed 467.05\n",
      "\u001b[32m[2020-07-19 11:25:33] __main__ INFO: \u001b[0mVal 4\n",
      "\u001b[32m[2020-07-19 11:25:50] __main__ INFO: \u001b[0mEpoch 4 loss 0.2050 acc@1 0.9358 acc@5 0.9992\n",
      "\u001b[32m[2020-07-19 11:25:50] __main__ INFO: \u001b[0mElapsed 16.76\n",
      "\u001b[32m[2020-07-19 11:25:50] __main__ INFO: \u001b[0mTrain 5 1404\n",
      "\u001b[32m[2020-07-19 11:28:03] __main__ INFO: \u001b[0mEpoch 5 Step 100/351 lr 0.001000 loss 0.0309 (0.0450) acc@1 0.9922 (0.9912) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-19 11:30:16] __main__ INFO: \u001b[0mEpoch 5 Step 200/351 lr 0.001000 loss 0.0728 (0.0462) acc@1 0.9688 (0.9904) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-19 11:32:29] __main__ INFO: \u001b[0mEpoch 5 Step 300/351 lr 0.001000 loss 0.0431 (0.0475) acc@1 0.9844 (0.9900) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-19 11:33:37] __main__ INFO: \u001b[0mEpoch 5 Step 351/351 lr 0.001000 loss 0.0548 (0.0486) acc@1 0.9844 (0.9894) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-19 11:33:37] __main__ INFO: \u001b[0mElapsed 467.68\n",
      "\u001b[32m[2020-07-19 11:33:37] __main__ INFO: \u001b[0mVal 5\n",
      "\u001b[32m[2020-07-19 11:33:54] __main__ INFO: \u001b[0mEpoch 5 loss 0.2061 acc@1 0.9356 acc@5 0.9996\n",
      "\u001b[32m[2020-07-19 11:33:54] __main__ INFO: \u001b[0mElapsed 16.77\n",
      "\u001b[32m[2020-07-19 11:33:54] __main__ INFO: \u001b[0mTrain 6 1755\n",
      "\u001b[32m[2020-07-19 11:36:07] __main__ INFO: \u001b[0mEpoch 6 Step 100/351 lr 0.001000 loss 0.0259 (0.0411) acc@1 1.0000 (0.9912) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 11:38:20] __main__ INFO: \u001b[0mEpoch 6 Step 200/351 lr 0.001000 loss 0.0442 (0.0413) acc@1 0.9922 (0.9918) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 11:40:33] __main__ INFO: \u001b[0mEpoch 6 Step 300/351 lr 0.001000 loss 0.0415 (0.0426) acc@1 0.9922 (0.9912) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-19 11:41:41] __main__ INFO: \u001b[0mEpoch 6 Step 351/351 lr 0.001000 loss 0.0335 (0.0421) acc@1 0.9922 (0.9916) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-19 11:41:41] __main__ INFO: \u001b[0mElapsed 466.98\n",
      "\u001b[32m[2020-07-19 11:41:41] __main__ INFO: \u001b[0mVal 6\n",
      "\u001b[32m[2020-07-19 11:41:58] __main__ INFO: \u001b[0mEpoch 6 loss 0.2106 acc@1 0.9352 acc@5 0.9992\n",
      "\u001b[32m[2020-07-19 11:41:58] __main__ INFO: \u001b[0mElapsed 16.76\n",
      "\u001b[32m[2020-07-19 11:41:58] __main__ INFO: \u001b[0mTrain 7 2106\n",
      "\u001b[32m[2020-07-19 11:44:11] __main__ INFO: \u001b[0mEpoch 7 Step 100/351 lr 0.001000 loss 0.0398 (0.0336) acc@1 0.9922 (0.9952) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 11:46:25] __main__ INFO: \u001b[0mEpoch 7 Step 200/351 lr 0.001000 loss 0.0256 (0.0346) acc@1 1.0000 (0.9945) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 11:48:38] __main__ INFO: \u001b[0mEpoch 7 Step 300/351 lr 0.001000 loss 0.0323 (0.0359) acc@1 0.9922 (0.9936) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 11:49:46] __main__ INFO: \u001b[0mEpoch 7 Step 351/351 lr 0.001000 loss 0.0329 (0.0358) acc@1 0.9922 (0.9937) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 11:49:46] __main__ INFO: \u001b[0mElapsed 468.26\n",
      "\u001b[32m[2020-07-19 11:49:46] __main__ INFO: \u001b[0mVal 7\n",
      "\u001b[32m[2020-07-19 11:50:03] __main__ INFO: \u001b[0mEpoch 7 loss 0.2054 acc@1 0.9352 acc@5 0.9992\n",
      "\u001b[32m[2020-07-19 11:50:03] __main__ INFO: \u001b[0mElapsed 16.79\n",
      "\u001b[32m[2020-07-19 11:50:03] __main__ INFO: \u001b[0mTrain 8 2457\n",
      "\u001b[32m[2020-07-19 11:52:16] __main__ INFO: \u001b[0mEpoch 8 Step 100/351 lr 0.001000 loss 0.0460 (0.0307) acc@1 0.9922 (0.9959) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 11:54:30] __main__ INFO: \u001b[0mEpoch 8 Step 200/351 lr 0.001000 loss 0.0209 (0.0303) acc@1 1.0000 (0.9959) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 11:56:43] __main__ INFO: \u001b[0mEpoch 8 Step 300/351 lr 0.001000 loss 0.0432 (0.0304) acc@1 0.9844 (0.9956) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 11:57:51] __main__ INFO: \u001b[0mEpoch 8 Step 351/351 lr 0.001000 loss 0.0154 (0.0309) acc@1 1.0000 (0.9953) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 11:57:51] __main__ INFO: \u001b[0mElapsed 467.85\n",
      "\u001b[32m[2020-07-19 11:57:51] __main__ INFO: \u001b[0mVal 8\n",
      "\u001b[32m[2020-07-19 11:58:08] __main__ INFO: \u001b[0mEpoch 8 loss 0.2117 acc@1 0.9378 acc@5 0.9994\n",
      "\u001b[32m[2020-07-19 11:58:08] __main__ INFO: \u001b[0mElapsed 16.76\n",
      "\u001b[32m[2020-07-19 11:58:08] __main__ INFO: \u001b[0mTrain 9 2808\n",
      "\u001b[32m[2020-07-19 12:00:21] __main__ INFO: \u001b[0mEpoch 9 Step 100/351 lr 0.001000 loss 0.0340 (0.0266) acc@1 1.0000 (0.9963) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 14:17:35] __main__ INFO: \u001b[0mEpoch 26 Step 100/351 lr 0.001000 loss 0.0096 (0.0088) acc@1 1.0000 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 14:19:48] __main__ INFO: \u001b[0mEpoch 26 Step 200/351 lr 0.001000 loss 0.0103 (0.0086) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 14:22:01] __main__ INFO: \u001b[0mEpoch 26 Step 300/351 lr 0.001000 loss 0.0135 (0.0088) acc@1 1.0000 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 14:23:09] __main__ INFO: \u001b[0mEpoch 26 Step 351/351 lr 0.001000 loss 0.0144 (0.0088) acc@1 1.0000 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 14:23:09] __main__ INFO: \u001b[0mElapsed 467.17\n",
      "\u001b[32m[2020-07-19 14:23:09] __main__ INFO: \u001b[0mVal 26\n",
      "\u001b[32m[2020-07-19 14:23:26] __main__ INFO: \u001b[0mEpoch 26 loss 0.2067 acc@1 0.9372 acc@5 0.9994\n",
      "\u001b[32m[2020-07-19 14:23:26] __main__ INFO: \u001b[0mElapsed 16.74\n",
      "\u001b[32m[2020-07-19 14:23:26] __main__ INFO: \u001b[0mTrain 27 9126\n",
      "\u001b[32m[2020-07-19 14:25:39] __main__ INFO: \u001b[0mEpoch 27 Step 100/351 lr 0.001000 loss 0.0032 (0.0086) acc@1 1.0000 (0.9995) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 14:27:52] __main__ INFO: \u001b[0mEpoch 27 Step 200/351 lr 0.001000 loss 0.0111 (0.0085) acc@1 1.0000 (0.9995) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 14:30:05] __main__ INFO: \u001b[0mEpoch 27 Step 300/351 lr 0.001000 loss 0.0073 (0.0085) acc@1 1.0000 (0.9996) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 14:31:13] __main__ INFO: \u001b[0mEpoch 27 Step 351/351 lr 0.001000 loss 0.0051 (0.0084) acc@1 1.0000 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 14:31:13] __main__ INFO: \u001b[0mElapsed 467.39\n",
      "\u001b[32m[2020-07-19 14:31:13] __main__ INFO: \u001b[0mVal 27\n",
      "\u001b[32m[2020-07-19 14:31:30] __main__ INFO: \u001b[0mEpoch 27 loss 0.2095 acc@1 0.9364 acc@5 0.9988\n",
      "\u001b[32m[2020-07-19 14:31:30] __main__ INFO: \u001b[0mElapsed 16.77\n",
      "\u001b[32m[2020-07-19 14:31:30] __main__ INFO: \u001b[0mTrain 28 9477\n",
      "\u001b[32m[2020-07-19 14:33:43] __main__ INFO: \u001b[0mEpoch 28 Step 100/351 lr 0.001000 loss 0.0123 (0.0080) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 14:35:57] __main__ INFO: \u001b[0mEpoch 28 Step 200/351 lr 0.001000 loss 0.0087 (0.0083) acc@1 1.0000 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 14:38:10] __main__ INFO: \u001b[0mEpoch 28 Step 300/351 lr 0.001000 loss 0.0101 (0.0081) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 14:39:18] __main__ INFO: \u001b[0mEpoch 28 Step 351/351 lr 0.001000 loss 0.0111 (0.0081) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 14:39:18] __main__ INFO: \u001b[0mElapsed 468.01\n",
      "\u001b[32m[2020-07-19 14:39:18] __main__ INFO: \u001b[0mVal 28\n",
      "\u001b[32m[2020-07-19 14:39:35] __main__ INFO: \u001b[0mEpoch 28 loss 0.2138 acc@1 0.9368 acc@5 0.9996\n",
      "\u001b[32m[2020-07-19 14:39:35] __main__ INFO: \u001b[0mElapsed 16.78\n",
      "\u001b[32m[2020-07-19 14:39:35] __main__ INFO: \u001b[0mTrain 29 9828\n",
      "\u001b[32m[2020-07-19 14:41:48] __main__ INFO: \u001b[0mEpoch 29 Step 100/351 lr 0.001000 loss 0.0100 (0.0076) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 14:44:01] __main__ INFO: \u001b[0mEpoch 29 Step 200/351 lr 0.001000 loss 0.0114 (0.0076) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 14:46:14] __main__ INFO: \u001b[0mEpoch 29 Step 300/351 lr 0.001000 loss 0.0061 (0.0076) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 14:47:22] __main__ INFO: \u001b[0mEpoch 29 Step 351/351 lr 0.001000 loss 0.0063 (0.0076) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 14:47:22] __main__ INFO: \u001b[0mElapsed 467.42\n",
      "\u001b[32m[2020-07-19 14:47:22] __main__ INFO: \u001b[0mVal 29\n",
      "\u001b[32m[2020-07-19 14:47:39] __main__ INFO: \u001b[0mEpoch 29 loss 0.2126 acc@1 0.9356 acc@5 0.9994\n",
      "\u001b[32m[2020-07-19 14:47:39] __main__ INFO: \u001b[0mElapsed 16.79\n",
      "\u001b[32m[2020-07-19 14:47:39] __main__ INFO: \u001b[0mTrain 30 10179\n",
      "\u001b[32m[2020-07-19 14:49:52] __main__ INFO: \u001b[0mEpoch 30 Step 100/351 lr 0.001000 loss 0.0104 (0.0078) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 14:52:05] __main__ INFO: \u001b[0mEpoch 30 Step 200/351 lr 0.001000 loss 0.0081 (0.0076) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 14:54:18] __main__ INFO: \u001b[0mEpoch 30 Step 300/351 lr 0.001000 loss 0.0068 (0.0076) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 14:55:27] __main__ INFO: \u001b[0mEpoch 30 Step 351/351 lr 0.001000 loss 0.0080 (0.0076) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 14:55:27] __main__ INFO: \u001b[0mElapsed 467.47\n",
      "\u001b[32m[2020-07-19 14:55:27] __main__ INFO: \u001b[0mVal 30\n",
      "\u001b[32m[2020-07-19 14:55:43] __main__ INFO: \u001b[0mEpoch 30 loss 0.2070 acc@1 0.9402 acc@5 0.9994\n",
      "\u001b[32m[2020-07-19 14:55:43] __main__ INFO: \u001b[0mElapsed 16.91\n",
      "\u001b[32m[2020-07-19 14:55:43] __main__ INFO: \u001b[0mTrain 31 10530\n",
      "\u001b[32m[2020-07-19 14:57:58] __main__ INFO: \u001b[0mEpoch 31 Step 100/351 lr 0.001000 loss 0.0052 (0.0073) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 15:00:13] __main__ INFO: \u001b[0mEpoch 31 Step 200/351 lr 0.001000 loss 0.0106 (0.0073) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 15:02:27] __main__ INFO: \u001b[0mEpoch 31 Step 300/351 lr 0.001000 loss 0.0081 (0.0071) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 15:03:35] __main__ INFO: \u001b[0mEpoch 31 Step 351/351 lr 0.001000 loss 0.0070 (0.0072) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 15:03:35] __main__ INFO: \u001b[0mElapsed 471.54\n",
      "\u001b[32m[2020-07-19 15:03:35] __main__ INFO: \u001b[0mVal 31\n",
      "\u001b[32m[2020-07-19 15:03:52] __main__ INFO: \u001b[0mEpoch 31 loss 0.2138 acc@1 0.9372 acc@5 0.9992\n",
      "\u001b[32m[2020-07-19 15:03:52] __main__ INFO: \u001b[0mElapsed 16.76\n",
      "\u001b[32m[2020-07-19 15:03:52] __main__ INFO: \u001b[0mTrain 32 10881\n",
      "\u001b[32m[2020-07-19 15:06:04] __main__ INFO: \u001b[0mEpoch 32 Step 100/351 lr 0.001000 loss 0.0051 (0.0066) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 15:08:16] __main__ INFO: \u001b[0mEpoch 32 Step 200/351 lr 0.001000 loss 0.0072 (0.0068) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 15:10:29] __main__ INFO: \u001b[0mEpoch 32 Step 300/351 lr 0.001000 loss 0.0061 (0.0069) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 15:11:36] __main__ INFO: \u001b[0mEpoch 32 Step 351/351 lr 0.001000 loss 0.0052 (0.0069) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 15:11:36] __main__ INFO: \u001b[0mElapsed 464.63\n",
      "\u001b[32m[2020-07-19 15:11:36] __main__ INFO: \u001b[0mVal 32\n",
      "\u001b[32m[2020-07-19 15:11:53] __main__ INFO: \u001b[0mEpoch 32 loss 0.2147 acc@1 0.9344 acc@5 0.9994\n",
      "\u001b[32m[2020-07-19 15:11:53] __main__ INFO: \u001b[0mElapsed 16.64\n",
      "\u001b[32m[2020-07-19 15:11:53] __main__ INFO: \u001b[0mTrain 33 11232\n",
      "\u001b[32m[2020-07-19 15:14:06] __main__ INFO: \u001b[0mEpoch 33 Step 100/351 lr 0.001000 loss 0.0050 (0.0069) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 15:16:18] __main__ INFO: \u001b[0mEpoch 33 Step 200/351 lr 0.001000 loss 0.0057 (0.0067) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 15:18:30] __main__ INFO: \u001b[0mEpoch 33 Step 300/351 lr 0.001000 loss 0.0060 (0.0068) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 15:19:37] __main__ INFO: \u001b[0mEpoch 33 Step 351/351 lr 0.001000 loss 0.0044 (0.0069) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 15:19:37] __main__ INFO: \u001b[0mElapsed 464.28\n",
      "\u001b[32m[2020-07-19 15:19:37] __main__ INFO: \u001b[0mVal 33\n",
      "\u001b[32m[2020-07-19 15:19:54] __main__ INFO: \u001b[0mEpoch 33 loss 0.2089 acc@1 0.9382 acc@5 0.9996\n",
      "\u001b[32m[2020-07-19 15:19:54] __main__ INFO: \u001b[0mElapsed 16.64\n",
      "\u001b[32m[2020-07-19 15:19:54] __main__ INFO: \u001b[0mTrain 34 11583\n",
      "\u001b[32m[2020-07-19 15:22:06] __main__ INFO: \u001b[0mEpoch 34 Step 100/351 lr 0.001000 loss 0.0036 (0.0066) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 15:24:18] __main__ INFO: \u001b[0mEpoch 34 Step 200/351 lr 0.001000 loss 0.0097 (0.0066) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 15:26:30] __main__ INFO: \u001b[0mEpoch 34 Step 300/351 lr 0.001000 loss 0.0071 (0.0067) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 15:27:37] __main__ INFO: \u001b[0mEpoch 34 Step 351/351 lr 0.001000 loss 0.0066 (0.0067) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 15:27:37] __main__ INFO: \u001b[0mElapsed 463.34\n",
      "\u001b[32m[2020-07-19 15:27:37] __main__ INFO: \u001b[0mVal 34\n",
      "\u001b[32m[2020-07-19 15:27:54] __main__ INFO: \u001b[0mEpoch 34 loss 0.2068 acc@1 0.9376 acc@5 0.9992\n",
      "\u001b[32m[2020-07-19 15:27:54] __main__ INFO: \u001b[0mElapsed 16.63\n",
      "\u001b[32m[2020-07-19 15:27:54] __main__ INFO: \u001b[0mTrain 35 11934\n",
      "\u001b[32m[2020-07-19 15:30:06] __main__ INFO: \u001b[0mEpoch 35 Step 100/351 lr 0.001000 loss 0.0077 (0.0066) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 15:48:22] __main__ INFO: \u001b[0mEpoch 37 Step 200/351 lr 0.001000 loss 0.0073 (0.0060) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 15:50:36] __main__ INFO: \u001b[0mEpoch 37 Step 300/351 lr 0.001000 loss 0.0110 (0.0061) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 15:51:44] __main__ INFO: \u001b[0mEpoch 37 Step 351/351 lr 0.001000 loss 0.0081 (0.0061) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 15:51:44] __main__ INFO: \u001b[0mElapsed 468.83\n",
      "\u001b[32m[2020-07-19 15:51:44] __main__ INFO: \u001b[0mVal 37\n",
      "\u001b[32m[2020-07-19 15:52:01] __main__ INFO: \u001b[0mEpoch 37 loss 0.2110 acc@1 0.9380 acc@5 0.9990\n",
      "\u001b[32m[2020-07-19 15:52:01] __main__ INFO: \u001b[0mElapsed 16.74\n",
      "\u001b[32m[2020-07-19 15:52:01] __main__ INFO: \u001b[0mTrain 38 12987\n",
      "\u001b[32m[2020-07-19 15:54:14] __main__ INFO: \u001b[0mEpoch 38 Step 100/351 lr 0.001000 loss 0.0067 (0.0062) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 15:56:26] __main__ INFO: \u001b[0mEpoch 38 Step 200/351 lr 0.001000 loss 0.0084 (0.0063) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 15:58:38] __main__ INFO: \u001b[0mEpoch 38 Step 300/351 lr 0.001000 loss 0.0047 (0.0062) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 15:59:45] __main__ INFO: \u001b[0mEpoch 38 Step 351/351 lr 0.001000 loss 0.0047 (0.0062) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 15:59:45] __main__ INFO: \u001b[0mElapsed 464.15\n",
      "\u001b[32m[2020-07-19 15:59:45] __main__ INFO: \u001b[0mVal 38\n",
      "\u001b[32m[2020-07-19 16:00:02] __main__ INFO: \u001b[0mEpoch 38 loss 0.2060 acc@1 0.9390 acc@5 0.9990\n",
      "\u001b[32m[2020-07-19 16:00:02] __main__ INFO: \u001b[0mElapsed 16.64\n",
      "\u001b[32m[2020-07-19 16:00:02] __main__ INFO: \u001b[0mTrain 39 13338\n",
      "\u001b[32m[2020-07-19 16:02:14] __main__ INFO: \u001b[0mEpoch 39 Step 100/351 lr 0.001000 loss 0.0031 (0.0058) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 16:04:27] __main__ INFO: \u001b[0mEpoch 39 Step 200/351 lr 0.001000 loss 0.0052 (0.0059) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 16:06:39] __main__ INFO: \u001b[0mEpoch 39 Step 300/351 lr 0.001000 loss 0.0085 (0.0060) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 16:07:47] __main__ INFO: \u001b[0mEpoch 39 Step 351/351 lr 0.001000 loss 0.0056 (0.0061) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 16:07:47] __main__ INFO: \u001b[0mElapsed 464.90\n",
      "\u001b[32m[2020-07-19 16:07:47] __main__ INFO: \u001b[0mVal 39\n",
      "\u001b[32m[2020-07-19 16:08:03] __main__ INFO: \u001b[0mEpoch 39 loss 0.2055 acc@1 0.9402 acc@5 0.9990\n",
      "\u001b[32m[2020-07-19 16:08:03] __main__ INFO: \u001b[0mElapsed 16.70\n",
      "\u001b[32m[2020-07-19 16:08:03] __main__ INFO: \u001b[0mTrain 40 13689\n",
      "\u001b[32m[2020-07-19 16:10:16] __main__ INFO: \u001b[0mEpoch 40 Step 100/351 lr 0.001000 loss 0.0087 (0.0058) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 16:12:28] __main__ INFO: \u001b[0mEpoch 40 Step 200/351 lr 0.001000 loss 0.0056 (0.0055) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 16:14:40] __main__ INFO: \u001b[0mEpoch 40 Step 300/351 lr 0.001000 loss 0.0040 (0.0057) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 16:15:48] __main__ INFO: \u001b[0mEpoch 40 Step 351/351 lr 0.001000 loss 0.0096 (0.0057) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 16:15:48] __main__ INFO: \u001b[0mElapsed 464.55\n",
      "\u001b[32m[2020-07-19 16:15:48] __main__ INFO: \u001b[0mVal 40\n",
      "\u001b[32m[2020-07-19 16:16:05] __main__ INFO: \u001b[0mEpoch 40 loss 0.2088 acc@1 0.9376 acc@5 0.9988\n",
      "\u001b[32m[2020-07-19 16:16:05] __main__ INFO: \u001b[0mElapsed 16.69\n",
      "\u001b[32m[2020-07-19 16:16:05] __main__ INFO: \u001b[0mTrain 41 14040\n",
      "\u001b[32m[2020-07-19 16:18:17] __main__ INFO: \u001b[0mEpoch 41 Step 100/351 lr 0.001000 loss 0.0067 (0.0058) acc@1 1.0000 (1.0000) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 16:20:29] __main__ INFO: \u001b[0mEpoch 41 Step 200/351 lr 0.001000 loss 0.0047 (0.0058) acc@1 1.0000 (1.0000) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 16:22:41] __main__ INFO: \u001b[0mEpoch 41 Step 300/351 lr 0.001000 loss 0.0060 (0.0059) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 16:23:48] __main__ INFO: \u001b[0mEpoch 41 Step 351/351 lr 0.001000 loss 0.0051 (0.0059) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 16:23:48] __main__ INFO: \u001b[0mElapsed 463.82\n",
      "\u001b[32m[2020-07-19 16:23:48] __main__ INFO: \u001b[0mVal 41\n",
      "\u001b[32m[2020-07-19 16:24:05] __main__ INFO: \u001b[0mEpoch 41 loss 0.2091 acc@1 0.9386 acc@5 0.9990\n",
      "\u001b[32m[2020-07-19 16:24:05] __main__ INFO: \u001b[0mElapsed 16.74\n",
      "\u001b[32m[2020-07-19 16:24:05] __main__ INFO: \u001b[0mTrain 42 14391\n",
      "\u001b[32m[2020-07-19 16:26:19] __main__ INFO: \u001b[0mEpoch 42 Step 100/351 lr 0.001000 loss 0.0101 (0.0054) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 16:28:31] __main__ INFO: \u001b[0mEpoch 42 Step 200/351 lr 0.001000 loss 0.0038 (0.0056) acc@1 1.0000 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 16:30:43] __main__ INFO: \u001b[0mEpoch 42 Step 300/351 lr 0.001000 loss 0.0094 (0.0056) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 16:31:51] __main__ INFO: \u001b[0mEpoch 42 Step 351/351 lr 0.001000 loss 0.0098 (0.0055) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 16:31:51] __main__ INFO: \u001b[0mElapsed 465.90\n",
      "\u001b[32m[2020-07-19 16:31:51] __main__ INFO: \u001b[0mVal 42\n",
      "\u001b[32m[2020-07-19 16:32:08] __main__ INFO: \u001b[0mEpoch 42 loss 0.2103 acc@1 0.9386 acc@5 0.9990\n",
      "\u001b[32m[2020-07-19 16:32:08] __main__ INFO: \u001b[0mElapsed 16.72\n",
      "\u001b[32m[2020-07-19 16:32:08] __main__ INFO: \u001b[0mTrain 43 14742\n",
      "\u001b[32m[2020-07-19 16:34:20] __main__ INFO: \u001b[0mEpoch 43 Step 100/351 lr 0.001000 loss 0.0064 (0.0051) acc@1 1.0000 (1.0000) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 16:36:33] __main__ INFO: \u001b[0mEpoch 43 Step 200/351 lr 0.001000 loss 0.0037 (0.0051) acc@1 1.0000 (1.0000) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 16:38:45] __main__ INFO: \u001b[0mEpoch 43 Step 300/351 lr 0.001000 loss 0.0072 (0.0052) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 16:39:52] __main__ INFO: \u001b[0mEpoch 43 Step 351/351 lr 0.001000 loss 0.0041 (0.0052) acc@1 1.0000 (1.0000) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 16:39:52] __main__ INFO: \u001b[0mElapsed 464.58\n",
      "\u001b[32m[2020-07-19 16:39:52] __main__ INFO: \u001b[0mVal 43\n",
      "\u001b[32m[2020-07-19 16:40:09] __main__ INFO: \u001b[0mEpoch 43 loss 0.2088 acc@1 0.9362 acc@5 0.9992\n",
      "\u001b[32m[2020-07-19 16:40:09] __main__ INFO: \u001b[0mElapsed 16.67\n",
      "\u001b[32m[2020-07-19 16:40:09] __main__ INFO: \u001b[0mTrain 44 15093\n",
      "\u001b[32m[2020-07-19 16:42:22] __main__ INFO: \u001b[0mEpoch 44 Step 100/351 lr 0.001000 loss 0.0042 (0.0051) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 16:44:34] __main__ INFO: \u001b[0mEpoch 44 Step 200/351 lr 0.001000 loss 0.0040 (0.0054) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 16:46:46] __main__ INFO: \u001b[0mEpoch 44 Step 300/351 lr 0.001000 loss 0.0050 (0.0055) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 16:47:54] __main__ INFO: \u001b[0mEpoch 44 Step 351/351 lr 0.001000 loss 0.0034 (0.0054) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 16:47:54] __main__ INFO: \u001b[0mElapsed 464.82\n",
      "\u001b[32m[2020-07-19 16:47:54] __main__ INFO: \u001b[0mVal 44\n",
      "\u001b[32m[2020-07-19 16:48:11] __main__ INFO: \u001b[0mEpoch 44 loss 0.2070 acc@1 0.9386 acc@5 0.9990\n",
      "\u001b[32m[2020-07-19 16:48:11] __main__ INFO: \u001b[0mElapsed 16.68\n",
      "\u001b[32m[2020-07-19 16:48:11] __main__ INFO: \u001b[0mTrain 45 15444\n",
      "\u001b[32m[2020-07-19 16:50:23] __main__ INFO: \u001b[0mEpoch 45 Step 100/351 lr 0.001000 loss 0.0052 (0.0053) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 16:52:36] __main__ INFO: \u001b[0mEpoch 45 Step 200/351 lr 0.001000 loss 0.0035 (0.0054) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 16:54:48] __main__ INFO: \u001b[0mEpoch 45 Step 300/351 lr 0.001000 loss 0.0044 (0.0053) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 16:55:56] __main__ INFO: \u001b[0mEpoch 45 Step 351/351 lr 0.001000 loss 0.0053 (0.0052) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 16:55:56] __main__ INFO: \u001b[0mElapsed 465.13\n",
      "\u001b[32m[2020-07-19 16:55:56] __main__ INFO: \u001b[0mVal 45\n",
      "\u001b[32m[2020-07-19 16:56:12] __main__ INFO: \u001b[0mEpoch 45 loss 0.2067 acc@1 0.9378 acc@5 0.9992\n",
      "\u001b[32m[2020-07-19 16:56:12] __main__ INFO: \u001b[0mElapsed 16.67\n",
      "\u001b[32m[2020-07-19 16:56:12] __main__ INFO: \u001b[0mTrain 46 15795\n",
      "\u001b[32m[2020-07-19 16:58:25] __main__ INFO: \u001b[0mEpoch 46 Step 100/351 lr 0.001000 loss 0.0023 (0.0052) acc@1 1.0000 (1.0000) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 17:00:37] __main__ INFO: \u001b[0mEpoch 46 Step 200/351 lr 0.001000 loss 0.0062 (0.0051) acc@1 1.0000 (1.0000) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 17:02:49] __main__ INFO: \u001b[0mEpoch 46 Step 300/351 lr 0.001000 loss 0.0036 (0.0050) acc@1 1.0000 (1.0000) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 17:03:57] __main__ INFO: \u001b[0mEpoch 46 Step 351/351 lr 0.001000 loss 0.0095 (0.0050) acc@1 1.0000 (1.0000) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 17:03:57] __main__ INFO: \u001b[0mElapsed 464.58\n",
      "\u001b[32m[2020-07-19 17:03:57] __main__ INFO: \u001b[0mVal 46\n",
      "\u001b[32m[2020-07-19 17:04:14] __main__ INFO: \u001b[0mEpoch 46 loss 0.2084 acc@1 0.9376 acc@5 0.9988\n",
      "\u001b[32m[2020-07-19 17:04:14] __main__ INFO: \u001b[0mElapsed 16.72\n",
      "\u001b[32m[2020-07-19 17:04:14] __main__ INFO: \u001b[0mTrain 47 16146\n",
      "\u001b[32m[2020-07-19 17:06:27] __main__ INFO: \u001b[0mEpoch 47 Step 100/351 lr 0.001000 loss 0.0032 (0.0051) acc@1 1.0000 (1.0000) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 17:08:39] __main__ INFO: \u001b[0mEpoch 47 Step 200/351 lr 0.001000 loss 0.0035 (0.0051) acc@1 1.0000 (1.0000) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 17:10:52] __main__ INFO: \u001b[0mEpoch 47 Step 300/351 lr 0.001000 loss 0.0038 (0.0051) acc@1 1.0000 (1.0000) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 17:11:59] __main__ INFO: \u001b[0mEpoch 47 Step 351/351 lr 0.001000 loss 0.0044 (0.0051) acc@1 1.0000 (1.0000) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 17:11:59] __main__ INFO: \u001b[0mElapsed 465.39\n",
      "\u001b[32m[2020-07-19 17:11:59] __main__ INFO: \u001b[0mVal 47\n",
      "\u001b[32m[2020-07-19 17:12:16] __main__ INFO: \u001b[0mEpoch 47 loss 0.2139 acc@1 0.9376 acc@5 0.9986\n",
      "\u001b[32m[2020-07-19 17:12:16] __main__ INFO: \u001b[0mElapsed 16.63\n",
      "\u001b[32m[2020-07-19 17:12:16] __main__ INFO: \u001b[0mTrain 48 16497\n",
      "\u001b[32m[2020-07-19 17:14:28] __main__ INFO: \u001b[0mEpoch 48 Step 100/351 lr 0.001000 loss 0.0065 (0.0047) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 17:16:40] __main__ INFO: \u001b[0mEpoch 48 Step 200/351 lr 0.001000 loss 0.0050 (0.0049) acc@1 1.0000 (1.0000) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 17:18:53] __main__ INFO: \u001b[0mEpoch 48 Step 300/351 lr 0.001000 loss 0.0044 (0.0048) acc@1 1.0000 (1.0000) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 17:20:00] __main__ INFO: \u001b[0mEpoch 48 Step 351/351 lr 0.001000 loss 0.0060 (0.0048) acc@1 1.0000 (1.0000) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 17:20:00] __main__ INFO: \u001b[0mElapsed 464.52\n",
      "\u001b[32m[2020-07-19 17:20:00] __main__ INFO: \u001b[0mVal 48\n",
      "\u001b[32m[2020-07-19 17:20:17] __main__ INFO: \u001b[0mEpoch 48 loss 0.2121 acc@1 0.9370 acc@5 0.9992\n",
      "\u001b[32m[2020-07-19 17:20:17] __main__ INFO: \u001b[0mElapsed 16.65\n",
      "\u001b[32m[2020-07-19 17:20:17] __main__ INFO: \u001b[0mTrain 49 16848\n",
      "\u001b[32m[2020-07-19 17:22:29] __main__ INFO: \u001b[0mEpoch 49 Step 100/351 lr 0.001000 loss 0.0045 (0.0050) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 17:24:42] __main__ INFO: \u001b[0mEpoch 49 Step 200/351 lr 0.001000 loss 0.0049 (0.0049) acc@1 1.0000 (1.0000) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 17:26:54] __main__ INFO: \u001b[0mEpoch 49 Step 300/351 lr 0.001000 loss 0.0028 (0.0049) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 17:28:02] __main__ INFO: \u001b[0mEpoch 49 Step 351/351 lr 0.001000 loss 0.0027 (0.0048) acc@1 1.0000 (1.0000) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 17:28:02] __main__ INFO: \u001b[0mElapsed 464.85\n",
      "\u001b[32m[2020-07-19 17:28:02] __main__ INFO: \u001b[0mVal 49\n",
      "\u001b[32m[2020-07-19 17:28:18] __main__ INFO: \u001b[0mEpoch 49 loss 0.2118 acc@1 0.9364 acc@5 0.9990\n",
      "\u001b[32m[2020-07-19 17:28:18] __main__ INFO: \u001b[0mElapsed 16.70\n",
      "\u001b[32m[2020-07-19 17:28:18] __main__ INFO: \u001b[0mTrain 50 17199\n",
      "\u001b[32m[2020-07-19 17:30:31] __main__ INFO: \u001b[0mEpoch 50 Step 100/351 lr 0.001000 loss 0.0044 (0.0049) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 17:32:44] __main__ INFO: \u001b[0mEpoch 50 Step 200/351 lr 0.001000 loss 0.0032 (0.0050) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 17:34:58] __main__ INFO: \u001b[0mEpoch 50 Step 300/351 lr 0.001000 loss 0.0048 (0.0049) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 17:36:07] __main__ INFO: \u001b[0mEpoch 50 Step 351/351 lr 0.001000 loss 0.0036 (0.0049) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 17:36:07] __main__ INFO: \u001b[0mElapsed 468.44\n",
      "\u001b[32m[2020-07-19 17:36:07] __main__ INFO: \u001b[0mVal 50\n",
      "\u001b[32m[2020-07-19 17:36:24] __main__ INFO: \u001b[0mEpoch 50 loss 0.2098 acc@1 0.9386 acc@5 0.9986\n",
      "\u001b[32m[2020-07-19 17:36:24] __main__ INFO: \u001b[0mElapsed 16.82\n",
      "\u001b[32m[2020-07-19 17:36:24] fvcore.common.checkpoint INFO: \u001b[0mSaving checkpoint to /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_1_20_c10val/exp00_resume400_50/checkpoint_00050.pth\n"
     ]
    }
   ],
   "source": [
    "# Resume training with the un-augmented data\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "!python train.py --config /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_1_20_c10val/exp00/config.yaml \\\n",
    "    train.checkpoint /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_1_20_c10val/exp00/checkpoint_00400.pth \\\n",
    "    dataset.name CIFAR10 \\\n",
    "    model.resnext.cardinality 4 \\\n",
    "    train.batch_size 128 \\\n",
    "    train.base_lr 0.001 \\\n",
    "    train.output_dir /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_1_20_c10val/exp00_resume400_50 \\\n",
    "    scheduler.epochs 50\n",
    "\n",
    "#### Set LEARNING RATE based on ending LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-20 02:33:29] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_1_20_c10val/exp00_resume400_50/checkpoint_00050.pth\n",
      "Files already downloaded and verified\n",
      "100%|| 79/79 [00:33<00:00,  2.34it/s]\n",
      "\u001b[32m[2020-07-20 02:34:04] __main__ INFO: \u001b[0mElapsed 33.79\n",
      "\u001b[32m[2020-07-20 02:34:04] __main__ INFO: \u001b[0mLoss 0.2095 Accuracy 0.9367\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/resnext.yaml \\\n",
    "   model.resnext.cardinality 4 \\\n",
    "   test.batch_size 128 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_1_20_c10val/exp00_resume400_50/checkpoint_00050.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_1_20_c10val/exp00_resume400_50/test_results_0050_cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-20 02:34:17] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_1_20_c10val/exp00_resume400_50/checkpoint_00050.pth\n",
      "CIFAR 10.1\n",
      "100%|| 16/16 [00:07<00:00,  2.23it/s]\n",
      "\u001b[32m[2020-07-20 02:34:25] __main__ INFO: \u001b[0mElapsed 7.18\n",
      "\u001b[32m[2020-07-20 02:34:25] __main__ INFO: \u001b[0mLoss 0.4877 Accuracy 0.8565\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10.1 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/resnext.yaml \\\n",
    "   model.resnext.cardinality 4 \\\n",
    "   test.batch_size 128 \\\n",
    "   dataset.name CIFAR101 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_1_20_c10val/exp00_resume400_50/checkpoint_00050.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_1_20_c10val/exp00_resume400_50/test_results_0050_cifar101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-20 02:34:42] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_1_20_c10val/exp00/checkpoint_00400.pth\n",
      "Files already downloaded and verified\n",
      "100%|| 79/79 [00:34<00:00,  2.31it/s]\n",
      "\u001b[32m[2020-07-20 02:35:17] __main__ INFO: \u001b[0mElapsed 34.13\n",
      "\u001b[32m[2020-07-20 02:35:17] __main__ INFO: \u001b[0mLoss 0.4583 Accuracy 0.8627\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/resnext.yaml \\\n",
    "   model.resnext.cardinality 4 \\\n",
    "   test.batch_size 128 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_1_20_c10val/exp00/checkpoint_00400.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_1_20_c10val/exp00/test_results_0400_cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-20 02:35:34] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_1_20_c10val/exp00/checkpoint_00400.pth\n",
      "CIFAR 10.1\n",
      "100%|| 16/16 [00:07<00:00,  2.24it/s]\n",
      "\u001b[32m[2020-07-20 02:35:41] __main__ INFO: \u001b[0mElapsed 7.16\n",
      "\u001b[32m[2020-07-20 02:35:41] __main__ INFO: \u001b[0mLoss 0.8678 Accuracy 0.7390\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10.1 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "!python evaluate.py --config configs/cifar/resnext.yaml \\\n",
    "   model.resnext.cardinality 4 \\\n",
    "   test.batch_size 128 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_1_20_c10val/exp00/checkpoint_00400.pth \\\n",
    "   dataset.name CIFAR101 \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_1_20_c10val/exp00/test_results_0400_cifar101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Record the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Testset</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Original_Accuracy</th>\n",
       "      <th>Original_CI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>resnext_29_4x64d_ra_1_20_c10val</td>\n",
       "      <td>400</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.4583</td>\n",
       "      <td>0.8627</td>\n",
       "      <td>96.4</td>\n",
       "      <td>(96.0, 96.7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>resnext_29_4x64d_ra_1_20_c10val</td>\n",
       "      <td>400</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>0.8678</td>\n",
       "      <td>0.739</td>\n",
       "      <td>89.6</td>\n",
       "      <td>(88.2, 90.9)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>resnext_29_4x64d_ra_1_20_c10val_refined400</td>\n",
       "      <td>50</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>0.4877</td>\n",
       "      <td>0.8565</td>\n",
       "      <td>89.6</td>\n",
       "      <td>(88.2, 90.9)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>resnext_29_4x64d_ra_1_20_c10val_refined400</td>\n",
       "      <td>50</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.2095</td>\n",
       "      <td>0.9367</td>\n",
       "      <td>96.4</td>\n",
       "      <td>(96.0, 96.7)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Model Epoch    Testset    Loss  \\\n",
       "0             resnext_29_4x64d_ra_1_20_c10val   400    cifar10  0.4583   \n",
       "1             resnext_29_4x64d_ra_1_20_c10val   400  cifar10.1  0.8678   \n",
       "2  resnext_29_4x64d_ra_1_20_c10val_refined400    50  cifar10.1  0.4877   \n",
       "3  resnext_29_4x64d_ra_1_20_c10val_refined400    50    cifar10  0.2095   \n",
       "\n",
       "  Accuracy  Original_Accuracy   Original_CI  \n",
       "0   0.8627               96.4  (96.0, 96.7)  \n",
       "1    0.739               89.6  (88.2, 90.9)  \n",
       "2   0.8565               89.6  (88.2, 90.9)  \n",
       "3   0.9367               96.4  (96.0, 96.7)  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "model = 'resnext_29_4x64d_ra_1_20_c10val'\n",
    "model_refined = model + '_refined400'\n",
    "\n",
    "a = pd.Series([model, 400, 'cifar10', 0.4583, 0.8627])\n",
    "c = pd.Series([model, 400, 'cifar10.1', 0.8678, 0.7390])\n",
    "\n",
    "e = pd.Series([model_refined, 50, 'cifar10.1', 0.4877, 0.8565])\n",
    "f = pd.Series([model_refined, 50, 'cifar10', 0.2095, 0.9367])\n",
    "               \n",
    "df_results = pd.concat([a,c,e,f], axis=1).T\n",
    "df_results.columns = ['Model', 'Epoch', 'Testset', 'Loss', 'Accuracy']\n",
    "\n",
    "df_results['Original_Accuracy'] = df_results.apply((lambda row: 96.4 if row[2] == 'cifar10' else 89.6), axis=1)\n",
    "df_results['Original_CI'] = df_results.apply((lambda row: (96.0, 96.7) if row[2] == 'cifar10' else (88.2, 90.9)), axis=1)\n",
    "\n",
    "df_results.to_csv('/home/ec2-user/SageMaker/experiments/' + model + '/results.csv')\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the model checkpoints, configs, and results to S3 \n",
    "bucket='sagemaker-june29'\n",
    "prefix = 'sagemaker/results/original-models/resnext_29_4x64d_ra_1_20_c10val'\n",
    "path = '/home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_1_20_c10val'\n",
    "\n",
    "s3_resource = boto3.resource(\"s3\", region_name=\"us-east-1\")\n",
    "\n",
    "def uploadDirectory(local_path,bucket_name,s3_prefix):\n",
    "\n",
    "    my_bucket = s3_resource.Bucket(bucket_name)\n",
    "    \n",
    "    for path, subdirs, files in os.walk(local_path):\n",
    "        path = path.replace(\"\\\\\",\"/\")\n",
    "        directory_name = path.replace(local_path,\"\")\n",
    "        for file in files:\n",
    "            #print(\"Local File:\", os.path.join(path, file))\n",
    "            #print(\"      Dest:\", s3_prefix+directory_name+'/'+file)\n",
    "            my_bucket.upload_file(os.path.join(path, file), s3_prefix+directory_name+'/'+file)\n",
    "    \n",
    "uploadDirectory(path,bucket,prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
