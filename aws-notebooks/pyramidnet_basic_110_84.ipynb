{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import numpy \n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyramidNet (depth=110, alpha=84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sagemaker Notebook must be of type, conda_pytorch_p36\n",
    "!pip install -r '/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to add this to requirements.txt\n",
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model per the settings specified in the original paper\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "!python train.py --config configs/cifar/pyramidnet.yaml \\\n",
    "    model.pyramidnet.depth 110 \\\n",
    "    model.pyramidnet.alpha 84 \\\n",
    "    train.batch_size 128 \\\n",
    "    train.base_lr 0.1 \\\n",
    "    train.output_dir /home/ec2-user/SageMaker/experiments/pyramidnet_basic_110_84/exp00 \\\n",
    "    scheduler.epochs 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/pyramidnet.yaml \\\n",
    "    model.pyramidnet.depth 110 \\\n",
    "    model.pyramidnet.alpha 84 \\\n",
    "    test.checkpoint /home/ec2-user/SageMaker/experiments/pyramidnet_basic_110_84/exp00/checkpoint_00300.pth \\\n",
    "    test.output_dir /home/ec2-user/SageMaker/experiments/pyramidnet_basic_110_84/exp00/test_results_0300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/pyramidnet.yaml \\\n",
    "    model.pyramidnet.depth 110 \\\n",
    "    model.pyramidnet.alpha 84 \\\n",
    "    test.checkpoint /home/ec2-user/SageMaker/experiments/pyramidnet_basic_110_84/exp00/checkpoint_00200.pth \\\n",
    "    test.output_dir /home/ec2-user/SageMaker/experiments/pyramidnet_basic_110_84/exp00/test_results_0200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/pyramidnet.yaml \\\n",
    "    model.pyramidnet.depth 110 \\\n",
    "    model.pyramidnet.alpha 84 \\\n",
    "    test.checkpoint /home/ec2-user/SageMaker/experiments/pyramidnet_basic_110_84/exp00/checkpoint_00100.pth \\\n",
    "    test.output_dir /home/ec2-user/SageMaker/experiments/pyramidnet_basic_110_84/exp00/test_results_0100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the results to a CSV file so that we can analyze later.\n",
    "import pandas as pd\n",
    "\n",
    "results = {'Model': ['pyramidnet_basic_110_84', 'pyramidnet_basic_110_84'],\n",
    "           'Testset': ['cifar10', 'cifar10'],\n",
    "           'Epoch': [100, 200],\n",
    "           'Loss': [],\n",
    "           'Accuracy': [],\n",
    "           'Original_Accuracy': [],\n",
    "           'Original_CI': [(, ), (, )]\n",
    "           }\n",
    "\n",
    "df = pd.DataFrame(results, columns = ['Model', 'Testset', 'Epoch', 'Loss', 'Accuracy', \n",
    "                                      'Original_Accuracy', 'Original_CI'])\n",
    "\n",
    "df.to_csv('/home/ec2-user/SageMaker/experiments/pyramidnet_basic_110_84/exp00/results.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peak inside the output file for predictions\n",
    "import numpy as np\n",
    "output = '/home/ec2-user/SageMaker/experiments/pyramidnet_basic_110_84/exp00/test_results_0200/predictions.npz'\n",
    "npzfile = np.load(output)\n",
    "print(npzfile.files)\n",
    "npzfile['preds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10.1 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/pyramidnet.yaml \\\n",
    "    model.pyramidnet.depth 110 \\\n",
    "    model.pyramidnet.alpha 84 \\\n",
    "    dataset.name CIFAR101 \\\n",
    "    test.checkpoint /home/ec2-user/SageMaker/experiments/pyramidnet_basic_110_84/exp00/checkpoint_00300.pth \\\n",
    "    test.output_dir /home/ec2-user/SageMaker/experiments/pyramidnet_basic_110_84/exp00/test_results_0300_CIFAR101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the model checkpoints, configs, and results to S3 \n",
    "bucket='sagemaker-may29'\n",
    "prefix = 'sagemaker/results/original-models/pyramidnet_basic_110_84'\n",
    "path = '/home/ec2-user/SageMaker/experiments/pyramidnet_basic_110_84'\n",
    "\n",
    "s3_resource = boto3.resource(\"s3\", region_name=\"us-east-2\")\n",
    "\n",
    "def uploadDirectory(local_path,bucket_name,s3_prefix):\n",
    "\n",
    "    my_bucket = s3_resource.Bucket(bucket_name)\n",
    "    \n",
    "    for path, subdirs, files in os.walk(local_path):\n",
    "        path = path.replace(\"\\\\\",\"/\")\n",
    "        directory_name = path.replace(local_path,\"\")\n",
    "        for file in files:\n",
    "            #print(\"Local File:\", os.path.join(path, file))\n",
    "            #print(\"      Dest:\", s3_prefix+directory_name+'/'+file)\n",
    "            my_bucket.upload_file(os.path.join(path, file), s3_prefix+directory_name+'/'+file)\n",
    "    \n",
    "uploadDirectory(path,bucket,prefix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
