{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import numpy \n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyramidNet (depth=110, alpha=84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sagemaker Notebook must be of type, conda_pytorch_p36\n",
    "!pip install -r '/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to add this to requirements.txt\n",
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-13 01:55:39] __main__ INFO: \u001b[0mdevice: cuda\n",
      "cudnn:\n",
      "  benchmark: True\n",
      "  deterministic: False\n",
      "dataset:\n",
      "  name: CIFAR10\n",
      "  dataset_dir: ~/.torch/datasets/CIFAR10\n",
      "  image_size: 32\n",
      "  n_channels: 3\n",
      "  n_classes: 10\n",
      "model:\n",
      "  type: cifar\n",
      "  name: pyramidnet\n",
      "  init_mode: kaiming_fan_out\n",
      "  vgg:\n",
      "    n_channels: [64, 128, 256, 512, 512]\n",
      "    n_layers: [2, 2, 3, 3, 3]\n",
      "    use_bn: True\n",
      "  resnet:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "  resnet_preact:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "  wrn:\n",
      "    depth: 28\n",
      "    initial_channels: 16\n",
      "    widening_factor: 10\n",
      "    drop_rate: 0.0\n",
      "  densenet:\n",
      "    depth: 100\n",
      "    n_blocks: [6, 12, 24, 16]\n",
      "    block_type: bottleneck\n",
      "    growth_rate: 12\n",
      "    drop_rate: 0.0\n",
      "    compression_rate: 0.5\n",
      "  pyramidnet:\n",
      "    depth: 110\n",
      "    n_blocks: [3, 24, 36, 3]\n",
      "    initial_channels: 16\n",
      "    block_type: bottleneck\n",
      "    alpha: 84\n",
      "  resnext:\n",
      "    depth: 29\n",
      "    n_blocks: [3, 4, 6, 3]\n",
      "    initial_channels: 64\n",
      "    cardinality: 8\n",
      "    base_channels: 4\n",
      "  shake_shake:\n",
      "    depth: 26\n",
      "    initial_channels: 96\n",
      "    shake_forward: True\n",
      "    shake_backward: True\n",
      "    shake_image: True\n",
      "  se_resnet_preact:\n",
      "    depth: 110\n",
      "    initial_channels: 16\n",
      "    se_reduction: 16\n",
      "    block_type: basic\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "train:\n",
      "  checkpoint: ''\n",
      "  resume: False\n",
      "  precision: O0\n",
      "  batch_size: 128\n",
      "  subdivision: 1\n",
      "  optimizer: sgd\n",
      "  base_lr: 0.1\n",
      "  momentum: 0.9\n",
      "  nesterov: True\n",
      "  weight_decay: 0.0001\n",
      "  no_weight_decay_on_bn: False\n",
      "  gradient_clip: 0\n",
      "  start_epoch: 0\n",
      "  seed: 0\n",
      "  val_first: True\n",
      "  val_period: 1\n",
      "  val_ratio: 0.1\n",
      "  use_test_as_val: False\n",
      "  output_dir: /home/ec2-user/SageMaker/experiments/pyramidnet_basic_110_84/exp00\n",
      "  log_period: 100\n",
      "  checkpoint_period: 100\n",
      "  use_tensorboard: True\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: True\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "  distributed: False\n",
      "  dist:\n",
      "    backend: nccl\n",
      "    init_method: env://\n",
      "    world_size: -1\n",
      "    node_rank: -1\n",
      "    local_rank: 0\n",
      "    use_sync_bn: False\n",
      "tensorboard:\n",
      "  train_images: False\n",
      "  val_images: False\n",
      "  model_params: False\n",
      "optim:\n",
      "  adam:\n",
      "    betas: (0.9, 0.999)\n",
      "  lars:\n",
      "    eps: 1e-09\n",
      "    threshold: 0.01\n",
      "  adabound:\n",
      "    betas: (0.9, 0.999)\n",
      "    final_lr: 0.1\n",
      "    gamma: 0.001\n",
      "scheduler:\n",
      "  epochs: 300\n",
      "  warmup:\n",
      "    type: none\n",
      "    epochs: 0\n",
      "    start_factor: 0.001\n",
      "    exponent: 4\n",
      "  type: multistep\n",
      "  milestones: [150, 225]\n",
      "  lr_decay: 0.1\n",
      "  lr_min_factor: 0.001\n",
      "  T0: 10\n",
      "  T_mul: 1.0\n",
      "validation:\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: False\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "augmentation:\n",
      "  use_random_crop: True\n",
      "  use_random_horizontal_flip: True\n",
      "  use_cutout: False\n",
      "  use_random_erasing: False\n",
      "  use_dual_cutout: False\n",
      "  use_mixup: False\n",
      "  use_ricap: False\n",
      "  use_cutmix: False\n",
      "  use_label_smoothing: False\n",
      "  random_crop:\n",
      "    padding: 4\n",
      "    fill: 0\n",
      "    padding_mode: constant\n",
      "  random_horizontal_flip:\n",
      "    prob: 0.5\n",
      "  cutout:\n",
      "    prob: 1.0\n",
      "    mask_size: 16\n",
      "    cut_inside: False\n",
      "    mask_color: 0\n",
      "    dual_cutout_alpha: 0.1\n",
      "  random_erasing:\n",
      "    prob: 0.5\n",
      "    area_ratio_range: [0.02, 0.4]\n",
      "    min_aspect_ratio: 0.3\n",
      "    max_attempt: 20\n",
      "  mixup:\n",
      "    alpha: 1.0\n",
      "  ricap:\n",
      "    beta: 0.3\n",
      "  cutmix:\n",
      "    alpha: 1.0\n",
      "  label_smoothing:\n",
      "    epsilon: 0.1\n",
      "tta:\n",
      "  use_resize: False\n",
      "  use_center_crop: False\n",
      "  resize: 256\n",
      "test:\n",
      "  checkpoint: ''\n",
      "  output_dir: ''\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    pin_memory: False\n",
      "\u001b[32m[2020-06-13 01:55:39] __main__ INFO: \u001b[0menv_info:\n",
      "  pytorch_version: 1.4.0\n",
      "  cuda_version: 10.1\n",
      "  cudnn_version: 7603\n",
      "  num_gpus: 1\n",
      "  gpu_name: Tesla K80\n",
      "  gpu_capability: 3.7\n",
      "Files already downloaded and verified\n",
      "\u001b[32m[2020-06-13 01:55:43] __main__ INFO: \u001b[0mMACs  : 519.72M\n",
      "\u001b[32m[2020-06-13 01:55:43] __main__ INFO: \u001b[0m#params: 2.53M\n",
      "Selected optimization level O0:  Pure FP32 training.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\",)\n",
      "\u001b[32m[2020-06-13 01:55:43] __main__ INFO: \u001b[0mVal 0\n",
      "\u001b[32m[2020-06-13 01:56:01] __main__ INFO: \u001b[0mEpoch 0 loss 24705008680.9600 acc@1 0.0934 acc@5 0.4914\n",
      "\u001b[32m[2020-06-13 01:56:01] __main__ INFO: \u001b[0mElapsed 17.83\n",
      "\u001b[32m[2020-06-13 01:56:01] __main__ INFO: \u001b[0mTrain 1 0\n",
      "\u001b[32m[2020-06-13 01:57:25] __main__ INFO: \u001b[0mEpoch 1 Step 100/351 lr 0.100000 loss 1.8098 (2.3151) acc@1 0.2656 (0.2695) acc@5 0.8594 (0.7903)\n",
      "\u001b[32m[2020-06-13 01:58:41] __main__ INFO: \u001b[0mEpoch 1 Step 200/351 lr 0.100000 loss 1.5458 (2.0172) acc@1 0.4688 (0.3239) acc@5 0.8984 (0.8339)\n",
      "\u001b[32m[2020-06-13 01:59:57] __main__ INFO: \u001b[0mEpoch 1 Step 300/351 lr 0.100000 loss 1.3974 (1.8319) acc@1 0.5547 (0.3744) acc@5 0.8984 (0.8613)\n",
      "\u001b[32m[2020-06-13 02:00:36] __main__ INFO: \u001b[0mEpoch 1 Step 351/351 lr 0.100000 loss 1.3639 (1.7555) acc@1 0.4766 (0.3969) acc@5 0.9531 (0.8725)\n",
      "\u001b[32m[2020-06-13 02:00:36] __main__ INFO: \u001b[0mElapsed 274.87\n",
      "\u001b[32m[2020-06-13 02:00:36] __main__ INFO: \u001b[0mVal 1\n",
      "\u001b[32m[2020-06-13 02:00:45] __main__ INFO: \u001b[0mEpoch 1 loss 1.3334 acc@1 0.5288 acc@5 0.9356\n",
      "\u001b[32m[2020-06-13 02:00:45] __main__ INFO: \u001b[0mElapsed 9.46\n",
      "\u001b[32m[2020-06-13 02:00:45] __main__ INFO: \u001b[0mTrain 2 351\n",
      "\u001b[32m[2020-06-13 02:02:01] __main__ INFO: \u001b[0mEpoch 2 Step 100/351 lr 0.100000 loss 1.1051 (1.2240) acc@1 0.5859 (0.5615) acc@5 0.9688 (0.9502)\n",
      "\u001b[32m[2020-06-13 02:03:17] __main__ INFO: \u001b[0mEpoch 2 Step 200/351 lr 0.100000 loss 1.1713 (1.1625) acc@1 0.5391 (0.5848) acc@5 0.9609 (0.9559)\n",
      "\u001b[32m[2020-06-13 02:04:33] __main__ INFO: \u001b[0mEpoch 2 Step 300/351 lr 0.100000 loss 0.9631 (1.1197) acc@1 0.6562 (0.6011) acc@5 0.9688 (0.9587)\n",
      "\u001b[32m[2020-06-13 02:05:12] __main__ INFO: \u001b[0mEpoch 2 Step 351/351 lr 0.100000 loss 0.9039 (1.0934) acc@1 0.6875 (0.6112) acc@5 0.9844 (0.9602)\n",
      "\u001b[32m[2020-06-13 02:05:12] __main__ INFO: \u001b[0mElapsed 267.02\n",
      "\u001b[32m[2020-06-13 02:05:12] __main__ INFO: \u001b[0mVal 2\n",
      "\u001b[32m[2020-06-13 02:05:21] __main__ INFO: \u001b[0mEpoch 2 loss 1.0869 acc@1 0.6308 acc@5 0.9570\n",
      "\u001b[32m[2020-06-13 02:05:21] __main__ INFO: \u001b[0mElapsed 9.44\n",
      "\u001b[32m[2020-06-13 02:05:21] __main__ INFO: \u001b[0mTrain 3 702\n",
      "\u001b[32m[2020-06-13 02:06:38] __main__ INFO: \u001b[0mEpoch 3 Step 100/351 lr 0.100000 loss 0.9231 (0.8661) acc@1 0.6719 (0.6955) acc@5 0.9688 (0.9767)\n",
      "\u001b[32m[2020-06-13 02:07:54] __main__ INFO: \u001b[0mEpoch 3 Step 200/351 lr 0.100000 loss 0.6808 (0.8423) acc@1 0.7812 (0.7063) acc@5 0.9844 (0.9772)\n",
      "\u001b[32m[2020-06-13 02:09:10] __main__ INFO: \u001b[0mEpoch 3 Step 300/351 lr 0.100000 loss 0.7384 (0.8145) acc@1 0.7031 (0.7152) acc@5 0.9766 (0.9784)\n",
      "\u001b[32m[2020-06-13 02:09:49] __main__ INFO: \u001b[0mEpoch 3 Step 351/351 lr 0.100000 loss 0.6842 (0.8002) acc@1 0.7500 (0.7209) acc@5 0.9922 (0.9791)\n",
      "\u001b[32m[2020-06-13 02:09:49] __main__ INFO: \u001b[0mElapsed 267.13\n",
      "\u001b[32m[2020-06-13 02:09:49] __main__ INFO: \u001b[0mVal 3\n",
      "\u001b[32m[2020-06-13 02:09:58] __main__ INFO: \u001b[0mEpoch 3 loss 0.8841 acc@1 0.7172 acc@5 0.9750\n",
      "\u001b[32m[2020-06-13 02:09:58] __main__ INFO: \u001b[0mElapsed 9.43\n",
      "\u001b[32m[2020-06-13 02:09:58] __main__ INFO: \u001b[0mTrain 4 1053\n",
      "\u001b[32m[2020-06-13 02:11:14] __main__ INFO: \u001b[0mEpoch 4 Step 100/351 lr 0.100000 loss 0.7858 (0.6702) acc@1 0.7109 (0.7652) acc@5 0.9922 (0.9852)\n",
      "\u001b[32m[2020-06-13 02:12:30] __main__ INFO: \u001b[0mEpoch 4 Step 200/351 lr 0.100000 loss 0.5367 (0.6558) acc@1 0.8047 (0.7717) acc@5 1.0000 (0.9863)\n",
      "\u001b[32m[2020-06-13 02:13:46] __main__ INFO: \u001b[0mEpoch 4 Step 300/351 lr 0.100000 loss 0.5781 (0.6421) acc@1 0.7656 (0.7766) acc@5 0.9766 (0.9873)\n",
      "\u001b[32m[2020-06-13 02:14:25] __main__ INFO: \u001b[0mEpoch 4 Step 351/351 lr 0.100000 loss 0.5538 (0.6368) acc@1 0.8047 (0.7790) acc@5 0.9844 (0.9874)\n",
      "\u001b[32m[2020-06-13 02:14:25] __main__ INFO: \u001b[0mElapsed 267.00\n",
      "\u001b[32m[2020-06-13 02:14:25] __main__ INFO: \u001b[0mVal 4\n",
      "\u001b[32m[2020-06-13 02:14:34] __main__ INFO: \u001b[0mEpoch 4 loss 0.7764 acc@1 0.7398 acc@5 0.9856\n",
      "\u001b[32m[2020-06-13 02:14:34] __main__ INFO: \u001b[0mElapsed 9.42\n",
      "\u001b[32m[2020-06-13 02:14:34] __main__ INFO: \u001b[0mTrain 5 1404\n",
      "\u001b[32m[2020-06-13 02:15:50] __main__ INFO: \u001b[0mEpoch 5 Step 100/351 lr 0.100000 loss 0.3420 (0.5377) acc@1 0.9062 (0.8155) acc@5 1.0000 (0.9912)\n",
      "\u001b[32m[2020-06-13 02:17:06] __main__ INFO: \u001b[0mEpoch 5 Step 200/351 lr 0.100000 loss 0.5462 (0.5438) acc@1 0.8125 (0.8113) acc@5 1.0000 (0.9910)\n",
      "\u001b[32m[2020-06-13 02:18:23] __main__ INFO: \u001b[0mEpoch 5 Step 300/351 lr 0.100000 loss 0.5152 (0.5427) acc@1 0.8047 (0.8120) acc@5 0.9844 (0.9907)\n",
      "\u001b[32m[2020-06-13 02:19:01] __main__ INFO: \u001b[0mEpoch 5 Step 351/351 lr 0.100000 loss 0.4995 (0.5381) acc@1 0.8047 (0.8136) acc@5 0.9766 (0.9907)\n",
      "\u001b[32m[2020-06-13 02:19:01] __main__ INFO: \u001b[0mElapsed 266.86\n",
      "\u001b[32m[2020-06-13 02:19:01] __main__ INFO: \u001b[0mVal 5\n",
      "\u001b[32m[2020-06-13 02:19:11] __main__ INFO: \u001b[0mEpoch 5 loss 0.7122 acc@1 0.7638 acc@5 0.9842\n",
      "\u001b[32m[2020-06-13 02:19:11] __main__ INFO: \u001b[0mElapsed 9.43\n",
      "\u001b[32m[2020-06-13 02:19:11] __main__ INFO: \u001b[0mTrain 6 1755\n",
      "\u001b[32m[2020-06-13 02:20:27] __main__ INFO: \u001b[0mEpoch 6 Step 100/351 lr 0.100000 loss 0.4884 (0.4717) acc@1 0.8281 (0.8363) acc@5 0.9844 (0.9934)\n",
      "\u001b[32m[2020-06-13 02:21:43] __main__ INFO: \u001b[0mEpoch 6 Step 200/351 lr 0.100000 loss 0.4162 (0.4804) acc@1 0.8359 (0.8332) acc@5 1.0000 (0.9922)\n",
      "\u001b[32m[2020-06-13 02:22:59] __main__ INFO: \u001b[0mEpoch 6 Step 300/351 lr 0.100000 loss 0.4707 (0.4736) acc@1 0.8516 (0.8366) acc@5 1.0000 (0.9926)\n",
      "\u001b[32m[2020-06-13 02:23:38] __main__ INFO: \u001b[0mEpoch 6 Step 351/351 lr 0.100000 loss 0.4479 (0.4690) acc@1 0.8125 (0.8373) acc@5 0.9922 (0.9929)\n",
      "\u001b[32m[2020-06-13 02:23:38] __main__ INFO: \u001b[0mElapsed 266.88\n",
      "\u001b[32m[2020-06-13 02:23:38] __main__ INFO: \u001b[0mVal 6\n",
      "\u001b[32m[2020-06-13 02:23:47] __main__ INFO: \u001b[0mEpoch 6 loss 0.5872 acc@1 0.8132 acc@5 0.9870\n",
      "\u001b[32m[2020-06-13 02:23:47] __main__ INFO: \u001b[0mElapsed 9.42\n",
      "\u001b[32m[2020-06-13 02:23:47] __main__ INFO: \u001b[0mTrain 7 2106\n",
      "\u001b[32m[2020-06-13 02:25:03] __main__ INFO: \u001b[0mEpoch 7 Step 100/351 lr 0.100000 loss 0.4315 (0.4172) acc@1 0.8594 (0.8576) acc@5 1.0000 (0.9958)\n",
      "\u001b[32m[2020-06-13 02:26:19] __main__ INFO: \u001b[0mEpoch 7 Step 200/351 lr 0.100000 loss 0.5143 (0.4240) acc@1 0.8203 (0.8536) acc@5 0.9922 (0.9951)\n",
      "\u001b[32m[2020-06-13 02:27:35] __main__ INFO: \u001b[0mEpoch 7 Step 300/351 lr 0.100000 loss 0.3626 (0.4221) acc@1 0.8594 (0.8548) acc@5 1.0000 (0.9950)\n",
      "\u001b[32m[2020-06-13 02:28:14] __main__ INFO: \u001b[0mEpoch 7 Step 351/351 lr 0.100000 loss 0.3320 (0.4243) acc@1 0.8750 (0.8540) acc@5 0.9922 (0.9949)\n",
      "\u001b[32m[2020-06-13 02:28:14] __main__ INFO: \u001b[0mElapsed 266.89\n",
      "\u001b[32m[2020-06-13 02:28:14] __main__ INFO: \u001b[0mVal 7\n",
      "\u001b[32m[2020-06-13 02:28:23] __main__ INFO: \u001b[0mEpoch 7 loss 0.5568 acc@1 0.8130 acc@5 0.9900\n",
      "\u001b[32m[2020-06-13 02:28:23] __main__ INFO: \u001b[0mElapsed 9.48\n",
      "\u001b[32m[2020-06-13 02:28:23] __main__ INFO: \u001b[0mTrain 8 2457\n",
      "\u001b[32m[2020-06-13 02:29:39] __main__ INFO: \u001b[0mEpoch 8 Step 100/351 lr 0.100000 loss 0.4591 (0.3740) acc@1 0.8594 (0.8691) acc@5 0.9922 (0.9960)\n",
      "\u001b[32m[2020-06-13 02:30:55] __main__ INFO: \u001b[0mEpoch 8 Step 200/351 lr 0.100000 loss 0.2318 (0.3871) acc@1 0.9219 (0.8645) acc@5 1.0000 (0.9952)\n",
      "\u001b[32m[2020-06-13 02:32:10] __main__ INFO: \u001b[0mEpoch 8 Step 300/351 lr 0.100000 loss 0.4439 (0.3864) acc@1 0.8516 (0.8649) acc@5 1.0000 (0.9951)\n",
      "\u001b[32m[2020-06-13 02:32:48] __main__ INFO: \u001b[0mEpoch 8 Step 351/351 lr 0.100000 loss 0.5526 (0.3887) acc@1 0.8203 (0.8649) acc@5 0.9922 (0.9950)\n",
      "\u001b[32m[2020-06-13 02:32:48] __main__ INFO: \u001b[0mElapsed 264.55\n",
      "\u001b[32m[2020-06-13 02:32:48] __main__ INFO: \u001b[0mVal 8\n",
      "\u001b[32m[2020-06-13 02:32:57] __main__ INFO: \u001b[0mEpoch 8 loss 0.6292 acc@1 0.7908 acc@5 0.9872\n",
      "\u001b[32m[2020-06-13 02:32:57] __main__ INFO: \u001b[0mElapsed 9.22\n",
      "\u001b[32m[2020-06-13 02:32:57] __main__ INFO: \u001b[0mTrain 9 2808\n",
      "\u001b[32m[2020-06-13 02:34:12] __main__ INFO: \u001b[0mEpoch 9 Step 100/351 lr 0.100000 loss 0.3247 (0.3519) acc@1 0.8906 (0.8812) acc@5 1.0000 (0.9966)\n",
      "\u001b[32m[2020-06-13 02:35:27] __main__ INFO: \u001b[0mEpoch 9 Step 200/351 lr 0.100000 loss 0.3629 (0.3529) acc@1 0.8359 (0.8784) acc@5 1.0000 (0.9966)\n",
      "\u001b[32m[2020-06-13 02:36:42] __main__ INFO: \u001b[0mEpoch 9 Step 300/351 lr 0.100000 loss 0.2827 (0.3554) acc@1 0.9062 (0.8767) acc@5 1.0000 (0.9964)\n",
      "\u001b[32m[2020-06-13 02:37:21] __main__ INFO: \u001b[0mEpoch 9 Step 351/351 lr 0.100000 loss 0.4612 (0.3573) acc@1 0.8594 (0.8764) acc@5 1.0000 (0.9963)\n",
      "\u001b[32m[2020-06-13 02:37:21] __main__ INFO: \u001b[0mElapsed 263.53\n",
      "\u001b[32m[2020-06-13 02:37:21] __main__ INFO: \u001b[0mVal 9\n",
      "\u001b[32m[2020-06-13 02:37:30] __main__ INFO: \u001b[0mEpoch 9 loss 0.4670 acc@1 0.8398 acc@5 0.9912\n",
      "\u001b[32m[2020-06-13 02:37:30] __main__ INFO: \u001b[0mElapsed 9.30\n",
      "\u001b[32m[2020-06-13 02:37:30] __main__ INFO: \u001b[0mTrain 10 3159\n",
      "\u001b[32m[2020-06-13 02:38:45] __main__ INFO: \u001b[0mEpoch 10 Step 100/351 lr 0.100000 loss 0.3999 (0.3154) acc@1 0.8594 (0.8930) acc@5 1.0000 (0.9969)\n",
      "\u001b[32m[2020-06-13 02:40:00] __main__ INFO: \u001b[0mEpoch 10 Step 200/351 lr 0.100000 loss 0.2708 (0.3274) acc@1 0.9297 (0.8876) acc@5 1.0000 (0.9968)\n"
     ]
    }
   ],
   "source": [
    "# Train the model per the settings specified in the original paper\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "!python train.py --config configs/cifar/pyramidnet.yaml \\\n",
    "    model.pyramidnet.depth 110 \\\n",
    "    model.pyramidnet.alpha 84 \\\n",
    "    train.batch_size 128 \\\n",
    "    train.base_lr 0.1 \\\n",
    "    train.output_dir /home/ec2-user/SageMaker/experiments/pyramidnet_basic_110_84/exp00 \\\n",
    "    scheduler.epochs 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-14 03:15:25] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/pyramidnet_basic_110_84/exp00/checkpoint_00300.pth\n",
      "Files already downloaded and verified\n",
      "100%|███████████████████████████████████████████| 40/40 [00:20<00:00,  1.91it/s]\n",
      "\u001b[32m[2020-06-14 03:15:47] __main__ INFO: \u001b[0mElapsed 21.00\n",
      "\u001b[32m[2020-06-14 03:15:47] __main__ INFO: \u001b[0mLoss 0.1892 Accuracy 0.9525\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/pyramidnet.yaml \\\n",
    "    model.pyramidnet.depth 110 \\\n",
    "    model.pyramidnet.alpha 84 \\\n",
    "    test.checkpoint /home/ec2-user/SageMaker/experiments/pyramidnet_basic_110_84/exp00/checkpoint_00300.pth \\\n",
    "    test.output_dir /home/ec2-user/SageMaker/experiments/pyramidnet_basic_110_84/exp00/test_results_0300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-14 03:16:49] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/pyramidnet_basic_110_84/exp00/checkpoint_00200.pth\n",
      "Files already downloaded and verified\n",
      "100%|███████████████████████████████████████████| 40/40 [00:21<00:00,  1.90it/s]\n",
      "\u001b[32m[2020-06-14 03:17:11] __main__ INFO: \u001b[0mElapsed 21.05\n",
      "\u001b[32m[2020-06-14 03:17:11] __main__ INFO: \u001b[0mLoss 0.1904 Accuracy 0.9515\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/pyramidnet.yaml \\\n",
    "    model.pyramidnet.depth 110 \\\n",
    "    model.pyramidnet.alpha 84 \\\n",
    "    test.checkpoint /home/ec2-user/SageMaker/experiments/pyramidnet_basic_110_84/exp00/checkpoint_00200.pth \\\n",
    "    test.output_dir /home/ec2-user/SageMaker/experiments/pyramidnet_basic_110_84/exp00/test_results_0200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-14 03:17:21] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/pyramidnet_basic_110_84/exp00/checkpoint_00100.pth\n",
      "Files already downloaded and verified\n",
      "100%|███████████████████████████████████████████| 40/40 [00:21<00:00,  1.89it/s]\n",
      "\u001b[32m[2020-06-14 03:17:43] __main__ INFO: \u001b[0mElapsed 21.12\n",
      "\u001b[32m[2020-06-14 03:17:43] __main__ INFO: \u001b[0mLoss 0.3607 Accuracy 0.8951\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/pyramidnet.yaml \\\n",
    "    model.pyramidnet.depth 110 \\\n",
    "    model.pyramidnet.alpha 84 \\\n",
    "    test.checkpoint /home/ec2-user/SageMaker/experiments/pyramidnet_basic_110_84/exp00/checkpoint_00100.pth \\\n",
    "    test.output_dir /home/ec2-user/SageMaker/experiments/pyramidnet_basic_110_84/exp00/test_results_0100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-14 03:18:30] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/pyramidnet_basic_110_84/exp00/checkpoint_00300.pth\n",
      "CIFAR 10.1\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:04<00:00,  1.76it/s]\n",
      "\u001b[32m[2020-06-14 03:18:35] __main__ INFO: \u001b[0mElapsed 4.56\n",
      "\u001b[32m[2020-06-14 03:18:35] __main__ INFO: \u001b[0mLoss 0.4247 Accuracy 0.8880\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10.1 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/pyramidnet.yaml \\\n",
    "    model.pyramidnet.depth 110 \\\n",
    "    model.pyramidnet.alpha 84 \\\n",
    "    dataset.name CIFAR101 \\\n",
    "    test.checkpoint /home/ec2-user/SageMaker/experiments/pyramidnet_basic_110_84/exp00/checkpoint_00300.pth \\\n",
    "    test.output_dir /home/ec2-user/SageMaker/experiments/pyramidnet_basic_110_84/exp00/test_results_0300_CIFAR101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Testset</th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Original_Accuracy</th>\n",
       "      <th>Original_CI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pyramidnet_basic_110_84</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>100</td>\n",
       "      <td>0.3607</td>\n",
       "      <td>0.8951</td>\n",
       "      <td>95.7</td>\n",
       "      <td>(95.3, 96.1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pyramidnet_basic_110_84</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>200</td>\n",
       "      <td>0.1904</td>\n",
       "      <td>0.9515</td>\n",
       "      <td>95.7</td>\n",
       "      <td>(95.3, 96.1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pyramidnet_basic_110_84</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>300</td>\n",
       "      <td>0.1892</td>\n",
       "      <td>0.9525</td>\n",
       "      <td>95.7</td>\n",
       "      <td>(95.3, 96.1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pyramidnet_basic_110_84</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>300</td>\n",
       "      <td>0.4247</td>\n",
       "      <td>0.8880</td>\n",
       "      <td>89.3</td>\n",
       "      <td>(87.8, 90.6)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Model    Testset  Epoch    Loss  Accuracy  \\\n",
       "0  pyramidnet_basic_110_84    cifar10    100  0.3607    0.8951   \n",
       "1  pyramidnet_basic_110_84    cifar10    200  0.1904    0.9515   \n",
       "2  pyramidnet_basic_110_84    cifar10    300  0.1892    0.9525   \n",
       "3  pyramidnet_basic_110_84  cifar10.1    300  0.4247    0.8880   \n",
       "\n",
       "   Original_Accuracy   Original_CI  \n",
       "0               95.7  (95.3, 96.1)  \n",
       "1               95.7  (95.3, 96.1)  \n",
       "2               95.7  (95.3, 96.1)  \n",
       "3               89.3  (87.8, 90.6)  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write the results to a CSV file so that we can analyze later.\n",
    "import pandas as pd\n",
    "\n",
    "results = {'Model': ['pyramidnet_basic_110_84', 'pyramidnet_basic_110_84', 'pyramidnet_basic_110_84', 'pyramidnet_basic_110_84'],\n",
    "           'Testset': ['cifar10', 'cifar10', 'cifar10', 'cifar10.1'],\n",
    "           'Epoch': [100, 200, 300, 300],\n",
    "           'Loss': [0.3607, 0.1904, 0.1892, 0.4247],\n",
    "           'Accuracy': [0.8951, 0.9515, 0.9525, 0.8880],\n",
    "           'Original_Accuracy': [95.7, 95.7, 95.7, 89.3],\n",
    "           'Original_CI': [(95.3, 96.1), (95.3, 96.1), (95.3, 96.1), (87.8, 90.6)]\n",
    "           }\n",
    "\n",
    "df = pd.DataFrame(results, columns = ['Model', 'Testset', 'Epoch', 'Loss', 'Accuracy', \n",
    "                                      'Original_Accuracy', 'Original_CI'])\n",
    "\n",
    "df.to_csv('/home/ec2-user/SageMaker/experiments/pyramidnet_basic_110_84/exp00/results.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['preds', 'probs', 'labels', 'loss', 'acc']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.70036435,  0.49356735,  0.4804548 , ..., -2.060328  ,\n",
       "        -4.1004734 , -6.5416474 ],\n",
       "       [ 3.1254632 ,  8.758387  , -4.609853  , ..., -5.8822746 ,\n",
       "        17.848377  , -2.6368778 ],\n",
       "       [ 1.4870665 ,  9.748533  , -3.19706   , ..., -3.877824  ,\n",
       "        14.978767  ,  2.1129982 ],\n",
       "       ...,\n",
       "       [-4.3264318 , -2.2234082 , -0.4107626 , ...,  0.55405146,\n",
       "        -1.979837  , -3.3431463 ],\n",
       "       [-0.04168908, 15.804465  , -0.32656556, ..., -3.6241503 ,\n",
       "         0.44613537, -4.8128295 ],\n",
       "       [-2.5344796 , -0.6957764 , -2.932084  , ..., 20.587442  ,\n",
       "        -4.269977  , -1.4589888 ]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Peak inside the output file for predictions\n",
    "import numpy as np\n",
    "output = '/home/ec2-user/SageMaker/experiments/pyramidnet_basic_110_84/exp00/test_results_0200/predictions.npz'\n",
    "npzfile = np.load(output)\n",
    "print(npzfile.files)\n",
    "npzfile['preds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the model checkpoints, configs, and results to S3 \n",
    "bucket='sagemaker-may29'\n",
    "prefix = 'sagemaker/results/original-models/pyramidnet_basic_110_84'\n",
    "path = '/home/ec2-user/SageMaker/experiments/pyramidnet_basic_110_84'\n",
    "\n",
    "s3_resource = boto3.resource(\"s3\", region_name=\"us-east-2\")\n",
    "\n",
    "def uploadDirectory(local_path,bucket_name,s3_prefix):\n",
    "\n",
    "    my_bucket = s3_resource.Bucket(bucket_name)\n",
    "    \n",
    "    for path, subdirs, files in os.walk(local_path):\n",
    "        path = path.replace(\"\\\\\",\"/\")\n",
    "        directory_name = path.replace(local_path,\"\")\n",
    "        for file in files:\n",
    "            #print(\"Local File:\", os.path.join(path, file))\n",
    "            #print(\"      Dest:\", s3_prefix+directory_name+'/'+file)\n",
    "            my_bucket.upload_file(os.path.join(path, file), s3_prefix+directory_name+'/'+file)\n",
    "    \n",
    "uploadDirectory(path,bucket,prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
