{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import numpy \n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wide Residual Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 1)) (1.18.1)\n",
      "Requirement already satisfied: torch>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 2)) (1.4.0)\n",
      "Requirement already satisfied: torchvision in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (0.5.0)\n",
      "Requirement already satisfied: fvcore in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (0.1.1.post20200619)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 5)) (4.44.1)\n",
      "Requirement already satisfied: yacs in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 6)) (0.1.7)\n",
      "Requirement already satisfied: apex from git+https://github.com/NVIDIA/apex.git#egg=apex in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 7)) (0.1)\n",
      "Requirement already satisfied: termcolor in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 8)) (1.1.0)\n",
      "Requirement already satisfied: thop<0.0.31.post2004070130 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 9)) (0.0.31.post2001170342)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (1.14.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (7.0.0)\n",
      "Requirement already satisfied: tabulate in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (0.8.7)\n",
      "Requirement already satisfied: portalocker in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (1.7.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (5.3.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Sagemaker Notebook must be of type, conda_pytorch_p36\n",
    "\n",
    "!pip install -r '/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorboard in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (2.2.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (0.9.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (2.23.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.29.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (0.4.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.14.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.6.0.post3)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.18.1)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (3.12.0)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (0.34.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (46.1.3.post20200330)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.18.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (3.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (2020.4.5.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (1.25.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (3.4.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (4.1.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (0.2.8)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard) (1.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.1.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard) (0.4.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard) (2.2.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Need to add this to requirements.txt\n",
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model per the settings specified in the original paper\n",
    "# os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "# !python train.py --config configs/cifar/wrn.yaml \\\n",
    "#     model.wrn.depth 28 \\\n",
    "#     model.wrn.widening_factor 10 \\\n",
    "#     train.batch_size 128 \\\n",
    "#     train.base_lr 0.1 \\\n",
    "#     train.output_dir /home/ec2-user/SageMaker/experiments/wrn_28_10/exp00 \\\n",
    "#     scheduler.epochs 200\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-24 01:16:52] __main__ INFO: \u001b[0mdevice: cuda\n",
      "cudnn:\n",
      "  benchmark: True\n",
      "  deterministic: False\n",
      "dataset:\n",
      "  name: CIFAR10_RA_3_20\n",
      "  dataset_dir: ''\n",
      "  image_size: 32\n",
      "  n_channels: 3\n",
      "  n_classes: 10\n",
      "model:\n",
      "  type: cifar\n",
      "  name: wrn\n",
      "  init_mode: kaiming_fan_in\n",
      "  vgg:\n",
      "    n_channels: [64, 128, 256, 512, 512]\n",
      "    n_layers: [2, 2, 3, 3, 3]\n",
      "    use_bn: True\n",
      "  resnet:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "  resnet_preact:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "  wrn:\n",
      "    depth: 28\n",
      "    initial_channels: 16\n",
      "    widening_factor: 10\n",
      "    drop_rate: 0.0\n",
      "  densenet:\n",
      "    depth: 100\n",
      "    n_blocks: [6, 12, 24, 16]\n",
      "    block_type: bottleneck\n",
      "    growth_rate: 12\n",
      "    drop_rate: 0.0\n",
      "    compression_rate: 0.5\n",
      "  pyramidnet:\n",
      "    depth: 272\n",
      "    n_blocks: [3, 24, 36, 3]\n",
      "    initial_channels: 16\n",
      "    block_type: bottleneck\n",
      "    alpha: 200\n",
      "  resnext:\n",
      "    depth: 29\n",
      "    n_blocks: [3, 4, 6, 3]\n",
      "    initial_channels: 64\n",
      "    cardinality: 8\n",
      "    base_channels: 4\n",
      "  shake_shake:\n",
      "    depth: 26\n",
      "    initial_channels: 96\n",
      "    shake_forward: True\n",
      "    shake_backward: True\n",
      "    shake_image: True\n",
      "  se_resnet_preact:\n",
      "    depth: 110\n",
      "    initial_channels: 16\n",
      "    se_reduction: 16\n",
      "    block_type: basic\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "train:\n",
      "  checkpoint: ''\n",
      "  resume: False\n",
      "  precision: O0\n",
      "  batch_size: 128\n",
      "  subdivision: 1\n",
      "  optimizer: sgd\n",
      "  base_lr: 0.1\n",
      "  momentum: 0.9\n",
      "  nesterov: True\n",
      "  weight_decay: 0.0005\n",
      "  no_weight_decay_on_bn: False\n",
      "  gradient_clip: 0\n",
      "  start_epoch: 0\n",
      "  seed: 0\n",
      "  val_first: True\n",
      "  val_period: 1\n",
      "  val_ratio: 0.1\n",
      "  use_test_as_val: False\n",
      "  output_dir: /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_3_20/exp00\n",
      "  log_period: 100\n",
      "  checkpoint_period: 100\n",
      "  use_tensorboard: True\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: True\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "  distributed: False\n",
      "  dist:\n",
      "    backend: nccl\n",
      "    init_method: env://\n",
      "    world_size: -1\n",
      "    node_rank: -1\n",
      "    local_rank: 0\n",
      "    use_sync_bn: False\n",
      "tensorboard:\n",
      "  train_images: False\n",
      "  val_images: False\n",
      "  model_params: False\n",
      "optim:\n",
      "  adam:\n",
      "    betas: (0.9, 0.999)\n",
      "  lars:\n",
      "    eps: 1e-09\n",
      "    threshold: 0.01\n",
      "  adabound:\n",
      "    betas: (0.9, 0.999)\n",
      "    final_lr: 0.1\n",
      "    gamma: 0.001\n",
      "scheduler:\n",
      "  epochs: 400\n",
      "  warmup:\n",
      "    type: none\n",
      "    epochs: 0\n",
      "    start_factor: 0.001\n",
      "    exponent: 4\n",
      "  type: multistep\n",
      "  milestones: [60, 120, 160]\n",
      "  lr_decay: 0.2\n",
      "  lr_min_factor: 0.001\n",
      "  T0: 10\n",
      "  T_mul: 1.0\n",
      "validation:\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: False\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "augmentation:\n",
      "  use_random_crop: True\n",
      "  use_random_horizontal_flip: True\n",
      "  use_cutout: False\n",
      "  use_random_erasing: False\n",
      "  use_dual_cutout: False\n",
      "  use_mixup: False\n",
      "  use_ricap: False\n",
      "  use_cutmix: False\n",
      "  use_label_smoothing: False\n",
      "  random_crop:\n",
      "    padding: 4\n",
      "    fill: 0\n",
      "    padding_mode: constant\n",
      "  random_horizontal_flip:\n",
      "    prob: 0.5\n",
      "  cutout:\n",
      "    prob: 1.0\n",
      "    mask_size: 16\n",
      "    cut_inside: False\n",
      "    mask_color: 0\n",
      "    dual_cutout_alpha: 0.1\n",
      "  random_erasing:\n",
      "    prob: 0.5\n",
      "    area_ratio_range: [0.02, 0.4]\n",
      "    min_aspect_ratio: 0.3\n",
      "    max_attempt: 20\n",
      "  mixup:\n",
      "    alpha: 1.0\n",
      "  ricap:\n",
      "    beta: 0.3\n",
      "  cutmix:\n",
      "    alpha: 1.0\n",
      "  label_smoothing:\n",
      "    epsilon: 0.1\n",
      "tta:\n",
      "  use_resize: False\n",
      "  use_center_crop: False\n",
      "  resize: 256\n",
      "test:\n",
      "  checkpoint: ''\n",
      "  output_dir: ''\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    pin_memory: False\n",
      "\u001b[32m[2020-06-24 01:16:52] __main__ INFO: \u001b[0menv_info:\n",
      "  pytorch_version: 1.4.0\n",
      "  cuda_version: 10.1\n",
      "  cudnn_version: 7603\n",
      "  num_gpus: 1\n",
      "  gpu_name: Tesla K80\n",
      "  gpu_capability: 3.7\n",
      "(50000, 32, 32, 3)\n",
      "\u001b[32m[2020-06-24 01:16:59] __main__ INFO: \u001b[0mMACs  : 5.25G\n",
      "\u001b[32m[2020-06-24 01:16:59] __main__ INFO: \u001b[0m#params: 36.48M\n",
      "Selected optimization level O0:  Pure FP32 training.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\",)\n",
      "\u001b[32m[2020-06-24 01:16:59] __main__ INFO: \u001b[0mVal 0\n",
      "\u001b[32m[2020-06-24 01:17:19] __main__ INFO: \u001b[0mEpoch 0 loss 345.8863 acc@1 0.1016 acc@5 0.4968\n",
      "\u001b[32m[2020-06-24 01:17:19] __main__ INFO: \u001b[0mElapsed 19.65\n",
      "\u001b[32m[2020-06-24 01:17:19] __main__ INFO: \u001b[0mTrain 1 0\n",
      "\u001b[32m[2020-06-24 01:19:16] __main__ INFO: \u001b[0mEpoch 1 Step 100/351 lr 0.100000 loss 2.3589 (2.4215) acc@1 0.0859 (0.1057) acc@5 0.5312 (0.5087)\n"
     ]
    }
   ],
   "source": [
    "# Train the model per the settings specified in the original paper\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "!python train.py --config configs/cifar/wrn.yaml \\\n",
    "    model.wrn.depth 28 \\\n",
    "    model.wrn.widening_factor 10 \\\n",
    "    train.batch_size 128 \\\n",
    "    train.base_lr 0.1 \\\n",
    "    dataset.name CIFAR10_RA_3_20 \\\n",
    "    train.output_dir /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_3_20/exp00 \\\n",
    "    scheduler.epochs 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-22 23:40:28] __main__ INFO: \u001b[0mdevice: cuda\n",
      "cudnn:\n",
      "  benchmark: True\n",
      "  deterministic: False\n",
      "dataset:\n",
      "  name: CIFAR10\n",
      "  dataset_dir: ~/.torch/datasets/CIFAR10\n",
      "  image_size: 32\n",
      "  n_channels: 3\n",
      "  n_classes: 10\n",
      "model:\n",
      "  type: cifar\n",
      "  name: wrn\n",
      "  init_mode: kaiming_fan_in\n",
      "  vgg:\n",
      "    n_channels: [64, 128, 256, 512, 512]\n",
      "    n_layers: [2, 2, 3, 3, 3]\n",
      "    use_bn: True\n",
      "  resnet:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "  resnet_preact:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "  wrn:\n",
      "    depth: 28\n",
      "    initial_channels: 16\n",
      "    widening_factor: 10\n",
      "    drop_rate: 0.0\n",
      "  densenet:\n",
      "    depth: 100\n",
      "    n_blocks: [6, 12, 24, 16]\n",
      "    block_type: bottleneck\n",
      "    growth_rate: 12\n",
      "    drop_rate: 0.0\n",
      "    compression_rate: 0.5\n",
      "  pyramidnet:\n",
      "    depth: 272\n",
      "    n_blocks: [3, 24, 36, 3]\n",
      "    initial_channels: 16\n",
      "    block_type: bottleneck\n",
      "    alpha: 200\n",
      "  resnext:\n",
      "    depth: 29\n",
      "    n_blocks: [3, 4, 6, 3]\n",
      "    initial_channels: 64\n",
      "    cardinality: 8\n",
      "    base_channels: 4\n",
      "  shake_shake:\n",
      "    depth: 26\n",
      "    initial_channels: 96\n",
      "    shake_forward: True\n",
      "    shake_backward: True\n",
      "    shake_image: True\n",
      "  se_resnet_preact:\n",
      "    depth: 110\n",
      "    initial_channels: 16\n",
      "    se_reduction: 16\n",
      "    block_type: basic\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "train:\n",
      "  checkpoint: /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_5/exp00/checkpoint_00400.pth\n",
      "  resume: False\n",
      "  precision: O0\n",
      "  batch_size: 128\n",
      "  subdivision: 1\n",
      "  optimizer: sgd\n",
      "  base_lr: 0.0008\n",
      "  momentum: 0.9\n",
      "  nesterov: True\n",
      "  weight_decay: 0.0005\n",
      "  no_weight_decay_on_bn: False\n",
      "  gradient_clip: 0\n",
      "  start_epoch: 0\n",
      "  seed: 0\n",
      "  val_first: True\n",
      "  val_period: 1\n",
      "  val_ratio: 0.1\n",
      "  use_test_as_val: False\n",
      "  output_dir: /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_5/exp00_resume400_50\n",
      "  log_period: 100\n",
      "  checkpoint_period: 100\n",
      "  use_tensorboard: True\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: True\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "  distributed: False\n",
      "  dist:\n",
      "    backend: nccl\n",
      "    init_method: env://\n",
      "    world_size: -1\n",
      "    node_rank: -1\n",
      "    local_rank: 0\n",
      "    use_sync_bn: False\n",
      "tensorboard:\n",
      "  train_images: False\n",
      "  val_images: False\n",
      "  model_params: False\n",
      "optim:\n",
      "  adam:\n",
      "    betas: (0.9, 0.999)\n",
      "  lars:\n",
      "    eps: 1e-09\n",
      "    threshold: 0.01\n",
      "  adabound:\n",
      "    betas: (0.9, 0.999)\n",
      "    final_lr: 0.1\n",
      "    gamma: 0.001\n",
      "scheduler:\n",
      "  epochs: 50\n",
      "  warmup:\n",
      "    type: none\n",
      "    epochs: 0\n",
      "    start_factor: 0.001\n",
      "    exponent: 4\n",
      "  type: multistep\n",
      "  milestones: [60, 120, 160]\n",
      "  lr_decay: 0.2\n",
      "  lr_min_factor: 0.001\n",
      "  T0: 10\n",
      "  T_mul: 1.0\n",
      "validation:\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: False\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "augmentation:\n",
      "  use_random_crop: True\n",
      "  use_random_horizontal_flip: True\n",
      "  use_cutout: False\n",
      "  use_random_erasing: False\n",
      "  use_dual_cutout: False\n",
      "  use_mixup: False\n",
      "  use_ricap: False\n",
      "  use_cutmix: False\n",
      "  use_label_smoothing: False\n",
      "  random_crop:\n",
      "    padding: 4\n",
      "    fill: 0\n",
      "    padding_mode: constant\n",
      "  random_horizontal_flip:\n",
      "    prob: 0.5\n",
      "  cutout:\n",
      "    prob: 1.0\n",
      "    mask_size: 16\n",
      "    cut_inside: False\n",
      "    mask_color: 0\n",
      "    dual_cutout_alpha: 0.1\n",
      "  random_erasing:\n",
      "    prob: 0.5\n",
      "    area_ratio_range: [0.02, 0.4]\n",
      "    min_aspect_ratio: 0.3\n",
      "    max_attempt: 20\n",
      "  mixup:\n",
      "    alpha: 1.0\n",
      "  ricap:\n",
      "    beta: 0.3\n",
      "  cutmix:\n",
      "    alpha: 1.0\n",
      "  label_smoothing:\n",
      "    epsilon: 0.1\n",
      "tta:\n",
      "  use_resize: False\n",
      "  use_center_crop: False\n",
      "  resize: 256\n",
      "test:\n",
      "  checkpoint: ''\n",
      "  output_dir: ''\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    pin_memory: False\n",
      "\u001b[32m[2020-06-22 23:40:28] __main__ INFO: \u001b[0menv_info:\n",
      "  pytorch_version: 1.4.0\n",
      "  cuda_version: 10.1\n",
      "  cudnn_version: 7603\n",
      "  num_gpus: 1\n",
      "  gpu_name: Tesla K80\n",
      "  gpu_capability: 3.7\n",
      "Files already downloaded and verified\n",
      "\u001b[32m[2020-06-22 23:40:32] __main__ INFO: \u001b[0mMACs  : 5.25G\n",
      "\u001b[32m[2020-06-22 23:40:32] __main__ INFO: \u001b[0m#params: 36.48M\n",
      "Selected optimization level O0:  Pure FP32 training.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\",)\n",
      "\u001b[32m[2020-06-22 23:40:32] __main__ INFO: \u001b[0mVal 0\n",
      "\u001b[32m[2020-06-22 23:40:52] __main__ INFO: \u001b[0mEpoch 0 loss 0.4080 acc@1 0.9106 acc@5 0.9952\n",
      "\u001b[32m[2020-06-22 23:40:52] __main__ INFO: \u001b[0mElapsed 19.64\n",
      "\u001b[32m[2020-06-22 23:40:52] __main__ INFO: \u001b[0mTrain 1 0\n",
      "\u001b[32m[2020-06-22 23:42:49] __main__ INFO: \u001b[0mEpoch 1 Step 100/351 lr 0.000800 loss 0.0707 (0.1548) acc@1 0.9844 (0.9580) acc@5 1.0000 (0.9983)\n",
      "\u001b[32m[2020-06-22 23:44:42] __main__ INFO: \u001b[0mEpoch 1 Step 200/351 lr 0.000800 loss 0.1556 (0.1427) acc@1 0.9375 (0.9600) acc@5 1.0000 (0.9985)\n",
      "\u001b[32m[2020-06-22 23:46:34] __main__ INFO: \u001b[0mEpoch 1 Step 300/351 lr 0.000800 loss 0.1018 (0.1392) acc@1 0.9609 (0.9603) acc@5 1.0000 (0.9986)\n",
      "\u001b[32m[2020-06-22 23:47:32] __main__ INFO: \u001b[0mEpoch 1 Step 351/351 lr 0.000800 loss 0.0745 (0.1380) acc@1 0.9844 (0.9601) acc@5 1.0000 (0.9987)\n",
      "\u001b[32m[2020-06-22 23:47:32] __main__ INFO: \u001b[0mElapsed 400.01\n",
      "\u001b[32m[2020-06-22 23:47:32] __main__ INFO: \u001b[0mVal 1\n",
      "\u001b[32m[2020-06-22 23:47:45] __main__ INFO: \u001b[0mEpoch 1 loss 0.2307 acc@1 0.9276 acc@5 0.9976\n",
      "\u001b[32m[2020-06-22 23:47:45] __main__ INFO: \u001b[0mElapsed 13.27\n",
      "\u001b[32m[2020-06-22 23:47:45] __main__ INFO: \u001b[0mTrain 2 351\n",
      "\u001b[32m[2020-06-22 23:49:38] __main__ INFO: \u001b[0mEpoch 2 Step 100/351 lr 0.000800 loss 0.0668 (0.0929) acc@1 0.9766 (0.9714) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-06-22 23:51:30] __main__ INFO: \u001b[0mEpoch 2 Step 200/351 lr 0.000800 loss 0.0463 (0.0913) acc@1 0.9922 (0.9720) acc@5 1.0000 (0.9995)\n",
      "\u001b[32m[2020-06-22 23:53:23] __main__ INFO: \u001b[0mEpoch 2 Step 300/351 lr 0.000800 loss 0.1212 (0.0912) acc@1 0.9531 (0.9723) acc@5 1.0000 (0.9994)\n",
      "\u001b[32m[2020-06-22 23:54:20] __main__ INFO: \u001b[0mEpoch 2 Step 351/351 lr 0.000800 loss 0.1477 (0.0921) acc@1 0.9609 (0.9720) acc@5 1.0000 (0.9994)\n",
      "\u001b[32m[2020-06-22 23:54:20] __main__ INFO: \u001b[0mElapsed 395.00\n",
      "\u001b[32m[2020-06-22 23:54:20] __main__ INFO: \u001b[0mVal 2\n",
      "\u001b[32m[2020-06-22 23:54:33] __main__ INFO: \u001b[0mEpoch 2 loss 0.2349 acc@1 0.9314 acc@5 0.9972\n",
      "\u001b[32m[2020-06-22 23:54:33] __main__ INFO: \u001b[0mElapsed 13.29\n",
      "\u001b[32m[2020-06-22 23:54:33] __main__ INFO: \u001b[0mTrain 3 702\n",
      "\u001b[32m[2020-06-22 23:56:26] __main__ INFO: \u001b[0mEpoch 3 Step 100/351 lr 0.000800 loss 0.0497 (0.0655) acc@1 0.9844 (0.9827) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-22 23:58:19] __main__ INFO: \u001b[0mEpoch 3 Step 200/351 lr 0.000800 loss 0.0441 (0.0694) acc@1 0.9844 (0.9804) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-06-23 00:00:11] __main__ INFO: \u001b[0mEpoch 3 Step 300/351 lr 0.000800 loss 0.1163 (0.0692) acc@1 0.9531 (0.9801) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-06-23 00:01:08] __main__ INFO: \u001b[0mEpoch 3 Step 351/351 lr 0.000800 loss 0.0802 (0.0698) acc@1 0.9688 (0.9796) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-06-23 00:01:08] __main__ INFO: \u001b[0mElapsed 394.97\n",
      "\u001b[32m[2020-06-23 00:01:08] __main__ INFO: \u001b[0mVal 3\n",
      "\u001b[32m[2020-06-23 00:01:22] __main__ INFO: \u001b[0mEpoch 3 loss 0.2338 acc@1 0.9308 acc@5 0.9976\n",
      "\u001b[32m[2020-06-23 00:01:22] __main__ INFO: \u001b[0mElapsed 13.26\n",
      "\u001b[32m[2020-06-23 00:01:22] __main__ INFO: \u001b[0mTrain 4 1053\n",
      "\u001b[32m[2020-06-23 00:03:14] __main__ INFO: \u001b[0mEpoch 4 Step 100/351 lr 0.000800 loss 0.0187 (0.0469) acc@1 1.0000 (0.9865) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-23 00:05:07] __main__ INFO: \u001b[0mEpoch 4 Step 200/351 lr 0.000800 loss 0.0728 (0.0504) acc@1 0.9766 (0.9854) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-23 00:06:59] __main__ INFO: \u001b[0mEpoch 4 Step 300/351 lr 0.000800 loss 0.0420 (0.0526) acc@1 0.9844 (0.9850) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-23 00:07:57] __main__ INFO: \u001b[0mEpoch 4 Step 351/351 lr 0.000800 loss 0.1311 (0.0534) acc@1 0.9609 (0.9850) acc@5 0.9922 (0.9997)\n",
      "\u001b[32m[2020-06-23 00:07:57] __main__ INFO: \u001b[0mElapsed 394.94\n",
      "\u001b[32m[2020-06-23 00:07:57] __main__ INFO: \u001b[0mVal 4\n",
      "\u001b[32m[2020-06-23 00:08:10] __main__ INFO: \u001b[0mEpoch 4 loss 0.2389 acc@1 0.9344 acc@5 0.9978\n",
      "\u001b[32m[2020-06-23 00:08:10] __main__ INFO: \u001b[0mElapsed 13.28\n",
      "\u001b[32m[2020-06-23 00:08:10] __main__ INFO: \u001b[0mTrain 5 1404\n",
      "\u001b[32m[2020-06-23 00:10:03] __main__ INFO: \u001b[0mEpoch 5 Step 100/351 lr 0.000800 loss 0.0341 (0.0409) acc@1 0.9844 (0.9898) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-23 00:11:55] __main__ INFO: \u001b[0mEpoch 5 Step 200/351 lr 0.000800 loss 0.0618 (0.0411) acc@1 0.9844 (0.9897) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-06-23 00:13:48] __main__ INFO: \u001b[0mEpoch 5 Step 300/351 lr 0.000800 loss 0.0716 (0.0429) acc@1 0.9766 (0.9889) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-06-23 00:14:45] __main__ INFO: \u001b[0mEpoch 5 Step 351/351 lr 0.000800 loss 0.0225 (0.0434) acc@1 1.0000 (0.9887) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-23 00:14:45] __main__ INFO: \u001b[0mElapsed 395.02\n",
      "\u001b[32m[2020-06-23 00:14:45] __main__ INFO: \u001b[0mVal 5\n",
      "\u001b[32m[2020-06-23 00:14:58] __main__ INFO: \u001b[0mEpoch 5 loss 0.2437 acc@1 0.9332 acc@5 0.9974\n",
      "\u001b[32m[2020-06-23 00:14:58] __main__ INFO: \u001b[0mElapsed 13.27\n",
      "\u001b[32m[2020-06-23 00:14:58] __main__ INFO: \u001b[0mTrain 6 1755\n",
      "\u001b[32m[2020-06-23 00:16:51] __main__ INFO: \u001b[0mEpoch 6 Step 100/351 lr 0.000800 loss 0.0165 (0.0344) acc@1 1.0000 (0.9911) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-23 00:18:43] __main__ INFO: \u001b[0mEpoch 6 Step 200/351 lr 0.000800 loss 0.0526 (0.0348) acc@1 0.9766 (0.9908) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 00:20:36] __main__ INFO: \u001b[0mEpoch 6 Step 300/351 lr 0.000800 loss 0.0195 (0.0360) acc@1 1.0000 (0.9904) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 00:21:33] __main__ INFO: \u001b[0mEpoch 6 Step 351/351 lr 0.000800 loss 0.0072 (0.0360) acc@1 1.0000 (0.9904) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-23 00:21:33] __main__ INFO: \u001b[0mElapsed 394.97\n",
      "\u001b[32m[2020-06-23 00:21:33] __main__ INFO: \u001b[0mVal 6\n",
      "\u001b[32m[2020-06-23 00:21:46] __main__ INFO: \u001b[0mEpoch 6 loss 0.2455 acc@1 0.9364 acc@5 0.9972\n",
      "\u001b[32m[2020-06-23 00:21:46] __main__ INFO: \u001b[0mElapsed 13.26\n",
      "\u001b[32m[2020-06-23 00:21:46] __main__ INFO: \u001b[0mTrain 7 2106\n",
      "\u001b[32m[2020-06-23 00:23:39] __main__ INFO: \u001b[0mEpoch 7 Step 100/351 lr 0.000800 loss 0.0240 (0.0290) acc@1 0.9922 (0.9930) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 00:25:32] __main__ INFO: \u001b[0mEpoch 7 Step 200/351 lr 0.000800 loss 0.0258 (0.0296) acc@1 0.9922 (0.9928) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-23 00:27:24] __main__ INFO: \u001b[0mEpoch 7 Step 300/351 lr 0.000800 loss 0.0229 (0.0284) acc@1 0.9922 (0.9928) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-23 00:28:21] __main__ INFO: \u001b[0mEpoch 7 Step 351/351 lr 0.000800 loss 0.0343 (0.0296) acc@1 0.9844 (0.9924) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-23 00:28:21] __main__ INFO: \u001b[0mElapsed 395.00\n",
      "\u001b[32m[2020-06-23 00:28:21] __main__ INFO: \u001b[0mVal 7\n",
      "\u001b[32m[2020-06-23 00:28:35] __main__ INFO: \u001b[0mEpoch 7 loss 0.2450 acc@1 0.9348 acc@5 0.9976\n",
      "\u001b[32m[2020-06-23 00:28:35] __main__ INFO: \u001b[0mElapsed 13.26\n",
      "\u001b[32m[2020-06-23 00:28:35] __main__ INFO: \u001b[0mTrain 8 2457\n",
      "\u001b[32m[2020-06-23 00:30:27] __main__ INFO: \u001b[0mEpoch 8 Step 100/351 lr 0.000800 loss 0.0071 (0.0257) acc@1 1.0000 (0.9934) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-23 00:32:20] __main__ INFO: \u001b[0mEpoch 8 Step 200/351 lr 0.000800 loss 0.0513 (0.0260) acc@1 0.9922 (0.9934) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 00:34:12] __main__ INFO: \u001b[0mEpoch 8 Step 300/351 lr 0.000800 loss 0.0403 (0.0266) acc@1 0.9922 (0.9930) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-23 00:35:10] __main__ INFO: \u001b[0mEpoch 8 Step 351/351 lr 0.000800 loss 0.0255 (0.0269) acc@1 0.9922 (0.9931) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-23 00:35:10] __main__ INFO: \u001b[0mElapsed 394.89\n",
      "\u001b[32m[2020-06-23 00:35:10] __main__ INFO: \u001b[0mVal 8\n",
      "\u001b[32m[2020-06-23 00:35:23] __main__ INFO: \u001b[0mEpoch 8 loss 0.2517 acc@1 0.9370 acc@5 0.9974\n",
      "\u001b[32m[2020-06-23 00:35:23] __main__ INFO: \u001b[0mElapsed 13.27\n",
      "\u001b[32m[2020-06-23 00:35:23] __main__ INFO: \u001b[0mTrain 9 2808\n",
      "\u001b[32m[2020-06-23 00:37:15] __main__ INFO: \u001b[0mEpoch 9 Step 100/351 lr 0.000800 loss 0.0326 (0.0230) acc@1 0.9922 (0.9946) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-23 00:39:08] __main__ INFO: \u001b[0mEpoch 9 Step 200/351 lr 0.000800 loss 0.0083 (0.0224) acc@1 1.0000 (0.9948) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 00:41:00] __main__ INFO: \u001b[0mEpoch 9 Step 300/351 lr 0.000800 loss 0.0370 (0.0218) acc@1 0.9922 (0.9948) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 00:41:58] __main__ INFO: \u001b[0mEpoch 9 Step 351/351 lr 0.000800 loss 0.0094 (0.0215) acc@1 1.0000 (0.9950) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 00:41:58] __main__ INFO: \u001b[0mElapsed 394.86\n",
      "\u001b[32m[2020-06-23 00:41:58] __main__ INFO: \u001b[0mVal 9\n",
      "\u001b[32m[2020-06-23 00:42:11] __main__ INFO: \u001b[0mEpoch 9 loss 0.2518 acc@1 0.9366 acc@5 0.9968\n",
      "\u001b[32m[2020-06-23 00:42:11] __main__ INFO: \u001b[0mElapsed 13.26\n",
      "\u001b[32m[2020-06-23 00:42:11] __main__ INFO: \u001b[0mTrain 10 3159\n",
      "\u001b[32m[2020-06-23 00:44:03] __main__ INFO: \u001b[0mEpoch 10 Step 100/351 lr 0.000800 loss 0.0229 (0.0184) acc@1 0.9922 (0.9958) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 00:45:56] __main__ INFO: \u001b[0mEpoch 10 Step 200/351 lr 0.000800 loss 0.0081 (0.0187) acc@1 1.0000 (0.9957) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 00:47:48] __main__ INFO: \u001b[0mEpoch 10 Step 300/351 lr 0.000800 loss 0.0168 (0.0190) acc@1 0.9922 (0.9957) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-23 00:48:46] __main__ INFO: \u001b[0mEpoch 10 Step 351/351 lr 0.000800 loss 0.0086 (0.0190) acc@1 1.0000 (0.9956) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 00:48:46] __main__ INFO: \u001b[0mElapsed 394.80\n",
      "\u001b[32m[2020-06-23 00:48:46] __main__ INFO: \u001b[0mVal 10\n",
      "\u001b[32m[2020-06-23 00:48:59] __main__ INFO: \u001b[0mEpoch 10 loss 0.2546 acc@1 0.9344 acc@5 0.9978\n",
      "\u001b[32m[2020-06-23 00:48:59] __main__ INFO: \u001b[0mElapsed 13.26\n",
      "\u001b[32m[2020-06-23 00:48:59] __main__ INFO: \u001b[0mTrain 11 3510\n",
      "\u001b[32m[2020-06-23 00:50:52] __main__ INFO: \u001b[0mEpoch 11 Step 100/351 lr 0.000800 loss 0.0470 (0.0144) acc@1 0.9844 (0.9970) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 00:52:44] __main__ INFO: \u001b[0mEpoch 11 Step 200/351 lr 0.000800 loss 0.0150 (0.0157) acc@1 0.9922 (0.9964) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 00:54:36] __main__ INFO: \u001b[0mEpoch 11 Step 300/351 lr 0.000800 loss 0.0157 (0.0157) acc@1 1.0000 (0.9962) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 00:55:34] __main__ INFO: \u001b[0mEpoch 11 Step 351/351 lr 0.000800 loss 0.0126 (0.0158) acc@1 0.9922 (0.9963) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 00:55:34] __main__ INFO: \u001b[0mElapsed 394.83\n",
      "\u001b[32m[2020-06-23 00:55:34] __main__ INFO: \u001b[0mVal 11\n",
      "\u001b[32m[2020-06-23 00:55:47] __main__ INFO: \u001b[0mEpoch 11 loss 0.2664 acc@1 0.9334 acc@5 0.9980\n",
      "\u001b[32m[2020-06-23 00:55:47] __main__ INFO: \u001b[0mElapsed 13.27\n",
      "\u001b[32m[2020-06-23 00:55:47] __main__ INFO: \u001b[0mTrain 12 3861\n",
      "\u001b[32m[2020-06-23 00:57:40] __main__ INFO: \u001b[0mEpoch 12 Step 100/351 lr 0.000800 loss 0.0114 (0.0130) acc@1 1.0000 (0.9976) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 00:59:32] __main__ INFO: \u001b[0mEpoch 12 Step 200/351 lr 0.000800 loss 0.0224 (0.0149) acc@1 0.9922 (0.9968) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 01:01:24] __main__ INFO: \u001b[0mEpoch 12 Step 300/351 lr 0.000800 loss 0.0114 (0.0153) acc@1 1.0000 (0.9965) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 01:02:22] __main__ INFO: \u001b[0mEpoch 12 Step 351/351 lr 0.000800 loss 0.0148 (0.0154) acc@1 1.0000 (0.9966) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 01:02:22] __main__ INFO: \u001b[0mElapsed 394.61\n",
      "\u001b[32m[2020-06-23 01:02:22] __main__ INFO: \u001b[0mVal 12\n",
      "\u001b[32m[2020-06-23 01:02:35] __main__ INFO: \u001b[0mEpoch 12 loss 0.2633 acc@1 0.9358 acc@5 0.9974\n",
      "\u001b[32m[2020-06-23 01:02:35] __main__ INFO: \u001b[0mElapsed 13.25\n",
      "\u001b[32m[2020-06-23 01:02:35] __main__ INFO: \u001b[0mTrain 13 4212\n",
      "\u001b[32m[2020-06-23 01:04:28] __main__ INFO: \u001b[0mEpoch 13 Step 100/351 lr 0.000800 loss 0.0538 (0.0137) acc@1 0.9766 (0.9970) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 01:06:20] __main__ INFO: \u001b[0mEpoch 13 Step 200/351 lr 0.000800 loss 0.0167 (0.0134) acc@1 0.9922 (0.9969) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 01:08:12] __main__ INFO: \u001b[0mEpoch 13 Step 300/351 lr 0.000800 loss 0.0048 (0.0130) acc@1 1.0000 (0.9972) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 01:09:10] __main__ INFO: \u001b[0mEpoch 13 Step 351/351 lr 0.000800 loss 0.0167 (0.0129) acc@1 0.9922 (0.9972) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 01:09:10] __main__ INFO: \u001b[0mElapsed 394.78\n",
      "\u001b[32m[2020-06-23 01:09:10] __main__ INFO: \u001b[0mVal 13\n",
      "\u001b[32m[2020-06-23 01:09:23] __main__ INFO: \u001b[0mEpoch 13 loss 0.2644 acc@1 0.9364 acc@5 0.9986\n",
      "\u001b[32m[2020-06-23 01:09:23] __main__ INFO: \u001b[0mElapsed 13.23\n",
      "\u001b[32m[2020-06-23 01:09:23] __main__ INFO: \u001b[0mTrain 14 4563\n",
      "\u001b[32m[2020-06-23 01:11:15] __main__ INFO: \u001b[0mEpoch 14 Step 100/351 lr 0.000800 loss 0.0047 (0.0126) acc@1 1.0000 (0.9975) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 01:13:08] __main__ INFO: \u001b[0mEpoch 14 Step 200/351 lr 0.000800 loss 0.0059 (0.0127) acc@1 1.0000 (0.9973) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 01:15:00] __main__ INFO: \u001b[0mEpoch 14 Step 300/351 lr 0.000800 loss 0.0122 (0.0124) acc@1 0.9922 (0.9974) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 01:15:58] __main__ INFO: \u001b[0mEpoch 14 Step 351/351 lr 0.000800 loss 0.0047 (0.0125) acc@1 1.0000 (0.9973) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 01:15:58] __main__ INFO: \u001b[0mElapsed 394.63\n",
      "\u001b[32m[2020-06-23 01:15:58] __main__ INFO: \u001b[0mVal 14\n",
      "\u001b[32m[2020-06-23 01:16:11] __main__ INFO: \u001b[0mEpoch 14 loss 0.2694 acc@1 0.9330 acc@5 0.9976\n",
      "\u001b[32m[2020-06-23 01:16:11] __main__ INFO: \u001b[0mElapsed 13.27\n",
      "\u001b[32m[2020-06-23 01:16:11] __main__ INFO: \u001b[0mTrain 15 4914\n",
      "\u001b[32m[2020-06-23 01:18:03] __main__ INFO: \u001b[0mEpoch 15 Step 100/351 lr 0.000800 loss 0.0103 (0.0136) acc@1 1.0000 (0.9971) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 01:19:56] __main__ INFO: \u001b[0mEpoch 15 Step 200/351 lr 0.000800 loss 0.0082 (0.0126) acc@1 1.0000 (0.9975) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 01:21:48] __main__ INFO: \u001b[0mEpoch 15 Step 300/351 lr 0.000800 loss 0.0043 (0.0120) acc@1 1.0000 (0.9976) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 01:22:46] __main__ INFO: \u001b[0mEpoch 15 Step 351/351 lr 0.000800 loss 0.0126 (0.0122) acc@1 1.0000 (0.9974) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 01:22:46] __main__ INFO: \u001b[0mElapsed 394.69\n",
      "\u001b[32m[2020-06-23 01:22:46] __main__ INFO: \u001b[0mVal 15\n",
      "\u001b[32m[2020-06-23 01:22:59] __main__ INFO: \u001b[0mEpoch 15 loss 0.2611 acc@1 0.9362 acc@5 0.9980\n",
      "\u001b[32m[2020-06-23 01:22:59] __main__ INFO: \u001b[0mElapsed 13.26\n",
      "\u001b[32m[2020-06-23 01:22:59] __main__ INFO: \u001b[0mTrain 16 5265\n",
      "\u001b[32m[2020-06-23 01:24:51] __main__ INFO: \u001b[0mEpoch 16 Step 100/351 lr 0.000800 loss 0.0079 (0.0111) acc@1 1.0000 (0.9981) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 01:26:44] __main__ INFO: \u001b[0mEpoch 16 Step 200/351 lr 0.000800 loss 0.0105 (0.0102) acc@1 1.0000 (0.9982) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 01:28:36] __main__ INFO: \u001b[0mEpoch 16 Step 300/351 lr 0.000800 loss 0.0033 (0.0108) acc@1 1.0000 (0.9978) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 01:29:33] __main__ INFO: \u001b[0mEpoch 16 Step 351/351 lr 0.000800 loss 0.0053 (0.0106) acc@1 1.0000 (0.9978) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 01:29:34] __main__ INFO: \u001b[0mElapsed 394.65\n",
      "\u001b[32m[2020-06-23 01:29:34] __main__ INFO: \u001b[0mVal 16\n",
      "\u001b[32m[2020-06-23 01:29:47] __main__ INFO: \u001b[0mEpoch 16 loss 0.2565 acc@1 0.9366 acc@5 0.9982\n",
      "\u001b[32m[2020-06-23 01:29:47] __main__ INFO: \u001b[0mElapsed 13.31\n",
      "\u001b[32m[2020-06-23 01:29:47] __main__ INFO: \u001b[0mTrain 17 5616\n",
      "\u001b[32m[2020-06-23 01:31:39] __main__ INFO: \u001b[0mEpoch 17 Step 100/351 lr 0.000800 loss 0.0067 (0.0080) acc@1 1.0000 (0.9991) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 01:33:32] __main__ INFO: \u001b[0mEpoch 17 Step 200/351 lr 0.000800 loss 0.0048 (0.0090) acc@1 1.0000 (0.9986) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 01:35:24] __main__ INFO: \u001b[0mEpoch 17 Step 300/351 lr 0.000800 loss 0.0107 (0.0087) acc@1 0.9922 (0.9986) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 01:36:21] __main__ INFO: \u001b[0mEpoch 17 Step 351/351 lr 0.000800 loss 0.0038 (0.0088) acc@1 1.0000 (0.9985) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 01:36:21] __main__ INFO: \u001b[0mElapsed 394.60\n",
      "\u001b[32m[2020-06-23 01:36:21] __main__ INFO: \u001b[0mVal 17\n",
      "\u001b[32m[2020-06-23 01:36:35] __main__ INFO: \u001b[0mEpoch 17 loss 0.2742 acc@1 0.9340 acc@5 0.9978\n",
      "\u001b[32m[2020-06-23 01:36:35] __main__ INFO: \u001b[0mElapsed 13.25\n",
      "\u001b[32m[2020-06-23 01:36:35] __main__ INFO: \u001b[0mTrain 18 5967\n",
      "\u001b[32m[2020-06-23 01:38:27] __main__ INFO: \u001b[0mEpoch 18 Step 100/351 lr 0.000800 loss 0.0041 (0.0097) acc@1 1.0000 (0.9984) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 01:40:20] __main__ INFO: \u001b[0mEpoch 18 Step 200/351 lr 0.000800 loss 0.0044 (0.0094) acc@1 1.0000 (0.9985) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 01:42:12] __main__ INFO: \u001b[0mEpoch 18 Step 300/351 lr 0.000800 loss 0.0186 (0.0092) acc@1 0.9922 (0.9984) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 01:43:09] __main__ INFO: \u001b[0mEpoch 18 Step 351/351 lr 0.000800 loss 0.0169 (0.0095) acc@1 0.9922 (0.9983) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 01:43:09] __main__ INFO: \u001b[0mElapsed 394.69\n",
      "\u001b[32m[2020-06-23 01:43:09] __main__ INFO: \u001b[0mVal 18\n",
      "\u001b[32m[2020-06-23 01:43:23] __main__ INFO: \u001b[0mEpoch 18 loss 0.2801 acc@1 0.9344 acc@5 0.9976\n",
      "\u001b[32m[2020-06-23 01:43:23] __main__ INFO: \u001b[0mElapsed 13.27\n",
      "\u001b[32m[2020-06-23 01:43:23] __main__ INFO: \u001b[0mTrain 19 6318\n",
      "\u001b[32m[2020-06-23 01:45:15] __main__ INFO: \u001b[0mEpoch 19 Step 100/351 lr 0.000800 loss 0.0080 (0.0081) acc@1 1.0000 (0.9988) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 01:47:08] __main__ INFO: \u001b[0mEpoch 19 Step 200/351 lr 0.000800 loss 0.0085 (0.0090) acc@1 1.0000 (0.9984) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 01:49:00] __main__ INFO: \u001b[0mEpoch 19 Step 300/351 lr 0.000800 loss 0.0175 (0.0090) acc@1 0.9922 (0.9984) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 01:49:57] __main__ INFO: \u001b[0mEpoch 19 Step 351/351 lr 0.000800 loss 0.0246 (0.0091) acc@1 0.9844 (0.9983) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 01:49:57] __main__ INFO: \u001b[0mElapsed 394.65\n",
      "\u001b[32m[2020-06-23 01:49:57] __main__ INFO: \u001b[0mVal 19\n",
      "\u001b[32m[2020-06-23 01:50:11] __main__ INFO: \u001b[0mEpoch 19 loss 0.2750 acc@1 0.9364 acc@5 0.9972\n",
      "\u001b[32m[2020-06-23 01:50:11] __main__ INFO: \u001b[0mElapsed 13.24\n",
      "\u001b[32m[2020-06-23 01:50:11] __main__ INFO: \u001b[0mTrain 20 6669\n",
      "\u001b[32m[2020-06-23 01:52:03] __main__ INFO: \u001b[0mEpoch 20 Step 100/351 lr 0.000800 loss 0.0034 (0.0077) acc@1 1.0000 (0.9992) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 01:53:55] __main__ INFO: \u001b[0mEpoch 20 Step 200/351 lr 0.000800 loss 0.0034 (0.0078) acc@1 1.0000 (0.9991) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 01:55:48] __main__ INFO: \u001b[0mEpoch 20 Step 300/351 lr 0.000800 loss 0.0048 (0.0083) acc@1 1.0000 (0.9988) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 01:56:45] __main__ INFO: \u001b[0mEpoch 20 Step 351/351 lr 0.000800 loss 0.0045 (0.0082) acc@1 1.0000 (0.9988) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 01:56:45] __main__ INFO: \u001b[0mElapsed 394.59\n",
      "\u001b[32m[2020-06-23 01:56:45] __main__ INFO: \u001b[0mVal 20\n",
      "\u001b[32m[2020-06-23 01:56:58] __main__ INFO: \u001b[0mEpoch 20 loss 0.2608 acc@1 0.9400 acc@5 0.9972\n",
      "\u001b[32m[2020-06-23 01:56:58] __main__ INFO: \u001b[0mElapsed 13.26\n",
      "\u001b[32m[2020-06-23 01:56:58] __main__ INFO: \u001b[0mTrain 21 7020\n",
      "\u001b[32m[2020-06-23 01:58:51] __main__ INFO: \u001b[0mEpoch 21 Step 100/351 lr 0.000800 loss 0.0188 (0.0080) acc@1 0.9922 (0.9984) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 02:00:43] __main__ INFO: \u001b[0mEpoch 21 Step 200/351 lr 0.000800 loss 0.0055 (0.0087) acc@1 1.0000 (0.9984) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 02:02:36] __main__ INFO: \u001b[0mEpoch 21 Step 300/351 lr 0.000800 loss 0.0035 (0.0082) acc@1 1.0000 (0.9985) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 02:03:33] __main__ INFO: \u001b[0mEpoch 21 Step 351/351 lr 0.000800 loss 0.0087 (0.0084) acc@1 1.0000 (0.9985) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 02:03:33] __main__ INFO: \u001b[0mElapsed 394.67\n",
      "\u001b[32m[2020-06-23 02:03:33] __main__ INFO: \u001b[0mVal 21\n",
      "\u001b[32m[2020-06-23 02:03:46] __main__ INFO: \u001b[0mEpoch 21 loss 0.2776 acc@1 0.9372 acc@5 0.9970\n",
      "\u001b[32m[2020-06-23 02:03:46] __main__ INFO: \u001b[0mElapsed 13.25\n",
      "\u001b[32m[2020-06-23 02:03:46] __main__ INFO: \u001b[0mTrain 22 7371\n",
      "\u001b[32m[2020-06-23 02:05:39] __main__ INFO: \u001b[0mEpoch 22 Step 100/351 lr 0.000800 loss 0.0036 (0.0069) acc@1 1.0000 (0.9991) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 02:07:31] __main__ INFO: \u001b[0mEpoch 22 Step 200/351 lr 0.000800 loss 0.0035 (0.0068) acc@1 1.0000 (0.9989) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 02:09:24] __main__ INFO: \u001b[0mEpoch 22 Step 300/351 lr 0.000800 loss 0.0052 (0.0068) acc@1 1.0000 (0.9990) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 02:10:21] __main__ INFO: \u001b[0mEpoch 22 Step 351/351 lr 0.000800 loss 0.0030 (0.0067) acc@1 1.0000 (0.9990) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 02:10:21] __main__ INFO: \u001b[0mElapsed 394.58\n",
      "\u001b[32m[2020-06-23 02:10:21] __main__ INFO: \u001b[0mVal 22\n",
      "\u001b[32m[2020-06-23 02:10:34] __main__ INFO: \u001b[0mEpoch 22 loss 0.2758 acc@1 0.9374 acc@5 0.9976\n",
      "\u001b[32m[2020-06-23 02:10:34] __main__ INFO: \u001b[0mElapsed 13.24\n",
      "\u001b[32m[2020-06-23 02:10:34] __main__ INFO: \u001b[0mTrain 23 7722\n",
      "\u001b[32m[2020-06-23 02:12:27] __main__ INFO: \u001b[0mEpoch 23 Step 100/351 lr 0.000800 loss 0.0048 (0.0072) acc@1 1.0000 (0.9986) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 02:14:19] __main__ INFO: \u001b[0mEpoch 23 Step 200/351 lr 0.000800 loss 0.0131 (0.0075) acc@1 0.9922 (0.9988) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 02:16:11] __main__ INFO: \u001b[0mEpoch 23 Step 300/351 lr 0.000800 loss 0.0047 (0.0074) acc@1 1.0000 (0.9987) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 02:17:09] __main__ INFO: \u001b[0mEpoch 23 Step 351/351 lr 0.000800 loss 0.0033 (0.0073) acc@1 1.0000 (0.9987) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 02:17:09] __main__ INFO: \u001b[0mElapsed 394.62\n",
      "\u001b[32m[2020-06-23 02:17:09] __main__ INFO: \u001b[0mVal 23\n",
      "\u001b[32m[2020-06-23 02:17:22] __main__ INFO: \u001b[0mEpoch 23 loss 0.2638 acc@1 0.9400 acc@5 0.9970\n",
      "\u001b[32m[2020-06-23 02:17:22] __main__ INFO: \u001b[0mElapsed 13.25\n",
      "\u001b[32m[2020-06-23 02:17:22] __main__ INFO: \u001b[0mTrain 24 8073\n",
      "\u001b[32m[2020-06-23 02:19:15] __main__ INFO: \u001b[0mEpoch 24 Step 100/351 lr 0.000800 loss 0.0051 (0.0054) acc@1 1.0000 (0.9995) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 02:21:07] __main__ INFO: \u001b[0mEpoch 24 Step 200/351 lr 0.000800 loss 0.0046 (0.0057) acc@1 1.0000 (0.9993) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 02:22:59] __main__ INFO: \u001b[0mEpoch 24 Step 300/351 lr 0.000800 loss 0.0065 (0.0063) acc@1 1.0000 (0.9990) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 02:23:57] __main__ INFO: \u001b[0mEpoch 24 Step 351/351 lr 0.000800 loss 0.0029 (0.0065) acc@1 1.0000 (0.9989) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 02:23:57] __main__ INFO: \u001b[0mElapsed 394.67\n",
      "\u001b[32m[2020-06-23 02:23:57] __main__ INFO: \u001b[0mVal 24\n",
      "\u001b[32m[2020-06-23 02:24:10] __main__ INFO: \u001b[0mEpoch 24 loss 0.2821 acc@1 0.9352 acc@5 0.9972\n",
      "\u001b[32m[2020-06-23 02:24:10] __main__ INFO: \u001b[0mElapsed 13.28\n",
      "\u001b[32m[2020-06-23 02:24:10] __main__ INFO: \u001b[0mTrain 25 8424\n",
      "\u001b[32m[2020-06-23 02:26:02] __main__ INFO: \u001b[0mEpoch 25 Step 100/351 lr 0.000800 loss 0.0028 (0.0066) acc@1 1.0000 (0.9988) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 02:27:55] __main__ INFO: \u001b[0mEpoch 25 Step 200/351 lr 0.000800 loss 0.0032 (0.0067) acc@1 1.0000 (0.9988) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 02:29:47] __main__ INFO: \u001b[0mEpoch 25 Step 300/351 lr 0.000800 loss 0.0049 (0.0070) acc@1 1.0000 (0.9987) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 02:30:45] __main__ INFO: \u001b[0mEpoch 25 Step 351/351 lr 0.000800 loss 0.0068 (0.0068) acc@1 1.0000 (0.9988) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 02:30:45] __main__ INFO: \u001b[0mElapsed 394.59\n",
      "\u001b[32m[2020-06-23 02:30:45] __main__ INFO: \u001b[0mVal 25\n",
      "\u001b[32m[2020-06-23 02:30:58] __main__ INFO: \u001b[0mEpoch 25 loss 0.2909 acc@1 0.9352 acc@5 0.9966\n",
      "\u001b[32m[2020-06-23 02:30:58] __main__ INFO: \u001b[0mElapsed 13.27\n",
      "\u001b[32m[2020-06-23 02:30:58] __main__ INFO: \u001b[0mTrain 26 8775\n",
      "\u001b[32m[2020-06-23 02:32:50] __main__ INFO: \u001b[0mEpoch 26 Step 100/351 lr 0.000800 loss 0.0116 (0.0064) acc@1 0.9922 (0.9989) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 02:34:43] __main__ INFO: \u001b[0mEpoch 26 Step 200/351 lr 0.000800 loss 0.0291 (0.0063) acc@1 0.9922 (0.9990) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 02:36:35] __main__ INFO: \u001b[0mEpoch 26 Step 300/351 lr 0.000800 loss 0.0054 (0.0066) acc@1 1.0000 (0.9988) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 02:37:32] __main__ INFO: \u001b[0mEpoch 26 Step 351/351 lr 0.000800 loss 0.0026 (0.0067) acc@1 1.0000 (0.9987) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 02:37:32] __main__ INFO: \u001b[0mElapsed 394.64\n",
      "\u001b[32m[2020-06-23 02:37:32] __main__ INFO: \u001b[0mVal 26\n",
      "\u001b[32m[2020-06-23 02:37:46] __main__ INFO: \u001b[0mEpoch 26 loss 0.2757 acc@1 0.9364 acc@5 0.9970\n",
      "\u001b[32m[2020-06-23 02:37:46] __main__ INFO: \u001b[0mElapsed 13.28\n",
      "\u001b[32m[2020-06-23 02:37:46] __main__ INFO: \u001b[0mTrain 27 9126\n",
      "\u001b[32m[2020-06-23 02:39:38] __main__ INFO: \u001b[0mEpoch 27 Step 100/351 lr 0.000800 loss 0.0148 (0.0078) acc@1 0.9922 (0.9988) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 02:41:31] __main__ INFO: \u001b[0mEpoch 27 Step 200/351 lr 0.000800 loss 0.0062 (0.0068) acc@1 1.0000 (0.9989) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 02:43:23] __main__ INFO: \u001b[0mEpoch 27 Step 300/351 lr 0.000800 loss 0.0072 (0.0064) acc@1 1.0000 (0.9990) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 02:44:20] __main__ INFO: \u001b[0mEpoch 27 Step 351/351 lr 0.000800 loss 0.0119 (0.0064) acc@1 0.9922 (0.9990) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 02:44:20] __main__ INFO: \u001b[0mElapsed 394.63\n",
      "\u001b[32m[2020-06-23 02:44:20] __main__ INFO: \u001b[0mVal 27\n",
      "\u001b[32m[2020-06-23 02:44:34] __main__ INFO: \u001b[0mEpoch 27 loss 0.2828 acc@1 0.9356 acc@5 0.9964\n",
      "\u001b[32m[2020-06-23 02:44:34] __main__ INFO: \u001b[0mElapsed 13.27\n",
      "\u001b[32m[2020-06-23 02:44:34] __main__ INFO: \u001b[0mTrain 28 9477\n",
      "\u001b[32m[2020-06-23 02:46:26] __main__ INFO: \u001b[0mEpoch 28 Step 100/351 lr 0.000800 loss 0.0030 (0.0049) acc@1 1.0000 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 02:48:18] __main__ INFO: \u001b[0mEpoch 28 Step 200/351 lr 0.000800 loss 0.0058 (0.0054) acc@1 1.0000 (0.9995) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 02:50:11] __main__ INFO: \u001b[0mEpoch 28 Step 300/351 lr 0.000800 loss 0.0042 (0.0053) acc@1 1.0000 (0.9994) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 02:51:08] __main__ INFO: \u001b[0mEpoch 28 Step 351/351 lr 0.000800 loss 0.0038 (0.0054) acc@1 1.0000 (0.9994) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 02:51:08] __main__ INFO: \u001b[0mElapsed 394.55\n",
      "\u001b[32m[2020-06-23 02:51:08] __main__ INFO: \u001b[0mVal 28\n",
      "\u001b[32m[2020-06-23 02:51:21] __main__ INFO: \u001b[0mEpoch 28 loss 0.2894 acc@1 0.9344 acc@5 0.9962\n",
      "\u001b[32m[2020-06-23 02:51:21] __main__ INFO: \u001b[0mElapsed 13.25\n",
      "\u001b[32m[2020-06-23 02:51:21] __main__ INFO: \u001b[0mTrain 29 9828\n",
      "\u001b[32m[2020-06-23 02:53:14] __main__ INFO: \u001b[0mEpoch 29 Step 100/351 lr 0.000800 loss 0.0056 (0.0051) acc@1 1.0000 (0.9993) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 02:55:06] __main__ INFO: \u001b[0mEpoch 29 Step 200/351 lr 0.000800 loss 0.0035 (0.0057) acc@1 1.0000 (0.9992) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 02:56:59] __main__ INFO: \u001b[0mEpoch 29 Step 300/351 lr 0.000800 loss 0.0024 (0.0054) acc@1 1.0000 (0.9993) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 02:57:56] __main__ INFO: \u001b[0mEpoch 29 Step 351/351 lr 0.000800 loss 0.0027 (0.0053) acc@1 1.0000 (0.9993) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 02:57:56] __main__ INFO: \u001b[0mElapsed 394.51\n",
      "\u001b[32m[2020-06-23 02:57:56] __main__ INFO: \u001b[0mVal 29\n",
      "\u001b[32m[2020-06-23 02:58:09] __main__ INFO: \u001b[0mEpoch 29 loss 0.2908 acc@1 0.9376 acc@5 0.9968\n",
      "\u001b[32m[2020-06-23 02:58:09] __main__ INFO: \u001b[0mElapsed 13.26\n",
      "\u001b[32m[2020-06-23 02:58:09] __main__ INFO: \u001b[0mTrain 30 10179\n",
      "\u001b[32m[2020-06-23 03:00:02] __main__ INFO: \u001b[0mEpoch 30 Step 100/351 lr 0.000800 loss 0.0030 (0.0046) acc@1 1.0000 (0.9995) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 03:01:54] __main__ INFO: \u001b[0mEpoch 30 Step 200/351 lr 0.000800 loss 0.0039 (0.0042) acc@1 1.0000 (0.9996) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 03:03:46] __main__ INFO: \u001b[0mEpoch 30 Step 300/351 lr 0.000800 loss 0.0039 (0.0045) acc@1 1.0000 (0.9995) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 03:04:44] __main__ INFO: \u001b[0mEpoch 30 Step 351/351 lr 0.000800 loss 0.0048 (0.0045) acc@1 1.0000 (0.9995) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 03:04:44] __main__ INFO: \u001b[0mElapsed 394.58\n",
      "\u001b[32m[2020-06-23 03:04:44] __main__ INFO: \u001b[0mVal 30\n",
      "\u001b[32m[2020-06-23 03:04:57] __main__ INFO: \u001b[0mEpoch 30 loss 0.2878 acc@1 0.9352 acc@5 0.9964\n",
      "\u001b[32m[2020-06-23 03:04:57] __main__ INFO: \u001b[0mElapsed 13.24\n",
      "\u001b[32m[2020-06-23 03:04:57] __main__ INFO: \u001b[0mTrain 31 10530\n",
      "\u001b[32m[2020-06-23 03:06:50] __main__ INFO: \u001b[0mEpoch 31 Step 100/351 lr 0.000800 loss 0.0022 (0.0046) acc@1 1.0000 (0.9995) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 03:08:42] __main__ INFO: \u001b[0mEpoch 31 Step 200/351 lr 0.000800 loss 0.0041 (0.0048) acc@1 1.0000 (0.9994) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 03:10:34] __main__ INFO: \u001b[0mEpoch 31 Step 300/351 lr 0.000800 loss 0.0048 (0.0050) acc@1 1.0000 (0.9993) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 03:11:32] __main__ INFO: \u001b[0mEpoch 31 Step 351/351 lr 0.000800 loss 0.0027 (0.0050) acc@1 1.0000 (0.9993) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 03:11:32] __main__ INFO: \u001b[0mElapsed 394.65\n",
      "\u001b[32m[2020-06-23 03:11:32] __main__ INFO: \u001b[0mVal 31\n",
      "\u001b[32m[2020-06-23 03:11:45] __main__ INFO: \u001b[0mEpoch 31 loss 0.2786 acc@1 0.9382 acc@5 0.9964\n",
      "\u001b[32m[2020-06-23 03:11:45] __main__ INFO: \u001b[0mElapsed 13.25\n",
      "\u001b[32m[2020-06-23 03:11:45] __main__ INFO: \u001b[0mTrain 32 10881\n",
      "\u001b[32m[2020-06-23 03:13:37] __main__ INFO: \u001b[0mEpoch 32 Step 100/351 lr 0.000800 loss 0.0059 (0.0048) acc@1 1.0000 (0.9995) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 03:15:30] __main__ INFO: \u001b[0mEpoch 32 Step 200/351 lr 0.000800 loss 0.0030 (0.0047) acc@1 1.0000 (0.9995) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 03:17:22] __main__ INFO: \u001b[0mEpoch 32 Step 300/351 lr 0.000800 loss 0.0031 (0.0045) acc@1 1.0000 (0.9996) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 03:18:19] __main__ INFO: \u001b[0mEpoch 32 Step 351/351 lr 0.000800 loss 0.0077 (0.0045) acc@1 1.0000 (0.9996) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 03:18:20] __main__ INFO: \u001b[0mElapsed 394.54\n",
      "\u001b[32m[2020-06-23 03:18:20] __main__ INFO: \u001b[0mVal 32\n",
      "\u001b[32m[2020-06-23 03:18:33] __main__ INFO: \u001b[0mEpoch 32 loss 0.2802 acc@1 0.9366 acc@5 0.9970\n",
      "\u001b[32m[2020-06-23 03:18:33] __main__ INFO: \u001b[0mElapsed 13.29\n",
      "\u001b[32m[2020-06-23 03:18:33] __main__ INFO: \u001b[0mTrain 33 11232\n",
      "\u001b[32m[2020-06-23 03:20:25] __main__ INFO: \u001b[0mEpoch 33 Step 100/351 lr 0.000800 loss 0.0023 (0.0042) acc@1 1.0000 (0.9996) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 03:22:18] __main__ INFO: \u001b[0mEpoch 33 Step 200/351 lr 0.000800 loss 0.0044 (0.0046) acc@1 1.0000 (0.9994) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 03:24:10] __main__ INFO: \u001b[0mEpoch 33 Step 300/351 lr 0.000800 loss 0.0031 (0.0044) acc@1 1.0000 (0.9995) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 03:25:07] __main__ INFO: \u001b[0mEpoch 33 Step 351/351 lr 0.000800 loss 0.0019 (0.0047) acc@1 1.0000 (0.9994) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 03:25:07] __main__ INFO: \u001b[0mElapsed 394.65\n",
      "\u001b[32m[2020-06-23 03:25:07] __main__ INFO: \u001b[0mVal 33\n",
      "\u001b[32m[2020-06-23 03:25:21] __main__ INFO: \u001b[0mEpoch 33 loss 0.2763 acc@1 0.9380 acc@5 0.9976\n",
      "\u001b[32m[2020-06-23 03:25:21] __main__ INFO: \u001b[0mElapsed 13.26\n",
      "\u001b[32m[2020-06-23 03:25:21] __main__ INFO: \u001b[0mTrain 34 11583\n",
      "\u001b[32m[2020-06-23 03:27:13] __main__ INFO: \u001b[0mEpoch 34 Step 100/351 lr 0.000800 loss 0.0052 (0.0045) acc@1 1.0000 (0.9995) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 03:29:06] __main__ INFO: \u001b[0mEpoch 34 Step 200/351 lr 0.000800 loss 0.0054 (0.0046) acc@1 1.0000 (0.9994) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 03:30:58] __main__ INFO: \u001b[0mEpoch 34 Step 300/351 lr 0.000800 loss 0.0022 (0.0045) acc@1 1.0000 (0.9994) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 03:31:55] __main__ INFO: \u001b[0mEpoch 34 Step 351/351 lr 0.000800 loss 0.0061 (0.0044) acc@1 1.0000 (0.9995) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 03:31:55] __main__ INFO: \u001b[0mElapsed 394.73\n",
      "\u001b[32m[2020-06-23 03:31:55] __main__ INFO: \u001b[0mVal 34\n",
      "\u001b[32m[2020-06-23 03:32:09] __main__ INFO: \u001b[0mEpoch 34 loss 0.2865 acc@1 0.9352 acc@5 0.9974\n",
      "\u001b[32m[2020-06-23 03:32:09] __main__ INFO: \u001b[0mElapsed 13.29\n",
      "\u001b[32m[2020-06-23 03:32:09] __main__ INFO: \u001b[0mTrain 35 11934\n",
      "\u001b[32m[2020-06-23 03:34:01] __main__ INFO: \u001b[0mEpoch 35 Step 100/351 lr 0.000800 loss 0.0024 (0.0041) acc@1 1.0000 (0.9994) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 03:35:54] __main__ INFO: \u001b[0mEpoch 35 Step 200/351 lr 0.000800 loss 0.0021 (0.0043) acc@1 1.0000 (0.9994) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 03:37:46] __main__ INFO: \u001b[0mEpoch 35 Step 300/351 lr 0.000800 loss 0.0021 (0.0045) acc@1 1.0000 (0.9993) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 03:38:43] __main__ INFO: \u001b[0mEpoch 35 Step 351/351 lr 0.000800 loss 0.0024 (0.0048) acc@1 1.0000 (0.9992) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 03:38:43] __main__ INFO: \u001b[0mElapsed 394.65\n",
      "\u001b[32m[2020-06-23 03:38:43] __main__ INFO: \u001b[0mVal 35\n",
      "\u001b[32m[2020-06-23 03:38:57] __main__ INFO: \u001b[0mEpoch 35 loss 0.2875 acc@1 0.9348 acc@5 0.9968\n",
      "\u001b[32m[2020-06-23 03:38:57] __main__ INFO: \u001b[0mElapsed 13.27\n",
      "\u001b[32m[2020-06-23 03:38:57] __main__ INFO: \u001b[0mTrain 36 12285\n",
      "\u001b[32m[2020-06-23 03:40:49] __main__ INFO: \u001b[0mEpoch 36 Step 100/351 lr 0.000800 loss 0.0027 (0.0041) acc@1 1.0000 (0.9995) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 03:42:42] __main__ INFO: \u001b[0mEpoch 36 Step 200/351 lr 0.000800 loss 0.0026 (0.0045) acc@1 1.0000 (0.9993) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 03:44:34] __main__ INFO: \u001b[0mEpoch 36 Step 300/351 lr 0.000800 loss 0.0031 (0.0047) acc@1 1.0000 (0.9993) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 03:45:31] __main__ INFO: \u001b[0mEpoch 36 Step 351/351 lr 0.000800 loss 0.0022 (0.0051) acc@1 1.0000 (0.9991) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 03:45:31] __main__ INFO: \u001b[0mElapsed 394.73\n",
      "\u001b[32m[2020-06-23 03:45:31] __main__ INFO: \u001b[0mVal 36\n",
      "\u001b[32m[2020-06-23 03:45:45] __main__ INFO: \u001b[0mEpoch 36 loss 0.2923 acc@1 0.9376 acc@5 0.9976\n",
      "\u001b[32m[2020-06-23 03:45:45] __main__ INFO: \u001b[0mElapsed 13.28\n",
      "\u001b[32m[2020-06-23 03:45:45] __main__ INFO: \u001b[0mTrain 37 12636\n",
      "\u001b[32m[2020-06-23 03:47:37] __main__ INFO: \u001b[0mEpoch 37 Step 100/351 lr 0.000800 loss 0.0023 (0.0042) acc@1 1.0000 (0.9995) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 03:49:30] __main__ INFO: \u001b[0mEpoch 37 Step 200/351 lr 0.000800 loss 0.0027 (0.0040) acc@1 1.0000 (0.9996) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 03:51:22] __main__ INFO: \u001b[0mEpoch 37 Step 300/351 lr 0.000800 loss 0.0024 (0.0041) acc@1 1.0000 (0.9996) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 03:52:19] __main__ INFO: \u001b[0mEpoch 37 Step 351/351 lr 0.000800 loss 0.0056 (0.0042) acc@1 1.0000 (0.9996) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 03:52:19] __main__ INFO: \u001b[0mElapsed 394.61\n",
      "\u001b[32m[2020-06-23 03:52:19] __main__ INFO: \u001b[0mVal 37\n",
      "\u001b[32m[2020-06-23 03:52:33] __main__ INFO: \u001b[0mEpoch 37 loss 0.2971 acc@1 0.9350 acc@5 0.9972\n",
      "\u001b[32m[2020-06-23 03:52:33] __main__ INFO: \u001b[0mElapsed 13.29\n",
      "\u001b[32m[2020-06-23 03:52:33] __main__ INFO: \u001b[0mTrain 38 12987\n",
      "\u001b[32m[2020-06-23 03:54:25] __main__ INFO: \u001b[0mEpoch 38 Step 100/351 lr 0.000800 loss 0.0022 (0.0040) acc@1 1.0000 (0.9995) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 03:56:18] __main__ INFO: \u001b[0mEpoch 38 Step 200/351 lr 0.000800 loss 0.0026 (0.0040) acc@1 1.0000 (0.9995) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 03:58:10] __main__ INFO: \u001b[0mEpoch 38 Step 300/351 lr 0.000800 loss 0.0063 (0.0041) acc@1 1.0000 (0.9994) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 03:59:07] __main__ INFO: \u001b[0mEpoch 38 Step 351/351 lr 0.000800 loss 0.0036 (0.0040) acc@1 1.0000 (0.9994) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 03:59:07] __main__ INFO: \u001b[0mElapsed 394.72\n",
      "\u001b[32m[2020-06-23 03:59:07] __main__ INFO: \u001b[0mVal 38\n",
      "\u001b[32m[2020-06-23 03:59:21] __main__ INFO: \u001b[0mEpoch 38 loss 0.2922 acc@1 0.9380 acc@5 0.9976\n",
      "\u001b[32m[2020-06-23 03:59:21] __main__ INFO: \u001b[0mElapsed 13.29\n",
      "\u001b[32m[2020-06-23 03:59:21] __main__ INFO: \u001b[0mTrain 39 13338\n",
      "\u001b[32m[2020-06-23 04:01:13] __main__ INFO: \u001b[0mEpoch 39 Step 100/351 lr 0.000800 loss 0.0028 (0.0044) acc@1 1.0000 (0.9995) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 04:03:06] __main__ INFO: \u001b[0mEpoch 39 Step 200/351 lr 0.000800 loss 0.0191 (0.0042) acc@1 0.9922 (0.9995) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 04:04:58] __main__ INFO: \u001b[0mEpoch 39 Step 300/351 lr 0.000800 loss 0.0027 (0.0039) acc@1 1.0000 (0.9996) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 04:05:55] __main__ INFO: \u001b[0mEpoch 39 Step 351/351 lr 0.000800 loss 0.0023 (0.0039) acc@1 1.0000 (0.9996) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 04:05:55] __main__ INFO: \u001b[0mElapsed 394.69\n",
      "\u001b[32m[2020-06-23 04:05:55] __main__ INFO: \u001b[0mVal 39\n",
      "\u001b[32m[2020-06-23 04:06:09] __main__ INFO: \u001b[0mEpoch 39 loss 0.2889 acc@1 0.9386 acc@5 0.9974\n",
      "\u001b[32m[2020-06-23 04:06:09] __main__ INFO: \u001b[0mElapsed 13.27\n",
      "\u001b[32m[2020-06-23 04:06:09] __main__ INFO: \u001b[0mTrain 40 13689\n",
      "\u001b[32m[2020-06-23 04:08:01] __main__ INFO: \u001b[0mEpoch 40 Step 100/351 lr 0.000800 loss 0.0086 (0.0041) acc@1 1.0000 (0.9995) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 04:09:54] __main__ INFO: \u001b[0mEpoch 40 Step 200/351 lr 0.000800 loss 0.0019 (0.0042) acc@1 1.0000 (0.9994) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 04:11:46] __main__ INFO: \u001b[0mEpoch 40 Step 300/351 lr 0.000800 loss 0.0106 (0.0041) acc@1 0.9922 (0.9995) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 04:12:43] __main__ INFO: \u001b[0mEpoch 40 Step 351/351 lr 0.000800 loss 0.0127 (0.0041) acc@1 0.9922 (0.9994) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 04:12:43] __main__ INFO: \u001b[0mElapsed 394.84\n",
      "\u001b[32m[2020-06-23 04:12:43] __main__ INFO: \u001b[0mVal 40\n",
      "\u001b[32m[2020-06-23 04:12:57] __main__ INFO: \u001b[0mEpoch 40 loss 0.2954 acc@1 0.9358 acc@5 0.9966\n",
      "\u001b[32m[2020-06-23 04:12:57] __main__ INFO: \u001b[0mElapsed 13.32\n",
      "\u001b[32m[2020-06-23 04:12:57] __main__ INFO: \u001b[0mTrain 41 14040\n",
      "\u001b[32m[2020-06-23 04:14:49] __main__ INFO: \u001b[0mEpoch 41 Step 100/351 lr 0.000800 loss 0.0021 (0.0046) acc@1 1.0000 (0.9991) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 04:16:42] __main__ INFO: \u001b[0mEpoch 41 Step 200/351 lr 0.000800 loss 0.0050 (0.0041) acc@1 1.0000 (0.9994) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 04:18:34] __main__ INFO: \u001b[0mEpoch 41 Step 300/351 lr 0.000800 loss 0.0020 (0.0041) acc@1 1.0000 (0.9994) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 04:19:32] __main__ INFO: \u001b[0mEpoch 41 Step 351/351 lr 0.000800 loss 0.0024 (0.0043) acc@1 1.0000 (0.9994) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 04:19:32] __main__ INFO: \u001b[0mElapsed 394.84\n",
      "\u001b[32m[2020-06-23 04:19:32] __main__ INFO: \u001b[0mVal 41\n",
      "\u001b[32m[2020-06-23 04:19:45] __main__ INFO: \u001b[0mEpoch 41 loss 0.2925 acc@1 0.9352 acc@5 0.9968\n",
      "\u001b[32m[2020-06-23 04:19:45] __main__ INFO: \u001b[0mElapsed 13.25\n",
      "\u001b[32m[2020-06-23 04:19:45] __main__ INFO: \u001b[0mTrain 42 14391\n",
      "\u001b[32m[2020-06-23 04:21:37] __main__ INFO: \u001b[0mEpoch 42 Step 100/351 lr 0.000800 loss 0.0033 (0.0041) acc@1 1.0000 (0.9995) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 04:23:30] __main__ INFO: \u001b[0mEpoch 42 Step 200/351 lr 0.000800 loss 0.0020 (0.0038) acc@1 1.0000 (0.9995) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 04:25:22] __main__ INFO: \u001b[0mEpoch 42 Step 300/351 lr 0.000800 loss 0.0044 (0.0038) acc@1 1.0000 (0.9996) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 04:26:20] __main__ INFO: \u001b[0mEpoch 42 Step 351/351 lr 0.000800 loss 0.0044 (0.0039) acc@1 1.0000 (0.9995) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 04:26:20] __main__ INFO: \u001b[0mElapsed 394.94\n",
      "\u001b[32m[2020-06-23 04:26:20] __main__ INFO: \u001b[0mVal 42\n",
      "\u001b[32m[2020-06-23 04:26:33] __main__ INFO: \u001b[0mEpoch 42 loss 0.2860 acc@1 0.9350 acc@5 0.9968\n",
      "\u001b[32m[2020-06-23 04:26:33] __main__ INFO: \u001b[0mElapsed 13.27\n",
      "\u001b[32m[2020-06-23 04:26:33] __main__ INFO: \u001b[0mTrain 43 14742\n",
      "\u001b[32m[2020-06-23 04:28:26] __main__ INFO: \u001b[0mEpoch 43 Step 100/351 lr 0.000800 loss 0.0035 (0.0032) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 04:30:18] __main__ INFO: \u001b[0mEpoch 43 Step 200/351 lr 0.000800 loss 0.0017 (0.0034) acc@1 1.0000 (0.9996) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 04:32:11] __main__ INFO: \u001b[0mEpoch 43 Step 300/351 lr 0.000800 loss 0.0029 (0.0034) acc@1 1.0000 (0.9996) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 04:33:08] __main__ INFO: \u001b[0mEpoch 43 Step 351/351 lr 0.000800 loss 0.0018 (0.0034) acc@1 1.0000 (0.9996) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 04:33:08] __main__ INFO: \u001b[0mElapsed 394.93\n",
      "\u001b[32m[2020-06-23 04:33:08] __main__ INFO: \u001b[0mVal 43\n",
      "\u001b[32m[2020-06-23 04:33:21] __main__ INFO: \u001b[0mEpoch 43 loss 0.2944 acc@1 0.9366 acc@5 0.9964\n",
      "\u001b[32m[2020-06-23 04:33:21] __main__ INFO: \u001b[0mElapsed 13.29\n",
      "\u001b[32m[2020-06-23 04:33:21] __main__ INFO: \u001b[0mTrain 44 15093\n",
      "\u001b[32m[2020-06-23 04:35:14] __main__ INFO: \u001b[0mEpoch 44 Step 100/351 lr 0.000800 loss 0.0029 (0.0034) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 04:37:06] __main__ INFO: \u001b[0mEpoch 44 Step 200/351 lr 0.000800 loss 0.0023 (0.0036) acc@1 1.0000 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 04:38:59] __main__ INFO: \u001b[0mEpoch 44 Step 300/351 lr 0.000800 loss 0.0026 (0.0038) acc@1 1.0000 (0.9996) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 04:39:56] __main__ INFO: \u001b[0mEpoch 44 Step 351/351 lr 0.000800 loss 0.0032 (0.0038) acc@1 1.0000 (0.9996) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 04:39:56] __main__ INFO: \u001b[0mElapsed 394.92\n",
      "\u001b[32m[2020-06-23 04:39:56] __main__ INFO: \u001b[0mVal 44\n",
      "\u001b[32m[2020-06-23 04:40:09] __main__ INFO: \u001b[0mEpoch 44 loss 0.2935 acc@1 0.9350 acc@5 0.9954\n",
      "\u001b[32m[2020-06-23 04:40:09] __main__ INFO: \u001b[0mElapsed 13.30\n",
      "\u001b[32m[2020-06-23 04:40:09] __main__ INFO: \u001b[0mTrain 45 15444\n",
      "\u001b[32m[2020-06-23 04:42:02] __main__ INFO: \u001b[0mEpoch 45 Step 100/351 lr 0.000800 loss 0.0017 (0.0040) acc@1 1.0000 (0.9995) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 04:43:55] __main__ INFO: \u001b[0mEpoch 45 Step 200/351 lr 0.000800 loss 0.0051 (0.0041) acc@1 1.0000 (0.9995) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 04:45:47] __main__ INFO: \u001b[0mEpoch 45 Step 300/351 lr 0.000800 loss 0.0021 (0.0041) acc@1 1.0000 (0.9995) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 04:46:45] __main__ INFO: \u001b[0mEpoch 45 Step 351/351 lr 0.000800 loss 0.0023 (0.0040) acc@1 1.0000 (0.9995) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 04:46:45] __main__ INFO: \u001b[0mElapsed 395.08\n",
      "\u001b[32m[2020-06-23 04:46:45] __main__ INFO: \u001b[0mVal 45\n",
      "\u001b[32m[2020-06-23 04:46:58] __main__ INFO: \u001b[0mEpoch 45 loss 0.2826 acc@1 0.9366 acc@5 0.9966\n",
      "\u001b[32m[2020-06-23 04:46:58] __main__ INFO: \u001b[0mElapsed 13.29\n",
      "\u001b[32m[2020-06-23 04:46:58] __main__ INFO: \u001b[0mTrain 46 15795\n",
      "\u001b[32m[2020-06-23 04:48:50] __main__ INFO: \u001b[0mEpoch 46 Step 100/351 lr 0.000800 loss 0.0025 (0.0032) acc@1 1.0000 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 04:50:43] __main__ INFO: \u001b[0mEpoch 46 Step 200/351 lr 0.000800 loss 0.0022 (0.0039) acc@1 1.0000 (0.9994) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 04:52:36] __main__ INFO: \u001b[0mEpoch 46 Step 300/351 lr 0.000800 loss 0.0021 (0.0042) acc@1 1.0000 (0.9993) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 04:53:33] __main__ INFO: \u001b[0mEpoch 46 Step 351/351 lr 0.000800 loss 0.0043 (0.0042) acc@1 1.0000 (0.9993) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 04:53:33] __main__ INFO: \u001b[0mElapsed 395.08\n",
      "\u001b[32m[2020-06-23 04:53:33] __main__ INFO: \u001b[0mVal 46\n",
      "\u001b[32m[2020-06-23 04:53:46] __main__ INFO: \u001b[0mEpoch 46 loss 0.3060 acc@1 0.9334 acc@5 0.9968\n",
      "\u001b[32m[2020-06-23 04:53:46] __main__ INFO: \u001b[0mElapsed 13.29\n",
      "\u001b[32m[2020-06-23 04:53:46] __main__ INFO: \u001b[0mTrain 47 16146\n",
      "\u001b[32m[2020-06-23 04:55:39] __main__ INFO: \u001b[0mEpoch 47 Step 100/351 lr 0.000800 loss 0.0043 (0.0042) acc@1 1.0000 (0.9994) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 04:57:31] __main__ INFO: \u001b[0mEpoch 47 Step 200/351 lr 0.000800 loss 0.0022 (0.0041) acc@1 1.0000 (0.9993) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 04:59:24] __main__ INFO: \u001b[0mEpoch 47 Step 300/351 lr 0.000800 loss 0.0041 (0.0043) acc@1 1.0000 (0.9994) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 05:00:21] __main__ INFO: \u001b[0mEpoch 47 Step 351/351 lr 0.000800 loss 0.0020 (0.0041) acc@1 1.0000 (0.9994) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 05:00:21] __main__ INFO: \u001b[0mElapsed 395.02\n",
      "\u001b[32m[2020-06-23 05:00:21] __main__ INFO: \u001b[0mVal 47\n",
      "\u001b[32m[2020-06-23 05:00:35] __main__ INFO: \u001b[0mEpoch 47 loss 0.2955 acc@1 0.9374 acc@5 0.9966\n",
      "\u001b[32m[2020-06-23 05:00:35] __main__ INFO: \u001b[0mElapsed 13.28\n",
      "\u001b[32m[2020-06-23 05:00:35] __main__ INFO: \u001b[0mTrain 48 16497\n",
      "\u001b[32m[2020-06-23 05:02:27] __main__ INFO: \u001b[0mEpoch 48 Step 100/351 lr 0.000800 loss 0.0018 (0.0038) acc@1 1.0000 (0.9995) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 05:04:20] __main__ INFO: \u001b[0mEpoch 48 Step 200/351 lr 0.000800 loss 0.0024 (0.0042) acc@1 1.0000 (0.9993) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 05:06:12] __main__ INFO: \u001b[0mEpoch 48 Step 300/351 lr 0.000800 loss 0.0210 (0.0044) acc@1 0.9844 (0.9992) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 05:07:10] __main__ INFO: \u001b[0mEpoch 48 Step 351/351 lr 0.000800 loss 0.0054 (0.0044) acc@1 1.0000 (0.9992) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 05:07:10] __main__ INFO: \u001b[0mElapsed 395.10\n",
      "\u001b[32m[2020-06-23 05:07:10] __main__ INFO: \u001b[0mVal 48\n",
      "\u001b[32m[2020-06-23 05:07:23] __main__ INFO: \u001b[0mEpoch 48 loss 0.3007 acc@1 0.9364 acc@5 0.9960\n",
      "\u001b[32m[2020-06-23 05:07:23] __main__ INFO: \u001b[0mElapsed 13.29\n",
      "\u001b[32m[2020-06-23 05:07:23] __main__ INFO: \u001b[0mTrain 49 16848\n",
      "\u001b[32m[2020-06-23 05:09:16] __main__ INFO: \u001b[0mEpoch 49 Step 100/351 lr 0.000800 loss 0.0020 (0.0037) acc@1 1.0000 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 05:11:08] __main__ INFO: \u001b[0mEpoch 49 Step 200/351 lr 0.000800 loss 0.0055 (0.0047) acc@1 1.0000 (0.9992) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 05:13:01] __main__ INFO: \u001b[0mEpoch 49 Step 300/351 lr 0.000800 loss 0.0023 (0.0046) acc@1 1.0000 (0.9993) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 05:13:58] __main__ INFO: \u001b[0mEpoch 49 Step 351/351 lr 0.000800 loss 0.0025 (0.0046) acc@1 1.0000 (0.9993) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 05:13:58] __main__ INFO: \u001b[0mElapsed 395.03\n",
      "\u001b[32m[2020-06-23 05:13:58] __main__ INFO: \u001b[0mVal 49\n",
      "\u001b[32m[2020-06-23 05:14:11] __main__ INFO: \u001b[0mEpoch 49 loss 0.2973 acc@1 0.9372 acc@5 0.9966\n",
      "\u001b[32m[2020-06-23 05:14:11] __main__ INFO: \u001b[0mElapsed 13.30\n",
      "\u001b[32m[2020-06-23 05:14:11] __main__ INFO: \u001b[0mTrain 50 17199\n",
      "\u001b[32m[2020-06-23 05:16:04] __main__ INFO: \u001b[0mEpoch 50 Step 100/351 lr 0.000800 loss 0.0024 (0.0047) acc@1 1.0000 (0.9992) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 05:17:56] __main__ INFO: \u001b[0mEpoch 50 Step 200/351 lr 0.000800 loss 0.0043 (0.0043) acc@1 1.0000 (0.9995) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 05:19:49] __main__ INFO: \u001b[0mEpoch 50 Step 300/351 lr 0.000800 loss 0.0111 (0.0043) acc@1 0.9922 (0.9994) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 05:20:46] __main__ INFO: \u001b[0mEpoch 50 Step 351/351 lr 0.000800 loss 0.0024 (0.0042) acc@1 1.0000 (0.9994) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 05:20:46] __main__ INFO: \u001b[0mElapsed 395.08\n",
      "\u001b[32m[2020-06-23 05:20:46] __main__ INFO: \u001b[0mVal 50\n",
      "\u001b[32m[2020-06-23 05:21:00] __main__ INFO: \u001b[0mEpoch 50 loss 0.2960 acc@1 0.9376 acc@5 0.9966\n",
      "\u001b[32m[2020-06-23 05:21:00] __main__ INFO: \u001b[0mElapsed 13.27\n",
      "\u001b[32m[2020-06-23 05:21:00] fvcore.common.checkpoint INFO: \u001b[0mSaving checkpoint to /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_5/exp00_resume400_50/checkpoint_00050.pth\n"
     ]
    }
   ],
   "source": [
    "# Resume training with the un-augmented data\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "!python train.py --config /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_3_20/exp00/config.yaml \\\n",
    "    train.checkpoint /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_3_20/exp00/checkpoint_00400.pth \\\n",
    "    model.wrn.depth 28 \\\n",
    "    model.wrn.widening_factor 10 \\\n",
    "    train.batch_size 128 \\\n",
    "    train.base_lr .0008 \\\n",
    "    dataset.name CIFAR10 \\\n",
    "    train.output_dir /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_3_20/exp00_resume400_50 \\\n",
    "    scheduler.epochs 50\n",
    "\n",
    "#### Set LEARNING RATE based on ending LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume training with the un-augmented data\n",
    "# os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "# #!python train.py --config configs/cifar/resnet.yaml \\\n",
    "# !python train.py --config /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_5/exp00/config.yaml \\\n",
    "#     train.checkpoint /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_5/exp00/checkpoint_00300.pth \\\n",
    "#     dataset.name CIFAR10 \\\n",
    "#     train.base_lr .001 \\\n",
    "#     train.output_dir /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_5/exp00_resume300_150 \\\n",
    "#     scheduler.epochs 150\n",
    "\n",
    "#### Set LEARNING RATE based on ending LR\n",
    "#    train.resume True \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-23 12:42:38] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_5/exp00_resume400_50/checkpoint_00050.pth\n",
      "Files already downloaded and verified\n",
      "100%|| 40/40 [00:51<00:00,  1.28s/it]\n",
      "\u001b[32m[2020-06-23 12:43:30] __main__ INFO: \u001b[0mElapsed 51.36\n",
      "\u001b[32m[2020-06-23 12:43:30] __main__ INFO: \u001b[0mLoss 0.2627 Accuracy 0.9403\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/wrn.yaml \\\n",
    "    model.wrn.depth 28 \\\n",
    "    model.wrn.widening_factor 10 \\\n",
    "    test.checkpoint /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_3_20/exp00_resume400_50/checkpoint_00050.pth \\\n",
    "    test.output_dir /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_3_20/exp00_resume400_50/test_results_0050_cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-20 16:32:43] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_5/exp00_resume300_150/checkpoint_00150.pth\n",
      "Files already downloaded and verified\n",
      "100%|| 79/79 [00:02<00:00, 26.53it/s]\n",
      "\u001b[32m[2020-06-20 16:32:46] __main__ INFO: \u001b[0mElapsed 2.98\n",
      "\u001b[32m[2020-06-20 16:32:46] __main__ INFO: \u001b[0mLoss 0.4499 Accuracy 0.8795\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "# !python evaluate.py --config configs/cifar/wrn.yaml \\\n",
    "#     model.wrn.depth 28 \\\n",
    "#     model.wrn.widening_factor 10 \\\n",
    "#     train.batch_size 128 \\\n",
    "#     train.base_lr 0.00001 \\\n",
    "#     test.checkpoint /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_5/exp00_resume300_150/checkpoint_00150.pth \\\n",
    "#     test.output_dir /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_5/exp00_resume300_150/test_results_0150_cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-23 12:47:22] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_5//exp00_resume400_50/checkpoint_00050.pth\n",
      "CIFAR 10.1\n",
      "100%|| 8/8 [00:10<00:00,  1.34s/it]\n",
      "\u001b[32m[2020-06-23 12:47:33] __main__ INFO: \u001b[0mElapsed 10.69\n",
      "\u001b[32m[2020-06-23 12:47:33] __main__ INFO: \u001b[0mLoss 0.6479 Accuracy 0.8520\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10.1 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/wrn.yaml \\\n",
    "    model.wrn.depth 28 \\\n",
    "    model.wrn.widening_factor 10 \\\n",
    "    dataset.name CIFAR101 \\\n",
    "    test.checkpoint /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_3_20//exp00_resume400_50/checkpoint_00050.pth \\\n",
    "    test.output_dir /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_3_20/exp00_resume400_50/test_results_0050_cifar101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-20 16:33:19] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_5/exp00_resume300_150/checkpoint_00150.pth\n",
      "CIFAR 10.1\n",
      "100%|| 16/16 [00:00<00:00, 17.16it/s]\n",
      "\u001b[32m[2020-06-20 16:33:20] __main__ INFO: \u001b[0mElapsed 0.93\n",
      "\u001b[32m[2020-06-20 16:33:20] __main__ INFO: \u001b[0mLoss 0.8206 Accuracy 0.7710\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10.1 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "# !python evaluate.py --config configs/cifar/rwrn.yaml \\\n",
    "#     model.wrn.depth 28 \\\n",
    "#     model.wrn.widening_factor 10 \\\n",
    "#     dataset.name CIFAR101 \\\n",
    "#     test.checkpoint /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_5/exp00_resume300_150/checkpoint_00150.pth \\\n",
    "#     test.output_dir /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_5/exp00_resume300_150/test_results_0150_cifar101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-23 13:04:05] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_5/exp00/checkpoint_00400.pth\n",
      "CIFAR 10.1\n",
      "100%|| 8/8 [00:10<00:00,  1.33s/it]\n",
      "\u001b[32m[2020-06-23 13:04:16] __main__ INFO: \u001b[0mElapsed 10.63\n",
      "\u001b[32m[2020-06-23 13:04:16] __main__ INFO: \u001b[0mLoss 0.8676 Accuracy 0.8145\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/wrn.yaml \\\n",
    "    model.wrn.depth 28 \\\n",
    "    model.wrn.widening_factor 10 \\\n",
    "    dataset.name CIFAR101 \\\n",
    "    test.checkpoint /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_5/exp00/checkpoint_00400.pth \\\n",
    "    test.output_dir /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_5/exp00/test_results_0400_cifar101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-23 13:07:02] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_5/exp00/checkpoint_00300.pth\n",
      "Files already downloaded and verified\n",
      "100%|| 40/40 [00:51<00:00,  1.28s/it]\n",
      "\u001b[32m[2020-06-23 13:07:54] __main__ INFO: \u001b[0mElapsed 51.38\n",
      "\u001b[32m[2020-06-23 13:07:54] __main__ INFO: \u001b[0mLoss 0.3800 Accuracy 0.9128\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/wrn.yaml \\\n",
    "    model.wrn.depth 28 \\\n",
    "    model.wrn.widening_factor 10 \\\n",
    "    test.checkpoint /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_3_20/exp00/checkpoint_00300.pth \\\n",
    "    test.output_dir /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_3_20/exp00/test_results_0300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-23 13:09:51] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_5/exp00/checkpoint_00400.pth\n",
      "Files already downloaded and verified\n",
      "100%|| 40/40 [00:51<00:00,  1.29s/it]\n",
      "\u001b[32m[2020-06-23 13:10:44] __main__ INFO: \u001b[0mElapsed 51.42\n",
      "\u001b[32m[2020-06-23 13:10:44] __main__ INFO: \u001b[0mLoss 0.3942 Accuracy 0.9119\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10.1 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "!python evaluate.py --config configs/cifar/wrn.yaml \\\n",
    "    model.wrn.depth 28 \\\n",
    "    model.wrn.widening_factor 10 \\\n",
    "    test.checkpoint /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_3_20/exp00/checkpoint_00400.pth \\\n",
    "    test.output_dir /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_3_20/exp00/test_results_0400_cifar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-23 13:13:45] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_5/exp00/checkpoint_00300.pth\n",
      "CIFAR 10.1\n",
      "100%|| 8/8 [00:10<00:00,  1.34s/it]\n",
      "\u001b[32m[2020-06-23 13:13:56] __main__ INFO: \u001b[0mElapsed 10.69\n",
      "\u001b[32m[2020-06-23 13:13:56] __main__ INFO: \u001b[0mLoss 0.8150 Accuracy 0.8070\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10.1 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "!python evaluate.py --config configs/cifar/wrn.yaml \\\n",
    "    model.wrn.depth 28 \\\n",
    "    model.wrn.widening_factor 10 \\\n",
    "    test.checkpoint /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_3_20/exp00/checkpoint_00300.pth \\\n",
    "    dataset.name CIFAR101 \\\n",
    "    test.output_dir /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_3_20/exp00/test_results_0300_cifar101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Testset</th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Original_Accuracy</th>\n",
       "      <th>Original_CI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wrn_28_10</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>100</td>\n",
       "      <td>0.2299</td>\n",
       "      <td>0.9311</td>\n",
       "      <td>95.9</td>\n",
       "      <td>(95.5, 96.3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wrn_28_10</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>200</td>\n",
       "      <td>0.1760</td>\n",
       "      <td>0.9578</td>\n",
       "      <td>95.9</td>\n",
       "      <td>(95.5, 96.3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wrn_28_10</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.3896</td>\n",
       "      <td>0.8975</td>\n",
       "      <td>89.7</td>\n",
       "      <td>(88.3, 91.0)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Model    Testset  Epoch    Loss  Accuracy  Original_Accuracy  \\\n",
       "0  wrn_28_10    cifar10    100  0.2299    0.9311               95.9   \n",
       "1  wrn_28_10    cifar10    200  0.1760    0.9578               95.9   \n",
       "2  wrn_28_10  cifar10.1    200  0.3896    0.8975               89.7   \n",
       "\n",
       "    Original_CI  \n",
       "0  (95.5, 96.3)  \n",
       "1  (95.5, 96.3)  \n",
       "2  (88.3, 91.0)  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write the results to a CSV file so that we can analyze later.\n",
    "import pandas as pd\n",
    "\n",
    "results = {'Model': ['wrn_28_10', 'wrn_28_10', 'wrn_28_10'],\n",
    "           'Testset': ['cifar10', 'cifar10', 'cifar10.1'],\n",
    "           'Epoch': [100, 200, 200],\n",
    "           'Loss': [0.2299, 0.1760, 0.3896],\n",
    "           'Accuracy': [0.9311, 0.9578, 0.8975],\n",
    "           'Original_Accuracy': [95.9, 95.9, 89.7],\n",
    "           'Original_CI': [(95.5, 96.3), (95.5, 96.3), (88.3, 91.0)]\n",
    "           }\n",
    "\n",
    "df = pd.DataFrame(results, columns = ['Model', 'Testset', 'Epoch', 'Loss', 'Accuracy', \n",
    "                                      'Original_Accuracy', 'Original_CI'])\n",
    "\n",
    "df.to_csv('/home/ec2-user/SageMaker/experiments/wrn_28_10/exp00/results.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Testset</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Original_Accuracy</th>\n",
       "      <th>Original_CI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wrn_28_10_ra_2_5</td>\n",
       "      <td>400</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.3942</td>\n",
       "      <td>0.9119</td>\n",
       "      <td>95.9</td>\n",
       "      <td>(95.5, 96.3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wrn_28_10_ra_2_5</td>\n",
       "      <td>300</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.9128</td>\n",
       "      <td>95.9</td>\n",
       "      <td>(95.5, 96.3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wrn_28_10_ra_2_5</td>\n",
       "      <td>400</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>0.8676</td>\n",
       "      <td>0.8145</td>\n",
       "      <td>89.7</td>\n",
       "      <td>(88.3, 91.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wrn_28_10_ra_2_5</td>\n",
       "      <td>300</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>0.815</td>\n",
       "      <td>0.807</td>\n",
       "      <td>89.7</td>\n",
       "      <td>(88.3, 91.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wrn_28_10_ra_2_5_refined400</td>\n",
       "      <td>50</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>0.6479</td>\n",
       "      <td>0.852</td>\n",
       "      <td>89.7</td>\n",
       "      <td>(88.3, 91.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>wrn_28_10_ra_2_5_refined400</td>\n",
       "      <td>50</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.2627</td>\n",
       "      <td>0.9403</td>\n",
       "      <td>95.9</td>\n",
       "      <td>(95.5, 96.3)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Model Epoch    Testset    Loss Accuracy  \\\n",
       "0             wrn_28_10_ra_2_5   400    cifar10  0.3942   0.9119   \n",
       "1             wrn_28_10_ra_2_5   300    cifar10    0.38   0.9128   \n",
       "2             wrn_28_10_ra_2_5   400  cifar10.1  0.8676   0.8145   \n",
       "3             wrn_28_10_ra_2_5   300  cifar10.1   0.815    0.807   \n",
       "4  wrn_28_10_ra_2_5_refined400    50  cifar10.1  0.6479    0.852   \n",
       "5  wrn_28_10_ra_2_5_refined400    50    cifar10  0.2627   0.9403   \n",
       "\n",
       "   Original_Accuracy   Original_CI  \n",
       "0               95.9  (95.5, 96.3)  \n",
       "1               95.9  (95.5, 96.3)  \n",
       "2               89.7  (88.3, 91.0)  \n",
       "3               89.7  (88.3, 91.0)  \n",
       "4               89.7  (88.3, 91.0)  \n",
       "5               95.9  (95.5, 96.3)  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "a = pd.Series(['wrn_28_10_ra_3_20', 400, 'cifar10',])\n",
    "b = pd.Series(['wrn_28_10_ra_3_20', 300, 'cifar10', ])\n",
    "c = pd.Series(['wrn_28_10_ra_3_20', 400, 'cifar10.1', ])\n",
    "d = pd.Series(['wrn_28_10_ra_3_20', 300, 'cifar10.1', ])\n",
    "    \n",
    "e = pd.Series(['wrn_28_10_ra_3_20_refined400', 50, 'cifar10.1', ])\n",
    "f = pd.Series(['wrn_28_10_ra_3_20_refined400', 50, 'cifar10', ])\n",
    "#g = pd.Series(['resnet_basic_32_ra_2_5_refined300', 150, 'cifar10', 0.4499, 0.8795])\n",
    "#h = pd.Series(['resnet_basic_32_ra_2_5_refined300', 150, 'cifar10.1', 0.8206, 0.7710])\n",
    "               \n",
    "df_results = pd.concat([a,b,c,d,e,f], axis=1).T\n",
    "df_results.columns = ['Model', 'Epoch', 'Testset', 'Loss', 'Accuracy']\n",
    "\n",
    "df_results['Original_Accuracy'] = df_results.apply((lambda row: 95.9 if row[2] == 'cifar10' else 89.7), axis=1)\n",
    "df_results['Original_CI'] = df_results.apply((lambda row: (95.5, 96.3) if row[2] == 'cifar10' else (88.3, 91.0)), axis=1)\n",
    "\n",
    "df_results.to_csv('/home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_5/results.csv')\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['preds', 'probs', 'labels', 'loss', 'acc']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ -7.153804  ,  -0.1832159 ,  -0.69570637, ...,  -0.50926757,\n",
       "         -5.526208  , -12.987257  ],\n",
       "       [  2.862379  ,   7.963458  ,  -6.603018  , ...,  -4.740323  ,\n",
       "         25.90399   ,  -0.52988565],\n",
       "       [  4.25749   ,   8.408992  ,  -4.3299227 , ...,  -2.3715498 ,\n",
       "         13.468082  ,   4.5792727 ],\n",
       "       ...,\n",
       "       [ -4.7270765 ,  -1.2400844 ,   1.3852903 , ...,  -0.51062894,\n",
       "         -3.399443  ,  -2.4969094 ],\n",
       "       [ -2.7640457 ,  14.635863  ,   6.7449965 , ...,  -1.3011913 ,\n",
       "         -3.036379  ,  -7.061736  ],\n",
       "       [ -2.6933427 ,   1.8961854 ,  -3.6396854 , ...,  18.63456   ,\n",
       "         -2.7524152 ,  -3.2204888 ]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Peak inside the output file for predictions\n",
    "import numpy as np\n",
    "output = '/home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_5/exp00/test_results_0400/predictions.npz'\n",
    "npzfile = np.load(output)\n",
    "print(npzfile.files)\n",
    "npzfile['preds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the model checkpoints, configs, and results to S3 \n",
    "bucket='sagemaker-may29'\n",
    "prefix = 'sagemaker/results/original-models/wrn_28_10_ra_3_20'\n",
    "path = '/home/ec2-user/SageMaker/experiments/wrn_28_10_ra_3_20'\n",
    "\n",
    "s3_resource = boto3.resource(\"s3\", region_name=\"us-east-2\")\n",
    "\n",
    "def uploadDirectory(local_path,bucket_name,s3_prefix):\n",
    "\n",
    "    my_bucket = s3_resource.Bucket(bucket_name)\n",
    "    \n",
    "    for path, subdirs, files in os.walk(local_path):\n",
    "        path = path.replace(\"\\\\\",\"/\")\n",
    "        directory_name = path.replace(local_path,\"\")\n",
    "        for file in files:\n",
    "            #print(\"Local File:\", os.path.join(path, file))\n",
    "            #print(\"      Dest:\", s3_prefix+directory_name+'/'+file)\n",
    "            my_bucket.upload_file(os.path.join(path, file), s3_prefix+directory_name+'/'+file)\n",
    "    \n",
    "uploadDirectory(path,bucket,prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
