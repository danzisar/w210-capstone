{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import numpy \n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wide Residual Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 1)) (1.18.1)\n",
      "Requirement already satisfied: torch>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 2)) (1.4.0)\n",
      "Requirement already satisfied: torchvision in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (0.5.0)\n",
      "Requirement already satisfied: fvcore in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (0.1.1.post20200619)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 5)) (4.44.1)\n",
      "Requirement already satisfied: yacs in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 6)) (0.1.7)\n",
      "Requirement already satisfied: apex from git+https://github.com/NVIDIA/apex.git#egg=apex in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 7)) (0.1)\n",
      "Requirement already satisfied: termcolor in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 8)) (1.1.0)\n",
      "Requirement already satisfied: thop<0.0.31.post2004070130 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 9)) (0.0.31.post2001170342)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (1.14.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (7.0.0)\n",
      "Requirement already satisfied: portalocker in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (1.7.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (5.3.1)\n",
      "Requirement already satisfied: tabulate in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (0.8.7)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Sagemaker Notebook must be of type, conda_pytorch_p36\n",
    "\n",
    "!pip install -r '/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorboard in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (2.2.2)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.29.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (2.23.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.6.0.post3)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (0.34.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (0.4.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (3.2.2)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.18.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.14.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.18.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (46.1.3.post20200330)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (3.12.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (0.9.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (1.25.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (2020.4.5.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard) (1.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (4.1.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (3.4.2)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard) (2.2.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard) (0.4.8)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Need to add this to requirements.txt\n",
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model per the settings specified in the original paper\n",
    "# os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "# !python train.py --config configs/cifar/wrn.yaml \\\n",
    "#     model.wrn.depth 28 \\\n",
    "#     model.wrn.widening_factor 10 \\\n",
    "#     train.batch_size 128 \\\n",
    "#     train.base_lr 0.1 \\\n",
    "#     train.output_dir /home/ec2-user/SageMaker/experiments/wrn_28_10/exp00 \\\n",
    "#     scheduler.epochs 200\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-24 01:16:52] __main__ INFO: \u001b[0mdevice: cuda\n",
      "cudnn:\n",
      "  benchmark: True\n",
      "  deterministic: False\n",
      "dataset:\n",
      "  name: CIFAR10_RA_3_20\n",
      "  dataset_dir: ''\n",
      "  image_size: 32\n",
      "  n_channels: 3\n",
      "  n_classes: 10\n",
      "model:\n",
      "  type: cifar\n",
      "  name: wrn\n",
      "  init_mode: kaiming_fan_in\n",
      "  vgg:\n",
      "    n_channels: [64, 128, 256, 512, 512]\n",
      "    n_layers: [2, 2, 3, 3, 3]\n",
      "    use_bn: True\n",
      "  resnet:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "  resnet_preact:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "  wrn:\n",
      "    depth: 28\n",
      "    initial_channels: 16\n",
      "    widening_factor: 10\n",
      "    drop_rate: 0.0\n",
      "  densenet:\n",
      "    depth: 100\n",
      "    n_blocks: [6, 12, 24, 16]\n",
      "    block_type: bottleneck\n",
      "    growth_rate: 12\n",
      "    drop_rate: 0.0\n",
      "    compression_rate: 0.5\n",
      "  pyramidnet:\n",
      "    depth: 272\n",
      "    n_blocks: [3, 24, 36, 3]\n",
      "    initial_channels: 16\n",
      "    block_type: bottleneck\n",
      "    alpha: 200\n",
      "  resnext:\n",
      "    depth: 29\n",
      "    n_blocks: [3, 4, 6, 3]\n",
      "    initial_channels: 64\n",
      "    cardinality: 8\n",
      "    base_channels: 4\n",
      "  shake_shake:\n",
      "    depth: 26\n",
      "    initial_channels: 96\n",
      "    shake_forward: True\n",
      "    shake_backward: True\n",
      "    shake_image: True\n",
      "  se_resnet_preact:\n",
      "    depth: 110\n",
      "    initial_channels: 16\n",
      "    se_reduction: 16\n",
      "    block_type: basic\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "train:\n",
      "  checkpoint: ''\n",
      "  resume: False\n",
      "  precision: O0\n",
      "  batch_size: 128\n",
      "  subdivision: 1\n",
      "  optimizer: sgd\n",
      "  base_lr: 0.1\n",
      "  momentum: 0.9\n",
      "  nesterov: True\n",
      "  weight_decay: 0.0005\n",
      "  no_weight_decay_on_bn: False\n",
      "  gradient_clip: 0\n",
      "  start_epoch: 0\n",
      "  seed: 0\n",
      "  val_first: True\n",
      "  val_period: 1\n",
      "  val_ratio: 0.1\n",
      "  use_test_as_val: False\n",
      "  output_dir: /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_3_20/exp00\n",
      "  log_period: 100\n",
      "  checkpoint_period: 100\n",
      "  use_tensorboard: True\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: True\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "  distributed: False\n",
      "  dist:\n",
      "    backend: nccl\n",
      "    init_method: env://\n",
      "    world_size: -1\n",
      "    node_rank: -1\n",
      "    local_rank: 0\n",
      "    use_sync_bn: False\n",
      "tensorboard:\n",
      "  train_images: False\n",
      "  val_images: False\n",
      "  model_params: False\n",
      "optim:\n",
      "  adam:\n",
      "    betas: (0.9, 0.999)\n",
      "  lars:\n",
      "    eps: 1e-09\n",
      "    threshold: 0.01\n",
      "  adabound:\n",
      "    betas: (0.9, 0.999)\n",
      "    final_lr: 0.1\n",
      "    gamma: 0.001\n",
      "scheduler:\n",
      "  epochs: 400\n",
      "  warmup:\n",
      "    type: none\n",
      "    epochs: 0\n",
      "    start_factor: 0.001\n",
      "    exponent: 4\n",
      "  type: multistep\n",
      "  milestones: [60, 120, 160]\n",
      "  lr_decay: 0.2\n",
      "  lr_min_factor: 0.001\n",
      "  T0: 10\n",
      "  T_mul: 1.0\n",
      "validation:\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: False\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "augmentation:\n",
      "  use_random_crop: True\n",
      "  use_random_horizontal_flip: True\n",
      "  use_cutout: False\n",
      "  use_random_erasing: False\n",
      "  use_dual_cutout: False\n",
      "  use_mixup: False\n",
      "  use_ricap: False\n",
      "  use_cutmix: False\n",
      "  use_label_smoothing: False\n",
      "  random_crop:\n",
      "    padding: 4\n",
      "    fill: 0\n",
      "    padding_mode: constant\n",
      "  random_horizontal_flip:\n",
      "    prob: 0.5\n",
      "  cutout:\n",
      "    prob: 1.0\n",
      "    mask_size: 16\n",
      "    cut_inside: False\n",
      "    mask_color: 0\n",
      "    dual_cutout_alpha: 0.1\n",
      "  random_erasing:\n",
      "    prob: 0.5\n",
      "    area_ratio_range: [0.02, 0.4]\n",
      "    min_aspect_ratio: 0.3\n",
      "    max_attempt: 20\n",
      "  mixup:\n",
      "    alpha: 1.0\n",
      "  ricap:\n",
      "    beta: 0.3\n",
      "  cutmix:\n",
      "    alpha: 1.0\n",
      "  label_smoothing:\n",
      "    epsilon: 0.1\n",
      "tta:\n",
      "  use_resize: False\n",
      "  use_center_crop: False\n",
      "  resize: 256\n",
      "test:\n",
      "  checkpoint: ''\n",
      "  output_dir: ''\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    pin_memory: False\n",
      "\u001b[32m[2020-06-24 01:16:52] __main__ INFO: \u001b[0menv_info:\n",
      "  pytorch_version: 1.4.0\n",
      "  cuda_version: 10.1\n",
      "  cudnn_version: 7603\n",
      "  num_gpus: 1\n",
      "  gpu_name: Tesla K80\n",
      "  gpu_capability: 3.7\n",
      "(50000, 32, 32, 3)\n",
      "\u001b[32m[2020-06-24 01:16:59] __main__ INFO: \u001b[0mMACs  : 5.25G\n",
      "\u001b[32m[2020-06-24 01:16:59] __main__ INFO: \u001b[0m#params: 36.48M\n",
      "Selected optimization level O0:  Pure FP32 training.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\",)\n",
      "\u001b[32m[2020-06-24 01:16:59] __main__ INFO: \u001b[0mVal 0\n",
      "\u001b[32m[2020-06-24 01:17:19] __main__ INFO: \u001b[0mEpoch 0 loss 345.8863 acc@1 0.1016 acc@5 0.4968\n",
      "\u001b[32m[2020-06-24 01:17:19] __main__ INFO: \u001b[0mElapsed 19.65\n",
      "\u001b[32m[2020-06-24 01:17:19] __main__ INFO: \u001b[0mTrain 1 0\n",
      "\u001b[32m[2020-06-24 01:19:16] __main__ INFO: \u001b[0mEpoch 1 Step 100/351 lr 0.100000 loss 2.3589 (2.4215) acc@1 0.0859 (0.1057) acc@5 0.5312 (0.5087)\n",
      "\u001b[32m[2020-06-24 01:21:09] __main__ INFO: \u001b[0mEpoch 1 Step 200/351 lr 0.100000 loss 2.2821 (2.3632) acc@1 0.1484 (0.1075) acc@5 0.5625 (0.5240)\n",
      "\u001b[32m[2020-06-24 01:23:01] __main__ INFO: \u001b[0mEpoch 1 Step 300/351 lr 0.100000 loss 2.2609 (2.3373) acc@1 0.1250 (0.1117) acc@5 0.5391 (0.5336)\n",
      "\u001b[32m[2020-06-24 01:23:59] __main__ INFO: \u001b[0mEpoch 1 Step 351/351 lr 0.100000 loss 2.2597 (2.3292) acc@1 0.1953 (0.1129) acc@5 0.5781 (0.5367)\n",
      "\u001b[32m[2020-06-24 01:23:59] __main__ INFO: \u001b[0mElapsed 400.13\n",
      "\u001b[32m[2020-06-24 01:23:59] __main__ INFO: \u001b[0mVal 1\n",
      "\u001b[32m[2020-06-24 01:24:12] __main__ INFO: \u001b[0mEpoch 1 loss 2.2894 acc@1 0.1226 acc@5 0.5608\n",
      "\u001b[32m[2020-06-24 01:24:12] __main__ INFO: \u001b[0mElapsed 13.18\n",
      "\u001b[32m[2020-06-24 01:24:12] __main__ INFO: \u001b[0mTrain 2 351\n",
      "\u001b[32m[2020-06-24 01:26:04] __main__ INFO: \u001b[0mEpoch 2 Step 100/351 lr 0.100000 loss 2.2791 (2.2775) acc@1 0.0625 (0.1212) acc@5 0.5156 (0.5638)\n",
      "\u001b[32m[2020-06-24 01:27:56] __main__ INFO: \u001b[0mEpoch 2 Step 200/351 lr 0.100000 loss 2.2595 (2.2721) acc@1 0.1250 (0.1252) acc@5 0.6172 (0.5675)\n",
      "\u001b[32m[2020-06-24 01:29:48] __main__ INFO: \u001b[0mEpoch 2 Step 300/351 lr 0.100000 loss 2.2591 (2.2671) acc@1 0.0859 (0.1274) acc@5 0.5469 (0.5694)\n",
      "\u001b[32m[2020-06-24 01:30:45] __main__ INFO: \u001b[0mEpoch 2 Step 351/351 lr 0.100000 loss 2.2618 (2.2658) acc@1 0.0938 (0.1264) acc@5 0.5547 (0.5708)\n",
      "\u001b[32m[2020-06-24 01:30:45] __main__ INFO: \u001b[0mElapsed 393.55\n",
      "\u001b[32m[2020-06-24 01:30:45] __main__ INFO: \u001b[0mVal 2\n",
      "\u001b[32m[2020-06-24 01:30:59] __main__ INFO: \u001b[0mEpoch 2 loss 2.2341 acc@1 0.1444 acc@5 0.5942\n",
      "\u001b[32m[2020-06-24 01:30:59] __main__ INFO: \u001b[0mElapsed 13.23\n",
      "\u001b[32m[2020-06-24 01:30:59] __main__ INFO: \u001b[0mTrain 3 702\n",
      "\u001b[32m[2020-06-24 01:32:51] __main__ INFO: \u001b[0mEpoch 3 Step 100/351 lr 0.100000 loss 2.2947 (2.2517) acc@1 0.1172 (0.1410) acc@5 0.5625 (0.5853)\n",
      "\u001b[32m[2020-06-24 01:34:44] __main__ INFO: \u001b[0mEpoch 3 Step 200/351 lr 0.100000 loss 2.2379 (2.2481) acc@1 0.1328 (0.1381) acc@5 0.6484 (0.5899)\n",
      "\u001b[32m[2020-06-24 01:36:36] __main__ INFO: \u001b[0mEpoch 3 Step 300/351 lr 0.100000 loss 2.2317 (2.2460) acc@1 0.1562 (0.1387) acc@5 0.6328 (0.5901)\n",
      "\u001b[32m[2020-06-24 01:37:34] __main__ INFO: \u001b[0mEpoch 3 Step 351/351 lr 0.100000 loss 2.2398 (2.2450) acc@1 0.1406 (0.1388) acc@5 0.5547 (0.5898)\n",
      "\u001b[32m[2020-06-24 01:37:34] __main__ INFO: \u001b[0mElapsed 395.14\n",
      "\u001b[32m[2020-06-24 01:37:34] __main__ INFO: \u001b[0mVal 3\n",
      "\u001b[32m[2020-06-24 01:37:47] __main__ INFO: \u001b[0mEpoch 3 loss 2.2390 acc@1 0.1390 acc@5 0.5868\n",
      "\u001b[32m[2020-06-24 01:37:47] __main__ INFO: \u001b[0mElapsed 13.21\n",
      "\u001b[32m[2020-06-24 01:37:47] __main__ INFO: \u001b[0mTrain 4 1053\n",
      "\u001b[32m[2020-06-24 01:39:39] __main__ INFO: \u001b[0mEpoch 4 Step 100/351 lr 0.100000 loss 2.2708 (2.2333) acc@1 0.0938 (0.1445) acc@5 0.5547 (0.5959)\n",
      "\u001b[32m[2020-06-24 01:41:31] __main__ INFO: \u001b[0mEpoch 4 Step 200/351 lr 0.100000 loss 2.2032 (2.2322) acc@1 0.1719 (0.1478) acc@5 0.5938 (0.5975)\n",
      "\u001b[32m[2020-06-24 01:43:23] __main__ INFO: \u001b[0mEpoch 4 Step 300/351 lr 0.100000 loss 2.2618 (2.2288) acc@1 0.1328 (0.1504) acc@5 0.5312 (0.6006)\n",
      "\u001b[32m[2020-06-24 01:44:21] __main__ INFO: \u001b[0mEpoch 4 Step 351/351 lr 0.100000 loss 2.2311 (2.2279) acc@1 0.1016 (0.1510) acc@5 0.5703 (0.6003)\n",
      "\u001b[32m[2020-06-24 01:44:21] __main__ INFO: \u001b[0mElapsed 393.67\n",
      "\u001b[32m[2020-06-24 01:44:21] __main__ INFO: \u001b[0mVal 4\n",
      "\u001b[32m[2020-06-24 01:44:34] __main__ INFO: \u001b[0mEpoch 4 loss 2.2269 acc@1 0.1544 acc@5 0.6152\n",
      "\u001b[32m[2020-06-24 01:44:34] __main__ INFO: \u001b[0mElapsed 13.19\n",
      "\u001b[32m[2020-06-24 01:44:34] __main__ INFO: \u001b[0mTrain 5 1404\n",
      "\u001b[32m[2020-06-24 01:46:26] __main__ INFO: \u001b[0mEpoch 5 Step 100/351 lr 0.100000 loss 2.2027 (2.2174) acc@1 0.1953 (0.1566) acc@5 0.5938 (0.6070)\n",
      "\u001b[32m[2020-06-24 01:48:19] __main__ INFO: \u001b[0mEpoch 5 Step 200/351 lr 0.100000 loss 2.1913 (2.2183) acc@1 0.1875 (0.1534) acc@5 0.6406 (0.6055)\n",
      "\u001b[32m[2020-06-24 01:50:12] __main__ INFO: \u001b[0mEpoch 5 Step 300/351 lr 0.100000 loss 2.2242 (2.2162) acc@1 0.1406 (0.1546) acc@5 0.6172 (0.6076)\n",
      "\u001b[32m[2020-06-24 01:51:09] __main__ INFO: \u001b[0mEpoch 5 Step 351/351 lr 0.100000 loss 2.2327 (2.2156) acc@1 0.1328 (0.1551) acc@5 0.5547 (0.6067)\n",
      "\u001b[32m[2020-06-24 01:51:09] __main__ INFO: \u001b[0mElapsed 395.15\n",
      "\u001b[32m[2020-06-24 01:51:09] __main__ INFO: \u001b[0mVal 5\n",
      "\u001b[32m[2020-06-24 01:51:22] __main__ INFO: \u001b[0mEpoch 5 loss 2.2234 acc@1 0.1510 acc@5 0.6134\n",
      "\u001b[32m[2020-06-24 01:51:22] __main__ INFO: \u001b[0mElapsed 13.20\n",
      "\u001b[32m[2020-06-24 01:51:22] __main__ INFO: \u001b[0mTrain 6 1755\n",
      "\u001b[32m[2020-06-24 01:53:14] __main__ INFO: \u001b[0mEpoch 6 Step 100/351 lr 0.100000 loss 2.1760 (2.2087) acc@1 0.2422 (0.1575) acc@5 0.6719 (0.6088)\n",
      "\u001b[32m[2020-06-24 01:55:07] __main__ INFO: \u001b[0mEpoch 6 Step 200/351 lr 0.100000 loss 2.2725 (2.2057) acc@1 0.0938 (0.1597) acc@5 0.4688 (0.6083)\n",
      "\u001b[32m[2020-06-24 01:56:59] __main__ INFO: \u001b[0mEpoch 6 Step 300/351 lr 0.100000 loss 2.2033 (2.2068) acc@1 0.1094 (0.1599) acc@5 0.6094 (0.6105)\n",
      "\u001b[32m[2020-06-24 01:57:56] __main__ INFO: \u001b[0mEpoch 6 Step 351/351 lr 0.100000 loss 2.1925 (2.2045) acc@1 0.2031 (0.1607) acc@5 0.6172 (0.6116)\n",
      "\u001b[32m[2020-06-24 01:57:56] __main__ INFO: \u001b[0mElapsed 393.62\n",
      "\u001b[32m[2020-06-24 01:57:56] __main__ INFO: \u001b[0mVal 6\n",
      "\u001b[32m[2020-06-24 01:58:09] __main__ INFO: \u001b[0mEpoch 6 loss 2.2204 acc@1 0.1494 acc@5 0.5962\n",
      "\u001b[32m[2020-06-24 01:58:09] __main__ INFO: \u001b[0mElapsed 13.20\n",
      "\u001b[32m[2020-06-24 01:58:09] __main__ INFO: \u001b[0mTrain 7 2106\n",
      "\u001b[32m[2020-06-24 02:00:02] __main__ INFO: \u001b[0mEpoch 7 Step 100/351 lr 0.100000 loss 2.1892 (2.1923) acc@1 0.1719 (0.1673) acc@5 0.6641 (0.6148)\n",
      "\u001b[32m[2020-06-24 02:01:54] __main__ INFO: \u001b[0mEpoch 7 Step 200/351 lr 0.100000 loss 2.1650 (2.1910) acc@1 0.1875 (0.1689) acc@5 0.6172 (0.6180)\n",
      "\u001b[32m[2020-06-24 02:03:47] __main__ INFO: \u001b[0mEpoch 7 Step 300/351 lr 0.100000 loss 2.1172 (2.1867) acc@1 0.2031 (0.1686) acc@5 0.6641 (0.6228)\n",
      "\u001b[32m[2020-06-24 02:04:44] __main__ INFO: \u001b[0mEpoch 7 Step 351/351 lr 0.100000 loss 2.1066 (2.1851) acc@1 0.1875 (0.1694) acc@5 0.6484 (0.6228)\n",
      "\u001b[32m[2020-06-24 02:04:44] __main__ INFO: \u001b[0mElapsed 395.00\n",
      "\u001b[32m[2020-06-24 02:04:44] __main__ INFO: \u001b[0mVal 7\n",
      "\u001b[32m[2020-06-24 02:04:57] __main__ INFO: \u001b[0mEpoch 7 loss 2.1995 acc@1 0.1662 acc@5 0.6184\n",
      "\u001b[32m[2020-06-24 02:04:57] __main__ INFO: \u001b[0mElapsed 13.20\n",
      "\u001b[32m[2020-06-24 02:04:57] __main__ INFO: \u001b[0mTrain 8 2457\n",
      "\u001b[32m[2020-06-24 02:06:49] __main__ INFO: \u001b[0mEpoch 8 Step 100/351 lr 0.100000 loss 2.1464 (2.1755) acc@1 0.2188 (0.1737) acc@5 0.5859 (0.6288)\n",
      "\u001b[32m[2020-06-24 02:08:42] __main__ INFO: \u001b[0mEpoch 8 Step 200/351 lr 0.100000 loss 1.9600 (2.1678) acc@1 0.2422 (0.1763) acc@5 0.6641 (0.6304)\n",
      "\u001b[32m[2020-06-24 02:10:34] __main__ INFO: \u001b[0mEpoch 8 Step 300/351 lr 0.100000 loss 2.1513 (2.1658) acc@1 0.2188 (0.1765) acc@5 0.7500 (0.6324)\n",
      "\u001b[32m[2020-06-24 02:11:31] __main__ INFO: \u001b[0mEpoch 8 Step 351/351 lr 0.100000 loss 2.1665 (2.1652) acc@1 0.1562 (0.1763) acc@5 0.6406 (0.6323)\n",
      "\u001b[32m[2020-06-24 02:11:31] __main__ INFO: \u001b[0mElapsed 393.54\n",
      "\u001b[32m[2020-06-24 02:11:31] __main__ INFO: \u001b[0mVal 8\n",
      "\u001b[32m[2020-06-24 02:11:44] __main__ INFO: \u001b[0mEpoch 8 loss 2.3118 acc@1 0.1698 acc@5 0.6228\n",
      "\u001b[32m[2020-06-24 02:11:44] __main__ INFO: \u001b[0mElapsed 13.20\n",
      "\u001b[32m[2020-06-24 02:11:44] __main__ INFO: \u001b[0mTrain 9 2808\n",
      "\u001b[32m[2020-06-24 02:13:37] __main__ INFO: \u001b[0mEpoch 9 Step 100/351 lr 0.100000 loss 2.2131 (2.1484) acc@1 0.1875 (0.1873) acc@5 0.6328 (0.6353)\n",
      "\u001b[32m[2020-06-24 02:15:29] __main__ INFO: \u001b[0mEpoch 9 Step 200/351 lr 0.100000 loss 2.0277 (2.1441) acc@1 0.2734 (0.1898) acc@5 0.7031 (0.6394)\n",
      "\u001b[32m[2020-06-24 02:17:22] __main__ INFO: \u001b[0mEpoch 9 Step 300/351 lr 0.100000 loss 2.1839 (2.1437) acc@1 0.2109 (0.1886) acc@5 0.6406 (0.6360)\n",
      "\u001b[32m[2020-06-24 02:18:19] __main__ INFO: \u001b[0mEpoch 9 Step 351/351 lr 0.100000 loss 2.1849 (2.1425) acc@1 0.1562 (0.1890) acc@5 0.6641 (0.6366)\n",
      "\u001b[32m[2020-06-24 02:18:19] __main__ INFO: \u001b[0mElapsed 394.93\n",
      "\u001b[32m[2020-06-24 02:18:19] __main__ INFO: \u001b[0mVal 9\n",
      "\u001b[32m[2020-06-24 02:18:32] __main__ INFO: \u001b[0mEpoch 9 loss 2.1656 acc@1 0.1748 acc@5 0.6304\n",
      "\u001b[32m[2020-06-24 02:18:32] __main__ INFO: \u001b[0mElapsed 13.23\n",
      "\u001b[32m[2020-06-24 02:18:32] __main__ INFO: \u001b[0mTrain 10 3159\n",
      "\u001b[32m[2020-06-24 02:20:24] __main__ INFO: \u001b[0mEpoch 10 Step 100/351 lr 0.100000 loss 2.1833 (2.1284) acc@1 0.1797 (0.1955) acc@5 0.6406 (0.6445)\n",
      "\u001b[32m[2020-06-24 02:22:16] __main__ INFO: \u001b[0mEpoch 10 Step 200/351 lr 0.100000 loss 2.1597 (2.1228) acc@1 0.2578 (0.1988) acc@5 0.6172 (0.6462)\n",
      "\u001b[32m[2020-06-24 02:24:08] __main__ INFO: \u001b[0mEpoch 10 Step 300/351 lr 0.100000 loss 2.1813 (2.1234) acc@1 0.1797 (0.1991) acc@5 0.6094 (0.6467)\n",
      "\u001b[32m[2020-06-24 02:25:06] __main__ INFO: \u001b[0mEpoch 10 Step 351/351 lr 0.100000 loss 2.1499 (2.1226) acc@1 0.2344 (0.1984) acc@5 0.6250 (0.6463)\n",
      "\u001b[32m[2020-06-24 02:25:06] __main__ INFO: \u001b[0mElapsed 393.53\n",
      "\u001b[32m[2020-06-24 02:25:06] __main__ INFO: \u001b[0mVal 10\n",
      "\u001b[32m[2020-06-24 02:25:19] __main__ INFO: \u001b[0mEpoch 10 loss 2.1791 acc@1 0.1728 acc@5 0.6402\n",
      "\u001b[32m[2020-06-24 02:25:19] __main__ INFO: \u001b[0mElapsed 13.19\n",
      "\u001b[32m[2020-06-24 02:25:19] __main__ INFO: \u001b[0mTrain 11 3510\n",
      "\u001b[32m[2020-06-24 02:27:11] __main__ INFO: \u001b[0mEpoch 11 Step 100/351 lr 0.100000 loss 2.0697 (2.1082) acc@1 0.2656 (0.2080) acc@5 0.6797 (0.6553)\n",
      "\u001b[32m[2020-06-24 02:29:04] __main__ INFO: \u001b[0mEpoch 11 Step 200/351 lr 0.100000 loss 2.1029 (2.1059) acc@1 0.2344 (0.2066) acc@5 0.6172 (0.6540)\n",
      "\u001b[32m[2020-06-24 02:30:56] __main__ INFO: \u001b[0mEpoch 11 Step 300/351 lr 0.100000 loss 2.1165 (2.1055) acc@1 0.1953 (0.2083) acc@5 0.5938 (0.6541)\n",
      "\u001b[32m[2020-06-24 02:31:54] __main__ INFO: \u001b[0mEpoch 11 Step 351/351 lr 0.100000 loss 2.1458 (2.1063) acc@1 0.2109 (0.2073) acc@5 0.5547 (0.6531)\n",
      "\u001b[32m[2020-06-24 02:31:54] __main__ INFO: \u001b[0mElapsed 394.86\n",
      "\u001b[32m[2020-06-24 02:31:54] __main__ INFO: \u001b[0mVal 11\n",
      "\u001b[32m[2020-06-24 02:32:07] __main__ INFO: \u001b[0mEpoch 11 loss 2.1288 acc@1 0.1892 acc@5 0.6388\n",
      "\u001b[32m[2020-06-24 02:32:07] __main__ INFO: \u001b[0mElapsed 13.17\n",
      "\u001b[32m[2020-06-24 02:32:07] __main__ INFO: \u001b[0mTrain 12 3861\n",
      "\u001b[32m[2020-06-24 02:33:59] __main__ INFO: \u001b[0mEpoch 12 Step 100/351 lr 0.100000 loss 2.1011 (2.0855) acc@1 0.2500 (0.2175) acc@5 0.6641 (0.6548)\n",
      "\u001b[32m[2020-06-24 02:35:51] __main__ INFO: \u001b[0mEpoch 12 Step 200/351 lr 0.100000 loss 2.1661 (2.0899) acc@1 0.1484 (0.2148) acc@5 0.6641 (0.6555)\n",
      "\u001b[32m[2020-06-24 02:37:43] __main__ INFO: \u001b[0mEpoch 12 Step 300/351 lr 0.100000 loss 2.0862 (2.0856) acc@1 0.1953 (0.2152) acc@5 0.7031 (0.6601)\n",
      "\u001b[32m[2020-06-24 02:38:40] __main__ INFO: \u001b[0mEpoch 12 Step 351/351 lr 0.100000 loss 2.1336 (2.0864) acc@1 0.2031 (0.2148) acc@5 0.5938 (0.6594)\n",
      "\u001b[32m[2020-06-24 02:38:40] __main__ INFO: \u001b[0mElapsed 393.19\n",
      "\u001b[32m[2020-06-24 02:38:40] __main__ INFO: \u001b[0mVal 12\n",
      "\u001b[32m[2020-06-24 02:38:53] __main__ INFO: \u001b[0mEpoch 12 loss 2.1950 acc@1 0.1768 acc@5 0.6124\n",
      "\u001b[32m[2020-06-24 02:38:53] __main__ INFO: \u001b[0mElapsed 13.16\n",
      "\u001b[32m[2020-06-24 02:38:53] __main__ INFO: \u001b[0mTrain 13 4212\n",
      "\u001b[32m[2020-06-24 02:40:46] __main__ INFO: \u001b[0mEpoch 13 Step 100/351 lr 0.100000 loss 1.9633 (2.0701) acc@1 0.2656 (0.2223) acc@5 0.7344 (0.6585)\n",
      "\u001b[32m[2020-06-24 02:42:38] __main__ INFO: \u001b[0mEpoch 13 Step 200/351 lr 0.100000 loss 2.1886 (2.0740) acc@1 0.1797 (0.2202) acc@5 0.7031 (0.6565)\n",
      "\u001b[32m[2020-06-24 02:44:30] __main__ INFO: \u001b[0mEpoch 13 Step 300/351 lr 0.100000 loss 2.0464 (2.0715) acc@1 0.2344 (0.2193) acc@5 0.6484 (0.6595)\n",
      "\u001b[32m[2020-06-24 02:45:28] __main__ INFO: \u001b[0mEpoch 13 Step 351/351 lr 0.100000 loss 2.0927 (2.0704) acc@1 0.1875 (0.2196) acc@5 0.6484 (0.6591)\n",
      "\u001b[32m[2020-06-24 02:45:28] __main__ INFO: \u001b[0mElapsed 394.57\n",
      "\u001b[32m[2020-06-24 02:45:28] __main__ INFO: \u001b[0mVal 13\n",
      "\u001b[32m[2020-06-24 02:45:41] __main__ INFO: \u001b[0mEpoch 13 loss 2.2288 acc@1 0.1838 acc@5 0.6244\n",
      "\u001b[32m[2020-06-24 02:45:41] __main__ INFO: \u001b[0mElapsed 13.21\n",
      "\u001b[32m[2020-06-24 02:45:41] __main__ INFO: \u001b[0mTrain 14 4563\n",
      "\u001b[32m[2020-06-24 02:47:33] __main__ INFO: \u001b[0mEpoch 14 Step 100/351 lr 0.100000 loss 2.0413 (2.0651) acc@1 0.2500 (0.2234) acc@5 0.7031 (0.6659)\n",
      "\u001b[32m[2020-06-24 02:49:25] __main__ INFO: \u001b[0mEpoch 14 Step 200/351 lr 0.100000 loss 2.0581 (2.0557) acc@1 0.2344 (0.2264) acc@5 0.6641 (0.6629)\n",
      "\u001b[32m[2020-06-24 02:51:17] __main__ INFO: \u001b[0mEpoch 14 Step 300/351 lr 0.100000 loss 2.0913 (2.0554) acc@1 0.2109 (0.2261) acc@5 0.6406 (0.6639)\n",
      "\u001b[32m[2020-06-24 02:52:14] __main__ INFO: \u001b[0mEpoch 14 Step 351/351 lr 0.100000 loss 2.1400 (2.0553) acc@1 0.1641 (0.2264) acc@5 0.5938 (0.6640)\n",
      "\u001b[32m[2020-06-24 02:52:14] __main__ INFO: \u001b[0mElapsed 393.00\n",
      "\u001b[32m[2020-06-24 02:52:14] __main__ INFO: \u001b[0mVal 14\n",
      "\u001b[32m[2020-06-24 02:52:27] __main__ INFO: \u001b[0mEpoch 14 loss 2.4502 acc@1 0.1822 acc@5 0.6392\n",
      "\u001b[32m[2020-06-24 02:52:27] __main__ INFO: \u001b[0mElapsed 13.19\n",
      "\u001b[32m[2020-06-24 02:52:27] __main__ INFO: \u001b[0mTrain 15 4914\n",
      "\u001b[32m[2020-06-24 02:54:20] __main__ INFO: \u001b[0mEpoch 15 Step 100/351 lr 0.100000 loss 2.0109 (2.0402) acc@1 0.2266 (0.2341) acc@5 0.6719 (0.6708)\n",
      "\u001b[32m[2020-06-24 02:56:12] __main__ INFO: \u001b[0mEpoch 15 Step 200/351 lr 0.100000 loss 2.0250 (2.0411) acc@1 0.2969 (0.2353) acc@5 0.6797 (0.6684)\n",
      "\u001b[32m[2020-06-24 02:58:04] __main__ INFO: \u001b[0mEpoch 15 Step 300/351 lr 0.100000 loss 2.0109 (2.0393) acc@1 0.2422 (0.2348) acc@5 0.6328 (0.6684)\n",
      "\u001b[32m[2020-06-24 02:59:02] __main__ INFO: \u001b[0mEpoch 15 Step 351/351 lr 0.100000 loss 2.0663 (2.0404) acc@1 0.2188 (0.2339) acc@5 0.7578 (0.6678)\n",
      "\u001b[32m[2020-06-24 02:59:02] __main__ INFO: \u001b[0mElapsed 394.45\n",
      "\u001b[32m[2020-06-24 02:59:02] __main__ INFO: \u001b[0mVal 15\n",
      "\u001b[32m[2020-06-24 02:59:15] __main__ INFO: \u001b[0mEpoch 15 loss 2.2439 acc@1 0.1828 acc@5 0.6416\n",
      "\u001b[32m[2020-06-24 02:59:15] __main__ INFO: \u001b[0mElapsed 13.20\n",
      "\u001b[32m[2020-06-24 02:59:15] __main__ INFO: \u001b[0mTrain 16 5265\n",
      "\u001b[32m[2020-06-24 03:01:07] __main__ INFO: \u001b[0mEpoch 16 Step 100/351 lr 0.100000 loss 1.9735 (2.0244) acc@1 0.2812 (0.2341) acc@5 0.6719 (0.6670)\n",
      "\u001b[32m[2020-06-24 03:02:59] __main__ INFO: \u001b[0mEpoch 16 Step 200/351 lr 0.100000 loss 2.0119 (2.0264) acc@1 0.2422 (0.2355) acc@5 0.6328 (0.6676)\n",
      "\u001b[32m[2020-06-24 03:04:51] __main__ INFO: \u001b[0mEpoch 16 Step 300/351 lr 0.100000 loss 2.0470 (2.0241) acc@1 0.2031 (0.2371) acc@5 0.6875 (0.6695)\n",
      "\u001b[32m[2020-06-24 03:05:48] __main__ INFO: \u001b[0mEpoch 16 Step 351/351 lr 0.100000 loss 2.0382 (2.0241) acc@1 0.2812 (0.2365) acc@5 0.7344 (0.6698)\n",
      "\u001b[32m[2020-06-24 03:05:48] __main__ INFO: \u001b[0mElapsed 392.72\n",
      "\u001b[32m[2020-06-24 03:05:48] __main__ INFO: \u001b[0mVal 16\n",
      "\u001b[32m[2020-06-24 03:06:01] __main__ INFO: \u001b[0mEpoch 16 loss 2.0956 acc@1 0.2258 acc@5 0.6572\n",
      "\u001b[32m[2020-06-24 03:06:01] __main__ INFO: \u001b[0mElapsed 13.17\n",
      "\u001b[32m[2020-06-24 03:06:01] __main__ INFO: \u001b[0mTrain 17 5616\n",
      "\u001b[32m[2020-06-24 03:07:53] __main__ INFO: \u001b[0mEpoch 17 Step 100/351 lr 0.100000 loss 1.9543 (2.0009) acc@1 0.2578 (0.2469) acc@5 0.6719 (0.6809)\n",
      "\u001b[32m[2020-06-24 03:09:45] __main__ INFO: \u001b[0mEpoch 17 Step 200/351 lr 0.100000 loss 2.0324 (2.0069) acc@1 0.2266 (0.2466) acc@5 0.7031 (0.6783)\n",
      "\u001b[32m[2020-06-24 03:11:38] __main__ INFO: \u001b[0mEpoch 17 Step 300/351 lr 0.100000 loss 1.9745 (2.0084) acc@1 0.2344 (0.2459) acc@5 0.6719 (0.6766)\n",
      "\u001b[32m[2020-06-24 03:12:35] __main__ INFO: \u001b[0mEpoch 17 Step 351/351 lr 0.100000 loss 2.0171 (2.0102) acc@1 0.2344 (0.2459) acc@5 0.6797 (0.6766)\n",
      "\u001b[32m[2020-06-24 03:12:35] __main__ INFO: \u001b[0mElapsed 394.12\n",
      "\u001b[32m[2020-06-24 03:12:35] __main__ INFO: \u001b[0mVal 17\n",
      "\u001b[32m[2020-06-24 03:12:48] __main__ INFO: \u001b[0mEpoch 17 loss 2.0391 acc@1 0.2338 acc@5 0.6744\n",
      "\u001b[32m[2020-06-24 03:12:48] __main__ INFO: \u001b[0mElapsed 13.15\n",
      "\u001b[32m[2020-06-24 03:12:48] __main__ INFO: \u001b[0mTrain 18 5967\n",
      "\u001b[32m[2020-06-24 03:14:40] __main__ INFO: \u001b[0mEpoch 18 Step 100/351 lr 0.100000 loss 1.9674 (1.9961) acc@1 0.2422 (0.2517) acc@5 0.6797 (0.6752)\n",
      "\u001b[32m[2020-06-24 03:16:32] __main__ INFO: \u001b[0mEpoch 18 Step 200/351 lr 0.100000 loss 1.9980 (1.9979) acc@1 0.2500 (0.2504) acc@5 0.6250 (0.6763)\n",
      "\u001b[32m[2020-06-24 03:18:24] __main__ INFO: \u001b[0mEpoch 18 Step 300/351 lr 0.100000 loss 2.0515 (2.0019) acc@1 0.2500 (0.2491) acc@5 0.6875 (0.6763)\n",
      "\u001b[32m[2020-06-24 03:19:21] __main__ INFO: \u001b[0mEpoch 18 Step 351/351 lr 0.100000 loss 2.0596 (2.0023) acc@1 0.2656 (0.2481) acc@5 0.7031 (0.6757)\n",
      "\u001b[32m[2020-06-24 03:19:21] __main__ INFO: \u001b[0mElapsed 392.58\n",
      "\u001b[32m[2020-06-24 03:19:21] __main__ INFO: \u001b[0mVal 18\n",
      "\u001b[32m[2020-06-24 03:19:34] __main__ INFO: \u001b[0mEpoch 18 loss 2.0721 acc@1 0.2346 acc@5 0.6646\n",
      "\u001b[32m[2020-06-24 03:19:34] __main__ INFO: \u001b[0mElapsed 13.17\n",
      "\u001b[32m[2020-06-24 03:19:34] __main__ INFO: \u001b[0mTrain 19 6318\n",
      "\u001b[32m[2020-06-24 03:21:26] __main__ INFO: \u001b[0mEpoch 19 Step 100/351 lr 0.100000 loss 2.0767 (1.9982) acc@1 0.1953 (0.2460) acc@5 0.6953 (0.6830)\n",
      "\u001b[32m[2020-06-24 03:23:18] __main__ INFO: \u001b[0mEpoch 19 Step 200/351 lr 0.100000 loss 2.1142 (1.9962) acc@1 0.1797 (0.2471) acc@5 0.6016 (0.6799)\n",
      "\u001b[32m[2020-06-24 03:25:11] __main__ INFO: \u001b[0mEpoch 19 Step 300/351 lr 0.100000 loss 2.0335 (1.9961) acc@1 0.3047 (0.2490) acc@5 0.6719 (0.6779)\n",
      "\u001b[32m[2020-06-24 03:26:08] __main__ INFO: \u001b[0mEpoch 19 Step 351/351 lr 0.100000 loss 1.9978 (1.9943) acc@1 0.2969 (0.2506) acc@5 0.6875 (0.6792)\n",
      "\u001b[32m[2020-06-24 03:26:08] __main__ INFO: \u001b[0mElapsed 394.11\n",
      "\u001b[32m[2020-06-24 03:26:08] __main__ INFO: \u001b[0mVal 19\n",
      "\u001b[32m[2020-06-24 03:26:21] __main__ INFO: \u001b[0mEpoch 19 loss 2.2513 acc@1 0.1918 acc@5 0.6168\n",
      "\u001b[32m[2020-06-24 03:26:21] __main__ INFO: \u001b[0mElapsed 13.16\n",
      "\u001b[32m[2020-06-24 03:26:21] __main__ INFO: \u001b[0mTrain 20 6669\n",
      "\u001b[32m[2020-06-24 03:28:13] __main__ INFO: \u001b[0mEpoch 20 Step 100/351 lr 0.100000 loss 2.0401 (1.9701) acc@1 0.2578 (0.2641) acc@5 0.7422 (0.6866)\n",
      "\u001b[32m[2020-06-24 03:30:05] __main__ INFO: \u001b[0mEpoch 20 Step 200/351 lr 0.100000 loss 1.8447 (1.9783) acc@1 0.3594 (0.2605) acc@5 0.7656 (0.6861)\n",
      "\u001b[32m[2020-06-24 03:31:56] __main__ INFO: \u001b[0mEpoch 20 Step 300/351 lr 0.100000 loss 2.0318 (1.9797) acc@1 0.2344 (0.2596) acc@5 0.6250 (0.6852)\n",
      "\u001b[32m[2020-06-24 03:32:53] __main__ INFO: \u001b[0mEpoch 20 Step 351/351 lr 0.100000 loss 1.9791 (1.9800) acc@1 0.2969 (0.2581) acc@5 0.6484 (0.6822)\n",
      "\u001b[32m[2020-06-24 03:32:53] __main__ INFO: \u001b[0mElapsed 392.40\n",
      "\u001b[32m[2020-06-24 03:32:53] __main__ INFO: \u001b[0mVal 20\n",
      "\u001b[32m[2020-06-24 03:33:07] __main__ INFO: \u001b[0mEpoch 20 loss 3.1669 acc@1 0.2250 acc@5 0.6522\n",
      "\u001b[32m[2020-06-24 03:33:07] __main__ INFO: \u001b[0mElapsed 13.17\n",
      "\u001b[32m[2020-06-24 03:33:07] __main__ INFO: \u001b[0mTrain 21 7020\n",
      "\u001b[32m[2020-06-24 03:34:59] __main__ INFO: \u001b[0mEpoch 21 Step 100/351 lr 0.100000 loss 1.9883 (1.9726) acc@1 0.2812 (0.2564) acc@5 0.6719 (0.6822)\n",
      "\u001b[32m[2020-06-24 03:36:51] __main__ INFO: \u001b[0mEpoch 21 Step 200/351 lr 0.100000 loss 1.9021 (1.9720) acc@1 0.3281 (0.2611) acc@5 0.7656 (0.6842)\n",
      "\u001b[32m[2020-06-24 03:38:43] __main__ INFO: \u001b[0mEpoch 21 Step 300/351 lr 0.100000 loss 2.0274 (1.9739) acc@1 0.2578 (0.2595) acc@5 0.6641 (0.6832)\n",
      "\u001b[32m[2020-06-24 03:39:41] __main__ INFO: \u001b[0mEpoch 21 Step 351/351 lr 0.100000 loss 1.9665 (1.9732) acc@1 0.2656 (0.2599) acc@5 0.6641 (0.6832)\n",
      "\u001b[32m[2020-06-24 03:39:41] __main__ INFO: \u001b[0mElapsed 394.00\n",
      "\u001b[32m[2020-06-24 03:39:41] __main__ INFO: \u001b[0mVal 21\n",
      "\u001b[32m[2020-06-24 03:39:54] __main__ INFO: \u001b[0mEpoch 21 loss 2.0421 acc@1 0.2484 acc@5 0.6648\n",
      "\u001b[32m[2020-06-24 03:39:54] __main__ INFO: \u001b[0mElapsed 13.17\n",
      "\u001b[32m[2020-06-24 03:39:54] __main__ INFO: \u001b[0mTrain 22 7371\n",
      "\u001b[32m[2020-06-24 03:41:46] __main__ INFO: \u001b[0mEpoch 22 Step 100/351 lr 0.100000 loss 2.0002 (1.9608) acc@1 0.2344 (0.2636) acc@5 0.6328 (0.6772)\n",
      "\u001b[32m[2020-06-24 03:43:37] __main__ INFO: \u001b[0mEpoch 22 Step 200/351 lr 0.100000 loss 1.9956 (1.9609) acc@1 0.2344 (0.2632) acc@5 0.6562 (0.6762)\n",
      "\u001b[32m[2020-06-24 03:45:29] __main__ INFO: \u001b[0mEpoch 22 Step 300/351 lr 0.100000 loss 2.0980 (1.9637) acc@1 0.2266 (0.2621) acc@5 0.6719 (0.6792)\n",
      "\u001b[32m[2020-06-24 03:46:26] __main__ INFO: \u001b[0mEpoch 22 Step 351/351 lr 0.100000 loss 1.9170 (1.9655) acc@1 0.2734 (0.2622) acc@5 0.7188 (0.6793)\n",
      "\u001b[32m[2020-06-24 03:46:26] __main__ INFO: \u001b[0mElapsed 392.32\n",
      "\u001b[32m[2020-06-24 03:46:26] __main__ INFO: \u001b[0mVal 22\n",
      "\u001b[32m[2020-06-24 03:46:39] __main__ INFO: \u001b[0mEpoch 22 loss 2.0769 acc@1 0.2394 acc@5 0.6566\n",
      "\u001b[32m[2020-06-24 03:46:39] __main__ INFO: \u001b[0mElapsed 13.14\n",
      "\u001b[32m[2020-06-24 03:46:39] __main__ INFO: \u001b[0mTrain 23 7722\n",
      "\u001b[32m[2020-06-24 03:48:32] __main__ INFO: \u001b[0mEpoch 23 Step 100/351 lr 0.100000 loss 1.8991 (1.9585) acc@1 0.3047 (0.2634) acc@5 0.7188 (0.6913)\n",
      "\u001b[32m[2020-06-24 03:50:24] __main__ INFO: \u001b[0mEpoch 23 Step 200/351 lr 0.100000 loss 2.0593 (1.9569) acc@1 0.2031 (0.2664) acc@5 0.6484 (0.6880)\n",
      "\u001b[32m[2020-06-24 03:52:16] __main__ INFO: \u001b[0mEpoch 23 Step 300/351 lr 0.100000 loss 1.9892 (1.9554) acc@1 0.2266 (0.2685) acc@5 0.6875 (0.6884)\n",
      "\u001b[32m[2020-06-24 03:53:13] __main__ INFO: \u001b[0mEpoch 23 Step 351/351 lr 0.100000 loss 1.9361 (1.9560) acc@1 0.2891 (0.2677) acc@5 0.6328 (0.6882)\n",
      "\u001b[32m[2020-06-24 03:53:13] __main__ INFO: \u001b[0mElapsed 393.88\n",
      "\u001b[32m[2020-06-24 03:53:13] __main__ INFO: \u001b[0mVal 23\n",
      "\u001b[32m[2020-06-24 03:53:26] __main__ INFO: \u001b[0mEpoch 23 loss 2.1372 acc@1 0.2196 acc@5 0.6592\n",
      "\u001b[32m[2020-06-24 03:53:26] __main__ INFO: \u001b[0mElapsed 13.15\n",
      "\u001b[32m[2020-06-24 03:53:26] __main__ INFO: \u001b[0mTrain 24 8073\n",
      "\u001b[32m[2020-06-24 03:55:18] __main__ INFO: \u001b[0mEpoch 24 Step 100/351 lr 0.100000 loss 1.9002 (1.9470) acc@1 0.2578 (0.2714) acc@5 0.7891 (0.6924)\n",
      "\u001b[32m[2020-06-24 03:57:10] __main__ INFO: \u001b[0mEpoch 24 Step 200/351 lr 0.100000 loss 1.9819 (1.9509) acc@1 0.2500 (0.2701) acc@5 0.7188 (0.6907)\n",
      "\u001b[32m[2020-06-24 03:59:02] __main__ INFO: \u001b[0mEpoch 24 Step 300/351 lr 0.100000 loss 2.0552 (1.9484) acc@1 0.2344 (0.2696) acc@5 0.6641 (0.6912)\n",
      "\u001b[32m[2020-06-24 03:59:59] __main__ INFO: \u001b[0mEpoch 24 Step 351/351 lr 0.100000 loss 1.9831 (1.9506) acc@1 0.2656 (0.2689) acc@5 0.6797 (0.6915)\n",
      "\u001b[32m[2020-06-24 03:59:59] __main__ INFO: \u001b[0mElapsed 392.41\n",
      "\u001b[32m[2020-06-24 03:59:59] __main__ INFO: \u001b[0mVal 24\n",
      "\u001b[32m[2020-06-24 04:00:12] __main__ INFO: \u001b[0mEpoch 24 loss 2.0192 acc@1 0.2374 acc@5 0.6662\n",
      "\u001b[32m[2020-06-24 04:00:12] __main__ INFO: \u001b[0mElapsed 13.13\n",
      "\u001b[32m[2020-06-24 04:00:12] __main__ INFO: \u001b[0mTrain 25 8424\n",
      "\u001b[32m[2020-06-24 04:02:04] __main__ INFO: \u001b[0mEpoch 25 Step 100/351 lr 0.100000 loss 2.0018 (1.9240) acc@1 0.2344 (0.2725) acc@5 0.6797 (0.6923)\n",
      "\u001b[32m[2020-06-24 04:03:56] __main__ INFO: \u001b[0mEpoch 25 Step 200/351 lr 0.100000 loss 1.7881 (1.9375) acc@1 0.3750 (0.2728) acc@5 0.6875 (0.6893)\n",
      "\u001b[32m[2020-06-24 04:05:49] __main__ INFO: \u001b[0mEpoch 25 Step 300/351 lr 0.100000 loss 2.0006 (1.9411) acc@1 0.2656 (0.2725) acc@5 0.6406 (0.6889)\n",
      "\u001b[32m[2020-06-24 04:06:46] __main__ INFO: \u001b[0mEpoch 25 Step 351/351 lr 0.100000 loss 1.9753 (1.9403) acc@1 0.2812 (0.2721) acc@5 0.7188 (0.6887)\n",
      "\u001b[32m[2020-06-24 04:06:46] __main__ INFO: \u001b[0mElapsed 393.84\n",
      "\u001b[32m[2020-06-24 04:06:46] __main__ INFO: \u001b[0mVal 25\n",
      "\u001b[32m[2020-06-24 04:06:59] __main__ INFO: \u001b[0mEpoch 25 loss 2.3367 acc@1 0.2048 acc@5 0.6508\n",
      "\u001b[32m[2020-06-24 04:06:59] __main__ INFO: \u001b[0mElapsed 13.15\n",
      "\u001b[32m[2020-06-24 04:06:59] __main__ INFO: \u001b[0mTrain 26 8775\n",
      "\u001b[32m[2020-06-24 04:08:51] __main__ INFO: \u001b[0mEpoch 26 Step 100/351 lr 0.100000 loss 1.9524 (1.9297) acc@1 0.2969 (0.2737) acc@5 0.7188 (0.6914)\n",
      "\u001b[32m[2020-06-24 04:10:42] __main__ INFO: \u001b[0mEpoch 26 Step 200/351 lr 0.100000 loss 1.9175 (1.9308) acc@1 0.2656 (0.2713) acc@5 0.6719 (0.6914)\n",
      "\u001b[32m[2020-06-24 04:12:34] __main__ INFO: \u001b[0mEpoch 26 Step 300/351 lr 0.100000 loss 1.9959 (1.9346) acc@1 0.2188 (0.2727) acc@5 0.6953 (0.6898)\n",
      "\u001b[32m[2020-06-24 04:13:31] __main__ INFO: \u001b[0mEpoch 26 Step 351/351 lr 0.100000 loss 1.9314 (1.9352) acc@1 0.2422 (0.2723) acc@5 0.7031 (0.6888)\n",
      "\u001b[32m[2020-06-24 04:13:31] __main__ INFO: \u001b[0mElapsed 392.04\n",
      "\u001b[32m[2020-06-24 04:13:31] __main__ INFO: \u001b[0mVal 26\n",
      "\u001b[32m[2020-06-24 04:13:44] __main__ INFO: \u001b[0mEpoch 26 loss 2.1012 acc@1 0.2200 acc@5 0.6638\n",
      "\u001b[32m[2020-06-24 04:13:44] __main__ INFO: \u001b[0mElapsed 13.15\n",
      "\u001b[32m[2020-06-24 04:13:44] __main__ INFO: \u001b[0mTrain 27 9126\n",
      "\u001b[32m[2020-06-24 04:15:36] __main__ INFO: \u001b[0mEpoch 27 Step 100/351 lr 0.100000 loss 1.9055 (1.9309) acc@1 0.2812 (0.2821) acc@5 0.6797 (0.6884)\n",
      "\u001b[32m[2020-06-24 04:17:28] __main__ INFO: \u001b[0mEpoch 27 Step 200/351 lr 0.100000 loss 1.8981 (1.9320) acc@1 0.2656 (0.2809) acc@5 0.7812 (0.6879)\n",
      "\u001b[32m[2020-06-24 04:19:20] __main__ INFO: \u001b[0mEpoch 27 Step 300/351 lr 0.100000 loss 1.9155 (1.9286) acc@1 0.2344 (0.2800) acc@5 0.7188 (0.6907)\n",
      "\u001b[32m[2020-06-24 04:20:18] __main__ INFO: \u001b[0mEpoch 27 Step 351/351 lr 0.100000 loss 1.9050 (1.9313) acc@1 0.2969 (0.2773) acc@5 0.7031 (0.6907)\n",
      "\u001b[32m[2020-06-24 04:20:18] __main__ INFO: \u001b[0mElapsed 393.55\n",
      "\u001b[32m[2020-06-24 04:20:18] __main__ INFO: \u001b[0mVal 27\n",
      "\u001b[32m[2020-06-24 04:20:31] __main__ INFO: \u001b[0mEpoch 27 loss 2.0672 acc@1 0.2524 acc@5 0.6742\n",
      "\u001b[32m[2020-06-24 04:20:31] __main__ INFO: \u001b[0mElapsed 13.12\n",
      "\u001b[32m[2020-06-24 04:20:31] __main__ INFO: \u001b[0mTrain 28 9477\n",
      "\u001b[32m[2020-06-24 04:22:23] __main__ INFO: \u001b[0mEpoch 28 Step 100/351 lr 0.100000 loss 2.0287 (1.9015) acc@1 0.2656 (0.2849) acc@5 0.6406 (0.6966)\n",
      "\u001b[32m[2020-06-24 04:24:14] __main__ INFO: \u001b[0mEpoch 28 Step 200/351 lr 0.100000 loss 1.8891 (1.9201) acc@1 0.2734 (0.2762) acc@5 0.6797 (0.6917)\n",
      "\u001b[32m[2020-06-24 04:26:06] __main__ INFO: \u001b[0mEpoch 28 Step 300/351 lr 0.100000 loss 1.9286 (1.9207) acc@1 0.2734 (0.2767) acc@5 0.6562 (0.6914)\n",
      "\u001b[32m[2020-06-24 04:27:03] __main__ INFO: \u001b[0mEpoch 28 Step 351/351 lr 0.100000 loss 2.0262 (1.9245) acc@1 0.2578 (0.2760) acc@5 0.6641 (0.6908)\n",
      "\u001b[32m[2020-06-24 04:27:03] __main__ INFO: \u001b[0mElapsed 392.14\n",
      "\u001b[32m[2020-06-24 04:27:03] __main__ INFO: \u001b[0mVal 28\n",
      "\u001b[32m[2020-06-24 04:27:16] __main__ INFO: \u001b[0mEpoch 28 loss 2.0348 acc@1 0.2344 acc@5 0.6632\n",
      "\u001b[32m[2020-06-24 04:27:16] __main__ INFO: \u001b[0mElapsed 13.17\n",
      "\u001b[32m[2020-06-24 04:27:16] __main__ INFO: \u001b[0mTrain 29 9828\n",
      "\u001b[32m[2020-06-24 04:29:08] __main__ INFO: \u001b[0mEpoch 29 Step 100/351 lr 0.100000 loss 1.8433 (1.9107) acc@1 0.3047 (0.2816) acc@5 0.7812 (0.6895)\n",
      "\u001b[32m[2020-06-24 04:31:00] __main__ INFO: \u001b[0mEpoch 29 Step 200/351 lr 0.100000 loss 1.9460 (1.9139) acc@1 0.3203 (0.2827) acc@5 0.6797 (0.6900)\n",
      "\u001b[32m[2020-06-24 04:32:52] __main__ INFO: \u001b[0mEpoch 29 Step 300/351 lr 0.100000 loss 1.9317 (1.9168) acc@1 0.2969 (0.2828) acc@5 0.6797 (0.6921)\n",
      "\u001b[32m[2020-06-24 04:33:50] __main__ INFO: \u001b[0mEpoch 29 Step 351/351 lr 0.100000 loss 2.0670 (1.9170) acc@1 0.2266 (0.2815) acc@5 0.6328 (0.6921)\n",
      "\u001b[32m[2020-06-24 04:33:50] __main__ INFO: \u001b[0mElapsed 393.65\n",
      "\u001b[32m[2020-06-24 04:33:50] __main__ INFO: \u001b[0mVal 29\n",
      "\u001b[32m[2020-06-24 04:34:03] __main__ INFO: \u001b[0mEpoch 29 loss 2.0145 acc@1 0.2470 acc@5 0.6748\n",
      "\u001b[32m[2020-06-24 04:34:03] __main__ INFO: \u001b[0mElapsed 13.11\n",
      "\u001b[32m[2020-06-24 04:34:03] __main__ INFO: \u001b[0mTrain 30 10179\n",
      "\u001b[32m[2020-06-24 04:35:55] __main__ INFO: \u001b[0mEpoch 30 Step 100/351 lr 0.100000 loss 1.9716 (1.9229) acc@1 0.2734 (0.2851) acc@5 0.7031 (0.6886)\n",
      "\u001b[32m[2020-06-24 04:37:46] __main__ INFO: \u001b[0mEpoch 30 Step 200/351 lr 0.100000 loss 1.8364 (1.9155) acc@1 0.3125 (0.2836) acc@5 0.6875 (0.6923)\n",
      "\u001b[32m[2020-06-24 04:39:38] __main__ INFO: \u001b[0mEpoch 30 Step 300/351 lr 0.100000 loss 1.8152 (1.9190) acc@1 0.3828 (0.2814) acc@5 0.6562 (0.6908)\n",
      "\u001b[32m[2020-06-24 04:40:35] __main__ INFO: \u001b[0mEpoch 30 Step 351/351 lr 0.100000 loss 1.9827 (1.9183) acc@1 0.2344 (0.2816) acc@5 0.6328 (0.6908)\n",
      "\u001b[32m[2020-06-24 04:40:35] __main__ INFO: \u001b[0mElapsed 392.17\n",
      "\u001b[32m[2020-06-24 04:40:35] __main__ INFO: \u001b[0mVal 30\n",
      "\u001b[32m[2020-06-24 04:40:48] __main__ INFO: \u001b[0mEpoch 30 loss 2.1531 acc@1 0.2398 acc@5 0.6540\n",
      "\u001b[32m[2020-06-24 04:40:48] __main__ INFO: \u001b[0mElapsed 13.16\n",
      "\u001b[32m[2020-06-24 04:40:48] __main__ INFO: \u001b[0mTrain 31 10530\n",
      "\u001b[32m[2020-06-24 04:42:40] __main__ INFO: \u001b[0mEpoch 31 Step 100/351 lr 0.100000 loss 1.9620 (1.8961) acc@1 0.2812 (0.2891) acc@5 0.7188 (0.6976)\n",
      "\u001b[32m[2020-06-24 04:44:32] __main__ INFO: \u001b[0mEpoch 31 Step 200/351 lr 0.100000 loss 1.9719 (1.9057) acc@1 0.2500 (0.2884) acc@5 0.6641 (0.6983)\n",
      "\u001b[32m[2020-06-24 04:46:25] __main__ INFO: \u001b[0mEpoch 31 Step 300/351 lr 0.100000 loss 1.8808 (1.9110) acc@1 0.2734 (0.2855) acc@5 0.7578 (0.6954)\n",
      "\u001b[32m[2020-06-24 04:47:22] __main__ INFO: \u001b[0mEpoch 31 Step 351/351 lr 0.100000 loss 1.8533 (1.9119) acc@1 0.3047 (0.2859) acc@5 0.6641 (0.6945)\n",
      "\u001b[32m[2020-06-24 04:47:22] __main__ INFO: \u001b[0mElapsed 393.62\n",
      "\u001b[32m[2020-06-24 04:47:22] __main__ INFO: \u001b[0mVal 31\n",
      "\u001b[32m[2020-06-24 04:47:35] __main__ INFO: \u001b[0mEpoch 31 loss 1.9684 acc@1 0.2582 acc@5 0.6788\n",
      "\u001b[32m[2020-06-24 04:47:35] __main__ INFO: \u001b[0mElapsed 13.09\n",
      "\u001b[32m[2020-06-24 04:47:35] __main__ INFO: \u001b[0mTrain 32 10881\n",
      "\u001b[32m[2020-06-24 04:49:27] __main__ INFO: \u001b[0mEpoch 32 Step 100/351 lr 0.100000 loss 1.8972 (1.9065) acc@1 0.2578 (0.2877) acc@5 0.6562 (0.6972)\n",
      "\u001b[32m[2020-06-24 04:51:18] __main__ INFO: \u001b[0mEpoch 32 Step 200/351 lr 0.100000 loss 1.8371 (1.9066) acc@1 0.3203 (0.2869) acc@5 0.6328 (0.6963)\n",
      "\u001b[32m[2020-06-24 04:53:10] __main__ INFO: \u001b[0mEpoch 32 Step 300/351 lr 0.100000 loss 1.8803 (1.9056) acc@1 0.2656 (0.2874) acc@5 0.7734 (0.6967)\n",
      "\u001b[32m[2020-06-24 04:54:07] __main__ INFO: \u001b[0mEpoch 32 Step 351/351 lr 0.100000 loss 1.8444 (1.9060) acc@1 0.2812 (0.2871) acc@5 0.6719 (0.6953)\n",
      "\u001b[32m[2020-06-24 04:54:07] __main__ INFO: \u001b[0mElapsed 392.08\n",
      "\u001b[32m[2020-06-24 04:54:07] __main__ INFO: \u001b[0mVal 32\n",
      "\u001b[32m[2020-06-24 04:54:20] __main__ INFO: \u001b[0mEpoch 32 loss 2.0220 acc@1 0.2594 acc@5 0.6676\n",
      "\u001b[32m[2020-06-24 04:54:20] __main__ INFO: \u001b[0mElapsed 13.12\n",
      "\u001b[32m[2020-06-24 04:54:20] __main__ INFO: \u001b[0mTrain 33 11232\n",
      "\u001b[32m[2020-06-24 04:56:12] __main__ INFO: \u001b[0mEpoch 33 Step 100/351 lr 0.100000 loss 2.0529 (1.8878) acc@1 0.2109 (0.2921) acc@5 0.6641 (0.7013)\n",
      "\u001b[32m[2020-06-24 04:58:04] __main__ INFO: \u001b[0mEpoch 33 Step 200/351 lr 0.100000 loss 1.8750 (1.8999) acc@1 0.3125 (0.2879) acc@5 0.7344 (0.6940)\n",
      "\u001b[32m[2020-06-24 04:59:56] __main__ INFO: \u001b[0mEpoch 33 Step 300/351 lr 0.100000 loss 1.8878 (1.9011) acc@1 0.3203 (0.2874) acc@5 0.6719 (0.6923)\n",
      "\u001b[32m[2020-06-24 05:00:54] __main__ INFO: \u001b[0mEpoch 33 Step 351/351 lr 0.100000 loss 1.9404 (1.9040) acc@1 0.2812 (0.2864) acc@5 0.6953 (0.6918)\n",
      "\u001b[32m[2020-06-24 05:00:54] __main__ INFO: \u001b[0mElapsed 393.61\n",
      "\u001b[32m[2020-06-24 05:00:54] __main__ INFO: \u001b[0mVal 33\n",
      "\u001b[32m[2020-06-24 05:01:07] __main__ INFO: \u001b[0mEpoch 33 loss 1.9922 acc@1 0.2594 acc@5 0.6794\n",
      "\u001b[32m[2020-06-24 05:01:07] __main__ INFO: \u001b[0mElapsed 13.12\n",
      "\u001b[32m[2020-06-24 05:01:07] __main__ INFO: \u001b[0mTrain 34 11583\n",
      "\u001b[32m[2020-06-24 05:02:59] __main__ INFO: \u001b[0mEpoch 34 Step 100/351 lr 0.100000 loss 2.0293 (1.8960) acc@1 0.2422 (0.2900) acc@5 0.6016 (0.7052)\n",
      "\u001b[32m[2020-06-24 05:04:50] __main__ INFO: \u001b[0mEpoch 34 Step 200/351 lr 0.100000 loss 1.8455 (1.8974) acc@1 0.3516 (0.2889) acc@5 0.6875 (0.7023)\n",
      "\u001b[32m[2020-06-24 05:06:42] __main__ INFO: \u001b[0mEpoch 34 Step 300/351 lr 0.100000 loss 2.0001 (1.9003) acc@1 0.2891 (0.2875) acc@5 0.6797 (0.7027)\n",
      "\u001b[32m[2020-06-24 05:07:39] __main__ INFO: \u001b[0mEpoch 34 Step 351/351 lr 0.100000 loss 1.9082 (1.9009) acc@1 0.2812 (0.2882) acc@5 0.6562 (0.7015)\n",
      "\u001b[32m[2020-06-24 05:07:39] __main__ INFO: \u001b[0mElapsed 392.04\n",
      "\u001b[32m[2020-06-24 05:07:39] __main__ INFO: \u001b[0mVal 34\n",
      "\u001b[32m[2020-06-24 05:07:52] __main__ INFO: \u001b[0mEpoch 34 loss 1.9983 acc@1 0.2520 acc@5 0.6766\n",
      "\u001b[32m[2020-06-24 05:07:52] __main__ INFO: \u001b[0mElapsed 13.16\n",
      "\u001b[32m[2020-06-24 05:07:52] __main__ INFO: \u001b[0mTrain 35 11934\n",
      "\u001b[32m[2020-06-24 05:09:44] __main__ INFO: \u001b[0mEpoch 35 Step 100/351 lr 0.100000 loss 1.7265 (1.8793) acc@1 0.3672 (0.3016) acc@5 0.6797 (0.7030)\n",
      "\u001b[32m[2020-06-24 05:11:36] __main__ INFO: \u001b[0mEpoch 35 Step 200/351 lr 0.100000 loss 1.7907 (1.8919) acc@1 0.3125 (0.2930) acc@5 0.6641 (0.7014)\n",
      "\u001b[32m[2020-06-24 05:13:28] __main__ INFO: \u001b[0mEpoch 35 Step 300/351 lr 0.100000 loss 1.8934 (1.8946) acc@1 0.2969 (0.2935) acc@5 0.7422 (0.7003)\n",
      "\u001b[32m[2020-06-24 05:14:25] __main__ INFO: \u001b[0mEpoch 35 Step 351/351 lr 0.100000 loss 1.8513 (1.8958) acc@1 0.3281 (0.2927) acc@5 0.7031 (0.6995)\n",
      "\u001b[32m[2020-06-24 05:14:25] __main__ INFO: \u001b[0mElapsed 393.46\n",
      "\u001b[32m[2020-06-24 05:14:25] __main__ INFO: \u001b[0mVal 35\n",
      "\u001b[32m[2020-06-24 05:14:39] __main__ INFO: \u001b[0mEpoch 35 loss 2.2979 acc@1 0.2188 acc@5 0.6514\n",
      "\u001b[32m[2020-06-24 05:14:39] __main__ INFO: \u001b[0mElapsed 13.12\n",
      "\u001b[32m[2020-06-24 05:14:39] __main__ INFO: \u001b[0mTrain 36 12285\n",
      "\u001b[32m[2020-06-24 05:16:30] __main__ INFO: \u001b[0mEpoch 36 Step 100/351 lr 0.100000 loss 1.8750 (1.8910) acc@1 0.2812 (0.2923) acc@5 0.6875 (0.6963)\n",
      "\u001b[32m[2020-06-24 05:18:22] __main__ INFO: \u001b[0mEpoch 36 Step 200/351 lr 0.100000 loss 1.7778 (1.8937) acc@1 0.3672 (0.2920) acc@5 0.7109 (0.6949)\n",
      "\u001b[32m[2020-06-24 05:20:14] __main__ INFO: \u001b[0mEpoch 36 Step 300/351 lr 0.100000 loss 2.0564 (1.8960) acc@1 0.2109 (0.2918) acc@5 0.6094 (0.6944)\n",
      "\u001b[32m[2020-06-24 05:21:10] __main__ INFO: \u001b[0mEpoch 36 Step 351/351 lr 0.100000 loss 1.9734 (1.8956) acc@1 0.2344 (0.2913) acc@5 0.7031 (0.6947)\n",
      "\u001b[32m[2020-06-24 05:21:10] __main__ INFO: \u001b[0mElapsed 391.85\n",
      "\u001b[32m[2020-06-24 05:21:10] __main__ INFO: \u001b[0mVal 36\n",
      "\u001b[32m[2020-06-24 05:21:24] __main__ INFO: \u001b[0mEpoch 36 loss 2.0076 acc@1 0.2488 acc@5 0.6674\n",
      "\u001b[32m[2020-06-24 05:21:24] __main__ INFO: \u001b[0mElapsed 13.13\n",
      "\u001b[32m[2020-06-24 05:21:24] __main__ INFO: \u001b[0mTrain 37 12636\n",
      "\u001b[32m[2020-06-24 05:23:16] __main__ INFO: \u001b[0mEpoch 37 Step 100/351 lr 0.100000 loss 1.7597 (1.8744) acc@1 0.4062 (0.2948) acc@5 0.7188 (0.7016)\n",
      "\u001b[32m[2020-06-24 05:25:08] __main__ INFO: \u001b[0mEpoch 37 Step 200/351 lr 0.100000 loss 1.9323 (1.8841) acc@1 0.3047 (0.2920) acc@5 0.6953 (0.6989)\n",
      "\u001b[32m[2020-06-24 05:27:00] __main__ INFO: \u001b[0mEpoch 37 Step 300/351 lr 0.100000 loss 2.0327 (1.8881) acc@1 0.2266 (0.2912) acc@5 0.6484 (0.6990)\n",
      "\u001b[32m[2020-06-24 05:27:57] __main__ INFO: \u001b[0mEpoch 37 Step 351/351 lr 0.100000 loss 2.0275 (1.8901) acc@1 0.2969 (0.2915) acc@5 0.7031 (0.6987)\n",
      "\u001b[32m[2020-06-24 05:27:57] __main__ INFO: \u001b[0mElapsed 393.26\n",
      "\u001b[32m[2020-06-24 05:27:57] __main__ INFO: \u001b[0mVal 37\n",
      "\u001b[32m[2020-06-24 05:28:10] __main__ INFO: \u001b[0mEpoch 37 loss 1.9941 acc@1 0.2628 acc@5 0.6830\n",
      "\u001b[32m[2020-06-24 05:28:10] __main__ INFO: \u001b[0mElapsed 13.11\n",
      "\u001b[32m[2020-06-24 05:28:10] __main__ INFO: \u001b[0mTrain 38 12987\n",
      "\u001b[32m[2020-06-24 05:30:02] __main__ INFO: \u001b[0mEpoch 38 Step 100/351 lr 0.100000 loss 1.8584 (1.8612) acc@1 0.3672 (0.3016) acc@5 0.7109 (0.7037)\n",
      "\u001b[32m[2020-06-24 05:31:53] __main__ INFO: \u001b[0mEpoch 38 Step 200/351 lr 0.100000 loss 1.8801 (1.8809) acc@1 0.3047 (0.2937) acc@5 0.7109 (0.7014)\n",
      "\u001b[32m[2020-06-24 05:33:45] __main__ INFO: \u001b[0mEpoch 38 Step 300/351 lr 0.100000 loss 1.7575 (1.8806) acc@1 0.3672 (0.2928) acc@5 0.7422 (0.6995)\n",
      "\u001b[32m[2020-06-24 05:34:42] __main__ INFO: \u001b[0mEpoch 38 Step 351/351 lr 0.100000 loss 2.0059 (1.8842) acc@1 0.2344 (0.2923) acc@5 0.6484 (0.6988)\n",
      "\u001b[32m[2020-06-24 05:34:42] __main__ INFO: \u001b[0mElapsed 391.84\n",
      "\u001b[32m[2020-06-24 05:34:42] __main__ INFO: \u001b[0mVal 38\n",
      "\u001b[32m[2020-06-24 05:34:55] __main__ INFO: \u001b[0mEpoch 38 loss 1.9782 acc@1 0.2592 acc@5 0.6836\n",
      "\u001b[32m[2020-06-24 05:34:55] __main__ INFO: \u001b[0mElapsed 13.12\n",
      "\u001b[32m[2020-06-24 05:34:55] __main__ INFO: \u001b[0mTrain 39 13338\n",
      "\u001b[32m[2020-06-24 05:36:47] __main__ INFO: \u001b[0mEpoch 39 Step 100/351 lr 0.100000 loss 1.9218 (1.8727) acc@1 0.2422 (0.2979) acc@5 0.6406 (0.6999)\n",
      "\u001b[32m[2020-06-24 05:38:39] __main__ INFO: \u001b[0mEpoch 39 Step 200/351 lr 0.100000 loss 1.8232 (1.8785) acc@1 0.3281 (0.2956) acc@5 0.7031 (0.6977)\n",
      "\u001b[32m[2020-06-24 05:40:31] __main__ INFO: \u001b[0mEpoch 39 Step 300/351 lr 0.100000 loss 1.9442 (1.8850) acc@1 0.2188 (0.2929) acc@5 0.6875 (0.6975)\n",
      "\u001b[32m[2020-06-24 05:41:28] __main__ INFO: \u001b[0mEpoch 39 Step 351/351 lr 0.100000 loss 1.8626 (1.8856) acc@1 0.2500 (0.2928) acc@5 0.7266 (0.6968)\n",
      "\u001b[32m[2020-06-24 05:41:28] __main__ INFO: \u001b[0mElapsed 393.38\n",
      "\u001b[32m[2020-06-24 05:41:28] __main__ INFO: \u001b[0mVal 39\n",
      "\u001b[32m[2020-06-24 05:41:41] __main__ INFO: \u001b[0mEpoch 39 loss 2.1680 acc@1 0.2350 acc@5 0.6520\n",
      "\u001b[32m[2020-06-24 05:41:41] __main__ INFO: \u001b[0mElapsed 13.12\n",
      "\u001b[32m[2020-06-24 05:41:41] __main__ INFO: \u001b[0mTrain 40 13689\n",
      "\u001b[32m[2020-06-24 05:43:33] __main__ INFO: \u001b[0mEpoch 40 Step 100/351 lr 0.100000 loss 1.8946 (1.8776) acc@1 0.2734 (0.2973) acc@5 0.6406 (0.7048)\n",
      "\u001b[32m[2020-06-24 05:45:25] __main__ INFO: \u001b[0mEpoch 40 Step 200/351 lr 0.100000 loss 1.8420 (1.8784) acc@1 0.3438 (0.2983) acc@5 0.7188 (0.7019)\n",
      "\u001b[32m[2020-06-24 05:47:16] __main__ INFO: \u001b[0mEpoch 40 Step 300/351 lr 0.100000 loss 2.0100 (1.8797) acc@1 0.2500 (0.2971) acc@5 0.6953 (0.7013)\n",
      "\u001b[32m[2020-06-24 05:48:13] __main__ INFO: \u001b[0mEpoch 40 Step 351/351 lr 0.100000 loss 1.9109 (1.8819) acc@1 0.2578 (0.2953) acc@5 0.6641 (0.7007)\n",
      "\u001b[32m[2020-06-24 05:48:13] __main__ INFO: \u001b[0mElapsed 391.84\n",
      "\u001b[32m[2020-06-24 05:48:13] __main__ INFO: \u001b[0mVal 40\n",
      "\u001b[32m[2020-06-24 05:48:26] __main__ INFO: \u001b[0mEpoch 40 loss 1.9365 acc@1 0.2706 acc@5 0.6894\n",
      "\u001b[32m[2020-06-24 05:48:26] __main__ INFO: \u001b[0mElapsed 13.11\n",
      "\u001b[32m[2020-06-24 05:48:26] __main__ INFO: \u001b[0mTrain 41 14040\n",
      "\u001b[32m[2020-06-24 05:50:19] __main__ INFO: \u001b[0mEpoch 41 Step 100/351 lr 0.100000 loss 1.8077 (1.8762) acc@1 0.3516 (0.3002) acc@5 0.7578 (0.7030)\n",
      "\u001b[32m[2020-06-24 05:52:11] __main__ INFO: \u001b[0mEpoch 41 Step 200/351 lr 0.100000 loss 1.8729 (1.8764) acc@1 0.3203 (0.2976) acc@5 0.6875 (0.6999)\n",
      "\u001b[32m[2020-06-24 05:54:03] __main__ INFO: \u001b[0mEpoch 41 Step 300/351 lr 0.100000 loss 1.8626 (1.8782) acc@1 0.3203 (0.2956) acc@5 0.6641 (0.6972)\n",
      "\u001b[32m[2020-06-24 05:55:00] __main__ INFO: \u001b[0mEpoch 41 Step 351/351 lr 0.100000 loss 1.9586 (1.8802) acc@1 0.2656 (0.2948) acc@5 0.7031 (0.6971)\n",
      "\u001b[32m[2020-06-24 05:55:00] __main__ INFO: \u001b[0mElapsed 393.34\n",
      "\u001b[32m[2020-06-24 05:55:00] __main__ INFO: \u001b[0mVal 41\n",
      "\u001b[32m[2020-06-24 05:55:13] __main__ INFO: \u001b[0mEpoch 41 loss 1.9739 acc@1 0.2706 acc@5 0.6712\n",
      "\u001b[32m[2020-06-24 05:55:13] __main__ INFO: \u001b[0mElapsed 13.13\n",
      "\u001b[32m[2020-06-24 05:55:13] __main__ INFO: \u001b[0mTrain 42 14391\n",
      "\u001b[32m[2020-06-24 05:57:05] __main__ INFO: \u001b[0mEpoch 42 Step 100/351 lr 0.100000 loss 1.8833 (1.8722) acc@1 0.3516 (0.2935) acc@5 0.6172 (0.6955)\n",
      "\u001b[32m[2020-06-24 05:58:56] __main__ INFO: \u001b[0mEpoch 42 Step 200/351 lr 0.100000 loss 2.0127 (1.8722) acc@1 0.2344 (0.2959) acc@5 0.6875 (0.7029)\n",
      "\u001b[32m[2020-06-24 06:00:48] __main__ INFO: \u001b[0mEpoch 42 Step 300/351 lr 0.100000 loss 1.9913 (1.8751) acc@1 0.2266 (0.2952) acc@5 0.6328 (0.7017)\n",
      "\u001b[32m[2020-06-24 06:01:45] __main__ INFO: \u001b[0mEpoch 42 Step 351/351 lr 0.100000 loss 2.0328 (1.8765) acc@1 0.2500 (0.2952) acc@5 0.6797 (0.7013)\n",
      "\u001b[32m[2020-06-24 06:01:45] __main__ INFO: \u001b[0mElapsed 391.74\n",
      "\u001b[32m[2020-06-24 06:01:45] __main__ INFO: \u001b[0mVal 42\n",
      "\u001b[32m[2020-06-24 06:01:58] __main__ INFO: \u001b[0mEpoch 42 loss 1.9691 acc@1 0.2606 acc@5 0.6746\n",
      "\u001b[32m[2020-06-24 06:01:58] __main__ INFO: \u001b[0mElapsed 13.12\n",
      "\u001b[32m[2020-06-24 06:01:58] __main__ INFO: \u001b[0mTrain 43 14742\n",
      "\u001b[32m[2020-06-24 06:03:50] __main__ INFO: \u001b[0mEpoch 43 Step 100/351 lr 0.100000 loss 1.7969 (1.8709) acc@1 0.3516 (0.2942) acc@5 0.7266 (0.7031)\n",
      "\u001b[32m[2020-06-24 06:05:42] __main__ INFO: \u001b[0mEpoch 43 Step 200/351 lr 0.100000 loss 1.8667 (1.8700) acc@1 0.2812 (0.2984) acc@5 0.7031 (0.7058)\n",
      "\u001b[32m[2020-06-24 06:07:34] __main__ INFO: \u001b[0mEpoch 43 Step 300/351 lr 0.100000 loss 1.9005 (1.8732) acc@1 0.2422 (0.2984) acc@5 0.7344 (0.7054)\n",
      "\u001b[32m[2020-06-24 06:08:31] __main__ INFO: \u001b[0mEpoch 43 Step 351/351 lr 0.100000 loss 2.0211 (1.8737) acc@1 0.2812 (0.2986) acc@5 0.6641 (0.7041)\n",
      "\u001b[32m[2020-06-24 06:08:31] __main__ INFO: \u001b[0mElapsed 393.34\n",
      "\u001b[32m[2020-06-24 06:08:31] __main__ INFO: \u001b[0mVal 43\n",
      "\u001b[32m[2020-06-24 06:08:44] __main__ INFO: \u001b[0mEpoch 43 loss 1.9846 acc@1 0.2660 acc@5 0.6912\n",
      "\u001b[32m[2020-06-24 06:08:44] __main__ INFO: \u001b[0mElapsed 13.11\n",
      "\u001b[32m[2020-06-24 06:08:44] __main__ INFO: \u001b[0mTrain 44 15093\n",
      "\u001b[32m[2020-06-24 06:10:36] __main__ INFO: \u001b[0mEpoch 44 Step 100/351 lr 0.100000 loss 1.8234 (1.8524) acc@1 0.3047 (0.3085) acc@5 0.6797 (0.7049)\n",
      "\u001b[32m[2020-06-24 06:12:27] __main__ INFO: \u001b[0mEpoch 44 Step 200/351 lr 0.100000 loss 1.8113 (1.8647) acc@1 0.3594 (0.3000) acc@5 0.7109 (0.7027)\n",
      "\u001b[32m[2020-06-24 06:14:19] __main__ INFO: \u001b[0mEpoch 44 Step 300/351 lr 0.100000 loss 2.0770 (1.8709) acc@1 0.2734 (0.2982) acc@5 0.7266 (0.7020)\n",
      "\u001b[32m[2020-06-24 06:15:16] __main__ INFO: \u001b[0mEpoch 44 Step 351/351 lr 0.100000 loss 1.8376 (1.8743) acc@1 0.2969 (0.2970) acc@5 0.7500 (0.7017)\n",
      "\u001b[32m[2020-06-24 06:15:16] __main__ INFO: \u001b[0mElapsed 391.82\n",
      "\u001b[32m[2020-06-24 06:15:16] __main__ INFO: \u001b[0mVal 44\n",
      "\u001b[32m[2020-06-24 06:15:29] __main__ INFO: \u001b[0mEpoch 44 loss 2.1166 acc@1 0.2422 acc@5 0.6808\n",
      "\u001b[32m[2020-06-24 06:15:29] __main__ INFO: \u001b[0mElapsed 13.13\n",
      "\u001b[32m[2020-06-24 06:15:29] __main__ INFO: \u001b[0mTrain 45 15444\n",
      "\u001b[32m[2020-06-24 06:17:21] __main__ INFO: \u001b[0mEpoch 45 Step 100/351 lr 0.100000 loss 1.7103 (1.8456) acc@1 0.3672 (0.3086) acc@5 0.7109 (0.7041)\n",
      "\u001b[32m[2020-06-24 06:19:13] __main__ INFO: \u001b[0mEpoch 45 Step 200/351 lr 0.100000 loss 2.0059 (1.8618) acc@1 0.2422 (0.3075) acc@5 0.6328 (0.7006)\n",
      "\u001b[32m[2020-06-24 06:21:05] __main__ INFO: \u001b[0mEpoch 45 Step 300/351 lr 0.100000 loss 1.8941 (1.8644) acc@1 0.2812 (0.3046) acc@5 0.7266 (0.7015)\n",
      "\u001b[32m[2020-06-24 06:22:02] __main__ INFO: \u001b[0mEpoch 45 Step 351/351 lr 0.100000 loss 1.8383 (1.8666) acc@1 0.3281 (0.3040) acc@5 0.7031 (0.7016)\n",
      "\u001b[32m[2020-06-24 06:22:02] __main__ INFO: \u001b[0mElapsed 393.35\n",
      "\u001b[32m[2020-06-24 06:22:02] __main__ INFO: \u001b[0mVal 45\n",
      "\u001b[32m[2020-06-24 06:22:16] __main__ INFO: \u001b[0mEpoch 45 loss 1.9354 acc@1 0.2774 acc@5 0.6956\n",
      "\u001b[32m[2020-06-24 06:22:16] __main__ INFO: \u001b[0mElapsed 13.12\n",
      "\u001b[32m[2020-06-24 06:22:16] __main__ INFO: \u001b[0mTrain 46 15795\n",
      "\u001b[32m[2020-06-24 06:24:07] __main__ INFO: \u001b[0mEpoch 46 Step 100/351 lr 0.100000 loss 1.8523 (1.8536) acc@1 0.2891 (0.3054) acc@5 0.6875 (0.7057)\n",
      "\u001b[32m[2020-06-24 06:25:59] __main__ INFO: \u001b[0mEpoch 46 Step 200/351 lr 0.100000 loss 1.7759 (1.8643) acc@1 0.3047 (0.3014) acc@5 0.7031 (0.7049)\n",
      "\u001b[32m[2020-06-24 06:27:50] __main__ INFO: \u001b[0mEpoch 46 Step 300/351 lr 0.100000 loss 1.8600 (1.8624) acc@1 0.2969 (0.3040) acc@5 0.7500 (0.7066)\n",
      "\u001b[32m[2020-06-24 06:28:47] __main__ INFO: \u001b[0mEpoch 46 Step 351/351 lr 0.100000 loss 2.0357 (1.8656) acc@1 0.2266 (0.3022) acc@5 0.6875 (0.7037)\n",
      "\u001b[32m[2020-06-24 06:28:47] __main__ INFO: \u001b[0mElapsed 391.69\n",
      "\u001b[32m[2020-06-24 06:28:47] __main__ INFO: \u001b[0mVal 46\n",
      "\u001b[32m[2020-06-24 06:29:00] __main__ INFO: \u001b[0mEpoch 46 loss 1.9545 acc@1 0.2610 acc@5 0.6862\n",
      "\u001b[32m[2020-06-24 06:29:00] __main__ INFO: \u001b[0mElapsed 13.11\n",
      "\u001b[32m[2020-06-24 06:29:00] __main__ INFO: \u001b[0mTrain 47 16146\n",
      "\u001b[32m[2020-06-24 06:30:52] __main__ INFO: \u001b[0mEpoch 47 Step 100/351 lr 0.100000 loss 1.8707 (1.8491) acc@1 0.3047 (0.3076) acc@5 0.6172 (0.7111)\n",
      "\u001b[32m[2020-06-24 06:32:44] __main__ INFO: \u001b[0mEpoch 47 Step 200/351 lr 0.100000 loss 1.8724 (1.8641) acc@1 0.2891 (0.3005) acc@5 0.7656 (0.7062)\n",
      "\u001b[32m[2020-06-24 06:34:36] __main__ INFO: \u001b[0mEpoch 47 Step 300/351 lr 0.100000 loss 1.8606 (1.8681) acc@1 0.2734 (0.2985) acc@5 0.7656 (0.7047)\n",
      "\u001b[32m[2020-06-24 06:35:34] __main__ INFO: \u001b[0mEpoch 47 Step 351/351 lr 0.100000 loss 1.8307 (1.8703) acc@1 0.3516 (0.2983) acc@5 0.7500 (0.7041)\n",
      "\u001b[32m[2020-06-24 06:35:34] __main__ INFO: \u001b[0mElapsed 393.17\n",
      "\u001b[32m[2020-06-24 06:35:34] __main__ INFO: \u001b[0mVal 47\n",
      "\u001b[32m[2020-06-24 06:35:47] __main__ INFO: \u001b[0mEpoch 47 loss 2.0965 acc@1 0.2384 acc@5 0.6744\n",
      "\u001b[32m[2020-06-24 06:35:47] __main__ INFO: \u001b[0mElapsed 13.13\n",
      "\u001b[32m[2020-06-24 06:35:47] __main__ INFO: \u001b[0mTrain 48 16497\n",
      "\u001b[32m[2020-06-24 06:37:38] __main__ INFO: \u001b[0mEpoch 48 Step 100/351 lr 0.100000 loss 1.7090 (1.8591) acc@1 0.3438 (0.3027) acc@5 0.7188 (0.7093)\n",
      "\u001b[32m[2020-06-24 06:39:30] __main__ INFO: \u001b[0mEpoch 48 Step 200/351 lr 0.100000 loss 1.9890 (1.8568) acc@1 0.2109 (0.3043) acc@5 0.6875 (0.7086)\n",
      "\u001b[32m[2020-06-24 06:41:21] __main__ INFO: \u001b[0mEpoch 48 Step 300/351 lr 0.100000 loss 1.8265 (1.8633) acc@1 0.3203 (0.3026) acc@5 0.6875 (0.7059)\n",
      "\u001b[32m[2020-06-24 06:42:18] __main__ INFO: \u001b[0mEpoch 48 Step 351/351 lr 0.100000 loss 1.8237 (1.8656) acc@1 0.3203 (0.3017) acc@5 0.7188 (0.7038)\n",
      "\u001b[32m[2020-06-24 06:42:18] __main__ INFO: \u001b[0mElapsed 391.72\n",
      "\u001b[32m[2020-06-24 06:42:18] __main__ INFO: \u001b[0mVal 48\n",
      "\u001b[32m[2020-06-24 06:42:32] __main__ INFO: \u001b[0mEpoch 48 loss 1.9631 acc@1 0.2706 acc@5 0.6722\n",
      "\u001b[32m[2020-06-24 06:42:32] __main__ INFO: \u001b[0mElapsed 13.13\n",
      "\u001b[32m[2020-06-24 06:42:32] __main__ INFO: \u001b[0mTrain 49 16848\n",
      "\u001b[32m[2020-06-24 06:44:24] __main__ INFO: \u001b[0mEpoch 49 Step 100/351 lr 0.100000 loss 1.8975 (1.8545) acc@1 0.2969 (0.3030) acc@5 0.7109 (0.7088)\n",
      "\u001b[32m[2020-06-24 06:46:16] __main__ INFO: \u001b[0mEpoch 49 Step 200/351 lr 0.100000 loss 1.8182 (1.8585) acc@1 0.3203 (0.3056) acc@5 0.6875 (0.7072)\n",
      "\u001b[32m[2020-06-24 06:48:08] __main__ INFO: \u001b[0mEpoch 49 Step 300/351 lr 0.100000 loss 1.9783 (1.8611) acc@1 0.2734 (0.3041) acc@5 0.7344 (0.7063)\n",
      "\u001b[32m[2020-06-24 06:49:05] __main__ INFO: \u001b[0mEpoch 49 Step 351/351 lr 0.100000 loss 1.9933 (1.8607) acc@1 0.3281 (0.3049) acc@5 0.6562 (0.7066)\n",
      "\u001b[32m[2020-06-24 06:49:05] __main__ INFO: \u001b[0mElapsed 393.13\n",
      "\u001b[32m[2020-06-24 06:49:05] __main__ INFO: \u001b[0mVal 49\n",
      "\u001b[32m[2020-06-24 06:49:18] __main__ INFO: \u001b[0mEpoch 49 loss 2.0690 acc@1 0.2452 acc@5 0.6666\n",
      "\u001b[32m[2020-06-24 06:49:18] __main__ INFO: \u001b[0mElapsed 13.09\n",
      "\u001b[32m[2020-06-24 06:49:18] __main__ INFO: \u001b[0mTrain 50 17199\n",
      "\u001b[32m[2020-06-24 06:51:09] __main__ INFO: \u001b[0mEpoch 50 Step 100/351 lr 0.100000 loss 1.9064 (1.8501) acc@1 0.2891 (0.3080) acc@5 0.6875 (0.7009)\n",
      "\u001b[32m[2020-06-24 06:53:01] __main__ INFO: \u001b[0mEpoch 50 Step 200/351 lr 0.100000 loss 1.8242 (1.8586) acc@1 0.3047 (0.3057) acc@5 0.7656 (0.7023)\n",
      "\u001b[32m[2020-06-24 06:54:52] __main__ INFO: \u001b[0mEpoch 50 Step 300/351 lr 0.100000 loss 1.9722 (1.8614) acc@1 0.2344 (0.3042) acc@5 0.6328 (0.7019)\n",
      "\u001b[32m[2020-06-24 06:55:49] __main__ INFO: \u001b[0mEpoch 50 Step 351/351 lr 0.100000 loss 1.8852 (1.8638) acc@1 0.3281 (0.3038) acc@5 0.7188 (0.7014)\n",
      "\u001b[32m[2020-06-24 06:55:49] __main__ INFO: \u001b[0mElapsed 391.63\n",
      "\u001b[32m[2020-06-24 06:55:49] __main__ INFO: \u001b[0mVal 50\n",
      "\u001b[32m[2020-06-24 06:56:03] __main__ INFO: \u001b[0mEpoch 50 loss 2.0535 acc@1 0.2462 acc@5 0.6664\n",
      "\u001b[32m[2020-06-24 06:56:03] __main__ INFO: \u001b[0mElapsed 13.12\n",
      "\u001b[32m[2020-06-24 06:56:03] __main__ INFO: \u001b[0mTrain 51 17550\n",
      "\u001b[32m[2020-06-24 06:57:55] __main__ INFO: \u001b[0mEpoch 51 Step 100/351 lr 0.100000 loss 1.8354 (1.8477) acc@1 0.3203 (0.3080) acc@5 0.7188 (0.7073)\n",
      "\u001b[32m[2020-06-24 06:59:47] __main__ INFO: \u001b[0mEpoch 51 Step 200/351 lr 0.100000 loss 1.7628 (1.8503) acc@1 0.3359 (0.3082) acc@5 0.7109 (0.7053)\n",
      "\u001b[32m[2020-06-24 07:01:39] __main__ INFO: \u001b[0mEpoch 51 Step 300/351 lr 0.100000 loss 1.8867 (1.8572) acc@1 0.2891 (0.3051) acc@5 0.6953 (0.7045)\n",
      "\u001b[32m[2020-06-24 07:02:36] __main__ INFO: \u001b[0mEpoch 51 Step 351/351 lr 0.100000 loss 1.8491 (1.8591) acc@1 0.3281 (0.3054) acc@5 0.7188 (0.7039)\n",
      "\u001b[32m[2020-06-24 07:02:36] __main__ INFO: \u001b[0mElapsed 393.30\n",
      "\u001b[32m[2020-06-24 07:02:36] __main__ INFO: \u001b[0mVal 51\n",
      "\u001b[32m[2020-06-24 07:02:49] __main__ INFO: \u001b[0mEpoch 51 loss 1.9750 acc@1 0.2642 acc@5 0.6722\n",
      "\u001b[32m[2020-06-24 07:02:49] __main__ INFO: \u001b[0mElapsed 13.10\n",
      "\u001b[32m[2020-06-24 07:02:49] __main__ INFO: \u001b[0mTrain 52 17901\n",
      "\u001b[32m[2020-06-24 07:04:41] __main__ INFO: \u001b[0mEpoch 52 Step 100/351 lr 0.100000 loss 1.8621 (1.8457) acc@1 0.2891 (0.3105) acc@5 0.6875 (0.7085)\n",
      "\u001b[32m[2020-06-24 07:06:32] __main__ INFO: \u001b[0mEpoch 52 Step 200/351 lr 0.100000 loss 1.8759 (1.8516) acc@1 0.2500 (0.3084) acc@5 0.7188 (0.7058)\n",
      "\u001b[32m[2020-06-24 07:08:24] __main__ INFO: \u001b[0mEpoch 52 Step 300/351 lr 0.100000 loss 1.6971 (1.8528) acc@1 0.3594 (0.3071) acc@5 0.7500 (0.7037)\n",
      "\u001b[32m[2020-06-24 07:09:21] __main__ INFO: \u001b[0mEpoch 52 Step 351/351 lr 0.100000 loss 1.8001 (1.8528) acc@1 0.2969 (0.3072) acc@5 0.6875 (0.7042)\n",
      "\u001b[32m[2020-06-24 07:09:21] __main__ INFO: \u001b[0mElapsed 391.66\n",
      "\u001b[32m[2020-06-24 07:09:21] __main__ INFO: \u001b[0mVal 52\n",
      "\u001b[32m[2020-06-24 07:09:34] __main__ INFO: \u001b[0mEpoch 52 loss 2.2056 acc@1 0.2328 acc@5 0.6860\n",
      "\u001b[32m[2020-06-24 07:09:34] __main__ INFO: \u001b[0mElapsed 13.13\n",
      "\u001b[32m[2020-06-24 07:09:34] __main__ INFO: \u001b[0mTrain 53 18252\n",
      "\u001b[32m[2020-06-24 07:11:26] __main__ INFO: \u001b[0mEpoch 53 Step 100/351 lr 0.100000 loss 1.8636 (1.8454) acc@1 0.2656 (0.3127) acc@5 0.6719 (0.7049)\n",
      "\u001b[32m[2020-06-24 07:13:18] __main__ INFO: \u001b[0mEpoch 53 Step 200/351 lr 0.100000 loss 1.8080 (1.8566) acc@1 0.3594 (0.3080) acc@5 0.6953 (0.7050)\n",
      "\u001b[32m[2020-06-24 07:15:10] __main__ INFO: \u001b[0mEpoch 53 Step 300/351 lr 0.100000 loss 1.8110 (1.8556) acc@1 0.3125 (0.3091) acc@5 0.7266 (0.7064)\n",
      "\u001b[32m[2020-06-24 07:16:07] __main__ INFO: \u001b[0mEpoch 53 Step 351/351 lr 0.100000 loss 1.9791 (1.8558) acc@1 0.2500 (0.3090) acc@5 0.7031 (0.7063)\n",
      "\u001b[32m[2020-06-24 07:16:07] __main__ INFO: \u001b[0mElapsed 393.15\n",
      "\u001b[32m[2020-06-24 07:16:07] __main__ INFO: \u001b[0mVal 53\n",
      "\u001b[32m[2020-06-24 07:16:20] __main__ INFO: \u001b[0mEpoch 53 loss 1.9828 acc@1 0.2646 acc@5 0.6868\n",
      "\u001b[32m[2020-06-24 07:16:20] __main__ INFO: \u001b[0mElapsed 13.13\n",
      "\u001b[32m[2020-06-24 07:16:20] __main__ INFO: \u001b[0mTrain 54 18603\n",
      "\u001b[32m[2020-06-24 07:18:12] __main__ INFO: \u001b[0mEpoch 54 Step 100/351 lr 0.100000 loss 1.8294 (1.8499) acc@1 0.3047 (0.3068) acc@5 0.6797 (0.7055)\n",
      "\u001b[32m[2020-06-24 07:20:03] __main__ INFO: \u001b[0mEpoch 54 Step 200/351 lr 0.100000 loss 1.9377 (1.8531) acc@1 0.2344 (0.3064) acc@5 0.7266 (0.7053)\n",
      "\u001b[32m[2020-06-24 07:21:54] __main__ INFO: \u001b[0mEpoch 54 Step 300/351 lr 0.100000 loss 1.7353 (1.8554) acc@1 0.3359 (0.3069) acc@5 0.6953 (0.7052)\n",
      "\u001b[32m[2020-06-24 07:22:51] __main__ INFO: \u001b[0mEpoch 54 Step 351/351 lr 0.100000 loss 1.9079 (1.8578) acc@1 0.2734 (0.3060) acc@5 0.6875 (0.7054)\n",
      "\u001b[32m[2020-06-24 07:22:51] __main__ INFO: \u001b[0mElapsed 391.43\n",
      "\u001b[32m[2020-06-24 07:22:51] __main__ INFO: \u001b[0mVal 54\n",
      "\u001b[32m[2020-06-24 07:23:05] __main__ INFO: \u001b[0mEpoch 54 loss 1.9699 acc@1 0.2718 acc@5 0.6820\n",
      "\u001b[32m[2020-06-24 07:23:05] __main__ INFO: \u001b[0mElapsed 13.13\n",
      "\u001b[32m[2020-06-24 07:23:05] __main__ INFO: \u001b[0mTrain 55 18954\n",
      "\u001b[32m[2020-06-24 07:24:57] __main__ INFO: \u001b[0mEpoch 55 Step 100/351 lr 0.100000 loss 1.8587 (1.8326) acc@1 0.2812 (0.3127) acc@5 0.7422 (0.7050)\n",
      "\u001b[32m[2020-06-24 07:26:49] __main__ INFO: \u001b[0mEpoch 55 Step 200/351 lr 0.100000 loss 1.7792 (1.8440) acc@1 0.4062 (0.3085) acc@5 0.7734 (0.7066)\n",
      "\u001b[32m[2020-06-24 07:28:40] __main__ INFO: \u001b[0mEpoch 55 Step 300/351 lr 0.100000 loss 1.8194 (1.8452) acc@1 0.2812 (0.3086) acc@5 0.7188 (0.7059)\n",
      "\u001b[32m[2020-06-24 07:29:37] __main__ INFO: \u001b[0mEpoch 55 Step 351/351 lr 0.100000 loss 1.8297 (1.8498) acc@1 0.3047 (0.3074) acc@5 0.6641 (0.7050)\n",
      "\u001b[32m[2020-06-24 07:29:38] __main__ INFO: \u001b[0mElapsed 392.98\n",
      "\u001b[32m[2020-06-24 07:29:38] __main__ INFO: \u001b[0mVal 55\n",
      "\u001b[32m[2020-06-24 07:29:51] __main__ INFO: \u001b[0mEpoch 55 loss 2.0179 acc@1 0.2602 acc@5 0.6680\n",
      "\u001b[32m[2020-06-24 07:29:51] __main__ INFO: \u001b[0mElapsed 13.09\n",
      "\u001b[32m[2020-06-24 07:29:51] __main__ INFO: \u001b[0mTrain 56 19305\n",
      "\u001b[32m[2020-06-24 07:31:42] __main__ INFO: \u001b[0mEpoch 56 Step 100/351 lr 0.100000 loss 1.7998 (1.8299) acc@1 0.3516 (0.3160) acc@5 0.7109 (0.7109)\n",
      "\u001b[32m[2020-06-24 07:33:34] __main__ INFO: \u001b[0mEpoch 56 Step 200/351 lr 0.100000 loss 1.9383 (1.8434) acc@1 0.2578 (0.3127) acc@5 0.7500 (0.7061)\n",
      "\u001b[32m[2020-06-24 07:35:25] __main__ INFO: \u001b[0mEpoch 56 Step 300/351 lr 0.100000 loss 1.9598 (1.8512) acc@1 0.2891 (0.3108) acc@5 0.6797 (0.7037)\n",
      "\u001b[32m[2020-06-24 07:36:22] __main__ INFO: \u001b[0mEpoch 56 Step 351/351 lr 0.100000 loss 1.6784 (1.8518) acc@1 0.3594 (0.3094) acc@5 0.8047 (0.7038)\n",
      "\u001b[32m[2020-06-24 07:36:22] __main__ INFO: \u001b[0mElapsed 391.39\n",
      "\u001b[32m[2020-06-24 07:36:22] __main__ INFO: \u001b[0mVal 56\n",
      "\u001b[32m[2020-06-24 07:36:35] __main__ INFO: \u001b[0mEpoch 56 loss 1.9917 acc@1 0.2584 acc@5 0.6794\n",
      "\u001b[32m[2020-06-24 07:36:35] __main__ INFO: \u001b[0mElapsed 13.13\n",
      "\u001b[32m[2020-06-24 07:36:35] __main__ INFO: \u001b[0mTrain 57 19656\n",
      "\u001b[32m[2020-06-24 07:38:27] __main__ INFO: \u001b[0mEpoch 57 Step 100/351 lr 0.100000 loss 1.8407 (1.8409) acc@1 0.3047 (0.3180) acc@5 0.7578 (0.7084)\n",
      "\u001b[32m[2020-06-24 07:40:19] __main__ INFO: \u001b[0mEpoch 57 Step 200/351 lr 0.100000 loss 1.8206 (1.8467) acc@1 0.2734 (0.3101) acc@5 0.7109 (0.7087)\n",
      "\u001b[32m[2020-06-24 07:42:11] __main__ INFO: \u001b[0mEpoch 57 Step 300/351 lr 0.100000 loss 1.9227 (1.8473) acc@1 0.2656 (0.3088) acc@5 0.7031 (0.7058)\n",
      "\u001b[32m[2020-06-24 07:43:08] __main__ INFO: \u001b[0mEpoch 57 Step 351/351 lr 0.100000 loss 1.9178 (1.8501) acc@1 0.2500 (0.3066) acc@5 0.7031 (0.7050)\n",
      "\u001b[32m[2020-06-24 07:43:08] __main__ INFO: \u001b[0mElapsed 392.95\n",
      "\u001b[32m[2020-06-24 07:43:08] __main__ INFO: \u001b[0mVal 57\n",
      "\u001b[32m[2020-06-24 07:43:21] __main__ INFO: \u001b[0mEpoch 57 loss 1.9891 acc@1 0.2690 acc@5 0.6856\n",
      "\u001b[32m[2020-06-24 07:43:21] __main__ INFO: \u001b[0mElapsed 13.11\n",
      "\u001b[32m[2020-06-24 07:43:21] __main__ INFO: \u001b[0mTrain 58 20007\n",
      "\u001b[32m[2020-06-24 07:45:13] __main__ INFO: \u001b[0mEpoch 58 Step 100/351 lr 0.100000 loss 1.8601 (1.8318) acc@1 0.2891 (0.3149) acc@5 0.6797 (0.7073)\n",
      "\u001b[32m[2020-06-24 07:47:04] __main__ INFO: \u001b[0mEpoch 58 Step 200/351 lr 0.100000 loss 1.7391 (1.8445) acc@1 0.3359 (0.3102) acc@5 0.7266 (0.7080)\n",
      "\u001b[32m[2020-06-24 07:48:56] __main__ INFO: \u001b[0mEpoch 58 Step 300/351 lr 0.100000 loss 1.8411 (1.8444) acc@1 0.3359 (0.3096) acc@5 0.6953 (0.7086)\n",
      "\u001b[32m[2020-06-24 07:49:53] __main__ INFO: \u001b[0mEpoch 58 Step 351/351 lr 0.100000 loss 1.9353 (1.8474) acc@1 0.3047 (0.3085) acc@5 0.6797 (0.7083)\n",
      "\u001b[32m[2020-06-24 07:49:53] __main__ INFO: \u001b[0mElapsed 391.38\n",
      "\u001b[32m[2020-06-24 07:49:53] __main__ INFO: \u001b[0mVal 58\n",
      "\u001b[32m[2020-06-24 07:50:06] __main__ INFO: \u001b[0mEpoch 58 loss 1.9843 acc@1 0.2746 acc@5 0.6930\n",
      "\u001b[32m[2020-06-24 07:50:06] __main__ INFO: \u001b[0mElapsed 13.13\n",
      "\u001b[32m[2020-06-24 07:50:06] __main__ INFO: \u001b[0mTrain 59 20358\n",
      "\u001b[32m[2020-06-24 07:51:58] __main__ INFO: \u001b[0mEpoch 59 Step 100/351 lr 0.100000 loss 1.6541 (1.8286) acc@1 0.3594 (0.3151) acc@5 0.7031 (0.7049)\n",
      "\u001b[32m[2020-06-24 07:53:50] __main__ INFO: \u001b[0mEpoch 59 Step 200/351 lr 0.100000 loss 1.8070 (1.8454) acc@1 0.3438 (0.3089) acc@5 0.6641 (0.7025)\n",
      "\u001b[32m[2020-06-24 07:55:42] __main__ INFO: \u001b[0mEpoch 59 Step 300/351 lr 0.100000 loss 1.7402 (1.8482) acc@1 0.3438 (0.3079) acc@5 0.7109 (0.7026)\n",
      "\u001b[32m[2020-06-24 07:56:39] __main__ INFO: \u001b[0mEpoch 59 Step 351/351 lr 0.100000 loss 1.7646 (1.8465) acc@1 0.3359 (0.3093) acc@5 0.7344 (0.7047)\n",
      "\u001b[32m[2020-06-24 07:56:39] __main__ INFO: \u001b[0mElapsed 392.98\n",
      "\u001b[32m[2020-06-24 07:56:39] __main__ INFO: \u001b[0mVal 59\n",
      "\u001b[32m[2020-06-24 07:56:52] __main__ INFO: \u001b[0mEpoch 59 loss 1.9768 acc@1 0.2650 acc@5 0.6946\n",
      "\u001b[32m[2020-06-24 07:56:52] __main__ INFO: \u001b[0mElapsed 13.10\n",
      "\u001b[32m[2020-06-24 07:56:52] __main__ INFO: \u001b[0mTrain 60 20709\n",
      "\u001b[32m[2020-06-24 07:58:43] __main__ INFO: \u001b[0mEpoch 60 Step 100/351 lr 0.100000 loss 1.8317 (1.8254) acc@1 0.3438 (0.3182) acc@5 0.6875 (0.7137)\n",
      "\u001b[32m[2020-06-24 08:00:35] __main__ INFO: \u001b[0mEpoch 60 Step 200/351 lr 0.100000 loss 1.7784 (1.8339) acc@1 0.3203 (0.3152) acc@5 0.7031 (0.7077)\n",
      "\u001b[32m[2020-06-24 08:02:26] __main__ INFO: \u001b[0mEpoch 60 Step 300/351 lr 0.100000 loss 1.9533 (1.8393) acc@1 0.2734 (0.3123) acc@5 0.6719 (0.7077)\n",
      "\u001b[32m[2020-06-24 08:03:23] __main__ INFO: \u001b[0mEpoch 60 Step 351/351 lr 0.100000 loss 1.9124 (1.8451) acc@1 0.3203 (0.3097) acc@5 0.6094 (0.7057)\n",
      "\u001b[32m[2020-06-24 08:03:23] __main__ INFO: \u001b[0mElapsed 391.31\n",
      "\u001b[32m[2020-06-24 08:03:23] __main__ INFO: \u001b[0mVal 60\n",
      "\u001b[32m[2020-06-24 08:03:36] __main__ INFO: \u001b[0mEpoch 60 loss 1.9989 acc@1 0.2550 acc@5 0.6856\n",
      "\u001b[32m[2020-06-24 08:03:36] __main__ INFO: \u001b[0mElapsed 13.10\n",
      "\u001b[32m[2020-06-24 08:03:36] __main__ INFO: \u001b[0mTrain 61 21060\n",
      "\u001b[32m[2020-06-24 08:05:28] __main__ INFO: \u001b[0mEpoch 61 Step 100/351 lr 0.020000 loss 1.6811 (1.7442) acc@1 0.3828 (0.3487) acc@5 0.7656 (0.7211)\n",
      "\u001b[32m[2020-06-24 08:07:20] __main__ INFO: \u001b[0mEpoch 61 Step 200/351 lr 0.020000 loss 1.6086 (1.7214) acc@1 0.3750 (0.3543) acc@5 0.7344 (0.7252)\n",
      "\u001b[32m[2020-06-24 08:09:12] __main__ INFO: \u001b[0mEpoch 61 Step 300/351 lr 0.020000 loss 1.7252 (1.7130) acc@1 0.3516 (0.3573) acc@5 0.7656 (0.7262)\n",
      "\u001b[32m[2020-06-24 08:10:09] __main__ INFO: \u001b[0mEpoch 61 Step 351/351 lr 0.020000 loss 1.5432 (1.7079) acc@1 0.3906 (0.3590) acc@5 0.7578 (0.7263)\n",
      "\u001b[32m[2020-06-24 08:10:09] __main__ INFO: \u001b[0mElapsed 392.90\n",
      "\u001b[32m[2020-06-24 08:10:09] __main__ INFO: \u001b[0mVal 61\n",
      "\u001b[32m[2020-06-24 08:10:22] __main__ INFO: \u001b[0mEpoch 61 loss 1.8139 acc@1 0.3224 acc@5 0.7138\n",
      "\u001b[32m[2020-06-24 08:10:22] __main__ INFO: \u001b[0mElapsed 13.10\n",
      "\u001b[32m[2020-06-24 08:10:22] __main__ INFO: \u001b[0mTrain 62 21411\n",
      "\u001b[32m[2020-06-24 08:12:14] __main__ INFO: \u001b[0mEpoch 62 Step 100/351 lr 0.020000 loss 1.5947 (1.6563) acc@1 0.3906 (0.3738) acc@5 0.7344 (0.7320)\n",
      "\u001b[32m[2020-06-24 08:14:05] __main__ INFO: \u001b[0mEpoch 62 Step 200/351 lr 0.020000 loss 1.7941 (1.6559) acc@1 0.3750 (0.3760) acc@5 0.7422 (0.7300)\n",
      "\u001b[32m[2020-06-24 08:15:57] __main__ INFO: \u001b[0mEpoch 62 Step 300/351 lr 0.020000 loss 1.6797 (1.6575) acc@1 0.3438 (0.3748) acc@5 0.7344 (0.7275)\n",
      "\u001b[32m[2020-06-24 08:16:54] __main__ INFO: \u001b[0mEpoch 62 Step 351/351 lr 0.020000 loss 1.7773 (1.6611) acc@1 0.3750 (0.3746) acc@5 0.7266 (0.7276)\n",
      "\u001b[32m[2020-06-24 08:16:54] __main__ INFO: \u001b[0mElapsed 391.41\n",
      "\u001b[32m[2020-06-24 08:16:54] __main__ INFO: \u001b[0mVal 62\n",
      "\u001b[32m[2020-06-24 08:17:07] __main__ INFO: \u001b[0mEpoch 62 loss 1.8137 acc@1 0.3284 acc@5 0.7160\n",
      "\u001b[32m[2020-06-24 08:17:07] __main__ INFO: \u001b[0mElapsed 13.08\n",
      "\u001b[32m[2020-06-24 08:17:07] __main__ INFO: \u001b[0mTrain 63 21762\n",
      "\u001b[32m[2020-06-24 08:18:59] __main__ INFO: \u001b[0mEpoch 63 Step 100/351 lr 0.020000 loss 1.6093 (1.6118) acc@1 0.3672 (0.4001) acc@5 0.7656 (0.7343)\n",
      "\u001b[32m[2020-06-24 08:20:51] __main__ INFO: \u001b[0mEpoch 63 Step 200/351 lr 0.020000 loss 1.5379 (1.6304) acc@1 0.4453 (0.3893) acc@5 0.7266 (0.7334)\n",
      "\u001b[32m[2020-06-24 08:22:43] __main__ INFO: \u001b[0mEpoch 63 Step 300/351 lr 0.020000 loss 1.9180 (1.6385) acc@1 0.2578 (0.3834) acc@5 0.6719 (0.7320)\n",
      "\u001b[32m[2020-06-24 08:23:40] __main__ INFO: \u001b[0mEpoch 63 Step 351/351 lr 0.020000 loss 1.6025 (1.6393) acc@1 0.4062 (0.3828) acc@5 0.7422 (0.7315)\n",
      "\u001b[32m[2020-06-24 08:23:40] __main__ INFO: \u001b[0mElapsed 392.90\n",
      "\u001b[32m[2020-06-24 08:23:40] __main__ INFO: \u001b[0mVal 63\n",
      "\u001b[32m[2020-06-24 08:23:53] __main__ INFO: \u001b[0mEpoch 63 loss 1.8265 acc@1 0.3226 acc@5 0.7036\n",
      "\u001b[32m[2020-06-24 08:23:53] __main__ INFO: \u001b[0mElapsed 13.09\n",
      "\u001b[32m[2020-06-24 08:23:53] __main__ INFO: \u001b[0mTrain 64 22113\n",
      "\u001b[32m[2020-06-24 08:25:44] __main__ INFO: \u001b[0mEpoch 64 Step 100/351 lr 0.020000 loss 1.6982 (1.6040) acc@1 0.3438 (0.3954) acc@5 0.7188 (0.7375)\n",
      "\u001b[32m[2020-06-24 08:27:36] __main__ INFO: \u001b[0mEpoch 64 Step 200/351 lr 0.020000 loss 1.4571 (1.6095) acc@1 0.4453 (0.3936) acc@5 0.8047 (0.7377)\n",
      "\u001b[32m[2020-06-24 08:29:27] __main__ INFO: \u001b[0mEpoch 64 Step 300/351 lr 0.020000 loss 1.6743 (1.6205) acc@1 0.3594 (0.3887) acc@5 0.7734 (0.7341)\n",
      "\u001b[32m[2020-06-24 08:30:24] __main__ INFO: \u001b[0mEpoch 64 Step 351/351 lr 0.020000 loss 1.5769 (1.6217) acc@1 0.3438 (0.3879) acc@5 0.7344 (0.7343)\n",
      "\u001b[32m[2020-06-24 08:30:24] __main__ INFO: \u001b[0mElapsed 391.48\n",
      "\u001b[32m[2020-06-24 08:30:24] __main__ INFO: \u001b[0mVal 64\n",
      "\u001b[32m[2020-06-24 08:30:37] __main__ INFO: \u001b[0mEpoch 64 loss 1.8183 acc@1 0.3252 acc@5 0.7154\n",
      "\u001b[32m[2020-06-24 08:30:37] __main__ INFO: \u001b[0mElapsed 13.09\n",
      "\u001b[32m[2020-06-24 08:30:37] __main__ INFO: \u001b[0mTrain 65 22464\n",
      "\u001b[32m[2020-06-24 08:32:29] __main__ INFO: \u001b[0mEpoch 65 Step 100/351 lr 0.020000 loss 1.5646 (1.5838) acc@1 0.3984 (0.4058) acc@5 0.7109 (0.7388)\n",
      "\u001b[32m[2020-06-24 08:34:21] __main__ INFO: \u001b[0mEpoch 65 Step 200/351 lr 0.020000 loss 1.8707 (1.6017) acc@1 0.2969 (0.3987) acc@5 0.7578 (0.7394)\n",
      "\u001b[32m[2020-06-24 08:36:13] __main__ INFO: \u001b[0mEpoch 65 Step 300/351 lr 0.020000 loss 1.5750 (1.6092) acc@1 0.3984 (0.3948) acc@5 0.7812 (0.7378)\n",
      "\u001b[32m[2020-06-24 08:37:10] __main__ INFO: \u001b[0mEpoch 65 Step 351/351 lr 0.020000 loss 1.8557 (1.6142) acc@1 0.3203 (0.3930) acc@5 0.6641 (0.7357)\n",
      "\u001b[32m[2020-06-24 08:37:10] __main__ INFO: \u001b[0mElapsed 393.04\n",
      "\u001b[32m[2020-06-24 08:37:10] __main__ INFO: \u001b[0mVal 65\n",
      "\u001b[32m[2020-06-24 08:37:23] __main__ INFO: \u001b[0mEpoch 65 loss 1.8142 acc@1 0.3152 acc@5 0.7216\n",
      "\u001b[32m[2020-06-24 08:37:23] __main__ INFO: \u001b[0mElapsed 13.11\n",
      "\u001b[32m[2020-06-24 08:37:23] __main__ INFO: \u001b[0mTrain 66 22815\n",
      "\u001b[32m[2020-06-24 08:39:15] __main__ INFO: \u001b[0mEpoch 66 Step 100/351 lr 0.020000 loss 1.6877 (1.5813) acc@1 0.3203 (0.4049) acc@5 0.6953 (0.7383)\n",
      "\u001b[32m[2020-06-24 08:41:07] __main__ INFO: \u001b[0mEpoch 66 Step 200/351 lr 0.020000 loss 1.4543 (1.5844) acc@1 0.4766 (0.4049) acc@5 0.7578 (0.7398)\n",
      "\u001b[32m[2020-06-24 08:42:58] __main__ INFO: \u001b[0mEpoch 66 Step 300/351 lr 0.020000 loss 1.5910 (1.5947) acc@1 0.3672 (0.4005) acc@5 0.7734 (0.7396)\n",
      "\u001b[32m[2020-06-24 08:43:55] __main__ INFO: \u001b[0mEpoch 66 Step 351/351 lr 0.020000 loss 1.4787 (1.5979) acc@1 0.4766 (0.3994) acc@5 0.7734 (0.7396)\n",
      "\u001b[32m[2020-06-24 08:43:55] __main__ INFO: \u001b[0mElapsed 391.53\n",
      "\u001b[32m[2020-06-24 08:43:55] __main__ INFO: \u001b[0mVal 66\n",
      "\u001b[32m[2020-06-24 08:44:08] __main__ INFO: \u001b[0mEpoch 66 loss 1.8539 acc@1 0.3188 acc@5 0.7078\n",
      "\u001b[32m[2020-06-24 08:44:08] __main__ INFO: \u001b[0mElapsed 13.10\n",
      "\u001b[32m[2020-06-24 08:44:08] __main__ INFO: \u001b[0mTrain 67 23166\n",
      "\u001b[32m[2020-06-24 08:46:00] __main__ INFO: \u001b[0mEpoch 67 Step 100/351 lr 0.020000 loss 1.5032 (1.5761) acc@1 0.4688 (0.4062) acc@5 0.7109 (0.7323)\n",
      "\u001b[32m[2020-06-24 08:47:52] __main__ INFO: \u001b[0mEpoch 67 Step 200/351 lr 0.020000 loss 1.5181 (1.5810) acc@1 0.4531 (0.4028) acc@5 0.7188 (0.7354)\n",
      "\u001b[32m[2020-06-24 08:49:44] __main__ INFO: \u001b[0mEpoch 67 Step 300/351 lr 0.020000 loss 1.5852 (1.5865) acc@1 0.3594 (0.3996) acc@5 0.7578 (0.7364)\n",
      "\u001b[32m[2020-06-24 08:50:41] __main__ INFO: \u001b[0mEpoch 67 Step 351/351 lr 0.020000 loss 1.6117 (1.5933) acc@1 0.3906 (0.3970) acc@5 0.7656 (0.7344)\n",
      "\u001b[32m[2020-06-24 08:50:41] __main__ INFO: \u001b[0mElapsed 392.66\n",
      "\u001b[32m[2020-06-24 08:50:41] __main__ INFO: \u001b[0mVal 67\n",
      "\u001b[32m[2020-06-24 08:50:54] __main__ INFO: \u001b[0mEpoch 67 loss 1.8752 acc@1 0.3188 acc@5 0.7072\n",
      "\u001b[32m[2020-06-24 08:50:54] __main__ INFO: \u001b[0mElapsed 13.10\n",
      "\u001b[32m[2020-06-24 08:50:54] __main__ INFO: \u001b[0mTrain 68 23517\n",
      "\u001b[32m[2020-06-24 08:52:45] __main__ INFO: \u001b[0mEpoch 68 Step 100/351 lr 0.020000 loss 1.6285 (1.5542) acc@1 0.3750 (0.4114) acc@5 0.7656 (0.7391)\n",
      "\u001b[32m[2020-06-24 08:54:37] __main__ INFO: \u001b[0mEpoch 68 Step 200/351 lr 0.020000 loss 1.5683 (1.5676) acc@1 0.3906 (0.4068) acc@5 0.7422 (0.7399)\n",
      "\u001b[32m[2020-06-24 08:56:28] __main__ INFO: \u001b[0mEpoch 68 Step 300/351 lr 0.020000 loss 1.6309 (1.5838) acc@1 0.3828 (0.4018) acc@5 0.6797 (0.7386)\n",
      "\u001b[32m[2020-06-24 08:57:25] __main__ INFO: \u001b[0mEpoch 68 Step 351/351 lr 0.020000 loss 1.5953 (1.5856) acc@1 0.3828 (0.4002) acc@5 0.7422 (0.7367)\n",
      "\u001b[32m[2020-06-24 08:57:25] __main__ INFO: \u001b[0mElapsed 391.30\n",
      "\u001b[32m[2020-06-24 08:57:25] __main__ INFO: \u001b[0mVal 68\n",
      "\u001b[32m[2020-06-24 08:57:38] __main__ INFO: \u001b[0mEpoch 68 loss 1.8588 acc@1 0.3234 acc@5 0.7190\n",
      "\u001b[32m[2020-06-24 08:57:38] __main__ INFO: \u001b[0mElapsed 13.13\n",
      "\u001b[32m[2020-06-24 08:57:38] __main__ INFO: \u001b[0mTrain 69 23868\n",
      "\u001b[32m[2020-06-24 08:59:30] __main__ INFO: \u001b[0mEpoch 69 Step 100/351 lr 0.020000 loss 1.5533 (1.5394) acc@1 0.3984 (0.4221) acc@5 0.7578 (0.7424)\n",
      "\u001b[32m[2020-06-24 09:01:22] __main__ INFO: \u001b[0mEpoch 69 Step 200/351 lr 0.020000 loss 1.7021 (1.5597) acc@1 0.3594 (0.4131) acc@5 0.7656 (0.7392)\n",
      "\u001b[32m[2020-06-24 09:03:14] __main__ INFO: \u001b[0mEpoch 69 Step 300/351 lr 0.020000 loss 1.9260 (1.5749) acc@1 0.3047 (0.4064) acc@5 0.6406 (0.7366)\n",
      "\u001b[32m[2020-06-24 09:04:11] __main__ INFO: \u001b[0mEpoch 69 Step 351/351 lr 0.020000 loss 1.5081 (1.5804) acc@1 0.4062 (0.4044) acc@5 0.7812 (0.7366)\n",
      "\u001b[32m[2020-06-24 09:04:11] __main__ INFO: \u001b[0mElapsed 392.70\n",
      "\u001b[32m[2020-06-24 09:04:11] __main__ INFO: \u001b[0mVal 69\n",
      "\u001b[32m[2020-06-24 09:04:24] __main__ INFO: \u001b[0mEpoch 69 loss 1.8844 acc@1 0.3084 acc@5 0.7144\n",
      "\u001b[32m[2020-06-24 09:04:24] __main__ INFO: \u001b[0mElapsed 13.10\n",
      "\u001b[32m[2020-06-24 09:04:24] __main__ INFO: \u001b[0mTrain 70 24219\n",
      "\u001b[32m[2020-06-24 09:06:16] __main__ INFO: \u001b[0mEpoch 70 Step 100/351 lr 0.020000 loss 1.7288 (1.5371) acc@1 0.3203 (0.4163) acc@5 0.7188 (0.7432)\n",
      "\u001b[32m[2020-06-24 09:08:07] __main__ INFO: \u001b[0mEpoch 70 Step 200/351 lr 0.020000 loss 1.5763 (1.5656) acc@1 0.4297 (0.4065) acc@5 0.7109 (0.7374)\n",
      "\u001b[32m[2020-06-24 09:09:59] __main__ INFO: \u001b[0mEpoch 70 Step 300/351 lr 0.020000 loss 1.5675 (1.5710) acc@1 0.4766 (0.4053) acc@5 0.7500 (0.7366)\n",
      "\u001b[32m[2020-06-24 09:10:55] __main__ INFO: \u001b[0mEpoch 70 Step 351/351 lr 0.020000 loss 1.6555 (1.5739) acc@1 0.3516 (0.4047) acc@5 0.6641 (0.7366)\n",
      "\u001b[32m[2020-06-24 09:10:55] __main__ INFO: \u001b[0mElapsed 391.38\n",
      "\u001b[32m[2020-06-24 09:10:55] __main__ INFO: \u001b[0mVal 70\n",
      "\u001b[32m[2020-06-24 09:11:09] __main__ INFO: \u001b[0mEpoch 70 loss 1.8673 acc@1 0.3148 acc@5 0.7036\n",
      "\u001b[32m[2020-06-24 09:11:09] __main__ INFO: \u001b[0mElapsed 13.09\n",
      "\u001b[32m[2020-06-24 09:11:09] __main__ INFO: \u001b[0mTrain 71 24570\n",
      "\u001b[32m[2020-06-24 09:13:01] __main__ INFO: \u001b[0mEpoch 71 Step 100/351 lr 0.020000 loss 1.6088 (1.5363) acc@1 0.4062 (0.4194) acc@5 0.7422 (0.7423)\n",
      "\u001b[32m[2020-06-24 09:14:52] __main__ INFO: \u001b[0mEpoch 71 Step 200/351 lr 0.020000 loss 1.4763 (1.5496) acc@1 0.4844 (0.4163) acc@5 0.7500 (0.7394)\n",
      "\u001b[32m[2020-06-24 09:16:44] __main__ INFO: \u001b[0mEpoch 71 Step 300/351 lr 0.020000 loss 1.4400 (1.5525) acc@1 0.4766 (0.4150) acc@5 0.7500 (0.7376)\n",
      "\u001b[32m[2020-06-24 09:17:41] __main__ INFO: \u001b[0mEpoch 71 Step 351/351 lr 0.020000 loss 1.6718 (1.5598) acc@1 0.3750 (0.4121) acc@5 0.7266 (0.7365)\n",
      "\u001b[32m[2020-06-24 09:17:41] __main__ INFO: \u001b[0mElapsed 392.75\n",
      "\u001b[32m[2020-06-24 09:17:41] __main__ INFO: \u001b[0mVal 71\n",
      "\u001b[32m[2020-06-24 09:17:54] __main__ INFO: \u001b[0mEpoch 71 loss 1.9552 acc@1 0.2976 acc@5 0.7030\n",
      "\u001b[32m[2020-06-24 09:17:54] __main__ INFO: \u001b[0mElapsed 13.10\n",
      "\u001b[32m[2020-06-24 09:17:54] __main__ INFO: \u001b[0mTrain 72 24921\n",
      "\u001b[32m[2020-06-24 09:19:46] __main__ INFO: \u001b[0mEpoch 72 Step 100/351 lr 0.020000 loss 1.6171 (1.5285) acc@1 0.3828 (0.4216) acc@5 0.7500 (0.7438)\n",
      "\u001b[32m[2020-06-24 09:21:37] __main__ INFO: \u001b[0mEpoch 72 Step 200/351 lr 0.020000 loss 1.6891 (1.5413) acc@1 0.3281 (0.4154) acc@5 0.7578 (0.7434)\n",
      "\u001b[32m[2020-06-24 09:23:29] __main__ INFO: \u001b[0mEpoch 72 Step 300/351 lr 0.020000 loss 1.2932 (1.5498) acc@1 0.5391 (0.4110) acc@5 0.7812 (0.7415)\n",
      "\u001b[32m[2020-06-24 09:24:25] __main__ INFO: \u001b[0mEpoch 72 Step 351/351 lr 0.020000 loss 1.5487 (1.5551) acc@1 0.4141 (0.4093) acc@5 0.6875 (0.7418)\n",
      "\u001b[32m[2020-06-24 09:24:26] __main__ INFO: \u001b[0mElapsed 391.12\n",
      "\u001b[32m[2020-06-24 09:24:26] __main__ INFO: \u001b[0mVal 72\n",
      "\u001b[32m[2020-06-24 09:24:39] __main__ INFO: \u001b[0mEpoch 72 loss 1.9272 acc@1 0.3028 acc@5 0.6992\n",
      "\u001b[32m[2020-06-24 09:24:39] __main__ INFO: \u001b[0mElapsed 13.09\n",
      "\u001b[32m[2020-06-24 09:24:39] __main__ INFO: \u001b[0mTrain 73 25272\n",
      "\u001b[32m[2020-06-24 09:26:31] __main__ INFO: \u001b[0mEpoch 73 Step 100/351 lr 0.020000 loss 1.5691 (1.5143) acc@1 0.3828 (0.4277) acc@5 0.6406 (0.7464)\n",
      "\u001b[32m[2020-06-24 09:28:22] __main__ INFO: \u001b[0mEpoch 73 Step 200/351 lr 0.020000 loss 1.4653 (1.5361) acc@1 0.4297 (0.4192) acc@5 0.7344 (0.7445)\n",
      "\u001b[32m[2020-06-24 09:30:14] __main__ INFO: \u001b[0mEpoch 73 Step 300/351 lr 0.020000 loss 1.6520 (1.5433) acc@1 0.4375 (0.4163) acc@5 0.7656 (0.7424)\n",
      "\u001b[32m[2020-06-24 09:31:11] __main__ INFO: \u001b[0mEpoch 73 Step 351/351 lr 0.020000 loss 1.8359 (1.5465) acc@1 0.3516 (0.4155) acc@5 0.7500 (0.7428)\n",
      "\u001b[32m[2020-06-24 09:31:11] __main__ INFO: \u001b[0mElapsed 392.76\n",
      "\u001b[32m[2020-06-24 09:31:11] __main__ INFO: \u001b[0mVal 73\n",
      "\u001b[32m[2020-06-24 09:31:24] __main__ INFO: \u001b[0mEpoch 73 loss 1.9630 acc@1 0.3102 acc@5 0.6974\n",
      "\u001b[32m[2020-06-24 09:31:24] __main__ INFO: \u001b[0mElapsed 13.09\n",
      "\u001b[32m[2020-06-24 09:31:24] __main__ INFO: \u001b[0mTrain 74 25623\n",
      "\u001b[32m[2020-06-24 09:33:16] __main__ INFO: \u001b[0mEpoch 74 Step 100/351 lr 0.020000 loss 1.6425 (1.5077) acc@1 0.3828 (0.4295) acc@5 0.7031 (0.7412)\n",
      "\u001b[32m[2020-06-24 09:35:07] __main__ INFO: \u001b[0mEpoch 74 Step 200/351 lr 0.020000 loss 1.6358 (1.5279) acc@1 0.3984 (0.4196) acc@5 0.6797 (0.7426)\n",
      "\u001b[32m[2020-06-24 09:36:59] __main__ INFO: \u001b[0mEpoch 74 Step 300/351 lr 0.020000 loss 1.5122 (1.5334) acc@1 0.4219 (0.4187) acc@5 0.7031 (0.7424)\n",
      "\u001b[32m[2020-06-24 09:37:56] __main__ INFO: \u001b[0mEpoch 74 Step 351/351 lr 0.020000 loss 1.5025 (1.5391) acc@1 0.4297 (0.4169) acc@5 0.7656 (0.7426)\n",
      "\u001b[32m[2020-06-24 09:37:56] __main__ INFO: \u001b[0mElapsed 391.32\n",
      "\u001b[32m[2020-06-24 09:37:56] __main__ INFO: \u001b[0mVal 74\n",
      "\u001b[32m[2020-06-24 09:38:09] __main__ INFO: \u001b[0mEpoch 74 loss 1.9434 acc@1 0.3124 acc@5 0.7072\n",
      "\u001b[32m[2020-06-24 09:38:09] __main__ INFO: \u001b[0mElapsed 13.09\n",
      "\u001b[32m[2020-06-24 09:38:09] __main__ INFO: \u001b[0mTrain 75 25974\n",
      "\u001b[32m[2020-06-24 09:40:01] __main__ INFO: \u001b[0mEpoch 75 Step 100/351 lr 0.020000 loss 1.3551 (1.5105) acc@1 0.5078 (0.4254) acc@5 0.7188 (0.7462)\n",
      "\u001b[32m[2020-06-24 09:41:53] __main__ INFO: \u001b[0mEpoch 75 Step 200/351 lr 0.020000 loss 1.4269 (1.5198) acc@1 0.4297 (0.4240) acc@5 0.7812 (0.7456)\n",
      "\u001b[32m[2020-06-24 09:43:45] __main__ INFO: \u001b[0mEpoch 75 Step 300/351 lr 0.020000 loss 1.4592 (1.5307) acc@1 0.4375 (0.4200) acc@5 0.7500 (0.7412)\n",
      "\u001b[32m[2020-06-24 09:44:42] __main__ INFO: \u001b[0mEpoch 75 Step 351/351 lr 0.020000 loss 1.8046 (1.5339) acc@1 0.3438 (0.4187) acc@5 0.6562 (0.7424)\n",
      "\u001b[32m[2020-06-24 09:44:42] __main__ INFO: \u001b[0mElapsed 392.81\n",
      "\u001b[32m[2020-06-24 09:44:42] __main__ INFO: \u001b[0mVal 75\n",
      "\u001b[32m[2020-06-24 09:44:55] __main__ INFO: \u001b[0mEpoch 75 loss 1.9338 acc@1 0.3124 acc@5 0.6988\n",
      "\u001b[32m[2020-06-24 09:44:55] __main__ INFO: \u001b[0mElapsed 13.11\n",
      "\u001b[32m[2020-06-24 09:44:55] __main__ INFO: \u001b[0mTrain 76 26325\n",
      "\u001b[32m[2020-06-24 09:46:46] __main__ INFO: \u001b[0mEpoch 76 Step 100/351 lr 0.020000 loss 1.4907 (1.5091) acc@1 0.4531 (0.4220) acc@5 0.7344 (0.7448)\n",
      "\u001b[32m[2020-06-24 09:48:38] __main__ INFO: \u001b[0mEpoch 76 Step 200/351 lr 0.020000 loss 1.4881 (1.5058) acc@1 0.4297 (0.4254) acc@5 0.7578 (0.7452)\n",
      "\u001b[32m[2020-06-24 09:50:29] __main__ INFO: \u001b[0mEpoch 76 Step 300/351 lr 0.020000 loss 1.5601 (1.5155) acc@1 0.4297 (0.4226) acc@5 0.7969 (0.7449)\n",
      "\u001b[32m[2020-06-24 09:51:26] __main__ INFO: \u001b[0mEpoch 76 Step 351/351 lr 0.020000 loss 1.7553 (1.5260) acc@1 0.2969 (0.4190) acc@5 0.7031 (0.7431)\n",
      "\u001b[32m[2020-06-24 09:51:26] __main__ INFO: \u001b[0mElapsed 391.18\n",
      "\u001b[32m[2020-06-24 09:51:26] __main__ INFO: \u001b[0mVal 76\n",
      "\u001b[32m[2020-06-24 09:51:39] __main__ INFO: \u001b[0mEpoch 76 loss 1.9077 acc@1 0.3158 acc@5 0.7036\n",
      "\u001b[32m[2020-06-24 09:51:39] __main__ INFO: \u001b[0mElapsed 13.08\n",
      "\u001b[32m[2020-06-24 09:51:39] __main__ INFO: \u001b[0mTrain 77 26676\n",
      "\u001b[32m[2020-06-24 09:53:31] __main__ INFO: \u001b[0mEpoch 77 Step 100/351 lr 0.020000 loss 1.6440 (1.4807) acc@1 0.3516 (0.4393) acc@5 0.7344 (0.7539)\n",
      "\u001b[32m[2020-06-24 09:55:23] __main__ INFO: \u001b[0mEpoch 77 Step 200/351 lr 0.020000 loss 1.3931 (1.4970) acc@1 0.4609 (0.4319) acc@5 0.7500 (0.7496)\n",
      "\u001b[32m[2020-06-24 09:57:15] __main__ INFO: \u001b[0mEpoch 77 Step 300/351 lr 0.020000 loss 1.5912 (1.5115) acc@1 0.4141 (0.4256) acc@5 0.6953 (0.7465)\n",
      "\u001b[32m[2020-06-24 09:58:12] __main__ INFO: \u001b[0mEpoch 77 Step 351/351 lr 0.020000 loss 1.5932 (1.5177) acc@1 0.3906 (0.4227) acc@5 0.7188 (0.7452)\n",
      "\u001b[32m[2020-06-24 09:58:12] __main__ INFO: \u001b[0mElapsed 392.69\n",
      "\u001b[32m[2020-06-24 09:58:12] __main__ INFO: \u001b[0mVal 77\n",
      "\u001b[32m[2020-06-24 09:58:25] __main__ INFO: \u001b[0mEpoch 77 loss 2.0719 acc@1 0.2986 acc@5 0.7008\n",
      "\u001b[32m[2020-06-24 09:58:25] __main__ INFO: \u001b[0mElapsed 13.13\n",
      "\u001b[32m[2020-06-24 09:58:25] __main__ INFO: \u001b[0mTrain 78 27027\n",
      "\u001b[32m[2020-06-24 10:00:16] __main__ INFO: \u001b[0mEpoch 78 Step 100/351 lr 0.020000 loss 1.4003 (1.4864) acc@1 0.5000 (0.4372) acc@5 0.7891 (0.7434)\n",
      "\u001b[32m[2020-06-24 10:02:08] __main__ INFO: \u001b[0mEpoch 78 Step 200/351 lr 0.020000 loss 1.4913 (1.4940) acc@1 0.4609 (0.4345) acc@5 0.7188 (0.7445)\n",
      "\u001b[32m[2020-06-24 10:03:59] __main__ INFO: \u001b[0mEpoch 78 Step 300/351 lr 0.020000 loss 1.6093 (1.5031) acc@1 0.3281 (0.4285) acc@5 0.6797 (0.7425)\n",
      "\u001b[32m[2020-06-24 10:04:56] __main__ INFO: \u001b[0mEpoch 78 Step 351/351 lr 0.020000 loss 1.5516 (1.5072) acc@1 0.3594 (0.4278) acc@5 0.7734 (0.7434)\n",
      "\u001b[32m[2020-06-24 10:04:56] __main__ INFO: \u001b[0mElapsed 391.27\n",
      "\u001b[32m[2020-06-24 10:04:56] __main__ INFO: \u001b[0mVal 78\n",
      "\u001b[32m[2020-06-24 10:05:09] __main__ INFO: \u001b[0mEpoch 78 loss 1.9368 acc@1 0.3152 acc@5 0.7104\n",
      "\u001b[32m[2020-06-24 10:05:09] __main__ INFO: \u001b[0mElapsed 13.11\n",
      "\u001b[32m[2020-06-24 10:05:09] __main__ INFO: \u001b[0mTrain 79 27378\n",
      "\u001b[32m[2020-06-24 10:07:01] __main__ INFO: \u001b[0mEpoch 79 Step 100/351 lr 0.020000 loss 1.4598 (1.4705) acc@1 0.4766 (0.4438) acc@5 0.7500 (0.7478)\n",
      "\u001b[32m[2020-06-24 10:08:53] __main__ INFO: \u001b[0mEpoch 79 Step 200/351 lr 0.020000 loss 1.3910 (1.4902) acc@1 0.4688 (0.4362) acc@5 0.7969 (0.7473)\n",
      "\u001b[32m[2020-06-24 10:10:45] __main__ INFO: \u001b[0mEpoch 79 Step 300/351 lr 0.020000 loss 1.6500 (1.5041) acc@1 0.4297 (0.4319) acc@5 0.7344 (0.7453)\n",
      "\u001b[32m[2020-06-24 10:11:42] __main__ INFO: \u001b[0mEpoch 79 Step 351/351 lr 0.020000 loss 1.6309 (1.5029) acc@1 0.3828 (0.4316) acc@5 0.7031 (0.7445)\n",
      "\u001b[32m[2020-06-24 10:11:42] __main__ INFO: \u001b[0mElapsed 392.84\n",
      "\u001b[32m[2020-06-24 10:11:42] __main__ INFO: \u001b[0mVal 79\n",
      "\u001b[32m[2020-06-24 10:11:55] __main__ INFO: \u001b[0mEpoch 79 loss 2.0151 acc@1 0.3108 acc@5 0.6918\n",
      "\u001b[32m[2020-06-24 10:11:55] __main__ INFO: \u001b[0mElapsed 13.12\n",
      "\u001b[32m[2020-06-24 10:11:55] __main__ INFO: \u001b[0mTrain 80 27729\n",
      "\u001b[32m[2020-06-24 10:13:47] __main__ INFO: \u001b[0mEpoch 80 Step 100/351 lr 0.020000 loss 1.4283 (1.4675) acc@1 0.4531 (0.4395) acc@5 0.7031 (0.7427)\n",
      "\u001b[32m[2020-06-24 10:15:38] __main__ INFO: \u001b[0mEpoch 80 Step 200/351 lr 0.020000 loss 1.4352 (1.4708) acc@1 0.4531 (0.4402) acc@5 0.8047 (0.7462)\n",
      "\u001b[32m[2020-06-24 10:17:30] __main__ INFO: \u001b[0mEpoch 80 Step 300/351 lr 0.020000 loss 1.4939 (1.4928) acc@1 0.4219 (0.4333) acc@5 0.7188 (0.7447)\n",
      "\u001b[32m[2020-06-24 10:18:26] __main__ INFO: \u001b[0mEpoch 80 Step 351/351 lr 0.020000 loss 1.3749 (1.4957) acc@1 0.5156 (0.4326) acc@5 0.7266 (0.7443)\n",
      "\u001b[32m[2020-06-24 10:18:27] __main__ INFO: \u001b[0mElapsed 391.27\n",
      "\u001b[32m[2020-06-24 10:18:27] __main__ INFO: \u001b[0mVal 80\n",
      "\u001b[32m[2020-06-24 10:18:40] __main__ INFO: \u001b[0mEpoch 80 loss 2.0017 acc@1 0.3172 acc@5 0.6980\n",
      "\u001b[32m[2020-06-24 10:18:40] __main__ INFO: \u001b[0mElapsed 13.15\n",
      "\u001b[32m[2020-06-24 10:18:40] __main__ INFO: \u001b[0mTrain 81 28080\n",
      "\u001b[32m[2020-06-24 10:20:32] __main__ INFO: \u001b[0mEpoch 81 Step 100/351 lr 0.020000 loss 1.4797 (1.4457) acc@1 0.3984 (0.4516) acc@5 0.8047 (0.7500)\n",
      "\u001b[32m[2020-06-24 10:22:23] __main__ INFO: \u001b[0mEpoch 81 Step 200/351 lr 0.020000 loss 1.3921 (1.4742) acc@1 0.4922 (0.4427) acc@5 0.7969 (0.7480)\n",
      "\u001b[32m[2020-06-24 10:24:15] __main__ INFO: \u001b[0mEpoch 81 Step 300/351 lr 0.020000 loss 1.5222 (1.4898) acc@1 0.3984 (0.4345) acc@5 0.7500 (0.7459)\n",
      "\u001b[32m[2020-06-24 10:25:12] __main__ INFO: \u001b[0mEpoch 81 Step 351/351 lr 0.020000 loss 1.6183 (1.4919) acc@1 0.4062 (0.4336) acc@5 0.7109 (0.7453)\n",
      "\u001b[32m[2020-06-24 10:25:12] __main__ INFO: \u001b[0mElapsed 392.59\n",
      "\u001b[32m[2020-06-24 10:25:12] __main__ INFO: \u001b[0mVal 81\n",
      "\u001b[32m[2020-06-24 10:25:25] __main__ INFO: \u001b[0mEpoch 81 loss 2.0223 acc@1 0.2986 acc@5 0.7056\n",
      "\u001b[32m[2020-06-24 10:25:25] __main__ INFO: \u001b[0mElapsed 13.09\n",
      "\u001b[32m[2020-06-24 10:25:25] __main__ INFO: \u001b[0mTrain 82 28431\n",
      "\u001b[32m[2020-06-24 10:27:17] __main__ INFO: \u001b[0mEpoch 82 Step 100/351 lr 0.020000 loss 1.5370 (1.4573) acc@1 0.4141 (0.4505) acc@5 0.7344 (0.7511)\n",
      "\u001b[32m[2020-06-24 10:29:08] __main__ INFO: \u001b[0mEpoch 82 Step 200/351 lr 0.020000 loss 1.5196 (1.4644) acc@1 0.4219 (0.4478) acc@5 0.6875 (0.7492)\n",
      "\u001b[32m[2020-06-24 10:31:00] __main__ INFO: \u001b[0mEpoch 82 Step 300/351 lr 0.020000 loss 1.5726 (1.4735) acc@1 0.4062 (0.4441) acc@5 0.7109 (0.7481)\n",
      "\u001b[32m[2020-06-24 10:31:57] __main__ INFO: \u001b[0mEpoch 82 Step 351/351 lr 0.020000 loss 1.4293 (1.4767) acc@1 0.4453 (0.4433) acc@5 0.8438 (0.7476)\n",
      "\u001b[32m[2020-06-24 10:31:57] __main__ INFO: \u001b[0mElapsed 391.23\n",
      "\u001b[32m[2020-06-24 10:31:57] __main__ INFO: \u001b[0mVal 82\n",
      "\u001b[32m[2020-06-24 10:32:10] __main__ INFO: \u001b[0mEpoch 82 loss 1.9871 acc@1 0.3128 acc@5 0.7016\n",
      "\u001b[32m[2020-06-24 10:32:10] __main__ INFO: \u001b[0mElapsed 13.12\n",
      "\u001b[32m[2020-06-24 10:32:10] __main__ INFO: \u001b[0mTrain 83 28782\n",
      "\u001b[32m[2020-06-24 10:34:02] __main__ INFO: \u001b[0mEpoch 83 Step 100/351 lr 0.020000 loss 1.6763 (1.4427) acc@1 0.3281 (0.4550) acc@5 0.6641 (0.7501)\n",
      "\u001b[32m[2020-06-24 10:35:53] __main__ INFO: \u001b[0mEpoch 83 Step 200/351 lr 0.020000 loss 1.4075 (1.4508) acc@1 0.4688 (0.4499) acc@5 0.7266 (0.7531)\n",
      "\u001b[32m[2020-06-24 10:37:45] __main__ INFO: \u001b[0mEpoch 83 Step 300/351 lr 0.020000 loss 1.5228 (1.4667) acc@1 0.4219 (0.4430) acc@5 0.7734 (0.7497)\n",
      "\u001b[32m[2020-06-24 10:38:42] __main__ INFO: \u001b[0mEpoch 83 Step 351/351 lr 0.020000 loss 1.5799 (1.4714) acc@1 0.4062 (0.4417) acc@5 0.7734 (0.7490)\n",
      "\u001b[32m[2020-06-24 10:38:42] __main__ INFO: \u001b[0mElapsed 392.63\n",
      "\u001b[32m[2020-06-24 10:38:42] __main__ INFO: \u001b[0mVal 83\n",
      "\u001b[32m[2020-06-24 10:38:55] __main__ INFO: \u001b[0mEpoch 83 loss 2.0137 acc@1 0.3094 acc@5 0.7066\n",
      "\u001b[32m[2020-06-24 10:38:55] __main__ INFO: \u001b[0mElapsed 13.12\n",
      "\u001b[32m[2020-06-24 10:38:55] __main__ INFO: \u001b[0mTrain 84 29133\n",
      "\u001b[32m[2020-06-24 10:40:47] __main__ INFO: \u001b[0mEpoch 84 Step 100/351 lr 0.020000 loss 1.4622 (1.4237) acc@1 0.4844 (0.4570) acc@5 0.7812 (0.7513)\n",
      "\u001b[32m[2020-06-24 10:42:38] __main__ INFO: \u001b[0mEpoch 84 Step 200/351 lr 0.020000 loss 1.3260 (1.4444) acc@1 0.5078 (0.4492) acc@5 0.8047 (0.7479)\n",
      "\u001b[32m[2020-06-24 10:44:30] __main__ INFO: \u001b[0mEpoch 84 Step 300/351 lr 0.020000 loss 1.3721 (1.4541) acc@1 0.4609 (0.4466) acc@5 0.7891 (0.7482)\n",
      "\u001b[32m[2020-06-24 10:45:27] __main__ INFO: \u001b[0mEpoch 84 Step 351/351 lr 0.020000 loss 1.5272 (1.4618) acc@1 0.4297 (0.4441) acc@5 0.7422 (0.7481)\n",
      "\u001b[32m[2020-06-24 10:45:27] __main__ INFO: \u001b[0mElapsed 391.09\n",
      "\u001b[32m[2020-06-24 10:45:27] __main__ INFO: \u001b[0mVal 84\n",
      "\u001b[32m[2020-06-24 10:45:40] __main__ INFO: \u001b[0mEpoch 84 loss 1.9713 acc@1 0.3188 acc@5 0.7102\n",
      "\u001b[32m[2020-06-24 10:45:40] __main__ INFO: \u001b[0mElapsed 13.12\n",
      "\u001b[32m[2020-06-24 10:45:40] __main__ INFO: \u001b[0mTrain 85 29484\n",
      "\u001b[32m[2020-06-24 10:47:32] __main__ INFO: \u001b[0mEpoch 85 Step 100/351 lr 0.020000 loss 1.5205 (1.4374) acc@1 0.4375 (0.4518) acc@5 0.7344 (0.7473)\n",
      "\u001b[32m[2020-06-24 10:49:23] __main__ INFO: \u001b[0mEpoch 85 Step 200/351 lr 0.020000 loss 1.4025 (1.4465) acc@1 0.4844 (0.4504) acc@5 0.7734 (0.7495)\n",
      "\u001b[32m[2020-06-24 10:51:15] __main__ INFO: \u001b[0mEpoch 85 Step 300/351 lr 0.020000 loss 1.6756 (1.4611) acc@1 0.3672 (0.4452) acc@5 0.6953 (0.7482)\n",
      "\u001b[32m[2020-06-24 10:52:12] __main__ INFO: \u001b[0mEpoch 85 Step 351/351 lr 0.020000 loss 1.5832 (1.4633) acc@1 0.3672 (0.4444) acc@5 0.7109 (0.7483)\n",
      "\u001b[32m[2020-06-24 10:52:12] __main__ INFO: \u001b[0mElapsed 392.67\n",
      "\u001b[32m[2020-06-24 10:52:12] __main__ INFO: \u001b[0mVal 85\n",
      "\u001b[32m[2020-06-24 10:52:25] __main__ INFO: \u001b[0mEpoch 85 loss 2.0005 acc@1 0.3094 acc@5 0.7072\n",
      "\u001b[32m[2020-06-24 10:52:25] __main__ INFO: \u001b[0mElapsed 13.10\n",
      "\u001b[32m[2020-06-24 10:52:25] __main__ INFO: \u001b[0mTrain 86 29835\n",
      "\u001b[32m[2020-06-24 10:54:17] __main__ INFO: \u001b[0mEpoch 86 Step 100/351 lr 0.020000 loss 1.2714 (1.4204) acc@1 0.5391 (0.4598) acc@5 0.7500 (0.7487)\n",
      "\u001b[32m[2020-06-24 10:56:08] __main__ INFO: \u001b[0mEpoch 86 Step 200/351 lr 0.020000 loss 1.5092 (1.4327) acc@1 0.4297 (0.4568) acc@5 0.7500 (0.7482)\n",
      "\u001b[32m[2020-06-24 10:58:00] __main__ INFO: \u001b[0mEpoch 86 Step 300/351 lr 0.020000 loss 1.4345 (1.4441) acc@1 0.4453 (0.4515) acc@5 0.7188 (0.7468)\n",
      "\u001b[32m[2020-06-24 10:58:57] __main__ INFO: \u001b[0mEpoch 86 Step 351/351 lr 0.020000 loss 1.5531 (1.4493) acc@1 0.4141 (0.4498) acc@5 0.6719 (0.7467)\n",
      "\u001b[32m[2020-06-24 10:58:57] __main__ INFO: \u001b[0mElapsed 391.17\n",
      "\u001b[32m[2020-06-24 10:58:57] __main__ INFO: \u001b[0mVal 86\n",
      "\u001b[32m[2020-06-24 10:59:10] __main__ INFO: \u001b[0mEpoch 86 loss 2.0307 acc@1 0.2978 acc@5 0.6966\n",
      "\u001b[32m[2020-06-24 10:59:10] __main__ INFO: \u001b[0mElapsed 13.09\n",
      "\u001b[32m[2020-06-24 10:59:10] __main__ INFO: \u001b[0mTrain 87 30186\n",
      "\u001b[32m[2020-06-24 11:01:02] __main__ INFO: \u001b[0mEpoch 87 Step 100/351 lr 0.020000 loss 1.5498 (1.4253) acc@1 0.4219 (0.4573) acc@5 0.7500 (0.7489)\n",
      "\u001b[32m[2020-06-24 11:02:54] __main__ INFO: \u001b[0mEpoch 87 Step 200/351 lr 0.020000 loss 1.4899 (1.4204) acc@1 0.4297 (0.4570) acc@5 0.7656 (0.7498)\n",
      "\u001b[32m[2020-06-24 11:04:45] __main__ INFO: \u001b[0mEpoch 87 Step 300/351 lr 0.020000 loss 1.5093 (1.4346) acc@1 0.4141 (0.4510) acc@5 0.7031 (0.7466)\n",
      "\u001b[32m[2020-06-24 11:05:42] __main__ INFO: \u001b[0mEpoch 87 Step 351/351 lr 0.020000 loss 1.5766 (1.4422) acc@1 0.3828 (0.4480) acc@5 0.6484 (0.7460)\n",
      "\u001b[32m[2020-06-24 11:05:42] __main__ INFO: \u001b[0mElapsed 392.68\n",
      "\u001b[32m[2020-06-24 11:05:42] __main__ INFO: \u001b[0mVal 87\n",
      "\u001b[32m[2020-06-24 11:05:55] __main__ INFO: \u001b[0mEpoch 87 loss 2.0455 acc@1 0.3130 acc@5 0.7060\n",
      "\u001b[32m[2020-06-24 11:05:55] __main__ INFO: \u001b[0mElapsed 13.08\n",
      "\u001b[32m[2020-06-24 11:05:55] __main__ INFO: \u001b[0mTrain 88 30537\n",
      "\u001b[32m[2020-06-24 11:07:47] __main__ INFO: \u001b[0mEpoch 88 Step 100/351 lr 0.020000 loss 1.4849 (1.4003) acc@1 0.4453 (0.4744) acc@5 0.7500 (0.7538)\n",
      "\u001b[32m[2020-06-24 11:09:38] __main__ INFO: \u001b[0mEpoch 88 Step 200/351 lr 0.020000 loss 1.4353 (1.4173) acc@1 0.4531 (0.4634) acc@5 0.7656 (0.7538)\n",
      "\u001b[32m[2020-06-24 11:11:30] __main__ INFO: \u001b[0mEpoch 88 Step 300/351 lr 0.020000 loss 1.5302 (1.4340) acc@1 0.4141 (0.4562) acc@5 0.7188 (0.7506)\n",
      "\u001b[32m[2020-06-24 11:12:27] __main__ INFO: \u001b[0mEpoch 88 Step 351/351 lr 0.020000 loss 1.5545 (1.4385) acc@1 0.3984 (0.4540) acc@5 0.6797 (0.7492)\n",
      "\u001b[32m[2020-06-24 11:12:27] __main__ INFO: \u001b[0mElapsed 391.16\n",
      "\u001b[32m[2020-06-24 11:12:27] __main__ INFO: \u001b[0mVal 88\n",
      "\u001b[32m[2020-06-24 11:12:40] __main__ INFO: \u001b[0mEpoch 88 loss 2.0045 acc@1 0.3068 acc@5 0.6936\n",
      "\u001b[32m[2020-06-24 11:12:40] __main__ INFO: \u001b[0mElapsed 13.09\n",
      "\u001b[32m[2020-06-24 11:12:40] __main__ INFO: \u001b[0mTrain 89 30888\n",
      "\u001b[32m[2020-06-24 11:14:32] __main__ INFO: \u001b[0mEpoch 89 Step 100/351 lr 0.020000 loss 1.4050 (1.4041) acc@1 0.4609 (0.4684) acc@5 0.8047 (0.7545)\n",
      "\u001b[32m[2020-06-24 11:16:24] __main__ INFO: \u001b[0mEpoch 89 Step 200/351 lr 0.020000 loss 1.3686 (1.4156) acc@1 0.4688 (0.4639) acc@5 0.6875 (0.7508)\n",
      "\u001b[32m[2020-06-24 11:18:16] __main__ INFO: \u001b[0mEpoch 89 Step 300/351 lr 0.020000 loss 1.5007 (1.4298) acc@1 0.3984 (0.4570) acc@5 0.7031 (0.7505)\n",
      "\u001b[32m[2020-06-24 11:19:13] __main__ INFO: \u001b[0mEpoch 89 Step 351/351 lr 0.020000 loss 1.4119 (1.4352) acc@1 0.4844 (0.4547) acc@5 0.8047 (0.7505)\n",
      "\u001b[32m[2020-06-24 11:19:13] __main__ INFO: \u001b[0mElapsed 392.94\n",
      "\u001b[32m[2020-06-24 11:19:13] __main__ INFO: \u001b[0mVal 89\n",
      "\u001b[32m[2020-06-24 11:19:26] __main__ INFO: \u001b[0mEpoch 89 loss 2.0809 acc@1 0.3078 acc@5 0.6864\n",
      "\u001b[32m[2020-06-24 11:19:26] __main__ INFO: \u001b[0mElapsed 13.10\n",
      "\u001b[32m[2020-06-24 11:19:26] __main__ INFO: \u001b[0mTrain 90 31239\n",
      "\u001b[32m[2020-06-24 11:21:17] __main__ INFO: \u001b[0mEpoch 90 Step 100/351 lr 0.020000 loss 1.3794 (1.3964) acc@1 0.4844 (0.4712) acc@5 0.7734 (0.7492)\n",
      "\u001b[32m[2020-06-24 11:23:09] __main__ INFO: \u001b[0mEpoch 90 Step 200/351 lr 0.020000 loss 1.4934 (1.3965) acc@1 0.4375 (0.4689) acc@5 0.7266 (0.7521)\n",
      "\u001b[32m[2020-06-24 11:25:00] __main__ INFO: \u001b[0mEpoch 90 Step 300/351 lr 0.020000 loss 1.4265 (1.4146) acc@1 0.4531 (0.4616) acc@5 0.7422 (0.7492)\n",
      "\u001b[32m[2020-06-24 11:25:57] __main__ INFO: \u001b[0mEpoch 90 Step 351/351 lr 0.020000 loss 1.3773 (1.4221) acc@1 0.4766 (0.4587) acc@5 0.7969 (0.7486)\n",
      "\u001b[32m[2020-06-24 11:25:57] __main__ INFO: \u001b[0mElapsed 391.21\n",
      "\u001b[32m[2020-06-24 11:25:57] __main__ INFO: \u001b[0mVal 90\n",
      "\u001b[32m[2020-06-24 11:26:10] __main__ INFO: \u001b[0mEpoch 90 loss 2.0498 acc@1 0.3140 acc@5 0.6988\n",
      "\u001b[32m[2020-06-24 11:26:10] __main__ INFO: \u001b[0mElapsed 13.11\n",
      "\u001b[32m[2020-06-24 11:26:10] __main__ INFO: \u001b[0mTrain 91 31590\n",
      "\u001b[32m[2020-06-24 11:28:02] __main__ INFO: \u001b[0mEpoch 91 Step 100/351 lr 0.020000 loss 1.3524 (1.3975) acc@1 0.5234 (0.4650) acc@5 0.7734 (0.7553)\n",
      "\u001b[32m[2020-06-24 11:29:54] __main__ INFO: \u001b[0mEpoch 91 Step 200/351 lr 0.020000 loss 1.2908 (1.4067) acc@1 0.5391 (0.4634) acc@5 0.8594 (0.7566)\n",
      "\u001b[32m[2020-06-24 11:31:46] __main__ INFO: \u001b[0mEpoch 91 Step 300/351 lr 0.020000 loss 1.5538 (1.4171) acc@1 0.4062 (0.4593) acc@5 0.7266 (0.7544)\n",
      "\u001b[32m[2020-06-24 11:32:43] __main__ INFO: \u001b[0mEpoch 91 Step 351/351 lr 0.020000 loss 1.4898 (1.4201) acc@1 0.4219 (0.4580) acc@5 0.7422 (0.7529)\n",
      "\u001b[32m[2020-06-24 11:32:43] __main__ INFO: \u001b[0mElapsed 392.72\n",
      "\u001b[32m[2020-06-24 11:32:43] __main__ INFO: \u001b[0mVal 91\n",
      "\u001b[32m[2020-06-24 11:32:56] __main__ INFO: \u001b[0mEpoch 91 loss 2.0786 acc@1 0.3054 acc@5 0.7020\n",
      "\u001b[32m[2020-06-24 11:32:56] __main__ INFO: \u001b[0mElapsed 13.13\n",
      "\u001b[32m[2020-06-24 11:32:56] __main__ INFO: \u001b[0mTrain 92 31941\n",
      "\u001b[32m[2020-06-24 11:34:47] __main__ INFO: \u001b[0mEpoch 92 Step 100/351 lr 0.020000 loss 1.4445 (1.3855) acc@1 0.4922 (0.4737) acc@5 0.7422 (0.7601)\n",
      "\u001b[32m[2020-06-24 11:36:39] __main__ INFO: \u001b[0mEpoch 92 Step 200/351 lr 0.020000 loss 1.4815 (1.3941) acc@1 0.4688 (0.4695) acc@5 0.7891 (0.7558)\n",
      "\u001b[32m[2020-06-24 11:38:30] __main__ INFO: \u001b[0mEpoch 92 Step 300/351 lr 0.020000 loss 1.4889 (1.4073) acc@1 0.4297 (0.4651) acc@5 0.7422 (0.7535)\n",
      "\u001b[32m[2020-06-24 11:39:27] __main__ INFO: \u001b[0mEpoch 92 Step 351/351 lr 0.020000 loss 1.5882 (1.4147) acc@1 0.4141 (0.4621) acc@5 0.7344 (0.7528)\n",
      "\u001b[32m[2020-06-24 11:39:27] __main__ INFO: \u001b[0mElapsed 391.23\n",
      "\u001b[32m[2020-06-24 11:39:27] __main__ INFO: \u001b[0mVal 92\n",
      "\u001b[32m[2020-06-24 11:39:40] __main__ INFO: \u001b[0mEpoch 92 loss 2.0240 acc@1 0.3110 acc@5 0.6970\n",
      "\u001b[32m[2020-06-24 11:39:40] __main__ INFO: \u001b[0mElapsed 13.11\n",
      "\u001b[32m[2020-06-24 11:39:40] __main__ INFO: \u001b[0mTrain 93 32292\n",
      "\u001b[32m[2020-06-24 11:41:32] __main__ INFO: \u001b[0mEpoch 93 Step 100/351 lr 0.020000 loss 1.3224 (1.3855) acc@1 0.4766 (0.4745) acc@5 0.7891 (0.7545)\n",
      "\u001b[32m[2020-06-24 11:43:24] __main__ INFO: \u001b[0mEpoch 93 Step 200/351 lr 0.020000 loss 1.6199 (1.3863) acc@1 0.3672 (0.4734) acc@5 0.6797 (0.7540)\n",
      "\u001b[32m[2020-06-24 11:45:16] __main__ INFO: \u001b[0mEpoch 93 Step 300/351 lr 0.020000 loss 1.4330 (1.4077) acc@1 0.4531 (0.4642) acc@5 0.7422 (0.7512)\n",
      "\u001b[32m[2020-06-24 11:46:13] __main__ INFO: \u001b[0mEpoch 93 Step 351/351 lr 0.020000 loss 1.5024 (1.4086) acc@1 0.4219 (0.4633) acc@5 0.7344 (0.7516)\n",
      "\u001b[32m[2020-06-24 11:46:13] __main__ INFO: \u001b[0mElapsed 392.68\n",
      "\u001b[32m[2020-06-24 11:46:13] __main__ INFO: \u001b[0mVal 93\n",
      "\u001b[32m[2020-06-24 11:46:26] __main__ INFO: \u001b[0mEpoch 93 loss 2.1085 acc@1 0.3018 acc@5 0.7076\n",
      "\u001b[32m[2020-06-24 11:46:26] __main__ INFO: \u001b[0mElapsed 13.09\n",
      "\u001b[32m[2020-06-24 11:46:26] __main__ INFO: \u001b[0mTrain 94 32643\n",
      "\u001b[32m[2020-06-24 11:48:18] __main__ INFO: \u001b[0mEpoch 94 Step 100/351 lr 0.020000 loss 1.3249 (1.3711) acc@1 0.5078 (0.4805) acc@5 0.7734 (0.7619)\n",
      "\u001b[32m[2020-06-24 11:50:09] __main__ INFO: \u001b[0mEpoch 94 Step 200/351 lr 0.020000 loss 1.3682 (1.3841) acc@1 0.4844 (0.4749) acc@5 0.7734 (0.7557)\n",
      "\u001b[32m[2020-06-24 11:52:00] __main__ INFO: \u001b[0mEpoch 94 Step 300/351 lr 0.020000 loss 1.4842 (1.3941) acc@1 0.4219 (0.4699) acc@5 0.6953 (0.7542)\n",
      "\u001b[32m[2020-06-24 11:52:57] __main__ INFO: \u001b[0mEpoch 94 Step 351/351 lr 0.020000 loss 1.5556 (1.3997) acc@1 0.4219 (0.4673) acc@5 0.7344 (0.7522)\n",
      "\u001b[32m[2020-06-24 11:52:57] __main__ INFO: \u001b[0mElapsed 391.24\n",
      "\u001b[32m[2020-06-24 11:52:57] __main__ INFO: \u001b[0mVal 94\n",
      "\u001b[32m[2020-06-24 11:53:10] __main__ INFO: \u001b[0mEpoch 94 loss 2.1239 acc@1 0.2930 acc@5 0.6952\n",
      "\u001b[32m[2020-06-24 11:53:10] __main__ INFO: \u001b[0mElapsed 13.12\n",
      "\u001b[32m[2020-06-24 11:53:10] __main__ INFO: \u001b[0mTrain 95 32994\n",
      "\u001b[32m[2020-06-24 11:55:02] __main__ INFO: \u001b[0mEpoch 95 Step 100/351 lr 0.020000 loss 1.2759 (1.3686) acc@1 0.5000 (0.4762) acc@5 0.7891 (0.7516)\n",
      "\u001b[32m[2020-06-24 11:56:54] __main__ INFO: \u001b[0mEpoch 95 Step 200/351 lr 0.020000 loss 1.3281 (1.3807) acc@1 0.4922 (0.4718) acc@5 0.7891 (0.7534)\n",
      "\u001b[32m[2020-06-24 11:58:46] __main__ INFO: \u001b[0mEpoch 95 Step 300/351 lr 0.020000 loss 1.4953 (1.3929) acc@1 0.4375 (0.4675) acc@5 0.7188 (0.7503)\n",
      "\u001b[32m[2020-06-24 11:59:43] __main__ INFO: \u001b[0mEpoch 95 Step 351/351 lr 0.020000 loss 1.5398 (1.3962) acc@1 0.4297 (0.4671) acc@5 0.6719 (0.7507)\n",
      "\u001b[32m[2020-06-24 11:59:43] __main__ INFO: \u001b[0mElapsed 392.86\n",
      "\u001b[32m[2020-06-24 11:59:43] __main__ INFO: \u001b[0mVal 95\n",
      "\u001b[32m[2020-06-24 11:59:56] __main__ INFO: \u001b[0mEpoch 95 loss 1.9726 acc@1 0.3132 acc@5 0.7068\n",
      "\u001b[32m[2020-06-24 11:59:56] __main__ INFO: \u001b[0mElapsed 13.11\n",
      "\u001b[32m[2020-06-24 11:59:56] __main__ INFO: \u001b[0mTrain 96 33345\n",
      "\u001b[32m[2020-06-24 12:01:48] __main__ INFO: \u001b[0mEpoch 96 Step 100/351 lr 0.020000 loss 1.3189 (1.3568) acc@1 0.4922 (0.4863) acc@5 0.7109 (0.7548)\n",
      "\u001b[32m[2020-06-24 12:03:39] __main__ INFO: \u001b[0mEpoch 96 Step 200/351 lr 0.020000 loss 1.4439 (1.3828) acc@1 0.4297 (0.4722) acc@5 0.7734 (0.7516)\n",
      "\u001b[32m[2020-06-24 12:05:31] __main__ INFO: \u001b[0mEpoch 96 Step 300/351 lr 0.020000 loss 1.2721 (1.3872) acc@1 0.5312 (0.4708) acc@5 0.7891 (0.7530)\n",
      "\u001b[32m[2020-06-24 12:06:28] __main__ INFO: \u001b[0mEpoch 96 Step 351/351 lr 0.020000 loss 1.3889 (1.3862) acc@1 0.5078 (0.4717) acc@5 0.7109 (0.7547)\n",
      "\u001b[32m[2020-06-24 12:06:28] __main__ INFO: \u001b[0mElapsed 391.32\n",
      "\u001b[32m[2020-06-24 12:06:28] __main__ INFO: \u001b[0mVal 96\n",
      "\u001b[32m[2020-06-24 12:06:41] __main__ INFO: \u001b[0mEpoch 96 loss 2.1192 acc@1 0.3042 acc@5 0.6996\n",
      "\u001b[32m[2020-06-24 12:06:41] __main__ INFO: \u001b[0mElapsed 13.11\n",
      "\u001b[32m[2020-06-24 12:06:41] __main__ INFO: \u001b[0mTrain 97 33696\n",
      "\u001b[32m[2020-06-24 12:08:33] __main__ INFO: \u001b[0mEpoch 97 Step 100/351 lr 0.020000 loss 1.4689 (1.3609) acc@1 0.4688 (0.4791) acc@5 0.7500 (0.7544)\n",
      "\u001b[32m[2020-06-24 12:10:25] __main__ INFO: \u001b[0mEpoch 97 Step 200/351 lr 0.020000 loss 1.4888 (1.3785) acc@1 0.3906 (0.4736) acc@5 0.6641 (0.7516)\n",
      "\u001b[32m[2020-06-24 12:12:16] __main__ INFO: \u001b[0mEpoch 97 Step 300/351 lr 0.020000 loss 1.2807 (1.3835) acc@1 0.5156 (0.4720) acc@5 0.7734 (0.7538)\n",
      "\u001b[32m[2020-06-24 12:13:13] __main__ INFO: \u001b[0mEpoch 97 Step 351/351 lr 0.020000 loss 1.4777 (1.3864) acc@1 0.4609 (0.4711) acc@5 0.7188 (0.7546)\n",
      "\u001b[32m[2020-06-24 12:13:14] __main__ INFO: \u001b[0mElapsed 392.69\n",
      "\u001b[32m[2020-06-24 12:13:14] __main__ INFO: \u001b[0mVal 97\n",
      "\u001b[32m[2020-06-24 12:13:27] __main__ INFO: \u001b[0mEpoch 97 loss 2.0633 acc@1 0.3100 acc@5 0.6926\n",
      "\u001b[32m[2020-06-24 12:13:27] __main__ INFO: \u001b[0mElapsed 13.08\n",
      "\u001b[32m[2020-06-24 12:13:27] __main__ INFO: \u001b[0mTrain 98 34047\n",
      "\u001b[32m[2020-06-24 12:15:18] __main__ INFO: \u001b[0mEpoch 98 Step 100/351 lr 0.020000 loss 1.5016 (1.3488) acc@1 0.4297 (0.4827) acc@5 0.6641 (0.7561)\n",
      "\u001b[32m[2020-06-24 12:17:10] __main__ INFO: \u001b[0mEpoch 98 Step 200/351 lr 0.020000 loss 1.5037 (1.3662) acc@1 0.4297 (0.4797) acc@5 0.7734 (0.7578)\n",
      "\u001b[32m[2020-06-24 12:19:01] __main__ INFO: \u001b[0mEpoch 98 Step 300/351 lr 0.020000 loss 1.3304 (1.3779) acc@1 0.5078 (0.4749) acc@5 0.7891 (0.7575)\n",
      "\u001b[32m[2020-06-24 12:19:58] __main__ INFO: \u001b[0mEpoch 98 Step 351/351 lr 0.020000 loss 1.3580 (1.3813) acc@1 0.4922 (0.4734) acc@5 0.7578 (0.7574)\n",
      "\u001b[32m[2020-06-24 12:19:58] __main__ INFO: \u001b[0mElapsed 391.25\n",
      "\u001b[32m[2020-06-24 12:19:58] __main__ INFO: \u001b[0mVal 98\n",
      "\u001b[32m[2020-06-24 12:20:11] __main__ INFO: \u001b[0mEpoch 98 loss 2.1316 acc@1 0.2922 acc@5 0.7004\n",
      "\u001b[32m[2020-06-24 12:20:11] __main__ INFO: \u001b[0mElapsed 13.11\n",
      "\u001b[32m[2020-06-24 12:20:11] __main__ INFO: \u001b[0mTrain 99 34398\n",
      "\u001b[32m[2020-06-24 12:22:03] __main__ INFO: \u001b[0mEpoch 99 Step 100/351 lr 0.020000 loss 1.3893 (1.3436) acc@1 0.4922 (0.4878) acc@5 0.7578 (0.7649)\n",
      "\u001b[32m[2020-06-24 12:23:55] __main__ INFO: \u001b[0mEpoch 99 Step 200/351 lr 0.020000 loss 1.2459 (1.3677) acc@1 0.5156 (0.4774) acc@5 0.7734 (0.7574)\n",
      "\u001b[32m[2020-06-24 12:25:47] __main__ INFO: \u001b[0mEpoch 99 Step 300/351 lr 0.020000 loss 1.4469 (1.3765) acc@1 0.4375 (0.4734) acc@5 0.7500 (0.7568)\n",
      "\u001b[32m[2020-06-24 12:26:44] __main__ INFO: \u001b[0mEpoch 99 Step 351/351 lr 0.020000 loss 1.3855 (1.3793) acc@1 0.4688 (0.4723) acc@5 0.7734 (0.7552)\n",
      "\u001b[32m[2020-06-24 12:26:44] __main__ INFO: \u001b[0mElapsed 392.79\n",
      "\u001b[32m[2020-06-24 12:26:44] __main__ INFO: \u001b[0mVal 99\n",
      "\u001b[32m[2020-06-24 12:26:57] __main__ INFO: \u001b[0mEpoch 99 loss 2.1453 acc@1 0.3054 acc@5 0.7094\n",
      "\u001b[32m[2020-06-24 12:26:57] __main__ INFO: \u001b[0mElapsed 13.12\n",
      "\u001b[32m[2020-06-24 12:26:57] __main__ INFO: \u001b[0mTrain 100 34749\n",
      "\u001b[32m[2020-06-24 12:28:48] __main__ INFO: \u001b[0mEpoch 100 Step 100/351 lr 0.020000 loss 1.4220 (1.3511) acc@1 0.4609 (0.4836) acc@5 0.8125 (0.7542)\n",
      "\u001b[32m[2020-06-24 12:30:40] __main__ INFO: \u001b[0mEpoch 100 Step 200/351 lr 0.020000 loss 1.3683 (1.3618) acc@1 0.4297 (0.4799) acc@5 0.7656 (0.7549)\n",
      "\u001b[32m[2020-06-24 12:32:31] __main__ INFO: \u001b[0mEpoch 100 Step 300/351 lr 0.020000 loss 1.4708 (1.3737) acc@1 0.4219 (0.4762) acc@5 0.7422 (0.7540)\n",
      "\u001b[32m[2020-06-24 12:33:28] __main__ INFO: \u001b[0mEpoch 100 Step 351/351 lr 0.020000 loss 1.3129 (1.3774) acc@1 0.4922 (0.4740) acc@5 0.7031 (0.7526)\n",
      "\u001b[32m[2020-06-24 12:33:28] __main__ INFO: \u001b[0mElapsed 391.34\n",
      "\u001b[32m[2020-06-24 12:33:28] __main__ INFO: \u001b[0mVal 100\n",
      "\u001b[32m[2020-06-24 12:33:41] __main__ INFO: \u001b[0mEpoch 100 loss 2.1602 acc@1 0.2956 acc@5 0.7088\n",
      "\u001b[32m[2020-06-24 12:33:41] __main__ INFO: \u001b[0mElapsed 13.15\n",
      "\u001b[32m[2020-06-24 12:33:41] fvcore.common.checkpoint INFO: \u001b[0mSaving checkpoint to /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_3_20/exp00/checkpoint_00100.pth\n",
      "\u001b[32m[2020-06-24 12:33:42] __main__ INFO: \u001b[0mTrain 101 35100\n",
      "\u001b[32m[2020-06-24 12:35:34] __main__ INFO: \u001b[0mEpoch 101 Step 100/351 lr 0.020000 loss 1.2216 (1.3332) acc@1 0.5391 (0.4942) acc@5 0.8281 (0.7535)\n",
      "\u001b[32m[2020-06-24 12:37:26] __main__ INFO: \u001b[0mEpoch 101 Step 200/351 lr 0.020000 loss 1.4194 (1.3427) acc@1 0.4531 (0.4894) acc@5 0.6719 (0.7507)\n",
      "\u001b[32m[2020-06-24 12:39:17] __main__ INFO: \u001b[0mEpoch 101 Step 300/351 lr 0.020000 loss 1.4264 (1.3575) acc@1 0.4531 (0.4841) acc@5 0.7578 (0.7503)\n",
      "\u001b[32m[2020-06-24 12:40:14] __main__ INFO: \u001b[0mEpoch 101 Step 351/351 lr 0.020000 loss 1.3109 (1.3626) acc@1 0.5000 (0.4829) acc@5 0.7578 (0.7522)\n",
      "\u001b[32m[2020-06-24 12:40:14] __main__ INFO: \u001b[0mElapsed 392.75\n",
      "\u001b[32m[2020-06-24 12:40:14] __main__ INFO: \u001b[0mVal 101\n",
      "\u001b[32m[2020-06-24 12:40:28] __main__ INFO: \u001b[0mEpoch 101 loss 2.2161 acc@1 0.2898 acc@5 0.7018\n",
      "\u001b[32m[2020-06-24 12:40:28] __main__ INFO: \u001b[0mElapsed 13.13\n",
      "\u001b[32m[2020-06-24 12:40:28] __main__ INFO: \u001b[0mTrain 102 35451\n",
      "\u001b[32m[2020-06-24 12:42:19] __main__ INFO: \u001b[0mEpoch 102 Step 100/351 lr 0.020000 loss 1.3231 (1.3502) acc@1 0.4922 (0.4844) acc@5 0.7188 (0.7510)\n"
     ]
    }
   ],
   "source": [
    "# Train the model per the settings specified in the original paper\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "!python train.py --config configs/cifar/wrn.yaml \\\n",
    "    model.wrn.depth 28 \\\n",
    "    model.wrn.widening_factor 10 \\\n",
    "    train.batch_size 128 \\\n",
    "    train.base_lr 0.1 \\\n",
    "    dataset.name CIFAR10_RA_3_20 \\\n",
    "    train.output_dir /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_3_20/exp00 \\\n",
    "    scheduler.epochs 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-26 12:59:44] __main__ INFO: \u001b[0mdevice: cuda\n",
      "cudnn:\n",
      "  benchmark: True\n",
      "  deterministic: False\n",
      "dataset:\n",
      "  name: CIFAR10_RA_3_20\n",
      "  dataset_dir: ''\n",
      "  image_size: 32\n",
      "  n_channels: 3\n",
      "  n_classes: 10\n",
      "model:\n",
      "  type: cifar\n",
      "  name: wrn\n",
      "  init_mode: kaiming_fan_in\n",
      "  vgg:\n",
      "    n_channels: [64, 128, 256, 512, 512]\n",
      "    n_layers: [2, 2, 3, 3, 3]\n",
      "    use_bn: True\n",
      "  resnet:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "  resnet_preact:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "  wrn:\n",
      "    depth: 28\n",
      "    initial_channels: 16\n",
      "    widening_factor: 10\n",
      "    drop_rate: 0.0\n",
      "  densenet:\n",
      "    depth: 100\n",
      "    n_blocks: [6, 12, 24, 16]\n",
      "    block_type: bottleneck\n",
      "    growth_rate: 12\n",
      "    drop_rate: 0.0\n",
      "    compression_rate: 0.5\n",
      "  pyramidnet:\n",
      "    depth: 272\n",
      "    n_blocks: [3, 24, 36, 3]\n",
      "    initial_channels: 16\n",
      "    block_type: bottleneck\n",
      "    alpha: 200\n",
      "  resnext:\n",
      "    depth: 29\n",
      "    n_blocks: [3, 4, 6, 3]\n",
      "    initial_channels: 64\n",
      "    cardinality: 8\n",
      "    base_channels: 4\n",
      "  shake_shake:\n",
      "    depth: 26\n",
      "    initial_channels: 96\n",
      "    shake_forward: True\n",
      "    shake_backward: True\n",
      "    shake_image: True\n",
      "  se_resnet_preact:\n",
      "    depth: 110\n",
      "    initial_channels: 16\n",
      "    se_reduction: 16\n",
      "    block_type: basic\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "train:\n",
      "  checkpoint: /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_3_20/exp01/checkpoint_00200.pth\n",
      "  resume: False\n",
      "  precision: O0\n",
      "  batch_size: 128\n",
      "  subdivision: 1\n",
      "  optimizer: sgd\n",
      "  base_lr: 0.004\n",
      "  momentum: 0.9\n",
      "  nesterov: True\n",
      "  weight_decay: 0.0005\n",
      "  no_weight_decay_on_bn: False\n",
      "  gradient_clip: 0\n",
      "  start_epoch: 0\n",
      "  seed: 0\n",
      "  val_first: True\n",
      "  val_period: 1\n",
      "  val_ratio: 0.1\n",
      "  use_test_as_val: False\n",
      "  output_dir: /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_3_20/exp02\n",
      "  log_period: 100\n",
      "  checkpoint_period: 100\n",
      "  use_tensorboard: True\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: True\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "  distributed: False\n",
      "  dist:\n",
      "    backend: nccl\n",
      "    init_method: env://\n",
      "    world_size: -1\n",
      "    node_rank: -1\n",
      "    local_rank: 0\n",
      "    use_sync_bn: False\n",
      "tensorboard:\n",
      "  train_images: False\n",
      "  val_images: False\n",
      "  model_params: False\n",
      "optim:\n",
      "  adam:\n",
      "    betas: (0.9, 0.999)\n",
      "  lars:\n",
      "    eps: 1e-09\n",
      "    threshold: 0.01\n",
      "  adabound:\n",
      "    betas: (0.9, 0.999)\n",
      "    final_lr: 0.1\n",
      "    gamma: 0.001\n",
      "scheduler:\n",
      "  epochs: 200\n",
      "  warmup:\n",
      "    type: none\n",
      "    epochs: 0\n",
      "    start_factor: 0.001\n",
      "    exponent: 4\n",
      "  type: multistep\n",
      "  milestones: [60, 120, 160]\n",
      "  lr_decay: 0.2\n",
      "  lr_min_factor: 0.001\n",
      "  T0: 10\n",
      "  T_mul: 1.0\n",
      "validation:\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: False\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "augmentation:\n",
      "  use_random_crop: True\n",
      "  use_random_horizontal_flip: True\n",
      "  use_cutout: False\n",
      "  use_random_erasing: False\n",
      "  use_dual_cutout: False\n",
      "  use_mixup: False\n",
      "  use_ricap: False\n",
      "  use_cutmix: False\n",
      "  use_label_smoothing: False\n",
      "  random_crop:\n",
      "    padding: 4\n",
      "    fill: 0\n",
      "    padding_mode: constant\n",
      "  random_horizontal_flip:\n",
      "    prob: 0.5\n",
      "  cutout:\n",
      "    prob: 1.0\n",
      "    mask_size: 16\n",
      "    cut_inside: False\n",
      "    mask_color: 0\n",
      "    dual_cutout_alpha: 0.1\n",
      "  random_erasing:\n",
      "    prob: 0.5\n",
      "    area_ratio_range: [0.02, 0.4]\n",
      "    min_aspect_ratio: 0.3\n",
      "    max_attempt: 20\n",
      "  mixup:\n",
      "    alpha: 1.0\n",
      "  ricap:\n",
      "    beta: 0.3\n",
      "  cutmix:\n",
      "    alpha: 1.0\n",
      "  label_smoothing:\n",
      "    epsilon: 0.1\n",
      "tta:\n",
      "  use_resize: False\n",
      "  use_center_crop: False\n",
      "  resize: 256\n",
      "test:\n",
      "  checkpoint: ''\n",
      "  output_dir: ''\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    pin_memory: False\n",
      "\u001b[32m[2020-06-26 12:59:44] __main__ INFO: \u001b[0menv_info:\n",
      "  pytorch_version: 1.4.0\n",
      "  cuda_version: 10.1\n",
      "  cudnn_version: 7603\n",
      "  num_gpus: 1\n",
      "  gpu_name: Tesla K80\n",
      "  gpu_capability: 3.7\n",
      "(50000, 32, 32, 3)\n",
      "\u001b[32m[2020-06-26 12:59:50] __main__ INFO: \u001b[0mMACs  : 5.25G\n",
      "\u001b[32m[2020-06-26 12:59:50] __main__ INFO: \u001b[0m#params: 36.48M\n",
      "Selected optimization level O0:  Pure FP32 training.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\",)\n",
      "\u001b[32m[2020-06-26 12:59:50] __main__ INFO: \u001b[0mVal 0\n",
      "\u001b[32m[2020-06-26 13:00:10] __main__ INFO: \u001b[0mEpoch 0 loss 2.1212 acc@1 0.3274 acc@5 0.7112\n",
      "\u001b[32m[2020-06-26 13:00:10] __main__ INFO: \u001b[0mElapsed 19.46\n",
      "\u001b[32m[2020-06-26 13:00:10] __main__ INFO: \u001b[0mTrain 1 0\n",
      "\u001b[32m[2020-06-26 13:02:06] __main__ INFO: \u001b[0mEpoch 1 Step 100/351 lr 0.004000 loss 1.1625 (1.1229) acc@1 0.5078 (0.5649) acc@5 0.7422 (0.7689)\n",
      "\u001b[32m[2020-06-26 13:03:59] __main__ INFO: \u001b[0mEpoch 1 Step 200/351 lr 0.004000 loss 0.9970 (1.1275) acc@1 0.6172 (0.5638) acc@5 0.7969 (0.7660)\n",
      "\u001b[32m[2020-06-26 13:05:51] __main__ INFO: \u001b[0mEpoch 1 Step 300/351 lr 0.004000 loss 1.0796 (1.1269) acc@1 0.5703 (0.5632) acc@5 0.8047 (0.7645)\n",
      "\u001b[32m[2020-06-26 13:06:48] __main__ INFO: \u001b[0mEpoch 1 Step 351/351 lr 0.004000 loss 1.1199 (1.1283) acc@1 0.5547 (0.5614) acc@5 0.7422 (0.7626)\n",
      "\u001b[32m[2020-06-26 13:06:48] __main__ INFO: \u001b[0mElapsed 398.33\n",
      "\u001b[32m[2020-06-26 13:06:48] __main__ INFO: \u001b[0mVal 1\n",
      "\u001b[32m[2020-06-26 13:07:01] __main__ INFO: \u001b[0mEpoch 1 loss 2.1282 acc@1 0.3316 acc@5 0.7064\n",
      "\u001b[32m[2020-06-26 13:07:01] __main__ INFO: \u001b[0mElapsed 13.15\n",
      "\u001b[32m[2020-06-26 13:07:01] __main__ INFO: \u001b[0mTrain 2 351\n",
      "\u001b[32m[2020-06-26 13:08:53] __main__ INFO: \u001b[0mEpoch 2 Step 100/351 lr 0.004000 loss 1.2044 (1.1365) acc@1 0.5156 (0.5609) acc@5 0.6719 (0.7599)\n",
      "\u001b[32m[2020-06-26 13:10:45] __main__ INFO: \u001b[0mEpoch 2 Step 200/351 lr 0.004000 loss 1.2056 (1.1321) acc@1 0.5391 (0.5627) acc@5 0.7969 (0.7638)\n",
      "\u001b[32m[2020-06-26 13:12:37] __main__ INFO: \u001b[0mEpoch 2 Step 300/351 lr 0.004000 loss 0.9960 (1.1313) acc@1 0.5859 (0.5613) acc@5 0.7734 (0.7606)\n",
      "\u001b[32m[2020-06-26 13:13:34] __main__ INFO: \u001b[0mEpoch 2 Step 351/351 lr 0.004000 loss 1.2240 (1.1282) acc@1 0.5469 (0.5623) acc@5 0.7266 (0.7615)\n",
      "\u001b[32m[2020-06-26 13:13:34] __main__ INFO: \u001b[0mElapsed 393.23\n",
      "\u001b[32m[2020-06-26 13:13:34] __main__ INFO: \u001b[0mVal 2\n",
      "\u001b[32m[2020-06-26 13:13:47] __main__ INFO: \u001b[0mEpoch 2 loss 2.1130 acc@1 0.3238 acc@5 0.7062\n",
      "\u001b[32m[2020-06-26 13:13:47] __main__ INFO: \u001b[0mElapsed 13.14\n",
      "\u001b[32m[2020-06-26 13:13:47] __main__ INFO: \u001b[0mTrain 3 702\n",
      "\u001b[32m[2020-06-26 13:15:39] __main__ INFO: \u001b[0mEpoch 3 Step 100/351 lr 0.004000 loss 1.3448 (1.1229) acc@1 0.4609 (0.5617) acc@5 0.7891 (0.7617)\n",
      "\u001b[32m[2020-06-26 13:17:31] __main__ INFO: \u001b[0mEpoch 3 Step 200/351 lr 0.004000 loss 1.2578 (1.1252) acc@1 0.5469 (0.5613) acc@5 0.7578 (0.7607)\n",
      "\u001b[32m[2020-06-26 13:19:23] __main__ INFO: \u001b[0mEpoch 3 Step 300/351 lr 0.004000 loss 1.1006 (1.1246) acc@1 0.5547 (0.5610) acc@5 0.7656 (0.7613)\n",
      "\u001b[32m[2020-06-26 13:20:20] __main__ INFO: \u001b[0mEpoch 3 Step 351/351 lr 0.004000 loss 1.0638 (1.1281) acc@1 0.5703 (0.5605) acc@5 0.8047 (0.7605)\n",
      "\u001b[32m[2020-06-26 13:20:20] __main__ INFO: \u001b[0mElapsed 393.08\n",
      "\u001b[32m[2020-06-26 13:20:20] __main__ INFO: \u001b[0mVal 3\n",
      "\u001b[32m[2020-06-26 13:20:34] __main__ INFO: \u001b[0mEpoch 3 loss 2.1213 acc@1 0.3270 acc@5 0.7112\n",
      "\u001b[32m[2020-06-26 13:20:34] __main__ INFO: \u001b[0mElapsed 13.13\n",
      "\u001b[32m[2020-06-26 13:20:34] __main__ INFO: \u001b[0mTrain 4 1053\n",
      "\u001b[32m[2020-06-26 13:22:26] __main__ INFO: \u001b[0mEpoch 4 Step 100/351 lr 0.004000 loss 1.1390 (1.1274) acc@1 0.5391 (0.5623) acc@5 0.7500 (0.7623)\n",
      "\u001b[32m[2020-06-26 13:24:18] __main__ INFO: \u001b[0mEpoch 4 Step 200/351 lr 0.004000 loss 1.0616 (1.1332) acc@1 0.6172 (0.5611) acc@5 0.7812 (0.7611)\n",
      "\u001b[32m[2020-06-26 13:26:10] __main__ INFO: \u001b[0mEpoch 4 Step 300/351 lr 0.004000 loss 1.1149 (1.1275) acc@1 0.5703 (0.5628) acc@5 0.7812 (0.7632)\n",
      "\u001b[32m[2020-06-26 13:27:07] __main__ INFO: \u001b[0mEpoch 4 Step 351/351 lr 0.004000 loss 1.0909 (1.1279) acc@1 0.6094 (0.5625) acc@5 0.8203 (0.7623)\n",
      "\u001b[32m[2020-06-26 13:27:07] __main__ INFO: \u001b[0mElapsed 393.25\n",
      "\u001b[32m[2020-06-26 13:27:07] __main__ INFO: \u001b[0mVal 4\n",
      "\u001b[32m[2020-06-26 13:27:20] __main__ INFO: \u001b[0mEpoch 4 loss 2.1177 acc@1 0.3320 acc@5 0.7112\n",
      "\u001b[32m[2020-06-26 13:27:20] __main__ INFO: \u001b[0mElapsed 13.15\n",
      "\u001b[32m[2020-06-26 13:27:20] __main__ INFO: \u001b[0mTrain 5 1404\n",
      "\u001b[32m[2020-06-26 13:29:12] __main__ INFO: \u001b[0mEpoch 5 Step 100/351 lr 0.004000 loss 1.0465 (1.1218) acc@1 0.5625 (0.5656) acc@5 0.8672 (0.7646)\n",
      "\u001b[32m[2020-06-26 13:31:04] __main__ INFO: \u001b[0mEpoch 5 Step 200/351 lr 0.004000 loss 1.1360 (1.1287) acc@1 0.5547 (0.5628) acc@5 0.7500 (0.7617)\n",
      "\u001b[32m[2020-06-26 13:32:56] __main__ INFO: \u001b[0mEpoch 5 Step 300/351 lr 0.004000 loss 1.1581 (1.1299) acc@1 0.5547 (0.5620) acc@5 0.7266 (0.7606)\n",
      "\u001b[32m[2020-06-26 13:33:53] __main__ INFO: \u001b[0mEpoch 5 Step 351/351 lr 0.004000 loss 1.1210 (1.1286) acc@1 0.5781 (0.5622) acc@5 0.7266 (0.7608)\n",
      "\u001b[32m[2020-06-26 13:33:53] __main__ INFO: \u001b[0mElapsed 393.14\n",
      "\u001b[32m[2020-06-26 13:33:53] __main__ INFO: \u001b[0mVal 5\n",
      "\u001b[32m[2020-06-26 13:34:06] __main__ INFO: \u001b[0mEpoch 5 loss 2.1109 acc@1 0.3314 acc@5 0.7098\n",
      "\u001b[32m[2020-06-26 13:34:06] __main__ INFO: \u001b[0mElapsed 13.15\n",
      "\u001b[32m[2020-06-26 13:34:06] __main__ INFO: \u001b[0mTrain 6 1755\n",
      "\u001b[32m[2020-06-26 13:35:58] __main__ INFO: \u001b[0mEpoch 6 Step 100/351 lr 0.004000 loss 1.1858 (1.1364) acc@1 0.5391 (0.5591) acc@5 0.8125 (0.7596)\n",
      "\u001b[32m[2020-06-26 13:37:50] __main__ INFO: \u001b[0mEpoch 6 Step 200/351 lr 0.004000 loss 1.0666 (1.1364) acc@1 0.6016 (0.5594) acc@5 0.7812 (0.7600)\n",
      "\u001b[32m[2020-06-26 13:39:42] __main__ INFO: \u001b[0mEpoch 6 Step 300/351 lr 0.004000 loss 1.1840 (1.1304) acc@1 0.5547 (0.5610) acc@5 0.7500 (0.7609)\n",
      "\u001b[32m[2020-06-26 13:40:39] __main__ INFO: \u001b[0mEpoch 6 Step 351/351 lr 0.004000 loss 0.9745 (1.1275) acc@1 0.6250 (0.5620) acc@5 0.8281 (0.7615)\n",
      "\u001b[32m[2020-06-26 13:40:39] __main__ INFO: \u001b[0mElapsed 393.23\n",
      "\u001b[32m[2020-06-26 13:40:39] __main__ INFO: \u001b[0mVal 6\n",
      "\u001b[32m[2020-06-26 13:40:53] __main__ INFO: \u001b[0mEpoch 6 loss 2.0989 acc@1 0.3336 acc@5 0.7124\n",
      "\u001b[32m[2020-06-26 13:40:53] __main__ INFO: \u001b[0mElapsed 13.15\n",
      "\u001b[32m[2020-06-26 13:40:53] __main__ INFO: \u001b[0mTrain 7 2106\n",
      "\u001b[32m[2020-06-26 13:42:45] __main__ INFO: \u001b[0mEpoch 7 Step 100/351 lr 0.004000 loss 1.0260 (1.1365) acc@1 0.6016 (0.5596) acc@5 0.7656 (0.7551)\n",
      "\u001b[32m[2020-06-26 13:44:37] __main__ INFO: \u001b[0mEpoch 7 Step 200/351 lr 0.004000 loss 1.1884 (1.1324) acc@1 0.5078 (0.5599) acc@5 0.7734 (0.7586)\n",
      "\u001b[32m[2020-06-26 13:46:29] __main__ INFO: \u001b[0mEpoch 7 Step 300/351 lr 0.004000 loss 1.2235 (1.1260) acc@1 0.5078 (0.5625) acc@5 0.7500 (0.7609)\n",
      "\u001b[32m[2020-06-26 13:47:26] __main__ INFO: \u001b[0mEpoch 7 Step 351/351 lr 0.004000 loss 1.0303 (1.1280) acc@1 0.6094 (0.5616) acc@5 0.7969 (0.7609)\n",
      "\u001b[32m[2020-06-26 13:47:26] __main__ INFO: \u001b[0mElapsed 393.35\n",
      "\u001b[32m[2020-06-26 13:47:26] __main__ INFO: \u001b[0mVal 7\n",
      "\u001b[32m[2020-06-26 13:47:39] __main__ INFO: \u001b[0mEpoch 7 loss 2.1125 acc@1 0.3328 acc@5 0.7128\n",
      "\u001b[32m[2020-06-26 13:47:39] __main__ INFO: \u001b[0mElapsed 13.14\n",
      "\u001b[32m[2020-06-26 13:47:39] __main__ INFO: \u001b[0mTrain 8 2457\n",
      "\u001b[32m[2020-06-26 13:49:31] __main__ INFO: \u001b[0mEpoch 8 Step 100/351 lr 0.004000 loss 1.3179 (1.1288) acc@1 0.5156 (0.5641) acc@5 0.6953 (0.7648)\n",
      "\u001b[32m[2020-06-26 13:51:23] __main__ INFO: \u001b[0mEpoch 8 Step 200/351 lr 0.004000 loss 0.9060 (1.1274) acc@1 0.6250 (0.5633) acc@5 0.8438 (0.7627)\n",
      "\u001b[32m[2020-06-26 13:53:15] __main__ INFO: \u001b[0mEpoch 8 Step 300/351 lr 0.004000 loss 1.1065 (1.1269) acc@1 0.5938 (0.5628) acc@5 0.7344 (0.7623)\n",
      "\u001b[32m[2020-06-26 13:54:12] __main__ INFO: \u001b[0mEpoch 8 Step 351/351 lr 0.004000 loss 1.1010 (1.1281) acc@1 0.5938 (0.5620) acc@5 0.8125 (0.7617)\n",
      "\u001b[32m[2020-06-26 13:54:12] __main__ INFO: \u001b[0mElapsed 393.22\n",
      "\u001b[32m[2020-06-26 13:54:12] __main__ INFO: \u001b[0mVal 8\n",
      "\u001b[32m[2020-06-26 13:54:26] __main__ INFO: \u001b[0mEpoch 8 loss 2.1048 acc@1 0.3292 acc@5 0.7130\n",
      "\u001b[32m[2020-06-26 13:54:26] __main__ INFO: \u001b[0mElapsed 13.15\n",
      "\u001b[32m[2020-06-26 13:54:26] __main__ INFO: \u001b[0mTrain 9 2808\n",
      "\u001b[32m[2020-06-26 13:56:18] __main__ INFO: \u001b[0mEpoch 9 Step 100/351 lr 0.004000 loss 1.0701 (1.1247) acc@1 0.6094 (0.5650) acc@5 0.7891 (0.7613)\n",
      "\u001b[32m[2020-06-26 13:58:10] __main__ INFO: \u001b[0mEpoch 9 Step 200/351 lr 0.004000 loss 1.0804 (1.1297) acc@1 0.6094 (0.5614) acc@5 0.7969 (0.7615)\n",
      "\u001b[32m[2020-06-26 14:00:02] __main__ INFO: \u001b[0mEpoch 9 Step 300/351 lr 0.004000 loss 1.0315 (1.1283) acc@1 0.5625 (0.5620) acc@5 0.7656 (0.7614)\n",
      "\u001b[32m[2020-06-26 14:00:59] __main__ INFO: \u001b[0mEpoch 9 Step 351/351 lr 0.004000 loss 1.2095 (1.1273) acc@1 0.5078 (0.5619) acc@5 0.7422 (0.7612)\n",
      "\u001b[32m[2020-06-26 14:00:59] __main__ INFO: \u001b[0mElapsed 393.30\n",
      "\u001b[32m[2020-06-26 14:00:59] __main__ INFO: \u001b[0mVal 9\n",
      "\u001b[32m[2020-06-26 14:01:12] __main__ INFO: \u001b[0mEpoch 9 loss 2.1147 acc@1 0.3298 acc@5 0.7050\n",
      "\u001b[32m[2020-06-26 14:01:12] __main__ INFO: \u001b[0mElapsed 13.16\n",
      "\u001b[32m[2020-06-26 14:01:12] __main__ INFO: \u001b[0mTrain 10 3159\n",
      "\u001b[32m[2020-06-26 14:03:04] __main__ INFO: \u001b[0mEpoch 10 Step 100/351 lr 0.004000 loss 1.1507 (1.1295) acc@1 0.5469 (0.5595) acc@5 0.7734 (0.7608)\n",
      "\u001b[32m[2020-06-26 14:04:56] __main__ INFO: \u001b[0mEpoch 10 Step 200/351 lr 0.004000 loss 1.0592 (1.1301) acc@1 0.6250 (0.5600) acc@5 0.8047 (0.7611)\n",
      "\u001b[32m[2020-06-26 14:06:48] __main__ INFO: \u001b[0mEpoch 10 Step 300/351 lr 0.004000 loss 1.1860 (1.1277) acc@1 0.5469 (0.5616) acc@5 0.7266 (0.7612)\n",
      "\u001b[32m[2020-06-26 14:07:45] __main__ INFO: \u001b[0mEpoch 10 Step 351/351 lr 0.004000 loss 1.0296 (1.1275) acc@1 0.5547 (0.5616) acc@5 0.7344 (0.7601)\n",
      "\u001b[32m[2020-06-26 14:07:45] __main__ INFO: \u001b[0mElapsed 393.44\n",
      "\u001b[32m[2020-06-26 14:07:45] __main__ INFO: \u001b[0mVal 10\n",
      "\u001b[32m[2020-06-26 14:07:59] __main__ INFO: \u001b[0mEpoch 10 loss 2.0944 acc@1 0.3314 acc@5 0.7112\n",
      "\u001b[32m[2020-06-26 14:07:59] __main__ INFO: \u001b[0mElapsed 13.18\n",
      "\u001b[32m[2020-06-26 14:07:59] __main__ INFO: \u001b[0mTrain 11 3510\n",
      "\u001b[32m[2020-06-26 14:09:51] __main__ INFO: \u001b[0mEpoch 11 Step 100/351 lr 0.004000 loss 0.9854 (1.1206) acc@1 0.6172 (0.5633) acc@5 0.8359 (0.7619)\n",
      "\u001b[32m[2020-06-26 14:11:43] __main__ INFO: \u001b[0mEpoch 11 Step 200/351 lr 0.004000 loss 1.1308 (1.1273) acc@1 0.5625 (0.5611) acc@5 0.7266 (0.7612)\n",
      "\u001b[32m[2020-06-26 14:13:35] __main__ INFO: \u001b[0mEpoch 11 Step 300/351 lr 0.004000 loss 1.1910 (1.1271) acc@1 0.5234 (0.5624) acc@5 0.7344 (0.7608)\n",
      "\u001b[32m[2020-06-26 14:14:32] __main__ INFO: \u001b[0mEpoch 11 Step 351/351 lr 0.004000 loss 1.2445 (1.1279) acc@1 0.5078 (0.5620) acc@5 0.7188 (0.7597)\n",
      "\u001b[32m[2020-06-26 14:14:32] __main__ INFO: \u001b[0mElapsed 393.38\n",
      "\u001b[32m[2020-06-26 14:14:32] __main__ INFO: \u001b[0mVal 11\n",
      "\u001b[32m[2020-06-26 14:14:45] __main__ INFO: \u001b[0mEpoch 11 loss 2.1161 acc@1 0.3296 acc@5 0.7126\n",
      "\u001b[32m[2020-06-26 14:14:45] __main__ INFO: \u001b[0mElapsed 13.16\n",
      "\u001b[32m[2020-06-26 14:14:45] __main__ INFO: \u001b[0mTrain 12 3861\n",
      "\u001b[32m[2020-06-26 14:16:37] __main__ INFO: \u001b[0mEpoch 12 Step 100/351 lr 0.004000 loss 1.0245 (1.1437) acc@1 0.6250 (0.5558) acc@5 0.8203 (0.7554)\n",
      "\u001b[32m[2020-06-26 14:18:29] __main__ INFO: \u001b[0mEpoch 12 Step 200/351 lr 0.004000 loss 1.1007 (1.1330) acc@1 0.5938 (0.5601) acc@5 0.7578 (0.7622)\n",
      "\u001b[32m[2020-06-26 14:20:21] __main__ INFO: \u001b[0mEpoch 12 Step 300/351 lr 0.004000 loss 0.9522 (1.1260) acc@1 0.6484 (0.5635) acc@5 0.8281 (0.7645)\n",
      "\u001b[32m[2020-06-26 14:21:19] __main__ INFO: \u001b[0mEpoch 12 Step 351/351 lr 0.004000 loss 1.1787 (1.1271) acc@1 0.5234 (0.5619) acc@5 0.7188 (0.7631)\n",
      "\u001b[32m[2020-06-26 14:21:19] __main__ INFO: \u001b[0mElapsed 393.43\n",
      "\u001b[32m[2020-06-26 14:21:19] __main__ INFO: \u001b[0mVal 12\n",
      "\u001b[32m[2020-06-26 14:21:32] __main__ INFO: \u001b[0mEpoch 12 loss 2.0959 acc@1 0.3294 acc@5 0.7122\n",
      "\u001b[32m[2020-06-26 14:21:32] __main__ INFO: \u001b[0mElapsed 13.16\n",
      "\u001b[32m[2020-06-26 14:21:32] __main__ INFO: \u001b[0mTrain 13 4212\n",
      "\u001b[32m[2020-06-26 14:23:24] __main__ INFO: \u001b[0mEpoch 13 Step 100/351 lr 0.004000 loss 1.1313 (1.1253) acc@1 0.5547 (0.5654) acc@5 0.8125 (0.7610)\n",
      "\u001b[32m[2020-06-26 14:25:16] __main__ INFO: \u001b[0mEpoch 13 Step 200/351 lr 0.004000 loss 1.1857 (1.1310) acc@1 0.5391 (0.5622) acc@5 0.7578 (0.7595)\n",
      "\u001b[32m[2020-06-26 14:27:08] __main__ INFO: \u001b[0mEpoch 13 Step 300/351 lr 0.004000 loss 1.1162 (1.1279) acc@1 0.5625 (0.5630) acc@5 0.7891 (0.7616)\n",
      "\u001b[32m[2020-06-26 14:28:05] __main__ INFO: \u001b[0mEpoch 13 Step 351/351 lr 0.004000 loss 1.2390 (1.1270) acc@1 0.5391 (0.5629) acc@5 0.7266 (0.7614)\n",
      "\u001b[32m[2020-06-26 14:28:05] __main__ INFO: \u001b[0mElapsed 393.32\n",
      "\u001b[32m[2020-06-26 14:28:05] __main__ INFO: \u001b[0mVal 13\n",
      "\u001b[32m[2020-06-26 14:28:18] __main__ INFO: \u001b[0mEpoch 13 loss 2.1092 acc@1 0.3234 acc@5 0.6980\n",
      "\u001b[32m[2020-06-26 14:28:18] __main__ INFO: \u001b[0mElapsed 13.16\n",
      "\u001b[32m[2020-06-26 14:28:18] __main__ INFO: \u001b[0mTrain 14 4563\n",
      "\u001b[32m[2020-06-26 14:30:10] __main__ INFO: \u001b[0mEpoch 14 Step 100/351 lr 0.004000 loss 1.1038 (1.1334) acc@1 0.5547 (0.5616) acc@5 0.7578 (0.7584)\n",
      "\u001b[32m[2020-06-26 14:32:02] __main__ INFO: \u001b[0mEpoch 14 Step 200/351 lr 0.004000 loss 1.2372 (1.1317) acc@1 0.5312 (0.5616) acc@5 0.7891 (0.7589)\n",
      "\u001b[32m[2020-06-26 14:33:54] __main__ INFO: \u001b[0mEpoch 14 Step 300/351 lr 0.004000 loss 1.1157 (1.1293) acc@1 0.5625 (0.5623) acc@5 0.7891 (0.7596)\n",
      "\u001b[32m[2020-06-26 14:34:52] __main__ INFO: \u001b[0mEpoch 14 Step 351/351 lr 0.004000 loss 1.2134 (1.1280) acc@1 0.5469 (0.5628) acc@5 0.7031 (0.7599)\n",
      "\u001b[32m[2020-06-26 14:34:52] __main__ INFO: \u001b[0mElapsed 393.33\n",
      "\u001b[32m[2020-06-26 14:34:52] __main__ INFO: \u001b[0mVal 14\n",
      "\u001b[32m[2020-06-26 14:35:05] __main__ INFO: \u001b[0mEpoch 14 loss 2.0962 acc@1 0.3230 acc@5 0.7000\n",
      "\u001b[32m[2020-06-26 14:35:05] __main__ INFO: \u001b[0mElapsed 13.14\n",
      "\u001b[32m[2020-06-26 14:35:05] __main__ INFO: \u001b[0mTrain 15 4914\n",
      "\u001b[32m[2020-06-26 14:36:57] __main__ INFO: \u001b[0mEpoch 15 Step 100/351 lr 0.004000 loss 1.3503 (1.1391) acc@1 0.4766 (0.5591) acc@5 0.7422 (0.7627)\n",
      "\u001b[32m[2020-06-26 14:38:49] __main__ INFO: \u001b[0mEpoch 15 Step 200/351 lr 0.004000 loss 1.0996 (1.1324) acc@1 0.5781 (0.5616) acc@5 0.7656 (0.7626)\n",
      "\u001b[32m[2020-06-26 14:40:41] __main__ INFO: \u001b[0mEpoch 15 Step 300/351 lr 0.004000 loss 1.1078 (1.1261) acc@1 0.5469 (0.5634) acc@5 0.7109 (0.7625)\n",
      "\u001b[32m[2020-06-26 14:41:38] __main__ INFO: \u001b[0mEpoch 15 Step 351/351 lr 0.004000 loss 1.0836 (1.1275) acc@1 0.5547 (0.5622) acc@5 0.7656 (0.7610)\n",
      "\u001b[32m[2020-06-26 14:41:38] __main__ INFO: \u001b[0mElapsed 393.28\n",
      "\u001b[32m[2020-06-26 14:41:38] __main__ INFO: \u001b[0mVal 15\n",
      "\u001b[32m[2020-06-26 14:41:51] __main__ INFO: \u001b[0mEpoch 15 loss 2.0867 acc@1 0.3230 acc@5 0.7110\n",
      "\u001b[32m[2020-06-26 14:41:51] __main__ INFO: \u001b[0mElapsed 13.13\n",
      "\u001b[32m[2020-06-26 14:41:51] __main__ INFO: \u001b[0mTrain 16 5265\n",
      "\u001b[32m[2020-06-26 14:43:43] __main__ INFO: \u001b[0mEpoch 16 Step 100/351 lr 0.004000 loss 1.1869 (1.1375) acc@1 0.5156 (0.5574) acc@5 0.8203 (0.7601)\n",
      "\u001b[32m[2020-06-26 14:45:35] __main__ INFO: \u001b[0mEpoch 16 Step 200/351 lr 0.004000 loss 1.2440 (1.1341) acc@1 0.5000 (0.5589) acc@5 0.7031 (0.7602)\n",
      "\u001b[32m[2020-06-26 14:47:27] __main__ INFO: \u001b[0mEpoch 16 Step 300/351 lr 0.004000 loss 1.0501 (1.1264) acc@1 0.5859 (0.5633) acc@5 0.7891 (0.7628)\n",
      "\u001b[32m[2020-06-26 14:48:24] __main__ INFO: \u001b[0mEpoch 16 Step 351/351 lr 0.004000 loss 1.1640 (1.1272) acc@1 0.5469 (0.5624) acc@5 0.7891 (0.7616)\n",
      "\u001b[32m[2020-06-26 14:48:24] __main__ INFO: \u001b[0mElapsed 393.31\n",
      "\u001b[32m[2020-06-26 14:48:24] __main__ INFO: \u001b[0mVal 16\n",
      "\u001b[32m[2020-06-26 14:48:38] __main__ INFO: \u001b[0mEpoch 16 loss 2.0969 acc@1 0.3310 acc@5 0.7120\n",
      "\u001b[32m[2020-06-26 14:48:38] __main__ INFO: \u001b[0mElapsed 13.17\n",
      "\u001b[32m[2020-06-26 14:48:38] __main__ INFO: \u001b[0mTrain 17 5616\n",
      "\u001b[32m[2020-06-26 14:50:30] __main__ INFO: \u001b[0mEpoch 17 Step 100/351 lr 0.004000 loss 1.1291 (1.1276) acc@1 0.5625 (0.5646) acc@5 0.7500 (0.7673)\n",
      "\u001b[32m[2020-06-26 14:52:22] __main__ INFO: \u001b[0mEpoch 17 Step 200/351 lr 0.004000 loss 1.1719 (1.1314) acc@1 0.5625 (0.5624) acc@5 0.7734 (0.7635)\n",
      "\u001b[32m[2020-06-26 14:54:14] __main__ INFO: \u001b[0mEpoch 17 Step 300/351 lr 0.004000 loss 1.1297 (1.1293) acc@1 0.5938 (0.5625) acc@5 0.8047 (0.7628)\n",
      "\u001b[32m[2020-06-26 14:55:11] __main__ INFO: \u001b[0mEpoch 17 Step 351/351 lr 0.004000 loss 1.0967 (1.1278) acc@1 0.5859 (0.5626) acc@5 0.7812 (0.7624)\n",
      "\u001b[32m[2020-06-26 14:55:11] __main__ INFO: \u001b[0mElapsed 393.38\n",
      "\u001b[32m[2020-06-26 14:55:11] __main__ INFO: \u001b[0mVal 17\n",
      "\u001b[32m[2020-06-26 14:55:24] __main__ INFO: \u001b[0mEpoch 17 loss 2.0719 acc@1 0.3228 acc@5 0.7010\n",
      "\u001b[32m[2020-06-26 14:55:24] __main__ INFO: \u001b[0mElapsed 13.15\n",
      "\u001b[32m[2020-06-26 14:55:24] __main__ INFO: \u001b[0mTrain 18 5967\n",
      "\u001b[32m[2020-06-26 14:57:16] __main__ INFO: \u001b[0mEpoch 18 Step 100/351 lr 0.004000 loss 1.1347 (1.1248) acc@1 0.5781 (0.5636) acc@5 0.7656 (0.7619)\n",
      "\u001b[32m[2020-06-26 14:59:08] __main__ INFO: \u001b[0mEpoch 18 Step 200/351 lr 0.004000 loss 1.1965 (1.1204) acc@1 0.5312 (0.5643) acc@5 0.7266 (0.7620)\n",
      "\u001b[32m[2020-06-26 15:01:00] __main__ INFO: \u001b[0mEpoch 18 Step 300/351 lr 0.004000 loss 1.1332 (1.1288) acc@1 0.5547 (0.5604) acc@5 0.7734 (0.7616)\n",
      "\u001b[32m[2020-06-26 15:01:57] __main__ INFO: \u001b[0mEpoch 18 Step 351/351 lr 0.004000 loss 1.0598 (1.1278) acc@1 0.5938 (0.5613) acc@5 0.8203 (0.7614)\n",
      "\u001b[32m[2020-06-26 15:01:57] __main__ INFO: \u001b[0mElapsed 393.16\n",
      "\u001b[32m[2020-06-26 15:01:57] __main__ INFO: \u001b[0mVal 18\n",
      "\u001b[32m[2020-06-26 15:02:10] __main__ INFO: \u001b[0mEpoch 18 loss 2.0808 acc@1 0.3292 acc@5 0.7052\n",
      "\u001b[32m[2020-06-26 15:02:10] __main__ INFO: \u001b[0mElapsed 13.15\n",
      "\u001b[32m[2020-06-26 15:02:10] __main__ INFO: \u001b[0mTrain 19 6318\n",
      "\u001b[32m[2020-06-26 15:04:03] __main__ INFO: \u001b[0mEpoch 19 Step 100/351 lr 0.004000 loss 1.1212 (1.1319) acc@1 0.5469 (0.5613) acc@5 0.7734 (0.7616)\n",
      "\u001b[32m[2020-06-26 15:05:55] __main__ INFO: \u001b[0mEpoch 19 Step 200/351 lr 0.004000 loss 1.1908 (1.1344) acc@1 0.5469 (0.5612) acc@5 0.7344 (0.7589)\n",
      "\u001b[32m[2020-06-26 15:07:47] __main__ INFO: \u001b[0mEpoch 19 Step 300/351 lr 0.004000 loss 1.0146 (1.1320) acc@1 0.6016 (0.5616) acc@5 0.7344 (0.7599)\n",
      "\u001b[32m[2020-06-26 15:08:44] __main__ INFO: \u001b[0mEpoch 19 Step 351/351 lr 0.004000 loss 1.1506 (1.1283) acc@1 0.5547 (0.5626) acc@5 0.7734 (0.7606)\n",
      "\u001b[32m[2020-06-26 15:08:44] __main__ INFO: \u001b[0mElapsed 393.33\n",
      "\u001b[32m[2020-06-26 15:08:44] __main__ INFO: \u001b[0mVal 19\n",
      "\u001b[32m[2020-06-26 15:08:57] __main__ INFO: \u001b[0mEpoch 19 loss 2.0697 acc@1 0.3274 acc@5 0.7130\n",
      "\u001b[32m[2020-06-26 15:08:57] __main__ INFO: \u001b[0mElapsed 13.13\n",
      "\u001b[32m[2020-06-26 15:08:57] __main__ INFO: \u001b[0mTrain 20 6669\n",
      "\u001b[32m[2020-06-26 15:10:49] __main__ INFO: \u001b[0mEpoch 20 Step 100/351 lr 0.004000 loss 1.2129 (1.1303) acc@1 0.5469 (0.5605) acc@5 0.7656 (0.7644)\n",
      "\u001b[32m[2020-06-26 15:12:41] __main__ INFO: \u001b[0mEpoch 20 Step 200/351 lr 0.004000 loss 0.9796 (1.1285) acc@1 0.6562 (0.5621) acc@5 0.8047 (0.7637)\n",
      "\u001b[32m[2020-06-26 15:14:33] __main__ INFO: \u001b[0mEpoch 20 Step 300/351 lr 0.004000 loss 1.1528 (1.1289) acc@1 0.5703 (0.5626) acc@5 0.7812 (0.7630)\n",
      "\u001b[32m[2020-06-26 15:15:30] __main__ INFO: \u001b[0mEpoch 20 Step 351/351 lr 0.004000 loss 1.1686 (1.1275) acc@1 0.5312 (0.5623) acc@5 0.7812 (0.7618)\n",
      "\u001b[32m[2020-06-26 15:15:30] __main__ INFO: \u001b[0mElapsed 393.38\n",
      "\u001b[32m[2020-06-26 15:15:30] __main__ INFO: \u001b[0mVal 20\n",
      "\u001b[32m[2020-06-26 15:15:43] __main__ INFO: \u001b[0mEpoch 20 loss 2.0710 acc@1 0.3292 acc@5 0.7128\n",
      "\u001b[32m[2020-06-26 15:15:43] __main__ INFO: \u001b[0mElapsed 13.18\n",
      "\u001b[32m[2020-06-26 15:15:43] __main__ INFO: \u001b[0mTrain 21 7020\n",
      "\u001b[32m[2020-06-26 15:17:36] __main__ INFO: \u001b[0mEpoch 21 Step 100/351 lr 0.004000 loss 1.0297 (1.1355) acc@1 0.5859 (0.5596) acc@5 0.7656 (0.7646)\n",
      "\u001b[32m[2020-06-26 15:19:28] __main__ INFO: \u001b[0mEpoch 21 Step 200/351 lr 0.004000 loss 1.0137 (1.1247) acc@1 0.5703 (0.5632) acc@5 0.7656 (0.7640)\n",
      "\u001b[32m[2020-06-26 15:21:20] __main__ INFO: \u001b[0mEpoch 21 Step 300/351 lr 0.004000 loss 1.1548 (1.1306) acc@1 0.5625 (0.5606) acc@5 0.7812 (0.7625)\n",
      "\u001b[32m[2020-06-26 15:22:17] __main__ INFO: \u001b[0mEpoch 21 Step 351/351 lr 0.004000 loss 1.0504 (1.1274) acc@1 0.5781 (0.5619) acc@5 0.6875 (0.7627)\n",
      "\u001b[32m[2020-06-26 15:22:17] __main__ INFO: \u001b[0mElapsed 393.37\n",
      "\u001b[32m[2020-06-26 15:22:17] __main__ INFO: \u001b[0mVal 21\n",
      "\u001b[32m[2020-06-26 15:22:30] __main__ INFO: \u001b[0mEpoch 21 loss 2.0946 acc@1 0.3312 acc@5 0.7112\n",
      "\u001b[32m[2020-06-26 15:22:30] __main__ INFO: \u001b[0mElapsed 13.14\n",
      "\u001b[32m[2020-06-26 15:22:30] __main__ INFO: \u001b[0mTrain 22 7371\n",
      "\u001b[32m[2020-06-26 15:24:22] __main__ INFO: \u001b[0mEpoch 22 Step 100/351 lr 0.004000 loss 1.1366 (1.1367) acc@1 0.5312 (0.5596) acc@5 0.7656 (0.7600)\n",
      "\u001b[32m[2020-06-26 15:26:14] __main__ INFO: \u001b[0mEpoch 22 Step 200/351 lr 0.004000 loss 1.1914 (1.1307) acc@1 0.5547 (0.5611) acc@5 0.6641 (0.7607)\n",
      "\u001b[32m[2020-06-26 15:28:06] __main__ INFO: \u001b[0mEpoch 22 Step 300/351 lr 0.004000 loss 1.3479 (1.1269) acc@1 0.4688 (0.5627) acc@5 0.7344 (0.7599)\n",
      "\u001b[32m[2020-06-26 15:29:03] __main__ INFO: \u001b[0mEpoch 22 Step 351/351 lr 0.004000 loss 1.2287 (1.1276) acc@1 0.4922 (0.5623) acc@5 0.7266 (0.7602)\n",
      "\u001b[32m[2020-06-26 15:29:03] __main__ INFO: \u001b[0mElapsed 393.16\n",
      "\u001b[32m[2020-06-26 15:29:03] __main__ INFO: \u001b[0mVal 22\n",
      "\u001b[32m[2020-06-26 15:29:16] __main__ INFO: \u001b[0mEpoch 22 loss 2.0855 acc@1 0.3320 acc@5 0.7016\n",
      "\u001b[32m[2020-06-26 15:29:16] __main__ INFO: \u001b[0mElapsed 13.15\n",
      "\u001b[32m[2020-06-26 15:29:16] __main__ INFO: \u001b[0mTrain 23 7722\n",
      "\u001b[32m[2020-06-26 15:31:08] __main__ INFO: \u001b[0mEpoch 23 Step 100/351 lr 0.004000 loss 1.0175 (1.1203) acc@1 0.5938 (0.5637) acc@5 0.7734 (0.7605)\n",
      "\u001b[32m[2020-06-26 15:33:00] __main__ INFO: \u001b[0mEpoch 23 Step 200/351 lr 0.004000 loss 1.1057 (1.1353) acc@1 0.5703 (0.5574) acc@5 0.7109 (0.7603)\n",
      "\u001b[32m[2020-06-26 15:34:52] __main__ INFO: \u001b[0mEpoch 23 Step 300/351 lr 0.004000 loss 1.0977 (1.1267) acc@1 0.5703 (0.5621) acc@5 0.7969 (0.7628)\n",
      "\u001b[32m[2020-06-26 15:35:49] __main__ INFO: \u001b[0mEpoch 23 Step 351/351 lr 0.004000 loss 1.2150 (1.1280) acc@1 0.5234 (0.5609) acc@5 0.7266 (0.7625)\n",
      "\u001b[32m[2020-06-26 15:35:49] __main__ INFO: \u001b[0mElapsed 392.93\n",
      "\u001b[32m[2020-06-26 15:35:49] __main__ INFO: \u001b[0mVal 23\n",
      "\u001b[32m[2020-06-26 15:36:02] __main__ INFO: \u001b[0mEpoch 23 loss 2.0995 acc@1 0.3328 acc@5 0.7116\n",
      "\u001b[32m[2020-06-26 15:36:02] __main__ INFO: \u001b[0mElapsed 13.13\n",
      "\u001b[32m[2020-06-26 15:36:02] __main__ INFO: \u001b[0mTrain 24 8073\n",
      "\u001b[32m[2020-06-26 15:37:54] __main__ INFO: \u001b[0mEpoch 24 Step 100/351 lr 0.004000 loss 1.0987 (1.1415) acc@1 0.5781 (0.5559) acc@5 0.7344 (0.7552)\n",
      "\u001b[32m[2020-06-26 15:39:46] __main__ INFO: \u001b[0mEpoch 24 Step 200/351 lr 0.004000 loss 1.0292 (1.1337) acc@1 0.5781 (0.5592) acc@5 0.7812 (0.7588)\n",
      "\u001b[32m[2020-06-26 15:41:38] __main__ INFO: \u001b[0mEpoch 24 Step 300/351 lr 0.004000 loss 1.2056 (1.1284) acc@1 0.5547 (0.5613) acc@5 0.7266 (0.7603)\n",
      "\u001b[32m[2020-06-26 15:42:35] __main__ INFO: \u001b[0mEpoch 24 Step 351/351 lr 0.004000 loss 0.8990 (1.1275) acc@1 0.6484 (0.5621) acc@5 0.7812 (0.7609)\n",
      "\u001b[32m[2020-06-26 15:42:35] __main__ INFO: \u001b[0mElapsed 392.85\n",
      "\u001b[32m[2020-06-26 15:42:35] __main__ INFO: \u001b[0mVal 24\n",
      "\u001b[32m[2020-06-26 15:42:48] __main__ INFO: \u001b[0mEpoch 24 loss 2.0637 acc@1 0.3322 acc@5 0.7078\n",
      "\u001b[32m[2020-06-26 15:42:48] __main__ INFO: \u001b[0mElapsed 13.11\n",
      "\u001b[32m[2020-06-26 15:42:48] __main__ INFO: \u001b[0mTrain 25 8424\n",
      "\u001b[32m[2020-06-26 15:44:40] __main__ INFO: \u001b[0mEpoch 25 Step 100/351 lr 0.004000 loss 1.1034 (1.1209) acc@1 0.5312 (0.5643) acc@5 0.6953 (0.7607)\n",
      "\u001b[32m[2020-06-26 15:46:32] __main__ INFO: \u001b[0mEpoch 25 Step 200/351 lr 0.004000 loss 0.9959 (1.1266) acc@1 0.6094 (0.5624) acc@5 0.7812 (0.7612)\n",
      "\u001b[32m[2020-06-26 15:48:24] __main__ INFO: \u001b[0mEpoch 25 Step 300/351 lr 0.004000 loss 1.1304 (1.1263) acc@1 0.5469 (0.5615) acc@5 0.8125 (0.7621)\n",
      "\u001b[32m[2020-06-26 15:49:21] __main__ INFO: \u001b[0mEpoch 25 Step 351/351 lr 0.004000 loss 1.2041 (1.1282) acc@1 0.5625 (0.5607) acc@5 0.7656 (0.7609)\n",
      "\u001b[32m[2020-06-26 15:49:21] __main__ INFO: \u001b[0mElapsed 392.83\n",
      "\u001b[32m[2020-06-26 15:49:21] __main__ INFO: \u001b[0mVal 25\n",
      "\u001b[32m[2020-06-26 15:49:34] __main__ INFO: \u001b[0mEpoch 25 loss 2.1014 acc@1 0.3312 acc@5 0.7122\n",
      "\u001b[32m[2020-06-26 15:49:34] __main__ INFO: \u001b[0mElapsed 13.17\n",
      "\u001b[32m[2020-06-26 15:49:34] __main__ INFO: \u001b[0mTrain 26 8775\n",
      "\u001b[32m[2020-06-26 15:51:26] __main__ INFO: \u001b[0mEpoch 26 Step 100/351 lr 0.004000 loss 1.1896 (1.1363) acc@1 0.5703 (0.5627) acc@5 0.7734 (0.7668)\n",
      "\u001b[32m[2020-06-26 15:53:18] __main__ INFO: \u001b[0mEpoch 26 Step 200/351 lr 0.004000 loss 0.9555 (1.1338) acc@1 0.6484 (0.5603) acc@5 0.7969 (0.7646)\n",
      "\u001b[32m[2020-06-26 15:55:10] __main__ INFO: \u001b[0mEpoch 26 Step 300/351 lr 0.004000 loss 1.2059 (1.1262) acc@1 0.5469 (0.5623) acc@5 0.7422 (0.7636)\n",
      "\u001b[32m[2020-06-26 15:56:07] __main__ INFO: \u001b[0mEpoch 26 Step 351/351 lr 0.004000 loss 1.1061 (1.1271) acc@1 0.5625 (0.5615) acc@5 0.7734 (0.7631)\n",
      "\u001b[32m[2020-06-26 15:56:07] __main__ INFO: \u001b[0mElapsed 392.66\n",
      "\u001b[32m[2020-06-26 15:56:07] __main__ INFO: \u001b[0mVal 26\n",
      "\u001b[32m[2020-06-26 15:56:20] __main__ INFO: \u001b[0mEpoch 26 loss 2.0740 acc@1 0.3302 acc@5 0.7080\n",
      "\u001b[32m[2020-06-26 15:56:20] __main__ INFO: \u001b[0mElapsed 13.11\n",
      "\u001b[32m[2020-06-26 15:56:20] __main__ INFO: \u001b[0mTrain 27 9126\n",
      "\u001b[32m[2020-06-26 15:58:12] __main__ INFO: \u001b[0mEpoch 27 Step 100/351 lr 0.004000 loss 1.0655 (1.1246) acc@1 0.5859 (0.5617) acc@5 0.7656 (0.7591)\n",
      "\u001b[32m[2020-06-26 16:00:04] __main__ INFO: \u001b[0mEpoch 27 Step 200/351 lr 0.004000 loss 1.0298 (1.1267) acc@1 0.5625 (0.5599) acc@5 0.8359 (0.7593)\n",
      "\u001b[32m[2020-06-26 16:01:56] __main__ INFO: \u001b[0mEpoch 27 Step 300/351 lr 0.004000 loss 1.0100 (1.1232) acc@1 0.5938 (0.5629) acc@5 0.7500 (0.7612)\n",
      "\u001b[32m[2020-06-26 16:02:53] __main__ INFO: \u001b[0mEpoch 27 Step 351/351 lr 0.004000 loss 1.0234 (1.1269) acc@1 0.6250 (0.5622) acc@5 0.8359 (0.7601)\n",
      "\u001b[32m[2020-06-26 16:02:53] __main__ INFO: \u001b[0mElapsed 392.89\n",
      "\u001b[32m[2020-06-26 16:02:53] __main__ INFO: \u001b[0mVal 27\n",
      "\u001b[32m[2020-06-26 16:03:06] __main__ INFO: \u001b[0mEpoch 27 loss 2.0754 acc@1 0.3322 acc@5 0.7138\n",
      "\u001b[32m[2020-06-26 16:03:06] __main__ INFO: \u001b[0mElapsed 13.13\n",
      "\u001b[32m[2020-06-26 16:03:06] __main__ INFO: \u001b[0mTrain 28 9477\n",
      "\u001b[32m[2020-06-26 16:04:58] __main__ INFO: \u001b[0mEpoch 28 Step 100/351 lr 0.004000 loss 1.1699 (1.1234) acc@1 0.5547 (0.5630) acc@5 0.7734 (0.7588)\n",
      "\u001b[32m[2020-06-26 16:06:50] __main__ INFO: \u001b[0mEpoch 28 Step 200/351 lr 0.004000 loss 1.1512 (1.1265) acc@1 0.5469 (0.5614) acc@5 0.7422 (0.7588)\n",
      "\u001b[32m[2020-06-26 16:08:42] __main__ INFO: \u001b[0mEpoch 28 Step 300/351 lr 0.004000 loss 1.1666 (1.1274) acc@1 0.5859 (0.5609) acc@5 0.7734 (0.7596)\n",
      "\u001b[32m[2020-06-26 16:09:39] __main__ INFO: \u001b[0mEpoch 28 Step 351/351 lr 0.004000 loss 1.0123 (1.1265) acc@1 0.6094 (0.5621) acc@5 0.7891 (0.7596)\n",
      "\u001b[32m[2020-06-26 16:09:39] __main__ INFO: \u001b[0mElapsed 392.76\n",
      "\u001b[32m[2020-06-26 16:09:39] __main__ INFO: \u001b[0mVal 28\n",
      "\u001b[32m[2020-06-26 16:09:52] __main__ INFO: \u001b[0mEpoch 28 loss 2.0674 acc@1 0.3280 acc@5 0.7126\n",
      "\u001b[32m[2020-06-26 16:09:52] __main__ INFO: \u001b[0mElapsed 13.14\n",
      "\u001b[32m[2020-06-26 16:09:52] __main__ INFO: \u001b[0mTrain 29 9828\n",
      "\u001b[32m[2020-06-26 16:11:44] __main__ INFO: \u001b[0mEpoch 29 Step 100/351 lr 0.004000 loss 1.0789 (1.1334) acc@1 0.5547 (0.5564) acc@5 0.7812 (0.7585)\n",
      "\u001b[32m[2020-06-26 16:13:36] __main__ INFO: \u001b[0mEpoch 29 Step 200/351 lr 0.004000 loss 1.1732 (1.1309) acc@1 0.5391 (0.5603) acc@5 0.7109 (0.7602)\n",
      "\u001b[32m[2020-06-26 16:15:28] __main__ INFO: \u001b[0mEpoch 29 Step 300/351 lr 0.004000 loss 1.1231 (1.1274) acc@1 0.5781 (0.5614) acc@5 0.7656 (0.7625)\n",
      "\u001b[32m[2020-06-26 16:16:25] __main__ INFO: \u001b[0mEpoch 29 Step 351/351 lr 0.004000 loss 1.2438 (1.1286) acc@1 0.5078 (0.5612) acc@5 0.7578 (0.7617)\n",
      "\u001b[32m[2020-06-26 16:16:25] __main__ INFO: \u001b[0mElapsed 392.94\n",
      "\u001b[32m[2020-06-26 16:16:25] __main__ INFO: \u001b[0mVal 29\n",
      "\u001b[32m[2020-06-26 16:16:38] __main__ INFO: \u001b[0mEpoch 29 loss 2.0898 acc@1 0.3328 acc@5 0.7056\n",
      "\u001b[32m[2020-06-26 16:16:38] __main__ INFO: \u001b[0mElapsed 13.15\n",
      "\u001b[32m[2020-06-26 16:16:38] __main__ INFO: \u001b[0mTrain 30 10179\n",
      "\u001b[32m[2020-06-26 16:18:30] __main__ INFO: \u001b[0mEpoch 30 Step 100/351 lr 0.004000 loss 1.2331 (1.1270) acc@1 0.5156 (0.5620) acc@5 0.7891 (0.7647)\n",
      "\u001b[32m[2020-06-26 16:20:22] __main__ INFO: \u001b[0mEpoch 30 Step 200/351 lr 0.004000 loss 1.1287 (1.1318) acc@1 0.5547 (0.5601) acc@5 0.7656 (0.7620)\n",
      "\u001b[32m[2020-06-26 16:22:14] __main__ INFO: \u001b[0mEpoch 30 Step 300/351 lr 0.004000 loss 1.0630 (1.1312) acc@1 0.6016 (0.5602) acc@5 0.7578 (0.7616)\n",
      "\u001b[32m[2020-06-26 16:23:11] __main__ INFO: \u001b[0mEpoch 30 Step 351/351 lr 0.004000 loss 1.3185 (1.1275) acc@1 0.4922 (0.5618) acc@5 0.7109 (0.7627)\n",
      "\u001b[32m[2020-06-26 16:23:11] __main__ INFO: \u001b[0mElapsed 392.83\n",
      "\u001b[32m[2020-06-26 16:23:11] __main__ INFO: \u001b[0mVal 30\n",
      "\u001b[32m[2020-06-26 16:23:24] __main__ INFO: \u001b[0mEpoch 30 loss 2.0817 acc@1 0.3238 acc@5 0.7044\n",
      "\u001b[32m[2020-06-26 16:23:24] __main__ INFO: \u001b[0mElapsed 13.14\n",
      "\u001b[32m[2020-06-26 16:23:24] __main__ INFO: \u001b[0mTrain 31 10530\n",
      "\u001b[32m[2020-06-26 16:25:16] __main__ INFO: \u001b[0mEpoch 31 Step 100/351 lr 0.004000 loss 1.1670 (1.1194) acc@1 0.5078 (0.5686) acc@5 0.7656 (0.7683)\n",
      "\u001b[32m[2020-06-26 16:27:08] __main__ INFO: \u001b[0mEpoch 31 Step 200/351 lr 0.004000 loss 1.1570 (1.1216) acc@1 0.5391 (0.5672) acc@5 0.7188 (0.7663)\n",
      "\u001b[32m[2020-06-26 16:29:00] __main__ INFO: \u001b[0mEpoch 31 Step 300/351 lr 0.004000 loss 0.9911 (1.1283) acc@1 0.6484 (0.5629) acc@5 0.8438 (0.7626)\n",
      "\u001b[32m[2020-06-26 16:29:57] __main__ INFO: \u001b[0mEpoch 31 Step 351/351 lr 0.004000 loss 1.1495 (1.1279) acc@1 0.5547 (0.5626) acc@5 0.7188 (0.7615)\n",
      "\u001b[32m[2020-06-26 16:29:57] __main__ INFO: \u001b[0mElapsed 392.90\n",
      "\u001b[32m[2020-06-26 16:29:57] __main__ INFO: \u001b[0mVal 31\n",
      "\u001b[32m[2020-06-26 16:30:10] __main__ INFO: \u001b[0mEpoch 31 loss 2.0785 acc@1 0.3254 acc@5 0.7070\n",
      "\u001b[32m[2020-06-26 16:30:10] __main__ INFO: \u001b[0mElapsed 13.14\n",
      "\u001b[32m[2020-06-26 16:30:10] __main__ INFO: \u001b[0mTrain 32 10881\n",
      "\u001b[32m[2020-06-26 16:32:02] __main__ INFO: \u001b[0mEpoch 32 Step 100/351 lr 0.004000 loss 1.1085 (1.1339) acc@1 0.5547 (0.5598) acc@5 0.7812 (0.7626)\n",
      "\u001b[32m[2020-06-26 16:33:54] __main__ INFO: \u001b[0mEpoch 32 Step 200/351 lr 0.004000 loss 1.1873 (1.1284) acc@1 0.5234 (0.5602) acc@5 0.7969 (0.7598)\n",
      "\u001b[32m[2020-06-26 16:35:46] __main__ INFO: \u001b[0mEpoch 32 Step 300/351 lr 0.004000 loss 1.1373 (1.1257) acc@1 0.5625 (0.5623) acc@5 0.7656 (0.7616)\n",
      "\u001b[32m[2020-06-26 16:36:43] __main__ INFO: \u001b[0mEpoch 32 Step 351/351 lr 0.004000 loss 1.0808 (1.1271) acc@1 0.5703 (0.5620) acc@5 0.7812 (0.7608)\n",
      "\u001b[32m[2020-06-26 16:36:43] __main__ INFO: \u001b[0mElapsed 392.94\n",
      "\u001b[32m[2020-06-26 16:36:43] __main__ INFO: \u001b[0mVal 32\n",
      "\u001b[32m[2020-06-26 16:36:56] __main__ INFO: \u001b[0mEpoch 32 loss 2.0683 acc@1 0.3294 acc@5 0.7096\n",
      "\u001b[32m[2020-06-26 16:36:56] __main__ INFO: \u001b[0mElapsed 13.17\n",
      "\u001b[32m[2020-06-26 16:36:56] __main__ INFO: \u001b[0mTrain 33 11232\n",
      "\u001b[32m[2020-06-26 16:38:48] __main__ INFO: \u001b[0mEpoch 33 Step 100/351 lr 0.004000 loss 1.3664 (1.1190) acc@1 0.4453 (0.5667) acc@5 0.7344 (0.7623)\n",
      "\u001b[32m[2020-06-26 16:40:40] __main__ INFO: \u001b[0mEpoch 33 Step 200/351 lr 0.004000 loss 1.0440 (1.1295) acc@1 0.5859 (0.5602) acc@5 0.7734 (0.7623)\n",
      "\u001b[32m[2020-06-26 16:42:32] __main__ INFO: \u001b[0mEpoch 33 Step 300/351 lr 0.004000 loss 0.9953 (1.1256) acc@1 0.6250 (0.5603) acc@5 0.7500 (0.7621)\n",
      "\u001b[32m[2020-06-26 16:43:29] __main__ INFO: \u001b[0mEpoch 33 Step 351/351 lr 0.004000 loss 0.9923 (1.1280) acc@1 0.6406 (0.5597) acc@5 0.8047 (0.7623)\n",
      "\u001b[32m[2020-06-26 16:43:29] __main__ INFO: \u001b[0mElapsed 393.10\n",
      "\u001b[32m[2020-06-26 16:43:29] __main__ INFO: \u001b[0mVal 33\n",
      "\u001b[32m[2020-06-26 16:43:42] __main__ INFO: \u001b[0mEpoch 33 loss 2.0752 acc@1 0.3296 acc@5 0.7088\n",
      "\u001b[32m[2020-06-26 16:43:42] __main__ INFO: \u001b[0mElapsed 13.14\n",
      "\u001b[32m[2020-06-26 16:43:42] __main__ INFO: \u001b[0mTrain 34 11583\n",
      "\u001b[32m[2020-06-26 16:45:35] __main__ INFO: \u001b[0mEpoch 34 Step 100/351 lr 0.004000 loss 1.2211 (1.1258) acc@1 0.5156 (0.5621) acc@5 0.7422 (0.7635)\n",
      "\u001b[32m[2020-06-26 16:47:27] __main__ INFO: \u001b[0mEpoch 34 Step 200/351 lr 0.004000 loss 1.1771 (1.1218) acc@1 0.5156 (0.5633) acc@5 0.7500 (0.7625)\n",
      "\u001b[32m[2020-06-26 16:49:18] __main__ INFO: \u001b[0mEpoch 34 Step 300/351 lr 0.004000 loss 1.2018 (1.1253) acc@1 0.5469 (0.5623) acc@5 0.7266 (0.7620)\n",
      "\u001b[32m[2020-06-26 16:50:16] __main__ INFO: \u001b[0mEpoch 34 Step 351/351 lr 0.004000 loss 1.2628 (1.1267) acc@1 0.5078 (0.5618) acc@5 0.6797 (0.7618)\n",
      "\u001b[32m[2020-06-26 16:50:16] __main__ INFO: \u001b[0mElapsed 393.11\n",
      "\u001b[32m[2020-06-26 16:50:16] __main__ INFO: \u001b[0mVal 34\n",
      "\u001b[32m[2020-06-26 16:50:29] __main__ INFO: \u001b[0mEpoch 34 loss 2.0844 acc@1 0.3348 acc@5 0.7118\n",
      "\u001b[32m[2020-06-26 16:50:29] __main__ INFO: \u001b[0mElapsed 13.17\n",
      "\u001b[32m[2020-06-26 16:50:29] __main__ INFO: \u001b[0mTrain 35 11934\n",
      "\u001b[32m[2020-06-26 16:52:21] __main__ INFO: \u001b[0mEpoch 35 Step 100/351 lr 0.004000 loss 1.1113 (1.1298) acc@1 0.5547 (0.5602) acc@5 0.7344 (0.7595)\n",
      "\u001b[32m[2020-06-26 16:54:13] __main__ INFO: \u001b[0mEpoch 35 Step 200/351 lr 0.004000 loss 1.2612 (1.1268) acc@1 0.5000 (0.5616) acc@5 0.7266 (0.7606)\n",
      "\u001b[32m[2020-06-26 16:56:05] __main__ INFO: \u001b[0mEpoch 35 Step 300/351 lr 0.004000 loss 1.1500 (1.1273) acc@1 0.5391 (0.5619) acc@5 0.7734 (0.7611)\n",
      "\u001b[32m[2020-06-26 16:57:02] __main__ INFO: \u001b[0mEpoch 35 Step 351/351 lr 0.004000 loss 0.9698 (1.1271) acc@1 0.6016 (0.5616) acc@5 0.7500 (0.7611)\n",
      "\u001b[32m[2020-06-26 16:57:02] __main__ INFO: \u001b[0mElapsed 393.02\n",
      "\u001b[32m[2020-06-26 16:57:02] __main__ INFO: \u001b[0mVal 35\n",
      "\u001b[32m[2020-06-26 16:57:15] __main__ INFO: \u001b[0mEpoch 35 loss 2.0727 acc@1 0.3262 acc@5 0.7104\n",
      "\u001b[32m[2020-06-26 16:57:15] __main__ INFO: \u001b[0mElapsed 13.14\n",
      "\u001b[32m[2020-06-26 16:57:15] __main__ INFO: \u001b[0mTrain 36 12285\n",
      "\u001b[32m[2020-06-26 16:59:07] __main__ INFO: \u001b[0mEpoch 36 Step 100/351 lr 0.004000 loss 1.1402 (1.1266) acc@1 0.5469 (0.5618) acc@5 0.7578 (0.7614)\n",
      "\u001b[32m[2020-06-26 17:00:59] __main__ INFO: \u001b[0mEpoch 36 Step 200/351 lr 0.004000 loss 1.0319 (1.1297) acc@1 0.6016 (0.5617) acc@5 0.7969 (0.7625)\n",
      "\u001b[32m[2020-06-26 17:02:51] __main__ INFO: \u001b[0mEpoch 36 Step 300/351 lr 0.004000 loss 1.3335 (1.1291) acc@1 0.4531 (0.5607) acc@5 0.7344 (0.7603)\n",
      "\u001b[32m[2020-06-26 17:03:48] __main__ INFO: \u001b[0mEpoch 36 Step 351/351 lr 0.004000 loss 1.2559 (1.1271) acc@1 0.5469 (0.5614) acc@5 0.7422 (0.7612)\n",
      "\u001b[32m[2020-06-26 17:03:48] __main__ INFO: \u001b[0mElapsed 393.02\n",
      "\u001b[32m[2020-06-26 17:03:48] __main__ INFO: \u001b[0mVal 36\n",
      "\u001b[32m[2020-06-26 17:04:01] __main__ INFO: \u001b[0mEpoch 36 loss 2.0773 acc@1 0.3296 acc@5 0.7074\n",
      "\u001b[32m[2020-06-26 17:04:01] __main__ INFO: \u001b[0mElapsed 13.17\n",
      "\u001b[32m[2020-06-26 17:04:01] __main__ INFO: \u001b[0mTrain 37 12636\n",
      "\u001b[32m[2020-06-26 17:05:53] __main__ INFO: \u001b[0mEpoch 37 Step 100/351 lr 0.004000 loss 1.1004 (1.1168) acc@1 0.5859 (0.5622) acc@5 0.7812 (0.7618)\n",
      "\u001b[32m[2020-06-26 17:07:45] __main__ INFO: \u001b[0mEpoch 37 Step 200/351 lr 0.004000 loss 1.0626 (1.1225) acc@1 0.5859 (0.5627) acc@5 0.7578 (0.7606)\n",
      "\u001b[32m[2020-06-26 17:09:37] __main__ INFO: \u001b[0mEpoch 37 Step 300/351 lr 0.004000 loss 1.3526 (1.1263) acc@1 0.4609 (0.5614) acc@5 0.6797 (0.7606)\n",
      "\u001b[32m[2020-06-26 17:10:34] __main__ INFO: \u001b[0mEpoch 37 Step 351/351 lr 0.004000 loss 0.9578 (1.1277) acc@1 0.6250 (0.5606) acc@5 0.8047 (0.7606)\n",
      "\u001b[32m[2020-06-26 17:10:34] __main__ INFO: \u001b[0mElapsed 392.88\n",
      "\u001b[32m[2020-06-26 17:10:34] __main__ INFO: \u001b[0mVal 37\n",
      "\u001b[32m[2020-06-26 17:10:47] __main__ INFO: \u001b[0mEpoch 37 loss 2.0863 acc@1 0.3276 acc@5 0.6992\n",
      "\u001b[32m[2020-06-26 17:10:47] __main__ INFO: \u001b[0mElapsed 13.13\n",
      "\u001b[32m[2020-06-26 17:10:47] __main__ INFO: \u001b[0mTrain 38 12987\n",
      "\u001b[32m[2020-06-26 17:12:39] __main__ INFO: \u001b[0mEpoch 38 Step 100/351 lr 0.004000 loss 1.3497 (1.1163) acc@1 0.4531 (0.5651) acc@5 0.7344 (0.7648)\n",
      "\u001b[32m[2020-06-26 17:14:31] __main__ INFO: \u001b[0mEpoch 38 Step 200/351 lr 0.004000 loss 1.0664 (1.1275) acc@1 0.5938 (0.5620) acc@5 0.7734 (0.7630)\n",
      "\u001b[32m[2020-06-26 17:16:23] __main__ INFO: \u001b[0mEpoch 38 Step 300/351 lr 0.004000 loss 1.0599 (1.1275) acc@1 0.6250 (0.5609) acc@5 0.8047 (0.7629)\n",
      "\u001b[32m[2020-06-26 17:17:20] __main__ INFO: \u001b[0mEpoch 38 Step 351/351 lr 0.004000 loss 1.3677 (1.1275) acc@1 0.4766 (0.5613) acc@5 0.7031 (0.7627)\n",
      "\u001b[32m[2020-06-26 17:17:20] __main__ INFO: \u001b[0mElapsed 392.89\n",
      "\u001b[32m[2020-06-26 17:17:20] __main__ INFO: \u001b[0mVal 38\n",
      "\u001b[32m[2020-06-26 17:17:33] __main__ INFO: \u001b[0mEpoch 38 loss 2.1036 acc@1 0.3304 acc@5 0.7100\n",
      "\u001b[32m[2020-06-26 17:17:33] __main__ INFO: \u001b[0mElapsed 13.13\n",
      "\u001b[32m[2020-06-26 17:17:33] __main__ INFO: \u001b[0mTrain 39 13338\n",
      "\u001b[32m[2020-06-26 17:19:25] __main__ INFO: \u001b[0mEpoch 39 Step 100/351 lr 0.004000 loss 1.2599 (1.1222) acc@1 0.5547 (0.5645) acc@5 0.7500 (0.7605)\n",
      "\u001b[32m[2020-06-26 17:21:17] __main__ INFO: \u001b[0mEpoch 39 Step 200/351 lr 0.004000 loss 1.0936 (1.1227) acc@1 0.5625 (0.5638) acc@5 0.7109 (0.7605)\n",
      "\u001b[32m[2020-06-26 17:23:09] __main__ INFO: \u001b[0mEpoch 39 Step 300/351 lr 0.004000 loss 0.9952 (1.1266) acc@1 0.5859 (0.5612) acc@5 0.8359 (0.7597)\n",
      "\u001b[32m[2020-06-26 17:24:06] __main__ INFO: \u001b[0mEpoch 39 Step 351/351 lr 0.004000 loss 1.1175 (1.1282) acc@1 0.5781 (0.5610) acc@5 0.7656 (0.7597)\n",
      "\u001b[32m[2020-06-26 17:24:06] __main__ INFO: \u001b[0mElapsed 392.74\n",
      "\u001b[32m[2020-06-26 17:24:06] __main__ INFO: \u001b[0mVal 39\n",
      "\u001b[32m[2020-06-26 17:24:19] __main__ INFO: \u001b[0mEpoch 39 loss 2.1043 acc@1 0.3300 acc@5 0.7092\n",
      "\u001b[32m[2020-06-26 17:24:19] __main__ INFO: \u001b[0mElapsed 13.13\n",
      "\u001b[32m[2020-06-26 17:24:19] __main__ INFO: \u001b[0mTrain 40 13689\n",
      "\u001b[32m[2020-06-26 17:26:11] __main__ INFO: \u001b[0mEpoch 40 Step 100/351 lr 0.004000 loss 1.1391 (1.1435) acc@1 0.5312 (0.5566) acc@5 0.7188 (0.7622)\n",
      "\u001b[32m[2020-06-26 17:28:03] __main__ INFO: \u001b[0mEpoch 40 Step 200/351 lr 0.004000 loss 1.1527 (1.1269) acc@1 0.5547 (0.5613) acc@5 0.7812 (0.7643)\n",
      "\u001b[32m[2020-06-26 17:29:55] __main__ INFO: \u001b[0mEpoch 40 Step 300/351 lr 0.004000 loss 1.3712 (1.1298) acc@1 0.4375 (0.5605) acc@5 0.7266 (0.7619)\n",
      "\u001b[32m[2020-06-26 17:30:52] __main__ INFO: \u001b[0mEpoch 40 Step 351/351 lr 0.004000 loss 1.0676 (1.1283) acc@1 0.5938 (0.5614) acc@5 0.8047 (0.7621)\n",
      "\u001b[32m[2020-06-26 17:30:52] __main__ INFO: \u001b[0mElapsed 392.96\n",
      "\u001b[32m[2020-06-26 17:30:52] __main__ INFO: \u001b[0mVal 40\n",
      "\u001b[32m[2020-06-26 17:31:05] __main__ INFO: \u001b[0mEpoch 40 loss 2.0839 acc@1 0.3296 acc@5 0.7104\n",
      "\u001b[32m[2020-06-26 17:31:05] __main__ INFO: \u001b[0mElapsed 13.14\n",
      "\u001b[32m[2020-06-26 17:31:05] __main__ INFO: \u001b[0mTrain 41 14040\n",
      "\u001b[32m[2020-06-26 17:32:57] __main__ INFO: \u001b[0mEpoch 41 Step 100/351 lr 0.004000 loss 1.0414 (1.1122) acc@1 0.5938 (0.5723) acc@5 0.8047 (0.7651)\n",
      "\u001b[32m[2020-06-26 17:34:49] __main__ INFO: \u001b[0mEpoch 41 Step 200/351 lr 0.004000 loss 1.1334 (1.1261) acc@1 0.5625 (0.5643) acc@5 0.7422 (0.7611)\n",
      "\u001b[32m[2020-06-26 17:36:41] __main__ INFO: \u001b[0mEpoch 41 Step 300/351 lr 0.004000 loss 1.1548 (1.1282) acc@1 0.5625 (0.5624) acc@5 0.7734 (0.7602)\n",
      "\u001b[32m[2020-06-26 17:37:38] __main__ INFO: \u001b[0mEpoch 41 Step 351/351 lr 0.004000 loss 1.0772 (1.1270) acc@1 0.5938 (0.5625) acc@5 0.7734 (0.7606)\n",
      "\u001b[32m[2020-06-26 17:37:38] __main__ INFO: \u001b[0mElapsed 392.84\n",
      "\u001b[32m[2020-06-26 17:37:38] __main__ INFO: \u001b[0mVal 41\n",
      "\u001b[32m[2020-06-26 17:37:51] __main__ INFO: \u001b[0mEpoch 41 loss 2.0707 acc@1 0.3306 acc@5 0.6954\n",
      "\u001b[32m[2020-06-26 17:37:51] __main__ INFO: \u001b[0mElapsed 13.12\n",
      "\u001b[32m[2020-06-26 17:37:51] __main__ INFO: \u001b[0mTrain 42 14391\n",
      "\u001b[32m[2020-06-26 17:50:13] __main__ INFO: \u001b[0mEpoch 43 Step 300/351 lr 0.004000 loss 1.0539 (1.1260) acc@1 0.6172 (0.5624) acc@5 0.7734 (0.7620)\n",
      "\u001b[32m[2020-06-26 17:51:10] __main__ INFO: \u001b[0mEpoch 43 Step 351/351 lr 0.004000 loss 1.2270 (1.1284) acc@1 0.5000 (0.5614) acc@5 0.7422 (0.7612)\n",
      "\u001b[32m[2020-06-26 17:51:10] __main__ INFO: \u001b[0mElapsed 392.86\n",
      "\u001b[32m[2020-06-26 17:51:10] __main__ INFO: \u001b[0mVal 43\n",
      "\u001b[32m[2020-06-26 17:51:23] __main__ INFO: \u001b[0mEpoch 43 loss 2.0890 acc@1 0.3298 acc@5 0.7080\n",
      "\u001b[32m[2020-06-26 17:51:23] __main__ INFO: \u001b[0mElapsed 13.14\n",
      "\u001b[32m[2020-06-26 17:51:23] __main__ INFO: \u001b[0mTrain 44 15093\n",
      "\u001b[32m[2020-06-26 17:53:15] __main__ INFO: \u001b[0mEpoch 44 Step 100/351 lr 0.004000 loss 1.1529 (1.1241) acc@1 0.5391 (0.5641) acc@5 0.7344 (0.7646)\n",
      "\u001b[32m[2020-06-26 17:55:07] __main__ INFO: \u001b[0mEpoch 44 Step 200/351 lr 0.004000 loss 1.0817 (1.1280) acc@1 0.5547 (0.5614) acc@5 0.7656 (0.7627)\n",
      "\u001b[32m[2020-06-26 17:56:59] __main__ INFO: \u001b[0mEpoch 44 Step 300/351 lr 0.004000 loss 1.2259 (1.1297) acc@1 0.5156 (0.5608) acc@5 0.7344 (0.7619)\n",
      "\u001b[32m[2020-06-26 17:57:56] __main__ INFO: \u001b[0mEpoch 44 Step 351/351 lr 0.004000 loss 0.9901 (1.1280) acc@1 0.6406 (0.5622) acc@5 0.7812 (0.7624)\n",
      "\u001b[32m[2020-06-26 17:57:56] __main__ INFO: \u001b[0mElapsed 392.89\n",
      "\u001b[32m[2020-06-26 17:57:56] __main__ INFO: \u001b[0mVal 44\n",
      "\u001b[32m[2020-06-26 17:58:09] __main__ INFO: \u001b[0mEpoch 44 loss 2.0763 acc@1 0.3184 acc@5 0.6996\n",
      "\u001b[32m[2020-06-26 17:58:09] __main__ INFO: \u001b[0mElapsed 13.13\n",
      "\u001b[32m[2020-06-26 17:58:09] __main__ INFO: \u001b[0mTrain 45 15444\n",
      "\u001b[32m[2020-06-26 18:00:01] __main__ INFO: \u001b[0mEpoch 45 Step 100/351 lr 0.004000 loss 1.0811 (1.1216) acc@1 0.5703 (0.5645) acc@5 0.7812 (0.7680)\n",
      "\u001b[32m[2020-06-26 18:01:53] __main__ INFO: \u001b[0mEpoch 45 Step 200/351 lr 0.004000 loss 1.4595 (1.1267) acc@1 0.4297 (0.5617) acc@5 0.6875 (0.7626)\n",
      "\u001b[32m[2020-06-26 18:03:45] __main__ INFO: \u001b[0mEpoch 45 Step 300/351 lr 0.004000 loss 1.2191 (1.1250) acc@1 0.5547 (0.5633) acc@5 0.7734 (0.7613)\n",
      "\u001b[32m[2020-06-26 18:04:42] __main__ INFO: \u001b[0mEpoch 45 Step 351/351 lr 0.004000 loss 1.1191 (1.1269) acc@1 0.5625 (0.5625) acc@5 0.7266 (0.7613)\n",
      "\u001b[32m[2020-06-26 18:04:42] __main__ INFO: \u001b[0mElapsed 392.82\n",
      "\u001b[32m[2020-06-26 18:04:42] __main__ INFO: \u001b[0mVal 45\n",
      "\u001b[32m[2020-06-26 18:04:55] __main__ INFO: \u001b[0mEpoch 45 loss 2.0871 acc@1 0.3258 acc@5 0.6998\n",
      "\u001b[32m[2020-06-26 18:04:55] __main__ INFO: \u001b[0mElapsed 13.14\n",
      "\u001b[32m[2020-06-26 18:04:55] __main__ INFO: \u001b[0mTrain 46 15795\n",
      "\u001b[32m[2020-06-26 18:06:47] __main__ INFO: \u001b[0mEpoch 46 Step 100/351 lr 0.004000 loss 1.1504 (1.1284) acc@1 0.5625 (0.5610) acc@5 0.7734 (0.7616)\n",
      "\u001b[32m[2020-06-26 18:08:39] __main__ INFO: \u001b[0mEpoch 46 Step 200/351 lr 0.004000 loss 0.9582 (1.1353) acc@1 0.6328 (0.5595) acc@5 0.8047 (0.7618)\n",
      "\u001b[32m[2020-06-26 18:10:31] __main__ INFO: \u001b[0mEpoch 46 Step 300/351 lr 0.004000 loss 1.0260 (1.1287) acc@1 0.5859 (0.5622) acc@5 0.7891 (0.7643)\n",
      "\u001b[32m[2020-06-26 18:11:28] __main__ INFO: \u001b[0mEpoch 46 Step 351/351 lr 0.004000 loss 1.2771 (1.1275) acc@1 0.5234 (0.5621) acc@5 0.7500 (0.7631)\n",
      "\u001b[32m[2020-06-26 18:11:28] __main__ INFO: \u001b[0mElapsed 392.91\n",
      "\u001b[32m[2020-06-26 18:11:28] __main__ INFO: \u001b[0mVal 46\n",
      "\u001b[32m[2020-06-26 18:11:41] __main__ INFO: \u001b[0mEpoch 46 loss 2.0876 acc@1 0.3310 acc@5 0.7092\n",
      "\u001b[32m[2020-06-26 18:11:41] __main__ INFO: \u001b[0mElapsed 13.14\n",
      "\u001b[32m[2020-06-26 18:11:41] __main__ INFO: \u001b[0mTrain 47 16146\n",
      "\u001b[32m[2020-06-26 18:13:33] __main__ INFO: \u001b[0mEpoch 47 Step 100/351 lr 0.004000 loss 1.1032 (1.1117) acc@1 0.5625 (0.5705) acc@5 0.7031 (0.7652)\n",
      "\u001b[32m[2020-06-26 18:15:25] __main__ INFO: \u001b[0mEpoch 47 Step 200/351 lr 0.004000 loss 1.0081 (1.1218) acc@1 0.5938 (0.5645) acc@5 0.8594 (0.7624)\n",
      "\u001b[32m[2020-06-26 18:17:17] __main__ INFO: \u001b[0mEpoch 47 Step 300/351 lr 0.004000 loss 1.1202 (1.1274) acc@1 0.5547 (0.5623) acc@5 0.7422 (0.7608)\n",
      "\u001b[32m[2020-06-26 18:18:14] __main__ INFO: \u001b[0mEpoch 47 Step 351/351 lr 0.004000 loss 0.8676 (1.1275) acc@1 0.6719 (0.5623) acc@5 0.8047 (0.7609)\n",
      "\u001b[32m[2020-06-26 18:18:14] __main__ INFO: \u001b[0mElapsed 392.94\n",
      "\u001b[32m[2020-06-26 18:18:14] __main__ INFO: \u001b[0mVal 47\n",
      "\u001b[32m[2020-06-26 18:18:27] __main__ INFO: \u001b[0mEpoch 47 loss 2.0929 acc@1 0.3276 acc@5 0.7082\n",
      "\u001b[32m[2020-06-26 18:18:27] __main__ INFO: \u001b[0mElapsed 13.18\n",
      "\u001b[32m[2020-06-26 18:18:27] __main__ INFO: \u001b[0mTrain 48 16497\n",
      "\u001b[32m[2020-06-26 18:20:19] __main__ INFO: \u001b[0mEpoch 48 Step 100/351 lr 0.004000 loss 1.0666 (1.1357) acc@1 0.5859 (0.5587) acc@5 0.8047 (0.7600)\n",
      "\u001b[32m[2020-06-26 18:22:11] __main__ INFO: \u001b[0mEpoch 48 Step 200/351 lr 0.004000 loss 1.1545 (1.1210) acc@1 0.5547 (0.5637) acc@5 0.7734 (0.7637)\n",
      "\u001b[32m[2020-06-26 18:24:03] __main__ INFO: \u001b[0mEpoch 48 Step 300/351 lr 0.004000 loss 1.1373 (1.1244) acc@1 0.5625 (0.5634) acc@5 0.7734 (0.7628)\n",
      "\u001b[32m[2020-06-26 18:25:00] __main__ INFO: \u001b[0mEpoch 48 Step 351/351 lr 0.004000 loss 1.0835 (1.1276) acc@1 0.5703 (0.5616) acc@5 0.7734 (0.7612)\n",
      "\u001b[32m[2020-06-26 18:25:00] __main__ INFO: \u001b[0mElapsed 392.82\n",
      "\u001b[32m[2020-06-26 18:25:00] __main__ INFO: \u001b[0mVal 48\n",
      "\u001b[32m[2020-06-26 18:25:13] __main__ INFO: \u001b[0mEpoch 48 loss 2.0815 acc@1 0.3222 acc@5 0.7014\n",
      "\u001b[32m[2020-06-26 18:25:13] __main__ INFO: \u001b[0mElapsed 13.13\n",
      "\u001b[32m[2020-06-26 18:25:13] __main__ INFO: \u001b[0mTrain 49 16848\n",
      "\u001b[32m[2020-06-26 18:27:05] __main__ INFO: \u001b[0mEpoch 49 Step 100/351 lr 0.004000 loss 1.0921 (1.1257) acc@1 0.6094 (0.5620) acc@5 0.7656 (0.7609)\n",
      "\u001b[32m[2020-06-26 18:28:57] __main__ INFO: \u001b[0mEpoch 49 Step 200/351 lr 0.004000 loss 1.0810 (1.1295) acc@1 0.6250 (0.5605) acc@5 0.7500 (0.7616)\n",
      "\u001b[32m[2020-06-26 18:30:49] __main__ INFO: \u001b[0mEpoch 49 Step 300/351 lr 0.004000 loss 1.1882 (1.1299) acc@1 0.5391 (0.5610) acc@5 0.7266 (0.7616)\n",
      "\u001b[32m[2020-06-26 18:31:46] __main__ INFO: \u001b[0mEpoch 49 Step 351/351 lr 0.004000 loss 1.1405 (1.1279) acc@1 0.5547 (0.5619) acc@5 0.7188 (0.7620)\n",
      "\u001b[32m[2020-06-26 18:31:46] __main__ INFO: \u001b[0mElapsed 392.75\n",
      "\u001b[32m[2020-06-26 18:31:46] __main__ INFO: \u001b[0mVal 49\n",
      "\u001b[32m[2020-06-26 18:31:59] __main__ INFO: \u001b[0mEpoch 49 loss 2.0704 acc@1 0.3302 acc@5 0.7116\n",
      "\u001b[32m[2020-06-26 18:31:59] __main__ INFO: \u001b[0mElapsed 13.16\n",
      "\u001b[32m[2020-06-26 18:31:59] __main__ INFO: \u001b[0mTrain 50 17199\n",
      "\u001b[32m[2020-06-26 18:33:51] __main__ INFO: \u001b[0mEpoch 50 Step 100/351 lr 0.004000 loss 1.2580 (1.1216) acc@1 0.5469 (0.5655) acc@5 0.7266 (0.7635)\n",
      "\u001b[32m[2020-06-26 18:35:43] __main__ INFO: \u001b[0mEpoch 50 Step 200/351 lr 0.004000 loss 1.1912 (1.1239) acc@1 0.5156 (0.5627) acc@5 0.7656 (0.7610)\n",
      "\u001b[32m[2020-06-26 18:37:35] __main__ INFO: \u001b[0mEpoch 50 Step 300/351 lr 0.004000 loss 1.2571 (1.1276) acc@1 0.5156 (0.5620) acc@5 0.8125 (0.7606)\n",
      "\u001b[32m[2020-06-26 18:38:32] __main__ INFO: \u001b[0mEpoch 50 Step 351/351 lr 0.004000 loss 1.0823 (1.1280) acc@1 0.6172 (0.5623) acc@5 0.8125 (0.7611)\n",
      "\u001b[32m[2020-06-26 18:38:32] __main__ INFO: \u001b[0mElapsed 392.84\n",
      "\u001b[32m[2020-06-26 18:38:32] __main__ INFO: \u001b[0mVal 50\n",
      "\u001b[32m[2020-06-26 18:38:45] __main__ INFO: \u001b[0mEpoch 50 loss 2.0783 acc@1 0.3304 acc@5 0.6996\n",
      "\u001b[32m[2020-06-26 18:38:45] __main__ INFO: \u001b[0mElapsed 13.12\n",
      "\u001b[32m[2020-06-26 18:38:45] __main__ INFO: \u001b[0mTrain 51 17550\n",
      "\u001b[32m[2020-06-26 18:40:37] __main__ INFO: \u001b[0mEpoch 51 Step 100/351 lr 0.004000 loss 1.0803 (1.1383) acc@1 0.5703 (0.5573) acc@5 0.7578 (0.7570)\n",
      "\u001b[32m[2020-06-26 18:42:29] __main__ INFO: \u001b[0mEpoch 51 Step 200/351 lr 0.004000 loss 1.0993 (1.1290) acc@1 0.5547 (0.5619) acc@5 0.8125 (0.7616)\n",
      "\u001b[32m[2020-06-26 18:44:21] __main__ INFO: \u001b[0mEpoch 51 Step 300/351 lr 0.004000 loss 1.1907 (1.1282) acc@1 0.5156 (0.5617) acc@5 0.7578 (0.7618)\n",
      "\u001b[32m[2020-06-26 18:45:18] __main__ INFO: \u001b[0mEpoch 51 Step 351/351 lr 0.004000 loss 1.0486 (1.1282) acc@1 0.5625 (0.5614) acc@5 0.7422 (0.7608)\n",
      "\u001b[32m[2020-06-26 18:45:18] __main__ INFO: \u001b[0mElapsed 392.83\n",
      "\u001b[32m[2020-06-26 18:45:18] __main__ INFO: \u001b[0mVal 51\n",
      "\u001b[32m[2020-06-26 18:45:31] __main__ INFO: \u001b[0mEpoch 51 loss 2.0813 acc@1 0.3300 acc@5 0.6970\n",
      "\u001b[32m[2020-06-26 18:45:31] __main__ INFO: \u001b[0mElapsed 13.13\n",
      "\u001b[32m[2020-06-26 18:45:31] __main__ INFO: \u001b[0mTrain 52 17901\n",
      "\u001b[32m[2020-06-26 18:47:23] __main__ INFO: \u001b[0mEpoch 52 Step 100/351 lr 0.004000 loss 1.1548 (1.1243) acc@1 0.5234 (0.5604) acc@5 0.7500 (0.7634)\n",
      "\u001b[32m[2020-06-26 18:49:15] __main__ INFO: \u001b[0mEpoch 52 Step 200/351 lr 0.004000 loss 1.2196 (1.1329) acc@1 0.5547 (0.5597) acc@5 0.7656 (0.7607)\n",
      "\u001b[32m[2020-06-26 18:51:07] __main__ INFO: \u001b[0mEpoch 52 Step 300/351 lr 0.004000 loss 1.1065 (1.1301) acc@1 0.6094 (0.5609) acc@5 0.7422 (0.7616)\n",
      "\u001b[32m[2020-06-26 18:52:04] __main__ INFO: \u001b[0mEpoch 52 Step 351/351 lr 0.004000 loss 1.1537 (1.1268) acc@1 0.5234 (0.5620) acc@5 0.7578 (0.7620)\n",
      "\u001b[32m[2020-06-26 18:52:04] __main__ INFO: \u001b[0mElapsed 392.74\n",
      "\u001b[32m[2020-06-26 18:52:04] __main__ INFO: \u001b[0mVal 52\n",
      "\u001b[32m[2020-06-26 18:52:17] __main__ INFO: \u001b[0mEpoch 52 loss 2.0824 acc@1 0.3300 acc@5 0.7080\n",
      "\u001b[32m[2020-06-26 18:52:17] __main__ INFO: \u001b[0mElapsed 13.14\n",
      "\u001b[32m[2020-06-26 18:52:17] __main__ INFO: \u001b[0mTrain 53 18252\n",
      "\u001b[32m[2020-06-26 18:54:09] __main__ INFO: \u001b[0mEpoch 53 Step 100/351 lr 0.004000 loss 1.1910 (1.1364) acc@1 0.5156 (0.5563) acc@5 0.7578 (0.7602)\n",
      "\u001b[32m[2020-06-26 18:56:01] __main__ INFO: \u001b[0mEpoch 53 Step 200/351 lr 0.004000 loss 1.1834 (1.1332) acc@1 0.5781 (0.5604) acc@5 0.7656 (0.7608)\n",
      "\u001b[32m[2020-06-26 18:57:53] __main__ INFO: \u001b[0mEpoch 53 Step 300/351 lr 0.004000 loss 1.1568 (1.1269) acc@1 0.5703 (0.5615) acc@5 0.7969 (0.7614)\n",
      "\u001b[32m[2020-06-26 18:58:50] __main__ INFO: \u001b[0mEpoch 53 Step 351/351 lr 0.004000 loss 1.2459 (1.1273) acc@1 0.5078 (0.5610) acc@5 0.7031 (0.7606)\n",
      "\u001b[32m[2020-06-26 18:58:50] __main__ INFO: \u001b[0mElapsed 392.79\n",
      "\u001b[32m[2020-06-26 18:58:50] __main__ INFO: \u001b[0mVal 53\n",
      "\u001b[32m[2020-06-26 18:59:03] __main__ INFO: \u001b[0mEpoch 53 loss 2.0823 acc@1 0.3390 acc@5 0.7070\n",
      "\u001b[32m[2020-06-26 18:59:03] __main__ INFO: \u001b[0mElapsed 13.16\n",
      "\u001b[32m[2020-06-26 18:59:03] __main__ INFO: \u001b[0mTrain 54 18603\n",
      "\u001b[32m[2020-06-26 19:00:55] __main__ INFO: \u001b[0mEpoch 54 Step 100/351 lr 0.004000 loss 1.0685 (1.1375) acc@1 0.5859 (0.5593) acc@5 0.7969 (0.7588)\n",
      "\u001b[32m[2020-06-26 19:02:47] __main__ INFO: \u001b[0mEpoch 54 Step 200/351 lr 0.004000 loss 1.2766 (1.1356) acc@1 0.5234 (0.5585) acc@5 0.7344 (0.7598)\n",
      "\u001b[32m[2020-06-26 19:04:39] __main__ INFO: \u001b[0mEpoch 54 Step 300/351 lr 0.004000 loss 0.8676 (1.1307) acc@1 0.6562 (0.5602) acc@5 0.8281 (0.7603)\n",
      "\u001b[32m[2020-06-26 19:05:36] __main__ INFO: \u001b[0mEpoch 54 Step 351/351 lr 0.004000 loss 1.1442 (1.1283) acc@1 0.5469 (0.5613) acc@5 0.7656 (0.7607)\n",
      "\u001b[32m[2020-06-26 19:05:36] __main__ INFO: \u001b[0mElapsed 392.69\n",
      "\u001b[32m[2020-06-26 19:05:36] __main__ INFO: \u001b[0mVal 54\n",
      "\u001b[32m[2020-06-26 19:05:49] __main__ INFO: \u001b[0mEpoch 54 loss 2.0787 acc@1 0.3276 acc@5 0.6946\n",
      "\u001b[32m[2020-06-26 19:05:49] __main__ INFO: \u001b[0mElapsed 13.15\n",
      "\u001b[32m[2020-06-26 19:05:49] __main__ INFO: \u001b[0mTrain 55 18954\n",
      "\u001b[32m[2020-06-26 19:07:41] __main__ INFO: \u001b[0mEpoch 55 Step 100/351 lr 0.004000 loss 1.2045 (1.1277) acc@1 0.5312 (0.5619) acc@5 0.7266 (0.7608)\n",
      "\u001b[32m[2020-06-26 19:09:33] __main__ INFO: \u001b[0mEpoch 55 Step 200/351 lr 0.004000 loss 1.0648 (1.1262) acc@1 0.5781 (0.5610) acc@5 0.7656 (0.7599)\n",
      "\u001b[32m[2020-06-26 19:11:24] __main__ INFO: \u001b[0mEpoch 55 Step 300/351 lr 0.004000 loss 1.1526 (1.1261) acc@1 0.5547 (0.5632) acc@5 0.7812 (0.7618)\n",
      "\u001b[32m[2020-06-26 19:12:22] __main__ INFO: \u001b[0mEpoch 55 Step 351/351 lr 0.004000 loss 0.9594 (1.1272) acc@1 0.6250 (0.5631) acc@5 0.7656 (0.7611)\n",
      "\u001b[32m[2020-06-26 19:12:22] __main__ INFO: \u001b[0mElapsed 392.74\n",
      "\u001b[32m[2020-06-26 19:12:22] __main__ INFO: \u001b[0mVal 55\n",
      "\u001b[32m[2020-06-26 19:12:35] __main__ INFO: \u001b[0mEpoch 55 loss 2.0888 acc@1 0.3310 acc@5 0.6960\n",
      "\u001b[32m[2020-06-26 19:12:35] __main__ INFO: \u001b[0mElapsed 13.13\n",
      "\u001b[32m[2020-06-26 19:12:35] __main__ INFO: \u001b[0mTrain 56 19305\n",
      "\u001b[32m[2020-06-26 19:14:27] __main__ INFO: \u001b[0mEpoch 56 Step 100/351 lr 0.004000 loss 0.9703 (1.1163) acc@1 0.6328 (0.5654) acc@5 0.8359 (0.7634)\n",
      "\u001b[32m[2020-06-26 19:16:18] __main__ INFO: \u001b[0mEpoch 56 Step 200/351 lr 0.004000 loss 1.0813 (1.1294) acc@1 0.5781 (0.5595) acc@5 0.7734 (0.7620)\n",
      "\u001b[32m[2020-06-26 19:18:10] __main__ INFO: \u001b[0mEpoch 56 Step 300/351 lr 0.004000 loss 1.2057 (1.1311) acc@1 0.5547 (0.5588) acc@5 0.7734 (0.7615)\n",
      "\u001b[32m[2020-06-26 19:19:07] __main__ INFO: \u001b[0mEpoch 56 Step 351/351 lr 0.004000 loss 1.0292 (1.1279) acc@1 0.5938 (0.5605) acc@5 0.7656 (0.7622)\n",
      "\u001b[32m[2020-06-26 19:19:07] __main__ INFO: \u001b[0mElapsed 392.69\n",
      "\u001b[32m[2020-06-26 19:19:07] __main__ INFO: \u001b[0mVal 56\n",
      "\u001b[32m[2020-06-26 19:19:20] __main__ INFO: \u001b[0mEpoch 56 loss 2.0519 acc@1 0.3312 acc@5 0.7058\n",
      "\u001b[32m[2020-06-26 19:19:20] __main__ INFO: \u001b[0mElapsed 13.12\n",
      "\u001b[32m[2020-06-26 19:19:20] __main__ INFO: \u001b[0mTrain 57 19656\n",
      "\u001b[32m[2020-06-26 19:21:12] __main__ INFO: \u001b[0mEpoch 57 Step 100/351 lr 0.004000 loss 1.1343 (1.1258) acc@1 0.5547 (0.5636) acc@5 0.7891 (0.7619)\n",
      "\u001b[32m[2020-06-26 19:23:04] __main__ INFO: \u001b[0mEpoch 57 Step 200/351 lr 0.004000 loss 1.0835 (1.1205) acc@1 0.5859 (0.5652) acc@5 0.7891 (0.7660)\n",
      "\u001b[32m[2020-06-26 19:24:56] __main__ INFO: \u001b[0mEpoch 57 Step 300/351 lr 0.004000 loss 1.3344 (1.1262) acc@1 0.5000 (0.5636) acc@5 0.7734 (0.7632)\n",
      "\u001b[32m[2020-06-26 19:25:53] __main__ INFO: \u001b[0mEpoch 57 Step 351/351 lr 0.004000 loss 1.0426 (1.1278) acc@1 0.6172 (0.5626) acc@5 0.7656 (0.7615)\n",
      "\u001b[32m[2020-06-26 19:25:53] __main__ INFO: \u001b[0mElapsed 392.73\n",
      "\u001b[32m[2020-06-26 19:25:53] __main__ INFO: \u001b[0mVal 57\n",
      "\u001b[32m[2020-06-26 19:26:06] __main__ INFO: \u001b[0mEpoch 57 loss 2.0701 acc@1 0.3306 acc@5 0.7054\n",
      "\u001b[32m[2020-06-26 19:26:06] __main__ INFO: \u001b[0mElapsed 13.14\n",
      "\u001b[32m[2020-06-26 19:26:06] __main__ INFO: \u001b[0mTrain 58 20007\n",
      "\u001b[32m[2020-06-26 19:27:58] __main__ INFO: \u001b[0mEpoch 58 Step 100/351 lr 0.004000 loss 1.0642 (1.1318) acc@1 0.5859 (0.5614) acc@5 0.7656 (0.7652)\n",
      "\u001b[32m[2020-06-26 19:29:50] __main__ INFO: \u001b[0mEpoch 58 Step 200/351 lr 0.004000 loss 1.1370 (1.1304) acc@1 0.5469 (0.5614) acc@5 0.7734 (0.7618)\n",
      "\u001b[32m[2020-06-26 19:31:42] __main__ INFO: \u001b[0mEpoch 58 Step 300/351 lr 0.004000 loss 1.0951 (1.1263) acc@1 0.5938 (0.5631) acc@5 0.7578 (0.7615)\n",
      "\u001b[32m[2020-06-26 19:32:39] __main__ INFO: \u001b[0mEpoch 58 Step 351/351 lr 0.004000 loss 1.0615 (1.1277) acc@1 0.5938 (0.5632) acc@5 0.7734 (0.7616)\n",
      "\u001b[32m[2020-06-26 19:32:39] __main__ INFO: \u001b[0mElapsed 392.81\n",
      "\u001b[32m[2020-06-26 19:32:39] __main__ INFO: \u001b[0mVal 58\n",
      "\u001b[32m[2020-06-26 19:32:52] __main__ INFO: \u001b[0mEpoch 58 loss 2.0753 acc@1 0.3292 acc@5 0.7084\n",
      "\u001b[32m[2020-06-26 19:32:52] __main__ INFO: \u001b[0mElapsed 13.16\n",
      "\u001b[32m[2020-06-26 19:32:52] __main__ INFO: \u001b[0mTrain 59 20358\n",
      "\u001b[32m[2020-06-26 19:34:44] __main__ INFO: \u001b[0mEpoch 59 Step 100/351 lr 0.004000 loss 1.1063 (1.1223) acc@1 0.5469 (0.5648) acc@5 0.7344 (0.7653)\n",
      "\u001b[32m[2020-06-26 19:36:36] __main__ INFO: \u001b[0mEpoch 59 Step 200/351 lr 0.004000 loss 1.2085 (1.1253) acc@1 0.4922 (0.5641) acc@5 0.7500 (0.7632)\n",
      "\u001b[32m[2020-06-26 19:38:28] __main__ INFO: \u001b[0mEpoch 59 Step 300/351 lr 0.004000 loss 0.9691 (1.1294) acc@1 0.6172 (0.5618) acc@5 0.7500 (0.7610)\n",
      "\u001b[32m[2020-06-26 19:39:25] __main__ INFO: \u001b[0mEpoch 59 Step 351/351 lr 0.004000 loss 1.0613 (1.1268) acc@1 0.5703 (0.5628) acc@5 0.7969 (0.7616)\n",
      "\u001b[32m[2020-06-26 19:39:25] __main__ INFO: \u001b[0mElapsed 392.79\n",
      "\u001b[32m[2020-06-26 19:39:25] __main__ INFO: \u001b[0mVal 59\n",
      "\u001b[32m[2020-06-26 19:39:38] __main__ INFO: \u001b[0mEpoch 59 loss 2.0516 acc@1 0.3344 acc@5 0.7096\n",
      "\u001b[32m[2020-06-26 19:39:38] __main__ INFO: \u001b[0mElapsed 13.15\n",
      "\u001b[32m[2020-06-26 19:39:38] __main__ INFO: \u001b[0mTrain 60 20709\n",
      "\u001b[32m[2020-06-26 19:41:30] __main__ INFO: \u001b[0mEpoch 60 Step 100/351 lr 0.004000 loss 1.1878 (1.1250) acc@1 0.5391 (0.5657) acc@5 0.7344 (0.7634)\n",
      "\u001b[32m[2020-06-26 19:43:22] __main__ INFO: \u001b[0mEpoch 60 Step 200/351 lr 0.004000 loss 1.0150 (1.1218) acc@1 0.6172 (0.5657) acc@5 0.7734 (0.7635)\n",
      "\u001b[32m[2020-06-26 19:45:14] __main__ INFO: \u001b[0mEpoch 60 Step 300/351 lr 0.004000 loss 1.1532 (1.1242) acc@1 0.5625 (0.5639) acc@5 0.7422 (0.7623)\n",
      "\u001b[32m[2020-06-26 19:46:11] __main__ INFO: \u001b[0mEpoch 60 Step 351/351 lr 0.004000 loss 1.2653 (1.1280) acc@1 0.4688 (0.5623) acc@5 0.6719 (0.7609)\n",
      "\u001b[32m[2020-06-26 19:46:11] __main__ INFO: \u001b[0mElapsed 392.86\n",
      "\u001b[32m[2020-06-26 19:46:11] __main__ INFO: \u001b[0mVal 60\n",
      "\u001b[32m[2020-06-26 19:46:24] __main__ INFO: \u001b[0mEpoch 60 loss 2.0633 acc@1 0.3328 acc@5 0.7102\n",
      "\u001b[32m[2020-06-26 19:46:24] __main__ INFO: \u001b[0mElapsed 13.12\n",
      "\u001b[32m[2020-06-26 19:46:24] __main__ INFO: \u001b[0mTrain 61 21060\n",
      "\u001b[32m[2020-06-26 19:48:16] __main__ INFO: \u001b[0mEpoch 61 Step 100/351 lr 0.000800 loss 1.2697 (1.1268) acc@1 0.5078 (0.5630) acc@5 0.7656 (0.7621)\n",
      "\u001b[32m[2020-06-26 19:50:08] __main__ INFO: \u001b[0mEpoch 61 Step 200/351 lr 0.000800 loss 1.1159 (1.1253) acc@1 0.5625 (0.5625) acc@5 0.8125 (0.7608)\n",
      "\u001b[32m[2020-06-26 19:52:00] __main__ INFO: \u001b[0mEpoch 61 Step 300/351 lr 0.000800 loss 1.2443 (1.1289) acc@1 0.5156 (0.5608) acc@5 0.7812 (0.7616)\n",
      "\u001b[32m[2020-06-26 19:52:57] __main__ INFO: \u001b[0mEpoch 61 Step 351/351 lr 0.000800 loss 0.9943 (1.1263) acc@1 0.6016 (0.5624) acc@5 0.7812 (0.7610)\n",
      "\u001b[32m[2020-06-26 19:52:57] __main__ INFO: \u001b[0mElapsed 392.78\n",
      "\u001b[32m[2020-06-26 19:52:57] __main__ INFO: \u001b[0mVal 61\n",
      "\u001b[32m[2020-06-26 19:53:10] __main__ INFO: \u001b[0mEpoch 61 loss 2.0543 acc@1 0.3356 acc@5 0.7110\n",
      "\u001b[32m[2020-06-26 19:53:10] __main__ INFO: \u001b[0mElapsed 13.12\n",
      "\u001b[32m[2020-06-26 19:53:10] __main__ INFO: \u001b[0mTrain 62 21411\n",
      "\u001b[32m[2020-06-26 19:55:02] __main__ INFO: \u001b[0mEpoch 62 Step 100/351 lr 0.000800 loss 1.1178 (1.1248) acc@1 0.5859 (0.5632) acc@5 0.7188 (0.7647)\n",
      "\u001b[32m[2020-06-26 19:56:54] __main__ INFO: \u001b[0mEpoch 62 Step 200/351 lr 0.000800 loss 1.2371 (1.1242) acc@1 0.5078 (0.5623) acc@5 0.7891 (0.7638)\n",
      "\u001b[32m[2020-06-26 19:58:46] __main__ INFO: \u001b[0mEpoch 62 Step 300/351 lr 0.000800 loss 1.1014 (1.1254) acc@1 0.5859 (0.5627) acc@5 0.7734 (0.7626)\n",
      "\u001b[32m[2020-06-26 19:59:43] __main__ INFO: \u001b[0mEpoch 62 Step 351/351 lr 0.000800 loss 1.1906 (1.1252) acc@1 0.5312 (0.5626) acc@5 0.7656 (0.7622)\n",
      "\u001b[32m[2020-06-26 19:59:43] __main__ INFO: \u001b[0mElapsed 392.78\n",
      "\u001b[32m[2020-06-26 19:59:43] __main__ INFO: \u001b[0mVal 62\n",
      "\u001b[32m[2020-06-26 19:59:56] __main__ INFO: \u001b[0mEpoch 62 loss 2.0713 acc@1 0.3336 acc@5 0.7104\n",
      "\u001b[32m[2020-06-26 19:59:56] __main__ INFO: \u001b[0mElapsed 13.14\n",
      "\u001b[32m[2020-06-26 19:59:56] __main__ INFO: \u001b[0mTrain 63 21762\n",
      "\u001b[32m[2020-06-26 20:01:48] __main__ INFO: \u001b[0mEpoch 63 Step 100/351 lr 0.000800 loss 1.0812 (1.1146) acc@1 0.5938 (0.5698) acc@5 0.7578 (0.7646)\n",
      "\u001b[32m[2020-06-26 20:03:40] __main__ INFO: \u001b[0mEpoch 63 Step 200/351 lr 0.000800 loss 1.0778 (1.1247) acc@1 0.5938 (0.5652) acc@5 0.8203 (0.7618)\n",
      "\u001b[32m[2020-06-26 20:05:32] __main__ INFO: \u001b[0mEpoch 63 Step 300/351 lr 0.000800 loss 1.1892 (1.1258) acc@1 0.5234 (0.5633) acc@5 0.7891 (0.7621)\n",
      "\u001b[32m[2020-06-26 20:06:29] __main__ INFO: \u001b[0mEpoch 63 Step 351/351 lr 0.000800 loss 1.1397 (1.1258) acc@1 0.5547 (0.5631) acc@5 0.7031 (0.7619)\n",
      "\u001b[32m[2020-06-26 20:06:29] __main__ INFO: \u001b[0mElapsed 392.84\n",
      "\u001b[32m[2020-06-26 20:06:29] __main__ INFO: \u001b[0mVal 63\n",
      "\u001b[32m[2020-06-26 20:06:42] __main__ INFO: \u001b[0mEpoch 63 loss 2.0756 acc@1 0.3308 acc@5 0.7114\n",
      "\u001b[32m[2020-06-26 20:06:42] __main__ INFO: \u001b[0mElapsed 13.11\n",
      "\u001b[32m[2020-06-26 20:06:42] __main__ INFO: \u001b[0mTrain 64 22113\n",
      "\u001b[32m[2020-06-26 20:08:34] __main__ INFO: \u001b[0mEpoch 64 Step 100/351 lr 0.000800 loss 1.2647 (1.1149) acc@1 0.5078 (0.5705) acc@5 0.7188 (0.7681)\n",
      "\u001b[32m[2020-06-26 20:10:26] __main__ INFO: \u001b[0mEpoch 64 Step 200/351 lr 0.000800 loss 1.0327 (1.1219) acc@1 0.6172 (0.5659) acc@5 0.7422 (0.7659)\n",
      "\u001b[32m[2020-06-26 20:12:18] __main__ INFO: \u001b[0mEpoch 64 Step 300/351 lr 0.000800 loss 1.2027 (1.1245) acc@1 0.5234 (0.5634) acc@5 0.7266 (0.7623)\n",
      "\u001b[32m[2020-06-26 20:13:15] __main__ INFO: \u001b[0mEpoch 64 Step 351/351 lr 0.000800 loss 1.0422 (1.1255) acc@1 0.5781 (0.5632) acc@5 0.7734 (0.7624)\n",
      "\u001b[32m[2020-06-26 20:13:15] __main__ INFO: \u001b[0mElapsed 392.65\n",
      "\u001b[32m[2020-06-26 20:13:15] __main__ INFO: \u001b[0mVal 64\n",
      "\u001b[32m[2020-06-26 20:13:28] __main__ INFO: \u001b[0mEpoch 64 loss 2.0624 acc@1 0.3332 acc@5 0.7104\n",
      "\u001b[32m[2020-06-26 20:13:28] __main__ INFO: \u001b[0mElapsed 13.17\n",
      "\u001b[32m[2020-06-26 20:13:28] __main__ INFO: \u001b[0mTrain 65 22464\n",
      "\u001b[32m[2020-06-26 20:15:20] __main__ INFO: \u001b[0mEpoch 65 Step 100/351 lr 0.000800 loss 1.1007 (1.1201) acc@1 0.5859 (0.5677) acc@5 0.8047 (0.7642)\n",
      "\u001b[32m[2020-06-26 20:17:12] __main__ INFO: \u001b[0mEpoch 65 Step 200/351 lr 0.000800 loss 1.2544 (1.1229) acc@1 0.5469 (0.5662) acc@5 0.7891 (0.7665)\n",
      "\u001b[32m[2020-06-26 20:19:04] __main__ INFO: \u001b[0mEpoch 65 Step 300/351 lr 0.000800 loss 1.0767 (1.1249) acc@1 0.6016 (0.5642) acc@5 0.8359 (0.7645)\n",
      "\u001b[32m[2020-06-26 20:20:01] __main__ INFO: \u001b[0mEpoch 65 Step 351/351 lr 0.000800 loss 1.1674 (1.1251) acc@1 0.5781 (0.5638) acc@5 0.7500 (0.7635)\n",
      "\u001b[32m[2020-06-26 20:20:01] __main__ INFO: \u001b[0mElapsed 392.72\n",
      "\u001b[32m[2020-06-26 20:20:01] __main__ INFO: \u001b[0mVal 65\n",
      "\u001b[32m[2020-06-26 20:20:14] __main__ INFO: \u001b[0mEpoch 65 loss 2.0628 acc@1 0.3324 acc@5 0.7096\n",
      "\u001b[32m[2020-06-26 20:20:14] __main__ INFO: \u001b[0mElapsed 13.13\n",
      "\u001b[32m[2020-06-26 20:20:14] __main__ INFO: \u001b[0mTrain 66 22815\n",
      "\u001b[32m[2020-06-26 20:22:06] __main__ INFO: \u001b[0mEpoch 66 Step 100/351 lr 0.000800 loss 1.0446 (1.1390) acc@1 0.6172 (0.5531) acc@5 0.7734 (0.7607)\n",
      "\u001b[32m[2020-06-26 20:23:58] __main__ INFO: \u001b[0mEpoch 66 Step 200/351 lr 0.000800 loss 0.9191 (1.1289) acc@1 0.6562 (0.5597) acc@5 0.8359 (0.7610)\n",
      "\u001b[32m[2020-06-26 20:25:49] __main__ INFO: \u001b[0mEpoch 66 Step 300/351 lr 0.000800 loss 1.1709 (1.1253) acc@1 0.5547 (0.5623) acc@5 0.8125 (0.7612)\n",
      "\u001b[32m[2020-06-26 20:26:46] __main__ INFO: \u001b[0mEpoch 66 Step 351/351 lr 0.000800 loss 0.9001 (1.1251) acc@1 0.6562 (0.5628) acc@5 0.8672 (0.7622)\n",
      "\u001b[32m[2020-06-26 20:26:46] __main__ INFO: \u001b[0mElapsed 392.72\n",
      "\u001b[32m[2020-06-26 20:26:46] __main__ INFO: \u001b[0mVal 66\n",
      "\u001b[32m[2020-06-26 20:27:00] __main__ INFO: \u001b[0mEpoch 66 loss 2.0547 acc@1 0.3316 acc@5 0.7108\n",
      "\u001b[32m[2020-06-26 20:27:00] __main__ INFO: \u001b[0mElapsed 13.12\n",
      "\u001b[32m[2020-06-26 20:27:00] __main__ INFO: \u001b[0mTrain 67 23166\n",
      "\u001b[32m[2020-06-26 20:28:52] __main__ INFO: \u001b[0mEpoch 67 Step 100/351 lr 0.000800 loss 1.1148 (1.1227) acc@1 0.5391 (0.5628) acc@5 0.7891 (0.7616)\n",
      "\u001b[32m[2020-06-26 20:30:43] __main__ INFO: \u001b[0mEpoch 67 Step 200/351 lr 0.000800 loss 1.1538 (1.1201) acc@1 0.5312 (0.5640) acc@5 0.7344 (0.7647)\n",
      "\u001b[32m[2020-06-26 20:32:35] __main__ INFO: \u001b[0mEpoch 67 Step 300/351 lr 0.000800 loss 1.1537 (1.1197) acc@1 0.5625 (0.5641) acc@5 0.7734 (0.7647)\n",
      "\u001b[32m[2020-06-26 20:33:32] __main__ INFO: \u001b[0mEpoch 67 Step 351/351 lr 0.000800 loss 1.0815 (1.1246) acc@1 0.5938 (0.5626) acc@5 0.8125 (0.7639)\n",
      "\u001b[32m[2020-06-26 20:33:32] __main__ INFO: \u001b[0mElapsed 392.74\n",
      "\u001b[32m[2020-06-26 20:33:32] __main__ INFO: \u001b[0mVal 67\n",
      "\u001b[32m[2020-06-26 20:33:45] __main__ INFO: \u001b[0mEpoch 67 loss 2.0691 acc@1 0.3334 acc@5 0.7108\n",
      "\u001b[32m[2020-06-26 20:33:45] __main__ INFO: \u001b[0mElapsed 13.11\n",
      "\u001b[32m[2020-06-26 20:33:45] __main__ INFO: \u001b[0mTrain 68 23517\n",
      "\u001b[32m[2020-06-26 20:35:37] __main__ INFO: \u001b[0mEpoch 68 Step 100/351 lr 0.000800 loss 1.1010 (1.1114) acc@1 0.5625 (0.5664) acc@5 0.7891 (0.7677)\n",
      "\u001b[32m[2020-06-26 20:37:29] __main__ INFO: \u001b[0mEpoch 68 Step 200/351 lr 0.000800 loss 1.1495 (1.1210) acc@1 0.5703 (0.5622) acc@5 0.7266 (0.7639)\n",
      "\u001b[32m[2020-06-26 20:39:21] __main__ INFO: \u001b[0mEpoch 68 Step 300/351 lr 0.000800 loss 1.1319 (1.1249) acc@1 0.5234 (0.5625) acc@5 0.7656 (0.7641)\n",
      "\u001b[32m[2020-06-26 20:40:18] __main__ INFO: \u001b[0mEpoch 68 Step 351/351 lr 0.000800 loss 1.0769 (1.1249) acc@1 0.5625 (0.5623) acc@5 0.8125 (0.7631)\n",
      "\u001b[32m[2020-06-26 20:40:18] __main__ INFO: \u001b[0mElapsed 392.68\n",
      "\u001b[32m[2020-06-26 20:40:18] __main__ INFO: \u001b[0mVal 68\n",
      "\u001b[32m[2020-06-26 20:40:31] __main__ INFO: \u001b[0mEpoch 68 loss 2.0668 acc@1 0.3316 acc@5 0.7074\n",
      "\u001b[32m[2020-06-26 20:40:31] __main__ INFO: \u001b[0mElapsed 13.16\n",
      "\u001b[32m[2020-06-26 20:40:31] __main__ INFO: \u001b[0mTrain 69 23868\n",
      "\u001b[32m[2020-06-26 20:42:23] __main__ INFO: \u001b[0mEpoch 69 Step 100/351 lr 0.000800 loss 1.0439 (1.1056) acc@1 0.5859 (0.5701) acc@5 0.7734 (0.7663)\n",
      "\u001b[32m[2020-06-26 20:44:15] __main__ INFO: \u001b[0mEpoch 69 Step 200/351 lr 0.000800 loss 1.1649 (1.1155) acc@1 0.5625 (0.5652) acc@5 0.7500 (0.7648)\n",
      "\u001b[32m[2020-06-26 20:46:07] __main__ INFO: \u001b[0mEpoch 69 Step 300/351 lr 0.000800 loss 1.2722 (1.1229) acc@1 0.5156 (0.5632) acc@5 0.7656 (0.7620)\n",
      "\u001b[32m[2020-06-26 20:47:04] __main__ INFO: \u001b[0mEpoch 69 Step 351/351 lr 0.000800 loss 1.0427 (1.1254) acc@1 0.6172 (0.5625) acc@5 0.8125 (0.7619)\n",
      "\u001b[32m[2020-06-26 20:47:04] __main__ INFO: \u001b[0mElapsed 392.95\n",
      "\u001b[32m[2020-06-26 20:47:04] __main__ INFO: \u001b[0mVal 69\n",
      "\u001b[32m[2020-06-26 20:47:17] __main__ INFO: \u001b[0mEpoch 69 loss 2.0653 acc@1 0.3324 acc@5 0.7116\n",
      "\u001b[32m[2020-06-26 20:47:17] __main__ INFO: \u001b[0mElapsed 13.14\n",
      "\u001b[32m[2020-06-26 20:47:17] __main__ INFO: \u001b[0mTrain 70 24219\n",
      "\u001b[32m[2020-06-26 20:49:09] __main__ INFO: \u001b[0mEpoch 70 Step 100/351 lr 0.000800 loss 1.1846 (1.1045) acc@1 0.5391 (0.5716) acc@5 0.7109 (0.7658)\n",
      "\u001b[32m[2020-06-26 20:51:01] __main__ INFO: \u001b[0mEpoch 70 Step 200/351 lr 0.000800 loss 1.1415 (1.1222) acc@1 0.5781 (0.5641) acc@5 0.7188 (0.7619)\n",
      "\u001b[32m[2020-06-26 20:52:53] __main__ INFO: \u001b[0mEpoch 70 Step 300/351 lr 0.000800 loss 1.0091 (1.1246) acc@1 0.6094 (0.5632) acc@5 0.7969 (0.7614)\n",
      "\u001b[32m[2020-06-26 20:53:50] __main__ INFO: \u001b[0mEpoch 70 Step 351/351 lr 0.000800 loss 1.1937 (1.1244) acc@1 0.5078 (0.5635) acc@5 0.6641 (0.7613)\n",
      "\u001b[32m[2020-06-26 20:53:50] __main__ INFO: \u001b[0mElapsed 392.76\n",
      "\u001b[32m[2020-06-26 20:53:50] __main__ INFO: \u001b[0mVal 70\n",
      "\u001b[32m[2020-06-26 20:54:03] __main__ INFO: \u001b[0mEpoch 70 loss 2.0764 acc@1 0.3322 acc@5 0.7098\n",
      "\u001b[32m[2020-06-26 20:54:03] __main__ INFO: \u001b[0mElapsed 13.17\n",
      "\u001b[32m[2020-06-26 20:54:03] __main__ INFO: \u001b[0mTrain 71 24570\n",
      "\u001b[32m[2020-06-26 20:55:55] __main__ INFO: \u001b[0mEpoch 71 Step 100/351 lr 0.000800 loss 1.1161 (1.1306) acc@1 0.6094 (0.5613) acc@5 0.7734 (0.7605)\n",
      "\u001b[32m[2020-06-26 20:57:47] __main__ INFO: \u001b[0mEpoch 71 Step 200/351 lr 0.000800 loss 1.0067 (1.1299) acc@1 0.6016 (0.5624) acc@5 0.8281 (0.7626)\n",
      "\u001b[32m[2020-06-26 20:59:39] __main__ INFO: \u001b[0mEpoch 71 Step 300/351 lr 0.000800 loss 1.0056 (1.1240) acc@1 0.6250 (0.5637) acc@5 0.8281 (0.7634)\n",
      "\u001b[32m[2020-06-26 21:00:36] __main__ INFO: \u001b[0mEpoch 71 Step 351/351 lr 0.000800 loss 1.1199 (1.1249) acc@1 0.5781 (0.5631) acc@5 0.7891 (0.7630)\n",
      "\u001b[32m[2020-06-26 21:00:36] __main__ INFO: \u001b[0mElapsed 392.82\n",
      "\u001b[32m[2020-06-26 21:00:36] __main__ INFO: \u001b[0mVal 71\n",
      "\u001b[32m[2020-06-26 21:00:49] __main__ INFO: \u001b[0mEpoch 71 loss 2.0635 acc@1 0.3346 acc@5 0.7098\n",
      "\u001b[32m[2020-06-26 21:00:49] __main__ INFO: \u001b[0mElapsed 13.17\n",
      "\u001b[32m[2020-06-26 21:00:49] __main__ INFO: \u001b[0mTrain 72 24921\n",
      "\u001b[32m[2020-06-26 21:02:41] __main__ INFO: \u001b[0mEpoch 72 Step 100/351 lr 0.000800 loss 1.1094 (1.1145) acc@1 0.6016 (0.5687) acc@5 0.7812 (0.7679)\n",
      "\u001b[32m[2020-06-26 21:04:33] __main__ INFO: \u001b[0mEpoch 72 Step 200/351 lr 0.000800 loss 1.2205 (1.1231) acc@1 0.5547 (0.5630) acc@5 0.7734 (0.7624)\n",
      "\u001b[32m[2020-06-26 21:06:25] __main__ INFO: \u001b[0mEpoch 72 Step 300/351 lr 0.000800 loss 0.7903 (1.1238) acc@1 0.6797 (0.5624) acc@5 0.8750 (0.7629)\n",
      "\u001b[32m[2020-06-26 21:07:22] __main__ INFO: \u001b[0mEpoch 72 Step 351/351 lr 0.000800 loss 1.1726 (1.1244) acc@1 0.5156 (0.5627) acc@5 0.7109 (0.7638)\n",
      "\u001b[32m[2020-06-26 21:07:22] __main__ INFO: \u001b[0mElapsed 392.72\n",
      "\u001b[32m[2020-06-26 21:07:22] __main__ INFO: \u001b[0mVal 72\n",
      "\u001b[32m[2020-06-26 21:07:35] __main__ INFO: \u001b[0mEpoch 72 loss 2.0661 acc@1 0.3310 acc@5 0.7092\n",
      "\u001b[32m[2020-06-26 21:07:35] __main__ INFO: \u001b[0mElapsed 13.13\n",
      "\u001b[32m[2020-06-26 21:07:35] __main__ INFO: \u001b[0mTrain 73 25272\n",
      "\u001b[32m[2020-06-26 21:09:27] __main__ INFO: \u001b[0mEpoch 73 Step 100/351 lr 0.000800 loss 1.1636 (1.1157) acc@1 0.5234 (0.5664) acc@5 0.6953 (0.7630)\n",
      "\u001b[32m[2020-06-26 21:11:19] __main__ INFO: \u001b[0mEpoch 73 Step 200/351 lr 0.000800 loss 1.1208 (1.1240) acc@1 0.5469 (0.5628) acc@5 0.7266 (0.7639)\n",
      "\u001b[32m[2020-06-26 21:13:11] __main__ INFO: \u001b[0mEpoch 73 Step 300/351 lr 0.000800 loss 1.0108 (1.1245) acc@1 0.6094 (0.5625) acc@5 0.8125 (0.7636)\n",
      "\u001b[32m[2020-06-26 21:14:08] __main__ INFO: \u001b[0mEpoch 73 Step 351/351 lr 0.000800 loss 1.2711 (1.1246) acc@1 0.5078 (0.5625) acc@5 0.7266 (0.7643)\n",
      "\u001b[32m[2020-06-26 21:14:08] __main__ INFO: \u001b[0mElapsed 392.68\n",
      "\u001b[32m[2020-06-26 21:14:08] __main__ INFO: \u001b[0mVal 73\n",
      "\u001b[32m[2020-06-26 21:14:21] __main__ INFO: \u001b[0mEpoch 73 loss 2.0706 acc@1 0.3330 acc@5 0.7104\n",
      "\u001b[32m[2020-06-26 21:14:21] __main__ INFO: \u001b[0mElapsed 13.18\n",
      "\u001b[32m[2020-06-26 21:14:21] __main__ INFO: \u001b[0mTrain 74 25623\n",
      "\u001b[32m[2020-06-26 21:16:13] __main__ INFO: \u001b[0mEpoch 74 Step 100/351 lr 0.000800 loss 1.2749 (1.1285) acc@1 0.5078 (0.5617) acc@5 0.7109 (0.7607)\n",
      "\u001b[32m[2020-06-26 21:18:05] __main__ INFO: \u001b[0mEpoch 74 Step 200/351 lr 0.000800 loss 1.1526 (1.1226) acc@1 0.5312 (0.5642) acc@5 0.7422 (0.7635)\n",
      "\u001b[32m[2020-06-26 21:19:57] __main__ INFO: \u001b[0mEpoch 74 Step 300/351 lr 0.000800 loss 1.1395 (1.1247) acc@1 0.5312 (0.5635) acc@5 0.7109 (0.7629)\n",
      "\u001b[32m[2020-06-26 21:20:54] __main__ INFO: \u001b[0mEpoch 74 Step 351/351 lr 0.000800 loss 1.1137 (1.1251) acc@1 0.5703 (0.5634) acc@5 0.8047 (0.7633)\n",
      "\u001b[32m[2020-06-26 21:20:54] __main__ INFO: \u001b[0mElapsed 392.64\n",
      "\u001b[32m[2020-06-26 21:20:54] __main__ INFO: \u001b[0mVal 74\n",
      "\u001b[32m[2020-06-26 21:21:07] __main__ INFO: \u001b[0mEpoch 74 loss 2.0753 acc@1 0.3330 acc@5 0.7110\n",
      "\u001b[32m[2020-06-26 21:21:07] __main__ INFO: \u001b[0mElapsed 13.12\n",
      "\u001b[32m[2020-06-26 21:21:07] __main__ INFO: \u001b[0mTrain 75 25974\n",
      "\u001b[32m[2020-06-26 21:22:59] __main__ INFO: \u001b[0mEpoch 75 Step 100/351 lr 0.000800 loss 0.9949 (1.1253) acc@1 0.5938 (0.5632) acc@5 0.7812 (0.7580)\n",
      "\u001b[32m[2020-06-26 21:24:51] __main__ INFO: \u001b[0mEpoch 75 Step 200/351 lr 0.000800 loss 1.1001 (1.1227) acc@1 0.5859 (0.5638) acc@5 0.7734 (0.7630)\n",
      "\u001b[32m[2020-06-26 21:26:42] __main__ INFO: \u001b[0mEpoch 75 Step 300/351 lr 0.000800 loss 1.0313 (1.1242) acc@1 0.5781 (0.5635) acc@5 0.7500 (0.7622)\n",
      "\u001b[32m[2020-06-26 21:27:39] __main__ INFO: \u001b[0mEpoch 75 Step 351/351 lr 0.000800 loss 1.3507 (1.1246) acc@1 0.4766 (0.5636) acc@5 0.6484 (0.7624)\n",
      "\u001b[32m[2020-06-26 21:27:39] __main__ INFO: \u001b[0mElapsed 392.69\n",
      "\u001b[32m[2020-06-26 21:27:39] __main__ INFO: \u001b[0mVal 75\n",
      "\u001b[32m[2020-06-26 21:27:53] __main__ INFO: \u001b[0mEpoch 75 loss 2.0687 acc@1 0.3318 acc@5 0.7112\n",
      "\u001b[32m[2020-06-26 21:27:53] __main__ INFO: \u001b[0mElapsed 13.14\n",
      "\u001b[32m[2020-06-26 21:27:53] __main__ INFO: \u001b[0mTrain 76 26325\n",
      "\u001b[32m[2020-06-26 21:29:45] __main__ INFO: \u001b[0mEpoch 76 Step 100/351 lr 0.000800 loss 1.1956 (1.1370) acc@1 0.5391 (0.5566) acc@5 0.7188 (0.7620)\n",
      "\u001b[32m[2020-06-26 21:31:36] __main__ INFO: \u001b[0mEpoch 76 Step 200/351 lr 0.000800 loss 0.9722 (1.1240) acc@1 0.6094 (0.5624) acc@5 0.7969 (0.7623)\n",
      "\u001b[32m[2020-06-26 21:33:28] __main__ INFO: \u001b[0mEpoch 76 Step 300/351 lr 0.000800 loss 1.0663 (1.1228) acc@1 0.5859 (0.5628) acc@5 0.7812 (0.7624)\n",
      "\u001b[32m[2020-06-26 21:34:25] __main__ INFO: \u001b[0mEpoch 76 Step 351/351 lr 0.000800 loss 1.2089 (1.1251) acc@1 0.5000 (0.5615) acc@5 0.7188 (0.7618)\n",
      "\u001b[32m[2020-06-26 21:34:25] __main__ INFO: \u001b[0mElapsed 392.79\n",
      "\u001b[32m[2020-06-26 21:34:25] __main__ INFO: \u001b[0mVal 76\n",
      "\u001b[32m[2020-06-26 21:34:39] __main__ INFO: \u001b[0mEpoch 76 loss 2.0643 acc@1 0.3308 acc@5 0.7106\n",
      "\u001b[32m[2020-06-26 21:34:39] __main__ INFO: \u001b[0mElapsed 13.14\n",
      "\u001b[32m[2020-06-26 21:34:39] __main__ INFO: \u001b[0mTrain 77 26676\n",
      "\u001b[32m[2020-06-26 21:36:30] __main__ INFO: \u001b[0mEpoch 77 Step 100/351 lr 0.000800 loss 1.3377 (1.1183) acc@1 0.5000 (0.5648) acc@5 0.7656 (0.7677)\n",
      "\u001b[32m[2020-06-26 21:38:22] __main__ INFO: \u001b[0mEpoch 77 Step 200/351 lr 0.000800 loss 0.9932 (1.1222) acc@1 0.5938 (0.5639) acc@5 0.7578 (0.7655)\n",
      "\u001b[32m[2020-06-26 21:40:14] __main__ INFO: \u001b[0mEpoch 77 Step 300/351 lr 0.000800 loss 1.1350 (1.1236) acc@1 0.5625 (0.5622) acc@5 0.7969 (0.7641)\n",
      "\u001b[32m[2020-06-26 21:41:11] __main__ INFO: \u001b[0mEpoch 77 Step 351/351 lr 0.000800 loss 1.0981 (1.1246) acc@1 0.5703 (0.5619) acc@5 0.7812 (0.7632)\n",
      "\u001b[32m[2020-06-26 21:41:11] __main__ INFO: \u001b[0mElapsed 392.77\n",
      "\u001b[32m[2020-06-26 21:41:11] __main__ INFO: \u001b[0mVal 77\n",
      "\u001b[32m[2020-06-26 21:41:24] __main__ INFO: \u001b[0mEpoch 77 loss 2.0741 acc@1 0.3330 acc@5 0.7110\n",
      "\u001b[32m[2020-06-26 21:41:24] __main__ INFO: \u001b[0mElapsed 13.15\n",
      "\u001b[32m[2020-06-26 21:41:24] __main__ INFO: \u001b[0mTrain 78 27027\n",
      "\u001b[32m[2020-06-26 21:43:16] __main__ INFO: \u001b[0mEpoch 78 Step 100/351 lr 0.000800 loss 1.0434 (1.1293) acc@1 0.6016 (0.5597) acc@5 0.8047 (0.7625)\n",
      "\u001b[32m[2020-06-26 21:45:08] __main__ INFO: \u001b[0mEpoch 78 Step 200/351 lr 0.000800 loss 1.2064 (1.1255) acc@1 0.5312 (0.5637) acc@5 0.7109 (0.7618)\n",
      "\u001b[32m[2020-06-26 21:47:00] __main__ INFO: \u001b[0mEpoch 78 Step 300/351 lr 0.000800 loss 1.1524 (1.1270) acc@1 0.5859 (0.5628) acc@5 0.7578 (0.7603)\n",
      "\u001b[32m[2020-06-26 21:47:57] __main__ INFO: \u001b[0mEpoch 78 Step 351/351 lr 0.000800 loss 1.2569 (1.1242) acc@1 0.5156 (0.5639) acc@5 0.7734 (0.7611)\n",
      "\u001b[32m[2020-06-26 21:47:57] __main__ INFO: \u001b[0mElapsed 392.82\n",
      "\u001b[32m[2020-06-26 21:47:57] __main__ INFO: \u001b[0mVal 78\n",
      "\u001b[32m[2020-06-26 21:48:10] __main__ INFO: \u001b[0mEpoch 78 loss 2.0697 acc@1 0.3338 acc@5 0.7102\n",
      "\u001b[32m[2020-06-26 21:48:10] __main__ INFO: \u001b[0mElapsed 13.14\n",
      "\u001b[32m[2020-06-26 21:48:10] __main__ INFO: \u001b[0mTrain 79 27378\n",
      "\u001b[32m[2020-06-26 21:50:02] __main__ INFO: \u001b[0mEpoch 79 Step 100/351 lr 0.000800 loss 1.1315 (1.1229) acc@1 0.5391 (0.5627) acc@5 0.7656 (0.7640)\n",
      "\u001b[32m[2020-06-26 21:51:54] __main__ INFO: \u001b[0mEpoch 79 Step 200/351 lr 0.000800 loss 1.0613 (1.1200) acc@1 0.5859 (0.5659) acc@5 0.7656 (0.7654)\n",
      "\u001b[32m[2020-06-26 21:53:46] __main__ INFO: \u001b[0mEpoch 79 Step 300/351 lr 0.000800 loss 1.1318 (1.1251) acc@1 0.5547 (0.5626) acc@5 0.7969 (0.7635)\n",
      "\u001b[32m[2020-06-26 21:54:43] __main__ INFO: \u001b[0mEpoch 79 Step 351/351 lr 0.000800 loss 1.3186 (1.1251) acc@1 0.5000 (0.5631) acc@5 0.7266 (0.7626)\n",
      "\u001b[32m[2020-06-26 21:54:43] __main__ INFO: \u001b[0mElapsed 392.98\n",
      "\u001b[32m[2020-06-26 21:54:43] __main__ INFO: \u001b[0mVal 79\n",
      "\u001b[32m[2020-06-26 21:54:57] __main__ INFO: \u001b[0mEpoch 79 loss 2.0687 acc@1 0.3356 acc@5 0.7120\n",
      "\u001b[32m[2020-06-26 21:54:57] __main__ INFO: \u001b[0mElapsed 13.13\n",
      "\u001b[32m[2020-06-26 21:54:57] __main__ INFO: \u001b[0mTrain 80 27729\n",
      "\u001b[32m[2020-06-26 21:56:49] __main__ INFO: \u001b[0mEpoch 80 Step 100/351 lr 0.000800 loss 1.1199 (1.1265) acc@1 0.5625 (0.5639) acc@5 0.7500 (0.7600)\n",
      "\u001b[32m[2020-06-26 21:58:40] __main__ INFO: \u001b[0mEpoch 80 Step 200/351 lr 0.000800 loss 0.9343 (1.1177) acc@1 0.6562 (0.5653) acc@5 0.8047 (0.7624)\n",
      "\u001b[32m[2020-06-26 22:00:32] __main__ INFO: \u001b[0mEpoch 80 Step 300/351 lr 0.000800 loss 1.1144 (1.1264) acc@1 0.5703 (0.5619) acc@5 0.7344 (0.7627)\n",
      "\u001b[32m[2020-06-26 22:01:29] __main__ INFO: \u001b[0mEpoch 80 Step 351/351 lr 0.000800 loss 1.0833 (1.1244) acc@1 0.5547 (0.5631) acc@5 0.7109 (0.7626)\n",
      "\u001b[32m[2020-06-26 22:01:29] __main__ INFO: \u001b[0mElapsed 392.75\n",
      "\u001b[32m[2020-06-26 22:01:29] __main__ INFO: \u001b[0mVal 80\n",
      "\u001b[32m[2020-06-26 22:01:42] __main__ INFO: \u001b[0mEpoch 80 loss 2.0656 acc@1 0.3350 acc@5 0.7126\n",
      "\u001b[32m[2020-06-26 22:01:42] __main__ INFO: \u001b[0mElapsed 13.13\n",
      "\u001b[32m[2020-06-26 22:01:42] __main__ INFO: \u001b[0mTrain 81 28080\n",
      "\u001b[32m[2020-06-26 22:03:34] __main__ INFO: \u001b[0mEpoch 81 Step 100/351 lr 0.000800 loss 1.1121 (1.1048) acc@1 0.6016 (0.5710) acc@5 0.7969 (0.7730)\n",
      "\u001b[32m[2020-06-26 22:05:26] __main__ INFO: \u001b[0mEpoch 81 Step 200/351 lr 0.000800 loss 1.0031 (1.1183) acc@1 0.6328 (0.5661) acc@5 0.7812 (0.7668)\n",
      "\u001b[32m[2020-06-26 22:07:18] __main__ INFO: \u001b[0mEpoch 81 Step 300/351 lr 0.000800 loss 1.0265 (1.1250) acc@1 0.5781 (0.5627) acc@5 0.7891 (0.7638)\n",
      "\u001b[32m[2020-06-26 22:08:15] __main__ INFO: \u001b[0mEpoch 81 Step 351/351 lr 0.000800 loss 1.1931 (1.1248) acc@1 0.5312 (0.5627) acc@5 0.7109 (0.7628)\n",
      "\u001b[32m[2020-06-26 22:08:15] __main__ INFO: \u001b[0mElapsed 392.75\n",
      "\u001b[32m[2020-06-26 22:08:15] __main__ INFO: \u001b[0mVal 81\n",
      "\u001b[32m[2020-06-26 22:08:28] __main__ INFO: \u001b[0mEpoch 81 loss 2.0616 acc@1 0.3332 acc@5 0.7118\n",
      "\u001b[32m[2020-06-26 22:08:28] __main__ INFO: \u001b[0mElapsed 13.12\n",
      "\u001b[32m[2020-06-26 22:08:28] __main__ INFO: \u001b[0mTrain 82 28431\n",
      "\u001b[32m[2020-06-26 22:10:20] __main__ INFO: \u001b[0mEpoch 82 Step 100/351 lr 0.000800 loss 1.2964 (1.1326) acc@1 0.5000 (0.5623) acc@5 0.7422 (0.7594)\n",
      "\u001b[32m[2020-06-26 22:12:12] __main__ INFO: \u001b[0mEpoch 82 Step 200/351 lr 0.000800 loss 1.2577 (1.1286) acc@1 0.5078 (0.5626) acc@5 0.7344 (0.7600)\n",
      "\u001b[32m[2020-06-26 22:14:04] __main__ INFO: \u001b[0mEpoch 82 Step 300/351 lr 0.000800 loss 1.2901 (1.1256) acc@1 0.4922 (0.5641) acc@5 0.7344 (0.7620)\n",
      "\u001b[32m[2020-06-26 22:15:01] __main__ INFO: \u001b[0mEpoch 82 Step 351/351 lr 0.000800 loss 1.0992 (1.1243) acc@1 0.5703 (0.5639) acc@5 0.7812 (0.7626)\n",
      "\u001b[32m[2020-06-26 22:15:01] __main__ INFO: \u001b[0mElapsed 392.91\n",
      "\u001b[32m[2020-06-26 22:15:01] __main__ INFO: \u001b[0mVal 82\n",
      "\u001b[32m[2020-06-26 22:15:14] __main__ INFO: \u001b[0mEpoch 82 loss 2.0647 acc@1 0.3320 acc@5 0.7106\n",
      "\u001b[32m[2020-06-26 22:15:14] __main__ INFO: \u001b[0mElapsed 13.16\n",
      "\u001b[32m[2020-06-26 22:15:14] __main__ INFO: \u001b[0mTrain 83 28782\n",
      "\u001b[32m[2020-06-26 22:17:06] __main__ INFO: \u001b[0mEpoch 83 Step 100/351 lr 0.000800 loss 1.3886 (1.1263) acc@1 0.4531 (0.5609) acc@5 0.7266 (0.7655)\n",
      "\u001b[32m[2020-06-26 22:18:58] __main__ INFO: \u001b[0mEpoch 83 Step 200/351 lr 0.000800 loss 1.1888 (1.1229) acc@1 0.5156 (0.5634) acc@5 0.7266 (0.7649)\n",
      "\u001b[32m[2020-06-26 22:20:50] __main__ INFO: \u001b[0mEpoch 83 Step 300/351 lr 0.000800 loss 1.2715 (1.1268) acc@1 0.5234 (0.5616) acc@5 0.7500 (0.7631)\n",
      "\u001b[32m[2020-06-26 22:21:47] __main__ INFO: \u001b[0mEpoch 83 Step 351/351 lr 0.000800 loss 1.0598 (1.1248) acc@1 0.6094 (0.5627) acc@5 0.8203 (0.7633)\n",
      "\u001b[32m[2020-06-26 22:21:47] __main__ INFO: \u001b[0mElapsed 392.70\n",
      "\u001b[32m[2020-06-26 22:21:47] __main__ INFO: \u001b[0mVal 83\n",
      "\u001b[32m[2020-06-26 22:22:00] __main__ INFO: \u001b[0mEpoch 83 loss 2.0562 acc@1 0.3308 acc@5 0.7114\n",
      "\u001b[32m[2020-06-26 22:22:00] __main__ INFO: \u001b[0mElapsed 13.17\n",
      "\u001b[32m[2020-06-26 22:22:00] __main__ INFO: \u001b[0mTrain 84 29133\n",
      "\u001b[32m[2020-06-26 22:23:52] __main__ INFO: \u001b[0mEpoch 84 Step 100/351 lr 0.000800 loss 1.2068 (1.1182) acc@1 0.5391 (0.5656) acc@5 0.7734 (0.7644)\n",
      "\u001b[32m[2020-06-26 22:25:44] __main__ INFO: \u001b[0mEpoch 84 Step 200/351 lr 0.000800 loss 0.9383 (1.1247) acc@1 0.6562 (0.5637) acc@5 0.8203 (0.7627)\n",
      "\u001b[32m[2020-06-26 22:27:36] __main__ INFO: \u001b[0mEpoch 84 Step 300/351 lr 0.000800 loss 1.0278 (1.1250) acc@1 0.6094 (0.5639) acc@5 0.7734 (0.7633)\n",
      "\u001b[32m[2020-06-26 22:28:33] __main__ INFO: \u001b[0mEpoch 84 Step 351/351 lr 0.000800 loss 1.1551 (1.1244) acc@1 0.5312 (0.5637) acc@5 0.7500 (0.7631)\n",
      "\u001b[32m[2020-06-26 22:28:33] __main__ INFO: \u001b[0mElapsed 392.72\n",
      "\u001b[32m[2020-06-26 22:28:33] __main__ INFO: \u001b[0mVal 84\n",
      "\u001b[32m[2020-06-26 22:28:46] __main__ INFO: \u001b[0mEpoch 84 loss 2.0745 acc@1 0.3316 acc@5 0.7108\n",
      "\u001b[32m[2020-06-26 22:28:46] __main__ INFO: \u001b[0mElapsed 13.14\n",
      "\u001b[32m[2020-06-26 22:28:46] __main__ INFO: \u001b[0mTrain 85 29484\n",
      "\u001b[32m[2020-06-26 22:30:38] __main__ INFO: \u001b[0mEpoch 85 Step 100/351 lr 0.000800 loss 1.2044 (1.1304) acc@1 0.5391 (0.5570) acc@5 0.7578 (0.7628)\n",
      "\u001b[32m[2020-06-26 22:32:30] __main__ INFO: \u001b[0mEpoch 85 Step 200/351 lr 0.000800 loss 1.0437 (1.1223) acc@1 0.6094 (0.5644) acc@5 0.8047 (0.7648)\n",
      "\u001b[32m[2020-06-26 22:34:22] __main__ INFO: \u001b[0mEpoch 85 Step 300/351 lr 0.000800 loss 1.1724 (1.1253) acc@1 0.5391 (0.5630) acc@5 0.7578 (0.7633)\n",
      "\u001b[32m[2020-06-26 22:35:19] __main__ INFO: \u001b[0mEpoch 85 Step 351/351 lr 0.000800 loss 1.2104 (1.1245) acc@1 0.5469 (0.5628) acc@5 0.7891 (0.7626)\n",
      "\u001b[32m[2020-06-26 22:35:19] __main__ INFO: \u001b[0mElapsed 392.68\n",
      "\u001b[32m[2020-06-26 22:35:19] __main__ INFO: \u001b[0mVal 85\n",
      "\u001b[32m[2020-06-26 22:35:32] __main__ INFO: \u001b[0mEpoch 85 loss 2.0683 acc@1 0.3340 acc@5 0.7092\n",
      "\u001b[32m[2020-06-26 22:35:32] __main__ INFO: \u001b[0mElapsed 13.13\n",
      "\u001b[32m[2020-06-26 22:35:32] __main__ INFO: \u001b[0mTrain 86 29835\n",
      "\u001b[32m[2020-06-26 22:37:24] __main__ INFO: \u001b[0mEpoch 86 Step 100/351 lr 0.000800 loss 0.9207 (1.1249) acc@1 0.6484 (0.5632) acc@5 0.7969 (0.7605)\n",
      "\u001b[32m[2020-06-26 22:39:16] __main__ INFO: \u001b[0mEpoch 86 Step 200/351 lr 0.000800 loss 1.1892 (1.1236) acc@1 0.5312 (0.5648) acc@5 0.7812 (0.7633)\n",
      "\u001b[32m[2020-06-26 22:41:07] __main__ INFO: \u001b[0mEpoch 86 Step 300/351 lr 0.000800 loss 1.1139 (1.1246) acc@1 0.5703 (0.5638) acc@5 0.7656 (0.7615)\n",
      "\u001b[32m[2020-06-26 22:42:04] __main__ INFO: \u001b[0mEpoch 86 Step 351/351 lr 0.000800 loss 1.1735 (1.1238) acc@1 0.5547 (0.5640) acc@5 0.7109 (0.7626)\n",
      "\u001b[32m[2020-06-26 22:42:04] __main__ INFO: \u001b[0mElapsed 392.59\n",
      "\u001b[32m[2020-06-26 22:42:04] __main__ INFO: \u001b[0mVal 86\n",
      "\u001b[32m[2020-06-26 22:42:18] __main__ INFO: \u001b[0mEpoch 86 loss 2.0622 acc@1 0.3310 acc@5 0.7104\n",
      "\u001b[32m[2020-06-26 22:42:18] __main__ INFO: \u001b[0mElapsed 13.17\n",
      "\u001b[32m[2020-06-26 22:42:18] __main__ INFO: \u001b[0mTrain 87 30186\n",
      "\u001b[32m[2020-06-26 22:44:10] __main__ INFO: \u001b[0mEpoch 87 Step 100/351 lr 0.000800 loss 1.1682 (1.1313) acc@1 0.5391 (0.5612) acc@5 0.7500 (0.7591)\n",
      "\u001b[32m[2020-06-26 22:46:01] __main__ INFO: \u001b[0mEpoch 87 Step 200/351 lr 0.000800 loss 1.1495 (1.1205) acc@1 0.6016 (0.5656) acc@5 0.8281 (0.7626)\n",
      "\u001b[32m[2020-06-26 22:47:53] __main__ INFO: \u001b[0mEpoch 87 Step 300/351 lr 0.000800 loss 1.1548 (1.1239) acc@1 0.5156 (0.5642) acc@5 0.7344 (0.7629)\n",
      "\u001b[32m[2020-06-26 22:48:50] __main__ INFO: \u001b[0mEpoch 87 Step 351/351 lr 0.000800 loss 1.2236 (1.1245) acc@1 0.5234 (0.5645) acc@5 0.7500 (0.7627)\n",
      "\u001b[32m[2020-06-26 22:48:50] __main__ INFO: \u001b[0mElapsed 392.61\n",
      "\u001b[32m[2020-06-26 22:48:50] __main__ INFO: \u001b[0mVal 87\n",
      "\u001b[32m[2020-06-26 22:49:03] __main__ INFO: \u001b[0mEpoch 87 loss 2.0679 acc@1 0.3332 acc@5 0.7108\n",
      "\u001b[32m[2020-06-26 22:49:03] __main__ INFO: \u001b[0mElapsed 13.14\n",
      "\u001b[32m[2020-06-26 22:49:03] __main__ INFO: \u001b[0mTrain 88 30537\n",
      "\u001b[32m[2020-06-26 22:50:55] __main__ INFO: \u001b[0mEpoch 88 Step 100/351 lr 0.000800 loss 1.1337 (1.1170) acc@1 0.5625 (0.5676) acc@5 0.7422 (0.7683)\n",
      "\u001b[32m[2020-06-26 22:52:47] __main__ INFO: \u001b[0mEpoch 88 Step 200/351 lr 0.000800 loss 1.2381 (1.1157) acc@1 0.5156 (0.5679) acc@5 0.7500 (0.7671)\n",
      "\u001b[32m[2020-06-26 22:54:39] __main__ INFO: \u001b[0mEpoch 88 Step 300/351 lr 0.000800 loss 1.1447 (1.1236) acc@1 0.5625 (0.5639) acc@5 0.7656 (0.7642)\n",
      "\u001b[32m[2020-06-26 22:55:36] __main__ INFO: \u001b[0mEpoch 88 Step 351/351 lr 0.000800 loss 1.2260 (1.1245) acc@1 0.5156 (0.5632) acc@5 0.6875 (0.7623)\n",
      "\u001b[32m[2020-06-26 22:55:36] __main__ INFO: \u001b[0mElapsed 392.68\n",
      "\u001b[32m[2020-06-26 22:55:36] __main__ INFO: \u001b[0mVal 88\n",
      "\u001b[32m[2020-06-26 22:55:49] __main__ INFO: \u001b[0mEpoch 88 loss 2.0681 acc@1 0.3308 acc@5 0.7100\n",
      "\u001b[32m[2020-06-26 22:55:49] __main__ INFO: \u001b[0mElapsed 13.15\n",
      "\u001b[32m[2020-06-26 22:55:49] __main__ INFO: \u001b[0mTrain 89 30888\n",
      "\u001b[32m[2020-06-26 22:57:41] __main__ INFO: \u001b[0mEpoch 89 Step 100/351 lr 0.000800 loss 1.0501 (1.1182) acc@1 0.5781 (0.5637) acc@5 0.7578 (0.7676)\n",
      "\u001b[32m[2020-06-26 22:59:33] __main__ INFO: \u001b[0mEpoch 89 Step 200/351 lr 0.000800 loss 1.1701 (1.1233) acc@1 0.5547 (0.5622) acc@5 0.7734 (0.7644)\n",
      "\u001b[32m[2020-06-26 23:01:25] __main__ INFO: \u001b[0mEpoch 89 Step 300/351 lr 0.000800 loss 1.2017 (1.1263) acc@1 0.5156 (0.5608) acc@5 0.7734 (0.7639)\n",
      "\u001b[32m[2020-06-26 23:02:22] __main__ INFO: \u001b[0mEpoch 89 Step 351/351 lr 0.000800 loss 1.1141 (1.1242) acc@1 0.5938 (0.5619) acc@5 0.7656 (0.7642)\n",
      "\u001b[32m[2020-06-26 23:02:22] __main__ INFO: \u001b[0mElapsed 392.78\n",
      "\u001b[32m[2020-06-26 23:02:22] __main__ INFO: \u001b[0mVal 89\n",
      "\u001b[32m[2020-06-26 23:02:35] __main__ INFO: \u001b[0mEpoch 89 loss 2.0624 acc@1 0.3328 acc@5 0.7096\n",
      "\u001b[32m[2020-06-26 23:02:35] __main__ INFO: \u001b[0mElapsed 13.15\n",
      "\u001b[32m[2020-06-26 23:02:35] __main__ INFO: \u001b[0mTrain 90 31239\n",
      "\u001b[32m[2020-06-26 23:04:27] __main__ INFO: \u001b[0mEpoch 90 Step 100/351 lr 0.000800 loss 1.0980 (1.1163) acc@1 0.5703 (0.5669) acc@5 0.7266 (0.7673)\n",
      "\u001b[32m[2020-06-26 23:06:19] __main__ INFO: \u001b[0mEpoch 90 Step 200/351 lr 0.000800 loss 1.2877 (1.1167) acc@1 0.5156 (0.5666) acc@5 0.7656 (0.7671)\n",
      "\u001b[32m[2020-06-26 23:08:11] __main__ INFO: \u001b[0mEpoch 90 Step 300/351 lr 0.000800 loss 1.1010 (1.1221) acc@1 0.5781 (0.5641) acc@5 0.7188 (0.7651)\n",
      "\u001b[32m[2020-06-26 23:09:08] __main__ INFO: \u001b[0mEpoch 90 Step 351/351 lr 0.000800 loss 1.1708 (1.1247) acc@1 0.5312 (0.5635) acc@5 0.7500 (0.7642)\n",
      "\u001b[32m[2020-06-26 23:09:08] __main__ INFO: \u001b[0mElapsed 392.70\n",
      "\u001b[32m[2020-06-26 23:09:08] __main__ INFO: \u001b[0mVal 90\n",
      "\u001b[32m[2020-06-26 23:09:21] __main__ INFO: \u001b[0mEpoch 90 loss 2.0650 acc@1 0.3308 acc@5 0.7098\n",
      "\u001b[32m[2020-06-26 23:09:21] __main__ INFO: \u001b[0mElapsed 13.11\n",
      "\u001b[32m[2020-06-26 23:09:21] __main__ INFO: \u001b[0mTrain 91 31590\n",
      "\u001b[32m[2020-06-26 23:11:13] __main__ INFO: \u001b[0mEpoch 91 Step 100/351 lr 0.000800 loss 1.0278 (1.1374) acc@1 0.5781 (0.5590) acc@5 0.7422 (0.7623)\n",
      "\u001b[32m[2020-06-26 23:13:05] __main__ INFO: \u001b[0mEpoch 91 Step 200/351 lr 0.000800 loss 0.9850 (1.1242) acc@1 0.6484 (0.5638) acc@5 0.8125 (0.7630)\n",
      "\u001b[32m[2020-06-26 23:14:57] __main__ INFO: \u001b[0mEpoch 91 Step 300/351 lr 0.000800 loss 1.2743 (1.1249) acc@1 0.5234 (0.5623) acc@5 0.7578 (0.7629)\n",
      "\u001b[32m[2020-06-26 23:15:54] __main__ INFO: \u001b[0mEpoch 91 Step 351/351 lr 0.000800 loss 1.1728 (1.1248) acc@1 0.5312 (0.5621) acc@5 0.7656 (0.7623)\n",
      "\u001b[32m[2020-06-26 23:15:54] __main__ INFO: \u001b[0mElapsed 392.69\n",
      "\u001b[32m[2020-06-26 23:15:54] __main__ INFO: \u001b[0mVal 91\n",
      "\u001b[32m[2020-06-26 23:16:07] __main__ INFO: \u001b[0mEpoch 91 loss 2.0642 acc@1 0.3316 acc@5 0.7094\n",
      "\u001b[32m[2020-06-26 23:16:07] __main__ INFO: \u001b[0mElapsed 13.11\n",
      "\u001b[32m[2020-06-26 23:16:07] __main__ INFO: \u001b[0mTrain 92 31941\n",
      "\u001b[32m[2020-06-26 23:17:59] __main__ INFO: \u001b[0mEpoch 92 Step 100/351 lr 0.000800 loss 1.2022 (1.1143) acc@1 0.5156 (0.5647) acc@5 0.7344 (0.7648)\n",
      "\u001b[32m[2020-06-26 23:19:51] __main__ INFO: \u001b[0mEpoch 92 Step 200/351 lr 0.000800 loss 1.1663 (1.1179) acc@1 0.6016 (0.5641) acc@5 0.8047 (0.7631)\n",
      "\u001b[32m[2020-06-26 23:21:42] __main__ INFO: \u001b[0mEpoch 92 Step 300/351 lr 0.000800 loss 1.0376 (1.1226) acc@1 0.6094 (0.5627) acc@5 0.8203 (0.7618)\n",
      "\u001b[32m[2020-06-26 23:22:39] __main__ INFO: \u001b[0mEpoch 92 Step 351/351 lr 0.000800 loss 1.2052 (1.1244) acc@1 0.5547 (0.5627) acc@5 0.7344 (0.7619)\n",
      "\u001b[32m[2020-06-26 23:22:39] __main__ INFO: \u001b[0mElapsed 392.53\n",
      "\u001b[32m[2020-06-26 23:22:39] __main__ INFO: \u001b[0mVal 92\n",
      "\u001b[32m[2020-06-26 23:22:53] __main__ INFO: \u001b[0mEpoch 92 loss 2.0645 acc@1 0.3322 acc@5 0.7098\n",
      "\u001b[32m[2020-06-26 23:22:53] __main__ INFO: \u001b[0mElapsed 13.18\n",
      "\u001b[32m[2020-06-26 23:22:53] __main__ INFO: \u001b[0mTrain 93 32292\n",
      "\u001b[32m[2020-06-26 23:24:44] __main__ INFO: \u001b[0mEpoch 93 Step 100/351 lr 0.000800 loss 1.0638 (1.1194) acc@1 0.5859 (0.5646) acc@5 0.8047 (0.7666)\n",
      "\u001b[32m[2020-06-26 23:26:36] __main__ INFO: \u001b[0mEpoch 93 Step 200/351 lr 0.000800 loss 1.3284 (1.1217) acc@1 0.4844 (0.5650) acc@5 0.7031 (0.7661)\n",
      "\u001b[32m[2020-06-26 23:28:28] __main__ INFO: \u001b[0mEpoch 93 Step 300/351 lr 0.000800 loss 1.1276 (1.1283) acc@1 0.5781 (0.5619) acc@5 0.7656 (0.7626)\n",
      "\u001b[32m[2020-06-26 23:29:25] __main__ INFO: \u001b[0mEpoch 93 Step 351/351 lr 0.000800 loss 1.1530 (1.1241) acc@1 0.5703 (0.5630) acc@5 0.7891 (0.7630)\n",
      "\u001b[32m[2020-06-26 23:29:25] __main__ INFO: \u001b[0mElapsed 392.55\n",
      "\u001b[32m[2020-06-26 23:29:25] __main__ INFO: \u001b[0mVal 93\n",
      "\u001b[32m[2020-06-26 23:29:38] __main__ INFO: \u001b[0mEpoch 93 loss 2.0631 acc@1 0.3320 acc@5 0.7102\n",
      "\u001b[32m[2020-06-26 23:29:38] __main__ INFO: \u001b[0mElapsed 13.14\n",
      "\u001b[32m[2020-06-26 23:29:38] __main__ INFO: \u001b[0mTrain 94 32643\n",
      "\u001b[32m[2020-06-26 23:31:30] __main__ INFO: \u001b[0mEpoch 94 Step 100/351 lr 0.000800 loss 1.1164 (1.1124) acc@1 0.5625 (0.5689) acc@5 0.7656 (0.7662)\n",
      "\u001b[32m[2020-06-26 23:33:22] __main__ INFO: \u001b[0mEpoch 94 Step 200/351 lr 0.000800 loss 1.0972 (1.1207) acc@1 0.5625 (0.5643) acc@5 0.7500 (0.7637)\n",
      "\u001b[32m[2020-06-26 23:35:14] __main__ INFO: \u001b[0mEpoch 94 Step 300/351 lr 0.000800 loss 1.1912 (1.1227) acc@1 0.5391 (0.5644) acc@5 0.7422 (0.7630)\n",
      "\u001b[32m[2020-06-26 23:36:11] __main__ INFO: \u001b[0mEpoch 94 Step 351/351 lr 0.000800 loss 1.2554 (1.1244) acc@1 0.5156 (0.5628) acc@5 0.7344 (0.7624)\n",
      "\u001b[32m[2020-06-26 23:36:11] __main__ INFO: \u001b[0mElapsed 392.45\n",
      "\u001b[32m[2020-06-26 23:36:11] __main__ INFO: \u001b[0mVal 94\n",
      "\u001b[32m[2020-06-26 23:36:24] __main__ INFO: \u001b[0mEpoch 94 loss 2.0614 acc@1 0.3322 acc@5 0.7102\n",
      "\u001b[32m[2020-06-26 23:36:24] __main__ INFO: \u001b[0mElapsed 13.11\n",
      "\u001b[32m[2020-06-26 23:36:24] __main__ INFO: \u001b[0mTrain 95 32994\n",
      "\u001b[32m[2020-06-26 23:38:16] __main__ INFO: \u001b[0mEpoch 95 Step 100/351 lr 0.000800 loss 1.0092 (1.1240) acc@1 0.6016 (0.5645) acc@5 0.7891 (0.7623)\n",
      "\u001b[32m[2020-06-26 23:40:07] __main__ INFO: \u001b[0mEpoch 95 Step 200/351 lr 0.000800 loss 0.9169 (1.1223) acc@1 0.6641 (0.5657) acc@5 0.7891 (0.7637)\n",
      "\u001b[32m[2020-06-26 23:41:59] __main__ INFO: \u001b[0mEpoch 95 Step 300/351 lr 0.000800 loss 1.2066 (1.1245) acc@1 0.5547 (0.5638) acc@5 0.7188 (0.7620)\n",
      "\u001b[32m[2020-06-26 23:42:56] __main__ INFO: \u001b[0mEpoch 95 Step 351/351 lr 0.000800 loss 1.2991 (1.1241) acc@1 0.5000 (0.5639) acc@5 0.7422 (0.7617)\n",
      "\u001b[32m[2020-06-26 23:42:56] __main__ INFO: \u001b[0mElapsed 392.55\n",
      "\u001b[32m[2020-06-26 23:42:56] __main__ INFO: \u001b[0mVal 95\n",
      "\u001b[32m[2020-06-26 23:43:09] __main__ INFO: \u001b[0mEpoch 95 loss 2.0666 acc@1 0.3336 acc@5 0.7088\n",
      "\u001b[32m[2020-06-26 23:43:09] __main__ INFO: \u001b[0mElapsed 13.14\n",
      "\u001b[32m[2020-06-26 23:43:09] __main__ INFO: \u001b[0mTrain 96 33345\n",
      "\u001b[32m[2020-06-26 23:45:01] __main__ INFO: \u001b[0mEpoch 96 Step 100/351 lr 0.000800 loss 1.0903 (1.1287) acc@1 0.5703 (0.5618) acc@5 0.7422 (0.7629)\n",
      "\u001b[32m[2020-06-26 23:46:53] __main__ INFO: \u001b[0mEpoch 96 Step 200/351 lr 0.000800 loss 1.1531 (1.1368) acc@1 0.5547 (0.5573) acc@5 0.7656 (0.7595)\n",
      "\u001b[32m[2020-06-26 23:48:45] __main__ INFO: \u001b[0mEpoch 96 Step 300/351 lr 0.000800 loss 0.9746 (1.1275) acc@1 0.6172 (0.5614) acc@5 0.7734 (0.7612)\n",
      "\u001b[32m[2020-06-26 23:49:42] __main__ INFO: \u001b[0mEpoch 96 Step 351/351 lr 0.000800 loss 1.2584 (1.1242) acc@1 0.5391 (0.5633) acc@5 0.7266 (0.7629)\n",
      "\u001b[32m[2020-06-26 23:49:42] __main__ INFO: \u001b[0mElapsed 392.55\n",
      "\u001b[32m[2020-06-26 23:49:42] __main__ INFO: \u001b[0mVal 96\n",
      "\u001b[32m[2020-06-26 23:49:55] __main__ INFO: \u001b[0mEpoch 96 loss 2.0684 acc@1 0.3332 acc@5 0.7098\n",
      "\u001b[32m[2020-06-26 23:49:55] __main__ INFO: \u001b[0mElapsed 13.13\n",
      "\u001b[32m[2020-06-26 23:49:55] __main__ INFO: \u001b[0mTrain 97 33696\n",
      "\u001b[32m[2020-06-26 23:51:47] __main__ INFO: \u001b[0mEpoch 97 Step 100/351 lr 0.000800 loss 1.1690 (1.1228) acc@1 0.5547 (0.5638) acc@5 0.7500 (0.7641)\n",
      "\u001b[32m[2020-06-26 23:53:39] __main__ INFO: \u001b[0mEpoch 97 Step 200/351 lr 0.000800 loss 1.2820 (1.1304) acc@1 0.4766 (0.5608) acc@5 0.6719 (0.7597)\n",
      "\u001b[32m[2020-06-26 23:55:31] __main__ INFO: \u001b[0mEpoch 97 Step 300/351 lr 0.000800 loss 1.0567 (1.1237) acc@1 0.5859 (0.5634) acc@5 0.8438 (0.7615)\n",
      "\u001b[32m[2020-06-26 23:56:28] __main__ INFO: \u001b[0mEpoch 97 Step 351/351 lr 0.000800 loss 1.2040 (1.1242) acc@1 0.5156 (0.5637) acc@5 0.7266 (0.7623)\n",
      "\u001b[32m[2020-06-26 23:56:28] __main__ INFO: \u001b[0mElapsed 392.55\n",
      "\u001b[32m[2020-06-26 23:56:28] __main__ INFO: \u001b[0mVal 97\n",
      "\u001b[32m[2020-06-26 23:56:41] __main__ INFO: \u001b[0mEpoch 97 loss 2.0716 acc@1 0.3324 acc@5 0.7102\n",
      "\u001b[32m[2020-06-26 23:56:41] __main__ INFO: \u001b[0mElapsed 13.13\n",
      "\u001b[32m[2020-06-26 23:56:41] __main__ INFO: \u001b[0mTrain 98 34047\n",
      "\u001b[32m[2020-06-26 23:58:33] __main__ INFO: \u001b[0mEpoch 98 Step 100/351 lr 0.000800 loss 1.2943 (1.1211) acc@1 0.5156 (0.5663) acc@5 0.7266 (0.7636)\n",
      "\u001b[32m[2020-06-27 00:00:25] __main__ INFO: \u001b[0mEpoch 98 Step 200/351 lr 0.000800 loss 1.2409 (1.1267) acc@1 0.5391 (0.5626) acc@5 0.7109 (0.7603)\n",
      "\u001b[32m[2020-06-27 00:02:16] __main__ INFO: \u001b[0mEpoch 98 Step 300/351 lr 0.000800 loss 0.9232 (1.1242) acc@1 0.6406 (0.5639) acc@5 0.8203 (0.7616)\n",
      "\u001b[32m[2020-06-27 00:03:13] __main__ INFO: \u001b[0mEpoch 98 Step 351/351 lr 0.000800 loss 1.0787 (1.1247) acc@1 0.6016 (0.5635) acc@5 0.8359 (0.7624)\n",
      "\u001b[32m[2020-06-27 00:03:13] __main__ INFO: \u001b[0mElapsed 392.59\n",
      "\u001b[32m[2020-06-27 00:03:13] __main__ INFO: \u001b[0mVal 98\n",
      "\u001b[32m[2020-06-27 00:03:27] __main__ INFO: \u001b[0mEpoch 98 loss 2.0746 acc@1 0.3348 acc@5 0.7110\n",
      "\u001b[32m[2020-06-27 00:03:27] __main__ INFO: \u001b[0mElapsed 13.12\n",
      "\u001b[32m[2020-06-27 00:03:27] __main__ INFO: \u001b[0mTrain 99 34398\n",
      "\u001b[32m[2020-06-27 00:05:18] __main__ INFO: \u001b[0mEpoch 99 Step 100/351 lr 0.000800 loss 1.2050 (1.1157) acc@1 0.5312 (0.5657) acc@5 0.7969 (0.7686)\n",
      "\u001b[32m[2020-06-27 00:07:10] __main__ INFO: \u001b[0mEpoch 99 Step 200/351 lr 0.000800 loss 0.9042 (1.1279) acc@1 0.6406 (0.5604) acc@5 0.7812 (0.7624)\n",
      "\u001b[32m[2020-06-27 00:09:02] __main__ INFO: \u001b[0mEpoch 99 Step 300/351 lr 0.000800 loss 1.0099 (1.1274) acc@1 0.6016 (0.5620) acc@5 0.7656 (0.7616)\n",
      "\u001b[32m[2020-06-27 00:09:59] __main__ INFO: \u001b[0mEpoch 99 Step 351/351 lr 0.000800 loss 1.0993 (1.1251) acc@1 0.5781 (0.5628) acc@5 0.7656 (0.7616)\n",
      "\u001b[32m[2020-06-27 00:09:59] __main__ INFO: \u001b[0mElapsed 392.60\n",
      "\u001b[32m[2020-06-27 00:09:59] __main__ INFO: \u001b[0mVal 99\n",
      "\u001b[32m[2020-06-27 00:10:12] __main__ INFO: \u001b[0mEpoch 99 loss 2.0555 acc@1 0.3308 acc@5 0.7116\n",
      "\u001b[32m[2020-06-27 00:10:12] __main__ INFO: \u001b[0mElapsed 13.12\n",
      "\u001b[32m[2020-06-27 00:10:12] __main__ INFO: \u001b[0mTrain 100 34749\n",
      "\u001b[32m[2020-06-27 00:12:04] __main__ INFO: \u001b[0mEpoch 100 Step 100/351 lr 0.000800 loss 0.9896 (1.1238) acc@1 0.6250 (0.5627) acc@5 0.7969 (0.7630)\n",
      "\u001b[32m[2020-06-27 00:13:56] __main__ INFO: \u001b[0mEpoch 100 Step 200/351 lr 0.000800 loss 1.1523 (1.1247) acc@1 0.5312 (0.5636) acc@5 0.7500 (0.7629)\n",
      "\u001b[32m[2020-06-27 00:15:48] __main__ INFO: \u001b[0mEpoch 100 Step 300/351 lr 0.000800 loss 1.0992 (1.1256) acc@1 0.5625 (0.5628) acc@5 0.7656 (0.7633)\n",
      "\u001b[32m[2020-06-27 00:16:45] __main__ INFO: \u001b[0mEpoch 100 Step 351/351 lr 0.000800 loss 1.0253 (1.1240) acc@1 0.5703 (0.5632) acc@5 0.7656 (0.7628)\n",
      "\u001b[32m[2020-06-27 00:16:45] __main__ INFO: \u001b[0mElapsed 392.66\n",
      "\u001b[32m[2020-06-27 00:16:45] __main__ INFO: \u001b[0mVal 100\n",
      "\u001b[32m[2020-06-27 00:16:58] __main__ INFO: \u001b[0mEpoch 100 loss 2.0639 acc@1 0.3304 acc@5 0.7102\n",
      "\u001b[32m[2020-06-27 00:16:58] __main__ INFO: \u001b[0mElapsed 13.15\n",
      "\u001b[32m[2020-06-27 00:16:58] fvcore.common.checkpoint INFO: \u001b[0mSaving checkpoint to /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_3_20/exp02/checkpoint_00100.pth\n",
      "\u001b[32m[2020-06-27 00:16:58] __main__ INFO: \u001b[0mTrain 101 35100\n",
      "\u001b[32m[2020-06-27 00:18:50] __main__ INFO: \u001b[0mEpoch 101 Step 100/351 lr 0.000800 loss 0.9889 (1.1243) acc@1 0.6094 (0.5634) acc@5 0.7969 (0.7623)\n",
      "\u001b[32m[2020-06-27 00:20:42] __main__ INFO: \u001b[0mEpoch 101 Step 200/351 lr 0.000800 loss 1.1845 (1.1203) acc@1 0.5391 (0.5648) acc@5 0.7969 (0.7620)\n",
      "\u001b[32m[2020-06-27 00:22:34] __main__ INFO: \u001b[0mEpoch 101 Step 300/351 lr 0.000800 loss 1.2026 (1.1254) acc@1 0.5234 (0.5628) acc@5 0.7734 (0.7615)\n",
      "\u001b[32m[2020-06-27 00:23:31] __main__ INFO: \u001b[0mEpoch 101 Step 351/351 lr 0.000800 loss 1.0996 (1.1245) acc@1 0.5703 (0.5632) acc@5 0.7422 (0.7636)\n",
      "\u001b[32m[2020-06-27 00:23:31] __main__ INFO: \u001b[0mElapsed 392.58\n",
      "\u001b[32m[2020-06-27 00:23:31] __main__ INFO: \u001b[0mVal 101\n",
      "\u001b[32m[2020-06-27 00:23:44] __main__ INFO: \u001b[0mEpoch 101 loss 2.0642 acc@1 0.3298 acc@5 0.7122\n",
      "\u001b[32m[2020-06-27 00:23:44] __main__ INFO: \u001b[0mElapsed 13.12\n",
      "\u001b[32m[2020-06-27 00:23:44] __main__ INFO: \u001b[0mTrain 102 35451\n",
      "\u001b[32m[2020-06-27 00:25:36] __main__ INFO: \u001b[0mEpoch 102 Step 100/351 lr 0.000800 loss 1.1424 (1.1370) acc@1 0.5625 (0.5607) acc@5 0.7422 (0.7577)\n",
      "\u001b[32m[2020-06-27 00:27:28] __main__ INFO: \u001b[0mEpoch 102 Step 200/351 lr 0.000800 loss 0.8660 (1.1361) acc@1 0.6875 (0.5594) acc@5 0.8047 (0.7582)\n",
      "\u001b[32m[2020-06-27 00:29:20] __main__ INFO: \u001b[0mEpoch 102 Step 300/351 lr 0.000800 loss 1.0555 (1.1314) acc@1 0.5859 (0.5617) acc@5 0.7578 (0.7605)\n",
      "\u001b[32m[2020-06-27 00:30:17] __main__ INFO: \u001b[0mEpoch 102 Step 351/351 lr 0.000800 loss 1.1816 (1.1248) acc@1 0.5859 (0.5641) acc@5 0.8203 (0.7619)\n",
      "\u001b[32m[2020-06-27 00:30:17] __main__ INFO: \u001b[0mElapsed 392.71\n",
      "\u001b[32m[2020-06-27 00:30:17] __main__ INFO: \u001b[0mVal 102\n",
      "\u001b[32m[2020-06-27 00:30:30] __main__ INFO: \u001b[0mEpoch 102 loss 2.0689 acc@1 0.3318 acc@5 0.7102\n",
      "\u001b[32m[2020-06-27 00:30:30] __main__ INFO: \u001b[0mElapsed 13.15\n",
      "\u001b[32m[2020-06-27 00:30:30] __main__ INFO: \u001b[0mTrain 103 35802\n",
      "\u001b[32m[2020-06-27 00:32:22] __main__ INFO: \u001b[0mEpoch 103 Step 100/351 lr 0.000800 loss 1.1474 (1.1400) acc@1 0.5703 (0.5580) acc@5 0.8047 (0.7573)\n",
      "\u001b[32m[2020-06-27 00:34:14] __main__ INFO: \u001b[0mEpoch 103 Step 200/351 lr 0.000800 loss 1.1227 (1.1251) acc@1 0.5859 (0.5636) acc@5 0.7500 (0.7620)\n"
     ]
    }
   ],
   "source": [
    "# The above stopped at 200 --- go figure!  retrain on augmented for 200 more epochs\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "!python train.py --config /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_3_20/exp01/config.yaml \\\n",
    "    train.checkpoint /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_3_20/exp01/checkpoint_00200.pth \\\n",
    "    model.wrn.depth 28 \\\n",
    "    model.wrn.widening_factor 10 \\\n",
    "    train.batch_size 128 \\\n",
    "    train.base_lr .004  \\\n",
    "    dataset.name CIFAR10_RA_3_20 \\\n",
    "    train.output_dir /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_3_20/exp02 \\\n",
    "    scheduler.epochs 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-27 16:14:41] __main__ INFO: \u001b[0mdevice: cuda\n",
      "cudnn:\n",
      "  benchmark: True\n",
      "  deterministic: False\n",
      "dataset:\n",
      "  name: CIFAR10\n",
      "  dataset_dir: ~/.torch/datasets/CIFAR10\n",
      "  image_size: 32\n",
      "  n_channels: 3\n",
      "  n_classes: 10\n",
      "model:\n",
      "  type: cifar\n",
      "  name: wrn\n",
      "  init_mode: kaiming_fan_in\n",
      "  vgg:\n",
      "    n_channels: [64, 128, 256, 512, 512]\n",
      "    n_layers: [2, 2, 3, 3, 3]\n",
      "    use_bn: True\n",
      "  resnet:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "  resnet_preact:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "  wrn:\n",
      "    depth: 28\n",
      "    initial_channels: 16\n",
      "    widening_factor: 10\n",
      "    drop_rate: 0.0\n",
      "  densenet:\n",
      "    depth: 100\n",
      "    n_blocks: [6, 12, 24, 16]\n",
      "    block_type: bottleneck\n",
      "    growth_rate: 12\n",
      "    drop_rate: 0.0\n",
      "    compression_rate: 0.5\n",
      "  pyramidnet:\n",
      "    depth: 272\n",
      "    n_blocks: [3, 24, 36, 3]\n",
      "    initial_channels: 16\n",
      "    block_type: bottleneck\n",
      "    alpha: 200\n",
      "  resnext:\n",
      "    depth: 29\n",
      "    n_blocks: [3, 4, 6, 3]\n",
      "    initial_channels: 64\n",
      "    cardinality: 8\n",
      "    base_channels: 4\n",
      "  shake_shake:\n",
      "    depth: 26\n",
      "    initial_channels: 96\n",
      "    shake_forward: True\n",
      "    shake_backward: True\n",
      "    shake_image: True\n",
      "  se_resnet_preact:\n",
      "    depth: 110\n",
      "    initial_channels: 16\n",
      "    se_reduction: 16\n",
      "    block_type: basic\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "train:\n",
      "  checkpoint: /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_3_20/exp02/checkpoint_00400.pth\n",
      "  resume: False\n",
      "  precision: O0\n",
      "  batch_size: 128\n",
      "  subdivision: 1\n",
      "  optimizer: sgd\n",
      "  base_lr: 3.2e-05\n",
      "  momentum: 0.9\n",
      "  nesterov: True\n",
      "  weight_decay: 0.0005\n",
      "  no_weight_decay_on_bn: False\n",
      "  gradient_clip: 0\n",
      "  start_epoch: 0\n",
      "  seed: 0\n",
      "  val_first: True\n",
      "  val_period: 1\n",
      "  val_ratio: 0.1\n",
      "  use_test_as_val: False\n",
      "  output_dir: /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_3_20/exp00_resume400_50\n",
      "  log_period: 100\n",
      "  checkpoint_period: 100\n",
      "  use_tensorboard: True\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: True\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "  distributed: False\n",
      "  dist:\n",
      "    backend: nccl\n",
      "    init_method: env://\n",
      "    world_size: -1\n",
      "    node_rank: -1\n",
      "    local_rank: 0\n",
      "    use_sync_bn: False\n",
      "tensorboard:\n",
      "  train_images: False\n",
      "  val_images: False\n",
      "  model_params: False\n",
      "optim:\n",
      "  adam:\n",
      "    betas: (0.9, 0.999)\n",
      "  lars:\n",
      "    eps: 1e-09\n",
      "    threshold: 0.01\n",
      "  adabound:\n",
      "    betas: (0.9, 0.999)\n",
      "    final_lr: 0.1\n",
      "    gamma: 0.001\n",
      "scheduler:\n",
      "  epochs: 50\n",
      "  warmup:\n",
      "    type: none\n",
      "    epochs: 0\n",
      "    start_factor: 0.001\n",
      "    exponent: 4\n",
      "  type: multistep\n",
      "  milestones: [60, 120, 160]\n",
      "  lr_decay: 0.2\n",
      "  lr_min_factor: 0.001\n",
      "  T0: 10\n",
      "  T_mul: 1.0\n",
      "validation:\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: False\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "augmentation:\n",
      "  use_random_crop: True\n",
      "  use_random_horizontal_flip: True\n",
      "  use_cutout: False\n",
      "  use_random_erasing: False\n",
      "  use_dual_cutout: False\n",
      "  use_mixup: False\n",
      "  use_ricap: False\n",
      "  use_cutmix: False\n",
      "  use_label_smoothing: False\n",
      "  random_crop:\n",
      "    padding: 4\n",
      "    fill: 0\n",
      "    padding_mode: constant\n",
      "  random_horizontal_flip:\n",
      "    prob: 0.5\n",
      "  cutout:\n",
      "    prob: 1.0\n",
      "    mask_size: 16\n",
      "    cut_inside: False\n",
      "    mask_color: 0\n",
      "    dual_cutout_alpha: 0.1\n",
      "  random_erasing:\n",
      "    prob: 0.5\n",
      "    area_ratio_range: [0.02, 0.4]\n",
      "    min_aspect_ratio: 0.3\n",
      "    max_attempt: 20\n",
      "  mixup:\n",
      "    alpha: 1.0\n",
      "  ricap:\n",
      "    beta: 0.3\n",
      "  cutmix:\n",
      "    alpha: 1.0\n",
      "  label_smoothing:\n",
      "    epsilon: 0.1\n",
      "tta:\n",
      "  use_resize: False\n",
      "  use_center_crop: False\n",
      "  resize: 256\n",
      "test:\n",
      "  checkpoint: ''\n",
      "  output_dir: ''\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    pin_memory: False\n",
      "\u001b[32m[2020-06-27 16:14:41] __main__ INFO: \u001b[0menv_info:\n",
      "  pytorch_version: 1.4.0\n",
      "  cuda_version: 10.1\n",
      "  cudnn_version: 7603\n",
      "  num_gpus: 1\n",
      "  gpu_name: Tesla K80\n",
      "  gpu_capability: 3.7\n",
      "Files already downloaded and verified\n",
      "\u001b[32m[2020-06-27 16:14:45] __main__ INFO: \u001b[0mMACs  : 5.25G\n",
      "\u001b[32m[2020-06-27 16:14:45] __main__ INFO: \u001b[0m#params: 36.48M\n",
      "Selected optimization level O0:  Pure FP32 training.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\",)\n",
      "\u001b[32m[2020-06-27 16:14:45] __main__ INFO: \u001b[0mVal 0\n",
      "\u001b[32m[2020-06-27 16:15:05] __main__ INFO: \u001b[0mEpoch 0 loss 0.9608 acc@1 0.7586 acc@5 0.9760\n",
      "\u001b[32m[2020-06-27 16:15:05] __main__ INFO: \u001b[0mElapsed 19.60\n",
      "\u001b[32m[2020-06-27 16:15:05] __main__ INFO: \u001b[0mTrain 1 0\n",
      "\u001b[32m[2020-06-27 16:17:02] __main__ INFO: \u001b[0mEpoch 1 Step 100/351 lr 0.000032 loss 0.4639 (0.6622) acc@1 0.8594 (0.8033) acc@5 0.9844 (0.9798)\n",
      "\u001b[32m[2020-06-27 16:18:55] __main__ INFO: \u001b[0mEpoch 1 Step 200/351 lr 0.000032 loss 0.4574 (0.6208) acc@1 0.8516 (0.8154) acc@5 0.9922 (0.9818)\n",
      "\u001b[32m[2020-06-27 16:20:47] __main__ INFO: \u001b[0mEpoch 1 Step 300/351 lr 0.000032 loss 0.3369 (0.5944) acc@1 0.8984 (0.8225) acc@5 1.0000 (0.9833)\n",
      "\u001b[32m[2020-06-27 16:21:45] __main__ INFO: \u001b[0mEpoch 1 Step 351/351 lr 0.000032 loss 0.4034 (0.5836) acc@1 0.8672 (0.8255) acc@5 0.9922 (0.9837)\n",
      "\u001b[32m[2020-06-27 16:21:45] __main__ INFO: \u001b[0mElapsed 399.86\n",
      "\u001b[32m[2020-06-27 16:21:45] __main__ INFO: \u001b[0mVal 1\n",
      "\u001b[32m[2020-06-27 16:21:58] __main__ INFO: \u001b[0mEpoch 1 loss 0.5575 acc@1 0.8272 acc@5 0.9884\n",
      "\u001b[32m[2020-06-27 16:21:58] __main__ INFO: \u001b[0mElapsed 13.27\n",
      "\u001b[32m[2020-06-27 16:21:58] __main__ INFO: \u001b[0mTrain 2 351\n",
      "\u001b[32m[2020-06-27 16:23:50] __main__ INFO: \u001b[0mEpoch 2 Step 100/351 lr 0.000032 loss 0.5707 (0.4922) acc@1 0.7969 (0.8487) acc@5 0.9922 (0.9898)\n",
      "\u001b[32m[2020-06-27 16:25:42] __main__ INFO: \u001b[0mEpoch 2 Step 200/351 lr 0.000032 loss 0.5065 (0.4920) acc@1 0.8438 (0.8478) acc@5 1.0000 (0.9900)\n",
      "\u001b[32m[2020-06-27 16:27:34] __main__ INFO: \u001b[0mEpoch 2 Step 300/351 lr 0.000032 loss 0.4836 (0.4812) acc@1 0.8438 (0.8514) acc@5 1.0000 (0.9903)\n",
      "\u001b[32m[2020-06-27 16:28:32] __main__ INFO: \u001b[0mEpoch 2 Step 351/351 lr 0.000032 loss 0.6902 (0.4767) acc@1 0.7891 (0.8525) acc@5 0.9922 (0.9903)\n",
      "\u001b[32m[2020-06-27 16:28:32] __main__ INFO: \u001b[0mElapsed 393.55\n",
      "\u001b[32m[2020-06-27 16:28:32] __main__ INFO: \u001b[0mVal 2\n",
      "\u001b[32m[2020-06-27 16:28:45] __main__ INFO: \u001b[0mEpoch 2 loss 0.5006 acc@1 0.8436 acc@5 0.9900\n",
      "\u001b[32m[2020-06-27 16:28:45] __main__ INFO: \u001b[0mElapsed 13.28\n",
      "\u001b[32m[2020-06-27 16:28:45] __main__ INFO: \u001b[0mTrain 3 702\n",
      "\u001b[32m[2020-06-27 16:30:37] __main__ INFO: \u001b[0mEpoch 3 Step 100/351 lr 0.000032 loss 0.5063 (0.4569) acc@1 0.8672 (0.8577) acc@5 0.9844 (0.9916)\n",
      "\u001b[32m[2020-06-27 16:32:30] __main__ INFO: \u001b[0mEpoch 3 Step 200/351 lr 0.000032 loss 0.4982 (0.4456) acc@1 0.8359 (0.8609) acc@5 1.0000 (0.9924)\n",
      "\u001b[32m[2020-06-27 16:34:22] __main__ INFO: \u001b[0mEpoch 3 Step 300/351 lr 0.000032 loss 0.4199 (0.4343) acc@1 0.8906 (0.8648) acc@5 0.9844 (0.9922)\n",
      "\u001b[32m[2020-06-27 16:35:20] __main__ INFO: \u001b[0mEpoch 3 Step 351/351 lr 0.000032 loss 0.5360 (0.4341) acc@1 0.8281 (0.8659) acc@5 1.0000 (0.9923)\n",
      "\u001b[32m[2020-06-27 16:35:20] __main__ INFO: \u001b[0mElapsed 394.96\n",
      "\u001b[32m[2020-06-27 16:35:20] __main__ INFO: \u001b[0mVal 3\n",
      "\u001b[32m[2020-06-27 16:35:33] __main__ INFO: \u001b[0mEpoch 3 loss 0.4703 acc@1 0.8516 acc@5 0.9918\n",
      "\u001b[32m[2020-06-27 16:35:33] __main__ INFO: \u001b[0mElapsed 13.23\n",
      "\u001b[32m[2020-06-27 16:35:33] __main__ INFO: \u001b[0mTrain 4 1053\n",
      "\u001b[32m[2020-06-27 16:37:25] __main__ INFO: \u001b[0mEpoch 4 Step 100/351 lr 0.000032 loss 0.5796 (0.4079) acc@1 0.8125 (0.8720) acc@5 0.9766 (0.9941)\n",
      "\u001b[32m[2020-06-27 16:39:17] __main__ INFO: \u001b[0mEpoch 4 Step 200/351 lr 0.000032 loss 0.3247 (0.4021) acc@1 0.9062 (0.8749) acc@5 0.9922 (0.9936)\n",
      "\u001b[32m[2020-06-27 16:41:09] __main__ INFO: \u001b[0mEpoch 4 Step 300/351 lr 0.000032 loss 0.4106 (0.4004) acc@1 0.8906 (0.8749) acc@5 1.0000 (0.9938)\n",
      "\u001b[32m[2020-06-27 16:42:06] __main__ INFO: \u001b[0mEpoch 4 Step 351/351 lr 0.000032 loss 0.4750 (0.4023) acc@1 0.8281 (0.8745) acc@5 0.9922 (0.9934)\n",
      "\u001b[32m[2020-06-27 16:42:06] __main__ INFO: \u001b[0mElapsed 393.24\n",
      "\u001b[32m[2020-06-27 16:42:06] __main__ INFO: \u001b[0mVal 4\n",
      "\u001b[32m[2020-06-27 16:42:19] __main__ INFO: \u001b[0mEpoch 4 loss 0.4486 acc@1 0.8572 acc@5 0.9924\n",
      "\u001b[32m[2020-06-27 16:42:19] __main__ INFO: \u001b[0mElapsed 13.23\n",
      "\u001b[32m[2020-06-27 16:42:19] __main__ INFO: \u001b[0mTrain 5 1404\n",
      "\u001b[32m[2020-06-27 16:44:12] __main__ INFO: \u001b[0mEpoch 5 Step 100/351 lr 0.000032 loss 0.4816 (0.3909) acc@1 0.8750 (0.8768) acc@5 0.9766 (0.9944)\n",
      "\u001b[32m[2020-06-27 16:46:04] __main__ INFO: \u001b[0mEpoch 5 Step 200/351 lr 0.000032 loss 0.4341 (0.3812) acc@1 0.8672 (0.8804) acc@5 0.9922 (0.9945)\n",
      "\u001b[32m[2020-06-27 16:47:56] __main__ INFO: \u001b[0mEpoch 5 Step 300/351 lr 0.000032 loss 0.5164 (0.3861) acc@1 0.8281 (0.8792) acc@5 1.0000 (0.9945)\n",
      "\u001b[32m[2020-06-27 16:48:54] __main__ INFO: \u001b[0mEpoch 5 Step 351/351 lr 0.000032 loss 0.3940 (0.3847) acc@1 0.8672 (0.8795) acc@5 0.9844 (0.9944)\n",
      "\u001b[32m[2020-06-27 16:48:54] __main__ INFO: \u001b[0mElapsed 394.21\n",
      "\u001b[32m[2020-06-27 16:48:54] __main__ INFO: \u001b[0mVal 5\n",
      "\u001b[32m[2020-06-27 16:49:07] __main__ INFO: \u001b[0mEpoch 5 loss 0.4346 acc@1 0.8612 acc@5 0.9926\n",
      "\u001b[32m[2020-06-27 16:49:07] __main__ INFO: \u001b[0mElapsed 13.28\n",
      "\u001b[32m[2020-06-27 16:49:07] __main__ INFO: \u001b[0mTrain 6 1755\n",
      "\u001b[32m[2020-06-27 16:50:59] __main__ INFO: \u001b[0mEpoch 6 Step 100/351 lr 0.000032 loss 0.2255 (0.3558) acc@1 0.9453 (0.8903) acc@5 1.0000 (0.9944)\n",
      "\u001b[32m[2020-06-27 16:52:51] __main__ INFO: \u001b[0mEpoch 6 Step 200/351 lr 0.000032 loss 0.3391 (0.3622) acc@1 0.8750 (0.8878) acc@5 0.9922 (0.9948)\n",
      "\u001b[32m[2020-06-27 16:54:42] __main__ INFO: \u001b[0mEpoch 6 Step 300/351 lr 0.000032 loss 0.4298 (0.3653) acc@1 0.8672 (0.8853) acc@5 0.9844 (0.9949)\n",
      "\u001b[32m[2020-06-27 16:55:39] __main__ INFO: \u001b[0mEpoch 6 Step 351/351 lr 0.000032 loss 0.3006 (0.3661) acc@1 0.9219 (0.8856) acc@5 1.0000 (0.9949)\n",
      "\u001b[32m[2020-06-27 16:55:39] __main__ INFO: \u001b[0mElapsed 392.38\n",
      "\u001b[32m[2020-06-27 16:55:39] __main__ INFO: \u001b[0mVal 6\n",
      "\u001b[32m[2020-06-27 16:55:53] __main__ INFO: \u001b[0mEpoch 6 loss 0.4216 acc@1 0.8660 acc@5 0.9926\n",
      "\u001b[32m[2020-06-27 16:55:53] __main__ INFO: \u001b[0mElapsed 13.24\n",
      "\u001b[32m[2020-06-27 16:55:53] __main__ INFO: \u001b[0mTrain 7 2106\n",
      "\u001b[32m[2020-06-27 16:57:45] __main__ INFO: \u001b[0mEpoch 7 Step 100/351 lr 0.000032 loss 0.3520 (0.3698) acc@1 0.8906 (0.8818) acc@5 1.0000 (0.9941)\n",
      "\u001b[32m[2020-06-27 16:59:37] __main__ INFO: \u001b[0mEpoch 7 Step 200/351 lr 0.000032 loss 0.4072 (0.3605) acc@1 0.8672 (0.8866) acc@5 0.9922 (0.9946)\n",
      "\u001b[32m[2020-06-27 17:01:29] __main__ INFO: \u001b[0mEpoch 7 Step 300/351 lr 0.000032 loss 0.3027 (0.3568) acc@1 0.8906 (0.8880) acc@5 0.9922 (0.9948)\n",
      "\u001b[32m[2020-06-27 17:02:26] __main__ INFO: \u001b[0mEpoch 7 Step 351/351 lr 0.000032 loss 0.3478 (0.3587) acc@1 0.9141 (0.8874) acc@5 0.9922 (0.9948)\n",
      "\u001b[32m[2020-06-27 17:02:26] __main__ INFO: \u001b[0mElapsed 393.85\n",
      "\u001b[32m[2020-06-27 17:02:26] __main__ INFO: \u001b[0mVal 7\n",
      "\u001b[32m[2020-06-27 17:02:40] __main__ INFO: \u001b[0mEpoch 7 loss 0.4130 acc@1 0.8670 acc@5 0.9938\n",
      "\u001b[32m[2020-06-27 17:02:40] __main__ INFO: \u001b[0mElapsed 13.22\n",
      "\u001b[32m[2020-06-27 17:02:40] __main__ INFO: \u001b[0mTrain 8 2457\n",
      "\u001b[32m[2020-06-27 17:04:31] __main__ INFO: \u001b[0mEpoch 8 Step 100/351 lr 0.000032 loss 0.2685 (0.3486) acc@1 0.9297 (0.8894) acc@5 0.9844 (0.9955)\n",
      "\u001b[32m[2020-06-27 17:06:23] __main__ INFO: \u001b[0mEpoch 8 Step 200/351 lr 0.000032 loss 0.3062 (0.3451) acc@1 0.8984 (0.8912) acc@5 0.9766 (0.9948)\n",
      "\u001b[32m[2020-06-27 17:08:15] __main__ INFO: \u001b[0mEpoch 8 Step 300/351 lr 0.000032 loss 0.2831 (0.3445) acc@1 0.9141 (0.8924) acc@5 1.0000 (0.9949)\n",
      "\u001b[32m[2020-06-27 17:09:12] __main__ INFO: \u001b[0mEpoch 8 Step 351/351 lr 0.000032 loss 0.3754 (0.3455) acc@1 0.8672 (0.8918) acc@5 1.0000 (0.9950)\n",
      "\u001b[32m[2020-06-27 17:09:12] __main__ INFO: \u001b[0mElapsed 392.08\n",
      "\u001b[32m[2020-06-27 17:09:12] __main__ INFO: \u001b[0mVal 8\n",
      "\u001b[32m[2020-06-27 17:09:25] __main__ INFO: \u001b[0mEpoch 8 loss 0.4050 acc@1 0.8694 acc@5 0.9944\n",
      "\u001b[32m[2020-06-27 17:09:25] __main__ INFO: \u001b[0mElapsed 13.20\n",
      "\u001b[32m[2020-06-27 17:09:25] __main__ INFO: \u001b[0mTrain 9 2808\n",
      "\u001b[32m[2020-06-27 17:11:17] __main__ INFO: \u001b[0mEpoch 9 Step 100/351 lr 0.000032 loss 0.3464 (0.3468) acc@1 0.9062 (0.8905) acc@5 0.9844 (0.9945)\n",
      "\u001b[32m[2020-06-27 17:13:09] __main__ INFO: \u001b[0mEpoch 9 Step 200/351 lr 0.000032 loss 0.2886 (0.3344) acc@1 0.9219 (0.8946) acc@5 0.9844 (0.9949)\n",
      "\u001b[32m[2020-06-27 17:15:02] __main__ INFO: \u001b[0mEpoch 9 Step 300/351 lr 0.000032 loss 0.3461 (0.3328) acc@1 0.8828 (0.8949) acc@5 1.0000 (0.9953)\n",
      "\u001b[32m[2020-06-27 17:15:59] __main__ INFO: \u001b[0mEpoch 9 Step 351/351 lr 0.000032 loss 0.2735 (0.3322) acc@1 0.9219 (0.8950) acc@5 0.9922 (0.9952)\n",
      "\u001b[32m[2020-06-27 17:15:59] __main__ INFO: \u001b[0mElapsed 393.83\n",
      "\u001b[32m[2020-06-27 17:15:59] __main__ INFO: \u001b[0mVal 9\n",
      "\u001b[32m[2020-06-27 17:16:12] __main__ INFO: \u001b[0mEpoch 9 loss 0.3980 acc@1 0.8716 acc@5 0.9934\n",
      "\u001b[32m[2020-06-27 17:16:12] __main__ INFO: \u001b[0mElapsed 13.23\n",
      "\u001b[32m[2020-06-27 17:16:12] __main__ INFO: \u001b[0mTrain 10 3159\n",
      "\u001b[32m[2020-06-27 17:18:04] __main__ INFO: \u001b[0mEpoch 10 Step 100/351 lr 0.000032 loss 0.4690 (0.3258) acc@1 0.8516 (0.8990) acc@5 0.9844 (0.9949)\n",
      "\u001b[32m[2020-06-27 17:19:56] __main__ INFO: \u001b[0mEpoch 10 Step 200/351 lr 0.000032 loss 0.2578 (0.3244) acc@1 0.8906 (0.8986) acc@5 1.0000 (0.9953)\n",
      "\u001b[32m[2020-06-27 17:21:48] __main__ INFO: \u001b[0mEpoch 10 Step 300/351 lr 0.000032 loss 0.2709 (0.3252) acc@1 0.9062 (0.8979) acc@5 1.0000 (0.9952)\n",
      "\u001b[32m[2020-06-27 17:22:45] __main__ INFO: \u001b[0mEpoch 10 Step 351/351 lr 0.000032 loss 0.2509 (0.3269) acc@1 0.9297 (0.8975) acc@5 1.0000 (0.9953)\n",
      "\u001b[32m[2020-06-27 17:22:45] __main__ INFO: \u001b[0mElapsed 392.56\n",
      "\u001b[32m[2020-06-27 17:22:45] __main__ INFO: \u001b[0mVal 10\n",
      "\u001b[32m[2020-06-27 17:22:58] __main__ INFO: \u001b[0mEpoch 10 loss 0.3919 acc@1 0.8740 acc@5 0.9938\n",
      "\u001b[32m[2020-06-27 17:22:58] __main__ INFO: \u001b[0mElapsed 13.24\n",
      "\u001b[32m[2020-06-27 17:22:58] __main__ INFO: \u001b[0mTrain 11 3510\n",
      "\u001b[32m[2020-06-27 17:24:50] __main__ INFO: \u001b[0mEpoch 11 Step 100/351 lr 0.000032 loss 0.3368 (0.3128) acc@1 0.9062 (0.9027) acc@5 1.0000 (0.9962)\n",
      "\u001b[32m[2020-06-27 17:26:43] __main__ INFO: \u001b[0mEpoch 11 Step 200/351 lr 0.000032 loss 0.2520 (0.3173) acc@1 0.9219 (0.9006) acc@5 1.0000 (0.9957)\n",
      "\u001b[32m[2020-06-27 17:28:35] __main__ INFO: \u001b[0mEpoch 11 Step 300/351 lr 0.000032 loss 0.3331 (0.3171) acc@1 0.9062 (0.9008) acc@5 1.0000 (0.9960)\n",
      "\u001b[32m[2020-06-27 17:29:32] __main__ INFO: \u001b[0mEpoch 11 Step 351/351 lr 0.000032 loss 0.2399 (0.3156) acc@1 0.9062 (0.9013) acc@5 1.0000 (0.9961)\n",
      "\u001b[32m[2020-06-27 17:29:32] __main__ INFO: \u001b[0mElapsed 394.30\n",
      "\u001b[32m[2020-06-27 17:29:32] __main__ INFO: \u001b[0mVal 11\n",
      "\u001b[32m[2020-06-27 17:29:45] __main__ INFO: \u001b[0mEpoch 11 loss 0.3868 acc@1 0.8758 acc@5 0.9944\n",
      "\u001b[32m[2020-06-27 17:29:45] __main__ INFO: \u001b[0mElapsed 13.23\n",
      "\u001b[32m[2020-06-27 17:29:45] __main__ INFO: \u001b[0mTrain 12 3861\n",
      "\u001b[32m[2020-06-27 17:31:37] __main__ INFO: \u001b[0mEpoch 12 Step 100/351 lr 0.000032 loss 0.4048 (0.3038) acc@1 0.8516 (0.9047) acc@5 0.9922 (0.9963)\n",
      "\u001b[32m[2020-06-27 17:33:29] __main__ INFO: \u001b[0mEpoch 12 Step 200/351 lr 0.000032 loss 0.2493 (0.3084) acc@1 0.9141 (0.9026) acc@5 0.9922 (0.9957)\n",
      "\u001b[32m[2020-06-27 17:35:21] __main__ INFO: \u001b[0mEpoch 12 Step 300/351 lr 0.000032 loss 0.3277 (0.3093) acc@1 0.8828 (0.9025) acc@5 1.0000 (0.9960)\n",
      "\u001b[32m[2020-06-27 17:36:18] __main__ INFO: \u001b[0mEpoch 12 Step 351/351 lr 0.000032 loss 0.3040 (0.3094) acc@1 0.9141 (0.9025) acc@5 0.9922 (0.9959)\n",
      "\u001b[32m[2020-06-27 17:36:18] __main__ INFO: \u001b[0mElapsed 392.66\n",
      "\u001b[32m[2020-06-27 17:36:18] __main__ INFO: \u001b[0mVal 12\n",
      "\u001b[32m[2020-06-27 17:36:31] __main__ INFO: \u001b[0mEpoch 12 loss 0.3819 acc@1 0.8788 acc@5 0.9940\n",
      "\u001b[32m[2020-06-27 17:36:31] __main__ INFO: \u001b[0mElapsed 13.28\n",
      "\u001b[32m[2020-06-27 17:36:31] __main__ INFO: \u001b[0mTrain 13 4212\n",
      "\u001b[32m[2020-06-27 17:38:24] __main__ INFO: \u001b[0mEpoch 13 Step 100/351 lr 0.000032 loss 0.4049 (0.3111) acc@1 0.8750 (0.9034) acc@5 0.9922 (0.9955)\n",
      "\u001b[32m[2020-06-27 17:40:16] __main__ INFO: \u001b[0mEpoch 13 Step 200/351 lr 0.000032 loss 0.2741 (0.3045) acc@1 0.9297 (0.9050) acc@5 1.0000 (0.9963)\n",
      "\u001b[32m[2020-06-27 17:42:08] __main__ INFO: \u001b[0mEpoch 13 Step 300/351 lr 0.000032 loss 0.3180 (0.3033) acc@1 0.8750 (0.9057) acc@5 1.0000 (0.9964)\n",
      "\u001b[32m[2020-06-27 17:43:06] __main__ INFO: \u001b[0mEpoch 13 Step 351/351 lr 0.000032 loss 0.2728 (0.3031) acc@1 0.9062 (0.9057) acc@5 1.0000 (0.9963)\n",
      "\u001b[32m[2020-06-27 17:43:06] __main__ INFO: \u001b[0mElapsed 394.29\n",
      "\u001b[32m[2020-06-27 17:43:06] __main__ INFO: \u001b[0mVal 13\n",
      "\u001b[32m[2020-06-27 17:43:19] __main__ INFO: \u001b[0mEpoch 13 loss 0.3776 acc@1 0.8778 acc@5 0.9944\n",
      "\u001b[32m[2020-06-27 17:43:19] __main__ INFO: \u001b[0mElapsed 13.27\n",
      "\u001b[32m[2020-06-27 17:43:19] __main__ INFO: \u001b[0mTrain 14 4563\n",
      "\u001b[32m[2020-06-27 17:45:11] __main__ INFO: \u001b[0mEpoch 14 Step 100/351 lr 0.000032 loss 0.3267 (0.2967) acc@1 0.8828 (0.9067) acc@5 0.9922 (0.9964)\n",
      "\u001b[32m[2020-06-27 17:47:03] __main__ INFO: \u001b[0mEpoch 14 Step 200/351 lr 0.000032 loss 0.2245 (0.2971) acc@1 0.9297 (0.9077) acc@5 1.0000 (0.9964)\n",
      "\u001b[32m[2020-06-27 17:48:54] __main__ INFO: \u001b[0mEpoch 14 Step 300/351 lr 0.000032 loss 0.3780 (0.2948) acc@1 0.8750 (0.9087) acc@5 0.9922 (0.9967)\n",
      "\u001b[32m[2020-06-27 17:49:51] __main__ INFO: \u001b[0mEpoch 14 Step 351/351 lr 0.000032 loss 0.2203 (0.2942) acc@1 0.9297 (0.9085) acc@5 1.0000 (0.9967)\n",
      "\u001b[32m[2020-06-27 17:49:52] __main__ INFO: \u001b[0mElapsed 392.63\n",
      "\u001b[32m[2020-06-27 17:49:52] __main__ INFO: \u001b[0mVal 14\n",
      "\u001b[32m[2020-06-27 17:50:05] __main__ INFO: \u001b[0mEpoch 14 loss 0.3750 acc@1 0.8784 acc@5 0.9946\n",
      "\u001b[32m[2020-06-27 17:50:05] __main__ INFO: \u001b[0mElapsed 13.24\n",
      "\u001b[32m[2020-06-27 17:50:05] __main__ INFO: \u001b[0mTrain 15 4914\n",
      "\u001b[32m[2020-06-27 17:51:57] __main__ INFO: \u001b[0mEpoch 15 Step 100/351 lr 0.000032 loss 0.2773 (0.2940) acc@1 0.9141 (0.9067) acc@5 1.0000 (0.9972)\n",
      "\u001b[32m[2020-06-27 17:53:49] __main__ INFO: \u001b[0mEpoch 15 Step 200/351 lr 0.000032 loss 0.2785 (0.2857) acc@1 0.9141 (0.9104) acc@5 0.9922 (0.9966)\n",
      "\u001b[32m[2020-06-27 17:55:42] __main__ INFO: \u001b[0mEpoch 15 Step 300/351 lr 0.000032 loss 0.2180 (0.2843) acc@1 0.9453 (0.9111) acc@5 1.0000 (0.9970)\n",
      "\u001b[32m[2020-06-27 17:56:39] __main__ INFO: \u001b[0mEpoch 15 Step 351/351 lr 0.000032 loss 0.2545 (0.2857) acc@1 0.9297 (0.9106) acc@5 1.0000 (0.9969)\n",
      "\u001b[32m[2020-06-27 17:56:39] __main__ INFO: \u001b[0mElapsed 394.33\n",
      "\u001b[32m[2020-06-27 17:56:39] __main__ INFO: \u001b[0mVal 15\n",
      "\u001b[32m[2020-06-27 17:56:52] __main__ INFO: \u001b[0mEpoch 15 loss 0.3727 acc@1 0.8786 acc@5 0.9950\n",
      "\u001b[32m[2020-06-27 17:56:52] __main__ INFO: \u001b[0mElapsed 13.25\n",
      "\u001b[32m[2020-06-27 17:56:52] __main__ INFO: \u001b[0mTrain 16 5265\n",
      "\u001b[32m[2020-06-27 17:58:44] __main__ INFO: \u001b[0mEpoch 16 Step 100/351 lr 0.000032 loss 0.3038 (0.2765) acc@1 0.9062 (0.9145) acc@5 1.0000 (0.9968)\n",
      "\u001b[32m[2020-06-27 18:00:36] __main__ INFO: \u001b[0mEpoch 16 Step 200/351 lr 0.000032 loss 0.3260 (0.2758) acc@1 0.8984 (0.9145) acc@5 1.0000 (0.9970)\n",
      "\u001b[32m[2020-06-27 18:02:28] __main__ INFO: \u001b[0mEpoch 16 Step 300/351 lr 0.000032 loss 0.1775 (0.2773) acc@1 0.9453 (0.9144) acc@5 1.0000 (0.9971)\n",
      "\u001b[32m[2020-06-27 18:03:25] __main__ INFO: \u001b[0mEpoch 16 Step 351/351 lr 0.000032 loss 0.1682 (0.2776) acc@1 0.9453 (0.9140) acc@5 1.0000 (0.9969)\n",
      "\u001b[32m[2020-06-27 18:03:25] __main__ INFO: \u001b[0mElapsed 392.79\n",
      "\u001b[32m[2020-06-27 18:03:25] __main__ INFO: \u001b[0mVal 16\n",
      "\u001b[32m[2020-06-27 18:03:38] __main__ INFO: \u001b[0mEpoch 16 loss 0.3689 acc@1 0.8818 acc@5 0.9948\n",
      "\u001b[32m[2020-06-27 18:03:38] __main__ INFO: \u001b[0mElapsed 13.26\n",
      "\u001b[32m[2020-06-27 18:03:38] __main__ INFO: \u001b[0mTrain 17 5616\n",
      "\u001b[32m[2020-06-27 18:05:31] __main__ INFO: \u001b[0mEpoch 17 Step 100/351 lr 0.000032 loss 0.3991 (0.2769) acc@1 0.8672 (0.9141) acc@5 0.9922 (0.9966)\n",
      "\u001b[32m[2020-06-27 18:07:23] __main__ INFO: \u001b[0mEpoch 17 Step 200/351 lr 0.000032 loss 0.3439 (0.2776) acc@1 0.8984 (0.9143) acc@5 1.0000 (0.9966)\n",
      "\u001b[32m[2020-06-27 18:09:15] __main__ INFO: \u001b[0mEpoch 17 Step 300/351 lr 0.000032 loss 0.2978 (0.2746) acc@1 0.9219 (0.9148) acc@5 1.0000 (0.9967)\n",
      "\u001b[32m[2020-06-27 18:10:13] __main__ INFO: \u001b[0mEpoch 17 Step 351/351 lr 0.000032 loss 0.2539 (0.2746) acc@1 0.9219 (0.9148) acc@5 0.9922 (0.9968)\n",
      "\u001b[32m[2020-06-27 18:10:13] __main__ INFO: \u001b[0mElapsed 394.38\n",
      "\u001b[32m[2020-06-27 18:10:13] __main__ INFO: \u001b[0mVal 17\n",
      "\u001b[32m[2020-06-27 18:10:26] __main__ INFO: \u001b[0mEpoch 17 loss 0.3651 acc@1 0.8826 acc@5 0.9944\n",
      "\u001b[32m[2020-06-27 18:10:26] __main__ INFO: \u001b[0mElapsed 13.23\n",
      "\u001b[32m[2020-06-27 18:10:26] __main__ INFO: \u001b[0mTrain 18 5967\n",
      "\u001b[32m[2020-06-27 18:12:18] __main__ INFO: \u001b[0mEpoch 18 Step 100/351 lr 0.000032 loss 0.1794 (0.2641) acc@1 0.9375 (0.9191) acc@5 1.0000 (0.9970)\n",
      "\u001b[32m[2020-06-27 18:14:10] __main__ INFO: \u001b[0mEpoch 18 Step 200/351 lr 0.000032 loss 0.2714 (0.2681) acc@1 0.9219 (0.9164) acc@5 0.9922 (0.9968)\n",
      "\u001b[32m[2020-06-27 18:16:02] __main__ INFO: \u001b[0mEpoch 18 Step 300/351 lr 0.000032 loss 0.2757 (0.2693) acc@1 0.8984 (0.9163) acc@5 0.9844 (0.9968)\n",
      "\u001b[32m[2020-06-27 18:16:59] __main__ INFO: \u001b[0mEpoch 18 Step 351/351 lr 0.000032 loss 0.2319 (0.2699) acc@1 0.9375 (0.9158) acc@5 1.0000 (0.9968)\n",
      "\u001b[32m[2020-06-27 18:16:59] __main__ INFO: \u001b[0mElapsed 392.78\n",
      "\u001b[32m[2020-06-27 18:16:59] __main__ INFO: \u001b[0mVal 18\n",
      "\u001b[32m[2020-06-27 18:17:12] __main__ INFO: \u001b[0mEpoch 18 loss 0.3610 acc@1 0.8832 acc@5 0.9946\n",
      "\u001b[32m[2020-06-27 18:17:12] __main__ INFO: \u001b[0mElapsed 13.24\n",
      "\u001b[32m[2020-06-27 18:17:12] __main__ INFO: \u001b[0mTrain 19 6318\n",
      "\u001b[32m[2020-06-27 18:19:04] __main__ INFO: \u001b[0mEpoch 19 Step 100/351 lr 0.000032 loss 0.3721 (0.2657) acc@1 0.8750 (0.9166) acc@5 1.0000 (0.9972)\n",
      "\u001b[32m[2020-06-27 18:20:57] __main__ INFO: \u001b[0mEpoch 19 Step 200/351 lr 0.000032 loss 0.4426 (0.2644) acc@1 0.8438 (0.9166) acc@5 1.0000 (0.9968)\n",
      "\u001b[32m[2020-06-27 18:22:49] __main__ INFO: \u001b[0mEpoch 19 Step 300/351 lr 0.000032 loss 0.2589 (0.2647) acc@1 0.9297 (0.9171) acc@5 0.9922 (0.9968)\n",
      "\u001b[32m[2020-06-27 18:23:46] __main__ INFO: \u001b[0mEpoch 19 Step 351/351 lr 0.000032 loss 0.2520 (0.2636) acc@1 0.9531 (0.9180) acc@5 0.9922 (0.9970)\n",
      "\u001b[32m[2020-06-27 18:23:46] __main__ INFO: \u001b[0mElapsed 394.43\n",
      "\u001b[32m[2020-06-27 18:23:46] __main__ INFO: \u001b[0mVal 19\n",
      "\u001b[32m[2020-06-27 18:24:00] __main__ INFO: \u001b[0mEpoch 19 loss 0.3604 acc@1 0.8838 acc@5 0.9938\n",
      "\u001b[32m[2020-06-27 18:24:00] __main__ INFO: \u001b[0mElapsed 13.23\n",
      "\u001b[32m[2020-06-27 18:24:00] __main__ INFO: \u001b[0mTrain 20 6669\n",
      "\u001b[32m[2020-06-27 18:25:52] __main__ INFO: \u001b[0mEpoch 20 Step 100/351 lr 0.000032 loss 0.2730 (0.2670) acc@1 0.9141 (0.9182) acc@5 0.9922 (0.9963)\n",
      "\u001b[32m[2020-06-27 18:27:44] __main__ INFO: \u001b[0mEpoch 20 Step 200/351 lr 0.000032 loss 0.2194 (0.2627) acc@1 0.9375 (0.9197) acc@5 1.0000 (0.9972)\n",
      "\u001b[32m[2020-06-27 18:29:35] __main__ INFO: \u001b[0mEpoch 20 Step 300/351 lr 0.000032 loss 0.2342 (0.2613) acc@1 0.9219 (0.9200) acc@5 1.0000 (0.9972)\n",
      "\u001b[32m[2020-06-27 18:30:33] __main__ INFO: \u001b[0mEpoch 20 Step 351/351 lr 0.000032 loss 0.2088 (0.2598) acc@1 0.9453 (0.9207) acc@5 0.9922 (0.9972)\n",
      "\u001b[32m[2020-06-27 18:30:33] __main__ INFO: \u001b[0mElapsed 392.89\n",
      "\u001b[32m[2020-06-27 18:30:33] __main__ INFO: \u001b[0mVal 20\n",
      "\u001b[32m[2020-06-27 18:30:46] __main__ INFO: \u001b[0mEpoch 20 loss 0.3573 acc@1 0.8846 acc@5 0.9946\n",
      "\u001b[32m[2020-06-27 18:30:46] __main__ INFO: \u001b[0mElapsed 13.25\n",
      "\u001b[32m[2020-06-27 18:30:46] __main__ INFO: \u001b[0mTrain 21 7020\n",
      "\u001b[32m[2020-06-27 18:32:38] __main__ INFO: \u001b[0mEpoch 21 Step 100/351 lr 0.000032 loss 0.2134 (0.2396) acc@1 0.9375 (0.9251) acc@5 1.0000 (0.9977)\n",
      "\u001b[32m[2020-06-27 18:34:31] __main__ INFO: \u001b[0mEpoch 21 Step 200/351 lr 0.000032 loss 0.2726 (0.2472) acc@1 0.9062 (0.9232) acc@5 1.0000 (0.9972)\n",
      "\u001b[32m[2020-06-27 18:36:23] __main__ INFO: \u001b[0mEpoch 21 Step 300/351 lr 0.000032 loss 0.1740 (0.2506) acc@1 0.9531 (0.9226) acc@5 1.0000 (0.9972)\n",
      "\u001b[32m[2020-06-27 18:37:20] __main__ INFO: \u001b[0mEpoch 21 Step 351/351 lr 0.000032 loss 0.3106 (0.2512) acc@1 0.8984 (0.9225) acc@5 0.9922 (0.9974)\n",
      "\u001b[32m[2020-06-27 18:37:20] __main__ INFO: \u001b[0mElapsed 394.48\n",
      "\u001b[32m[2020-06-27 18:37:20] __main__ INFO: \u001b[0mVal 21\n",
      "\u001b[32m[2020-06-27 18:37:34] __main__ INFO: \u001b[0mEpoch 21 loss 0.3542 acc@1 0.8858 acc@5 0.9942\n",
      "\u001b[32m[2020-06-27 18:37:34] __main__ INFO: \u001b[0mElapsed 13.23\n",
      "\u001b[32m[2020-06-27 18:37:34] __main__ INFO: \u001b[0mTrain 22 7371\n",
      "\u001b[32m[2020-06-27 18:39:26] __main__ INFO: \u001b[0mEpoch 22 Step 100/351 lr 0.000032 loss 0.1691 (0.2483) acc@1 0.9453 (0.9237) acc@5 1.0000 (0.9962)\n",
      "\u001b[32m[2020-06-27 18:41:17] __main__ INFO: \u001b[0mEpoch 22 Step 200/351 lr 0.000032 loss 0.2913 (0.2445) acc@1 0.8672 (0.9246) acc@5 1.0000 (0.9966)\n",
      "\u001b[32m[2020-06-27 18:43:09] __main__ INFO: \u001b[0mEpoch 22 Step 300/351 lr 0.000032 loss 0.3984 (0.2473) acc@1 0.8438 (0.9241) acc@5 1.0000 (0.9968)\n",
      "\u001b[32m[2020-06-27 18:44:06] __main__ INFO: \u001b[0mEpoch 22 Step 351/351 lr 0.000032 loss 0.2390 (0.2495) acc@1 0.9297 (0.9235) acc@5 1.0000 (0.9970)\n",
      "\u001b[32m[2020-06-27 18:44:06] __main__ INFO: \u001b[0mElapsed 392.82\n",
      "\u001b[32m[2020-06-27 18:44:06] __main__ INFO: \u001b[0mVal 22\n",
      "\u001b[32m[2020-06-27 18:44:20] __main__ INFO: \u001b[0mEpoch 22 loss 0.3564 acc@1 0.8842 acc@5 0.9946\n",
      "\u001b[32m[2020-06-27 18:44:20] __main__ INFO: \u001b[0mElapsed 13.27\n",
      "\u001b[32m[2020-06-27 18:44:20] __main__ INFO: \u001b[0mTrain 23 7722\n",
      "\u001b[32m[2020-06-27 18:46:12] __main__ INFO: \u001b[0mEpoch 23 Step 100/351 lr 0.000032 loss 0.2420 (0.2353) acc@1 0.9375 (0.9298) acc@5 1.0000 (0.9967)\n",
      "\u001b[32m[2020-06-27 18:48:04] __main__ INFO: \u001b[0mEpoch 23 Step 200/351 lr 0.000032 loss 0.2044 (0.2424) acc@1 0.9219 (0.9271) acc@5 1.0000 (0.9973)\n",
      "\u001b[32m[2020-06-27 18:49:57] __main__ INFO: \u001b[0mEpoch 23 Step 300/351 lr 0.000032 loss 0.2882 (0.2435) acc@1 0.9219 (0.9259) acc@5 0.9922 (0.9973)\n",
      "\u001b[32m[2020-06-27 18:50:54] __main__ INFO: \u001b[0mEpoch 23 Step 351/351 lr 0.000032 loss 0.3302 (0.2441) acc@1 0.8984 (0.9253) acc@5 0.9922 (0.9973)\n",
      "\u001b[32m[2020-06-27 18:50:54] __main__ INFO: \u001b[0mElapsed 394.44\n",
      "\u001b[32m[2020-06-27 18:50:54] __main__ INFO: \u001b[0mVal 23\n",
      "\u001b[32m[2020-06-27 18:51:07] __main__ INFO: \u001b[0mEpoch 23 loss 0.3536 acc@1 0.8846 acc@5 0.9946\n",
      "\u001b[32m[2020-06-27 18:51:07] __main__ INFO: \u001b[0mElapsed 13.26\n",
      "\u001b[32m[2020-06-27 18:51:07] __main__ INFO: \u001b[0mTrain 24 8073\n",
      "\u001b[32m[2020-06-27 18:52:59] __main__ INFO: \u001b[0mEpoch 24 Step 100/351 lr 0.000032 loss 0.3431 (0.2394) acc@1 0.9297 (0.9271) acc@5 1.0000 (0.9980)\n",
      "\u001b[32m[2020-06-27 18:54:51] __main__ INFO: \u001b[0mEpoch 24 Step 200/351 lr 0.000032 loss 0.1781 (0.2413) acc@1 0.9297 (0.9263) acc@5 1.0000 (0.9980)\n",
      "\u001b[32m[2020-06-27 18:56:43] __main__ INFO: \u001b[0mEpoch 24 Step 300/351 lr 0.000032 loss 0.4397 (0.2377) acc@1 0.8750 (0.9276) acc@5 0.9844 (0.9979)\n",
      "\u001b[32m[2020-06-27 18:57:40] __main__ INFO: \u001b[0mEpoch 24 Step 351/351 lr 0.000032 loss 0.1383 (0.2375) acc@1 0.9688 (0.9277) acc@5 1.0000 (0.9979)\n",
      "\u001b[32m[2020-06-27 18:57:40] __main__ INFO: \u001b[0mElapsed 392.81\n",
      "\u001b[32m[2020-06-27 18:57:40] __main__ INFO: \u001b[0mVal 24\n",
      "\u001b[32m[2020-06-27 18:57:53] __main__ INFO: \u001b[0mEpoch 24 loss 0.3509 acc@1 0.8856 acc@5 0.9942\n",
      "\u001b[32m[2020-06-27 18:57:53] __main__ INFO: \u001b[0mElapsed 13.24\n",
      "\u001b[32m[2020-06-27 18:57:53] __main__ INFO: \u001b[0mTrain 25 8424\n",
      "\u001b[32m[2020-06-27 18:59:46] __main__ INFO: \u001b[0mEpoch 25 Step 100/351 lr 0.000032 loss 0.3003 (0.2353) acc@1 0.9141 (0.9281) acc@5 0.9922 (0.9971)\n",
      "\u001b[32m[2020-06-27 19:01:38] __main__ INFO: \u001b[0mEpoch 25 Step 200/351 lr 0.000032 loss 0.2058 (0.2356) acc@1 0.9219 (0.9280) acc@5 1.0000 (0.9973)\n",
      "\u001b[32m[2020-06-27 19:03:30] __main__ INFO: \u001b[0mEpoch 25 Step 300/351 lr 0.000032 loss 0.2210 (0.2343) acc@1 0.9453 (0.9286) acc@5 1.0000 (0.9973)\n",
      "\u001b[32m[2020-06-27 19:04:28] __main__ INFO: \u001b[0mEpoch 25 Step 351/351 lr 0.000032 loss 0.1674 (0.2332) acc@1 0.9766 (0.9288) acc@5 1.0000 (0.9973)\n",
      "\u001b[32m[2020-06-27 19:04:28] __main__ INFO: \u001b[0mElapsed 394.28\n",
      "\u001b[32m[2020-06-27 19:04:28] __main__ INFO: \u001b[0mVal 25\n",
      "\u001b[32m[2020-06-27 19:04:41] __main__ INFO: \u001b[0mEpoch 25 loss 0.3503 acc@1 0.8874 acc@5 0.9940\n",
      "\u001b[32m[2020-06-27 19:04:41] __main__ INFO: \u001b[0mElapsed 13.22\n",
      "\u001b[32m[2020-06-27 19:04:41] __main__ INFO: \u001b[0mTrain 26 8775\n",
      "\u001b[32m[2020-06-27 19:06:33] __main__ INFO: \u001b[0mEpoch 26 Step 100/351 lr 0.000032 loss 0.1252 (0.2290) acc@1 0.9766 (0.9296) acc@5 1.0000 (0.9973)\n",
      "\u001b[32m[2020-06-27 19:08:25] __main__ INFO: \u001b[0mEpoch 26 Step 200/351 lr 0.000032 loss 0.2849 (0.2271) acc@1 0.9141 (0.9313) acc@5 0.9922 (0.9970)\n",
      "\u001b[32m[2020-06-27 19:10:17] __main__ INFO: \u001b[0mEpoch 26 Step 300/351 lr 0.000032 loss 0.2174 (0.2275) acc@1 0.9297 (0.9310) acc@5 1.0000 (0.9972)\n",
      "\u001b[32m[2020-06-27 19:11:14] __main__ INFO: \u001b[0mEpoch 26 Step 351/351 lr 0.000032 loss 0.2183 (0.2268) acc@1 0.9219 (0.9313) acc@5 1.0000 (0.9974)\n",
      "\u001b[32m[2020-06-27 19:11:14] __main__ INFO: \u001b[0mElapsed 392.78\n",
      "\u001b[32m[2020-06-27 19:11:14] __main__ INFO: \u001b[0mVal 26\n",
      "\u001b[32m[2020-06-27 19:11:27] __main__ INFO: \u001b[0mEpoch 26 loss 0.3462 acc@1 0.8878 acc@5 0.9948\n",
      "\u001b[32m[2020-06-27 19:11:27] __main__ INFO: \u001b[0mElapsed 13.24\n",
      "\u001b[32m[2020-06-27 19:11:27] __main__ INFO: \u001b[0mTrain 27 9126\n",
      "\u001b[32m[2020-06-27 19:13:19] __main__ INFO: \u001b[0mEpoch 27 Step 100/351 lr 0.000032 loss 0.1579 (0.2290) acc@1 0.9688 (0.9297) acc@5 1.0000 (0.9980)\n",
      "\u001b[32m[2020-06-27 19:15:12] __main__ INFO: \u001b[0mEpoch 27 Step 200/351 lr 0.000032 loss 0.2164 (0.2245) acc@1 0.9375 (0.9315) acc@5 1.0000 (0.9981)\n",
      "\u001b[32m[2020-06-27 19:17:04] __main__ INFO: \u001b[0mEpoch 27 Step 300/351 lr 0.000032 loss 0.1495 (0.2241) acc@1 0.9297 (0.9318) acc@5 1.0000 (0.9978)\n",
      "\u001b[32m[2020-06-27 19:18:01] __main__ INFO: \u001b[0mEpoch 27 Step 351/351 lr 0.000032 loss 0.2527 (0.2243) acc@1 0.9297 (0.9321) acc@5 0.9922 (0.9978)\n",
      "\u001b[32m[2020-06-27 19:18:01] __main__ INFO: \u001b[0mElapsed 394.28\n",
      "\u001b[32m[2020-06-27 19:18:01] __main__ INFO: \u001b[0mVal 27\n",
      "\u001b[32m[2020-06-27 19:18:14] __main__ INFO: \u001b[0mEpoch 27 loss 0.3458 acc@1 0.8888 acc@5 0.9944\n",
      "\u001b[32m[2020-06-27 19:18:14] __main__ INFO: \u001b[0mElapsed 13.25\n",
      "\u001b[32m[2020-06-27 19:18:14] __main__ INFO: \u001b[0mTrain 28 9477\n",
      "\u001b[32m[2020-06-27 19:20:06] __main__ INFO: \u001b[0mEpoch 28 Step 100/351 lr 0.000032 loss 0.2474 (0.2273) acc@1 0.9297 (0.9331) acc@5 1.0000 (0.9976)\n",
      "\u001b[32m[2020-06-27 19:21:58] __main__ INFO: \u001b[0mEpoch 28 Step 200/351 lr 0.000032 loss 0.2174 (0.2242) acc@1 0.9219 (0.9336) acc@5 1.0000 (0.9978)\n",
      "\u001b[32m[2020-06-27 19:23:50] __main__ INFO: \u001b[0mEpoch 28 Step 300/351 lr 0.000032 loss 0.1708 (0.2185) acc@1 0.9531 (0.9345) acc@5 1.0000 (0.9976)\n",
      "\u001b[32m[2020-06-27 19:24:47] __main__ INFO: \u001b[0mEpoch 28 Step 351/351 lr 0.000032 loss 0.1869 (0.2196) acc@1 0.9375 (0.9336) acc@5 1.0000 (0.9978)\n",
      "\u001b[32m[2020-06-27 19:24:47] __main__ INFO: \u001b[0mElapsed 392.83\n",
      "\u001b[32m[2020-06-27 19:24:47] __main__ INFO: \u001b[0mVal 28\n",
      "\u001b[32m[2020-06-27 19:25:01] __main__ INFO: \u001b[0mEpoch 28 loss 0.3445 acc@1 0.8880 acc@5 0.9942\n",
      "\u001b[32m[2020-06-27 19:25:01] __main__ INFO: \u001b[0mElapsed 13.24\n",
      "\u001b[32m[2020-06-27 19:25:01] __main__ INFO: \u001b[0mTrain 29 9828\n",
      "\u001b[32m[2020-06-27 19:26:53] __main__ INFO: \u001b[0mEpoch 29 Step 100/351 lr 0.000032 loss 0.2596 (0.2143) acc@1 0.9062 (0.9362) acc@5 1.0000 (0.9973)\n",
      "\u001b[32m[2020-06-27 19:28:45] __main__ INFO: \u001b[0mEpoch 29 Step 200/351 lr 0.000032 loss 0.1716 (0.2120) acc@1 0.9844 (0.9359) acc@5 1.0000 (0.9977)\n",
      "\u001b[32m[2020-06-27 19:30:38] __main__ INFO: \u001b[0mEpoch 29 Step 300/351 lr 0.000032 loss 0.2876 (0.2153) acc@1 0.9375 (0.9351) acc@5 1.0000 (0.9977)\n",
      "\u001b[32m[2020-06-27 19:31:35] __main__ INFO: \u001b[0mEpoch 29 Step 351/351 lr 0.000032 loss 0.1697 (0.2121) acc@1 0.9531 (0.9361) acc@5 1.0000 (0.9978)\n",
      "\u001b[32m[2020-06-27 19:31:35] __main__ INFO: \u001b[0mElapsed 394.40\n",
      "\u001b[32m[2020-06-27 19:31:35] __main__ INFO: \u001b[0mVal 29\n",
      "\u001b[32m[2020-06-27 19:31:48] __main__ INFO: \u001b[0mEpoch 29 loss 0.3451 acc@1 0.8890 acc@5 0.9948\n",
      "\u001b[32m[2020-06-27 19:31:48] __main__ INFO: \u001b[0mElapsed 13.25\n",
      "\u001b[32m[2020-06-27 19:31:48] __main__ INFO: \u001b[0mTrain 30 10179\n",
      "\u001b[32m[2020-06-27 19:33:40] __main__ INFO: \u001b[0mEpoch 30 Step 100/351 lr 0.000032 loss 0.2622 (0.2235) acc@1 0.9297 (0.9321) acc@5 0.9922 (0.9982)\n",
      "\u001b[32m[2020-06-27 19:35:32] __main__ INFO: \u001b[0mEpoch 30 Step 200/351 lr 0.000032 loss 0.2319 (0.2131) acc@1 0.9453 (0.9364) acc@5 0.9922 (0.9980)\n",
      "\u001b[32m[2020-06-27 19:37:24] __main__ INFO: \u001b[0mEpoch 30 Step 300/351 lr 0.000032 loss 0.2035 (0.2109) acc@1 0.9375 (0.9373) acc@5 1.0000 (0.9980)\n",
      "\u001b[32m[2020-06-27 19:38:21] __main__ INFO: \u001b[0mEpoch 30 Step 351/351 lr 0.000032 loss 0.1823 (0.2099) acc@1 0.9688 (0.9374) acc@5 1.0000 (0.9981)\n",
      "\u001b[32m[2020-06-27 19:38:21] __main__ INFO: \u001b[0mElapsed 392.83\n",
      "\u001b[32m[2020-06-27 19:38:21] __main__ INFO: \u001b[0mVal 30\n",
      "\u001b[32m[2020-06-27 19:38:34] __main__ INFO: \u001b[0mEpoch 30 loss 0.3421 acc@1 0.8912 acc@5 0.9946\n",
      "\u001b[32m[2020-06-27 19:38:34] __main__ INFO: \u001b[0mElapsed 13.28\n",
      "\u001b[32m[2020-06-27 19:38:34] __main__ INFO: \u001b[0mTrain 31 10530\n",
      "\u001b[32m[2020-06-27 19:40:27] __main__ INFO: \u001b[0mEpoch 31 Step 100/351 lr 0.000032 loss 0.2049 (0.2018) acc@1 0.9375 (0.9387) acc@5 0.9922 (0.9980)\n",
      "\u001b[32m[2020-06-27 19:42:19] __main__ INFO: \u001b[0mEpoch 31 Step 200/351 lr 0.000032 loss 0.1941 (0.2074) acc@1 0.9375 (0.9377) acc@5 1.0000 (0.9980)\n",
      "\u001b[32m[2020-06-27 19:44:11] __main__ INFO: \u001b[0mEpoch 31 Step 300/351 lr 0.000032 loss 0.2071 (0.2026) acc@1 0.9531 (0.9397) acc@5 1.0000 (0.9981)\n",
      "\u001b[32m[2020-06-27 19:45:09] __main__ INFO: \u001b[0mEpoch 31 Step 351/351 lr 0.000032 loss 0.1429 (0.2035) acc@1 0.9609 (0.9393) acc@5 1.0000 (0.9981)\n",
      "\u001b[32m[2020-06-27 19:45:09] __main__ INFO: \u001b[0mElapsed 394.43\n",
      "\u001b[32m[2020-06-27 19:45:09] __main__ INFO: \u001b[0mVal 31\n",
      "\u001b[32m[2020-06-27 19:45:22] __main__ INFO: \u001b[0mEpoch 31 loss 0.3416 acc@1 0.8906 acc@5 0.9948\n",
      "\u001b[32m[2020-06-27 19:45:22] __main__ INFO: \u001b[0mElapsed 13.22\n",
      "\u001b[32m[2020-06-27 19:45:22] __main__ INFO: \u001b[0mTrain 32 10881\n",
      "\u001b[32m[2020-06-27 19:47:14] __main__ INFO: \u001b[0mEpoch 32 Step 100/351 lr 0.000032 loss 0.1459 (0.2130) acc@1 0.9531 (0.9340) acc@5 1.0000 (0.9978)\n",
      "\u001b[32m[2020-06-27 19:49:06] __main__ INFO: \u001b[0mEpoch 32 Step 200/351 lr 0.000032 loss 0.1669 (0.2035) acc@1 0.9609 (0.9384) acc@5 1.0000 (0.9982)\n",
      "\u001b[32m[2020-06-27 19:50:58] __main__ INFO: \u001b[0mEpoch 32 Step 300/351 lr 0.000032 loss 0.2170 (0.2028) acc@1 0.9375 (0.9391) acc@5 1.0000 (0.9982)\n",
      "\u001b[32m[2020-06-27 19:51:55] __main__ INFO: \u001b[0mEpoch 32 Step 351/351 lr 0.000032 loss 0.1416 (0.2016) acc@1 0.9688 (0.9395) acc@5 1.0000 (0.9982)\n",
      "\u001b[32m[2020-06-27 19:51:55] __main__ INFO: \u001b[0mElapsed 392.85\n",
      "\u001b[32m[2020-06-27 19:51:55] __main__ INFO: \u001b[0mVal 32\n",
      "\u001b[32m[2020-06-27 19:52:08] __main__ INFO: \u001b[0mEpoch 32 loss 0.3404 acc@1 0.8898 acc@5 0.9946\n",
      "\u001b[32m[2020-06-27 19:52:08] __main__ INFO: \u001b[0mElapsed 13.27\n",
      "\u001b[32m[2020-06-27 19:52:08] __main__ INFO: \u001b[0mTrain 33 11232\n",
      "\u001b[32m[2020-06-27 19:54:01] __main__ INFO: \u001b[0mEpoch 33 Step 100/351 lr 0.000032 loss 0.1913 (0.1881) acc@1 0.9453 (0.9452) acc@5 1.0000 (0.9989)\n",
      "\u001b[32m[2020-06-27 19:55:53] __main__ INFO: \u001b[0mEpoch 33 Step 200/351 lr 0.000032 loss 0.1540 (0.1939) acc@1 0.9453 (0.9431) acc@5 1.0000 (0.9986)\n",
      "\u001b[32m[2020-06-27 19:57:45] __main__ INFO: \u001b[0mEpoch 33 Step 300/351 lr 0.000032 loss 0.2059 (0.1948) acc@1 0.9219 (0.9425) acc@5 1.0000 (0.9985)\n",
      "\u001b[32m[2020-06-27 19:58:42] __main__ INFO: \u001b[0mEpoch 33 Step 351/351 lr 0.000032 loss 0.2046 (0.1962) acc@1 0.9375 (0.9421) acc@5 1.0000 (0.9984)\n",
      "\u001b[32m[2020-06-27 19:58:42] __main__ INFO: \u001b[0mElapsed 394.37\n",
      "\u001b[32m[2020-06-27 19:58:42] __main__ INFO: \u001b[0mVal 33\n",
      "\u001b[32m[2020-06-27 19:58:56] __main__ INFO: \u001b[0mEpoch 33 loss 0.3396 acc@1 0.8910 acc@5 0.9944\n",
      "\u001b[32m[2020-06-27 19:58:56] __main__ INFO: \u001b[0mElapsed 13.25\n",
      "\u001b[32m[2020-06-27 19:58:56] __main__ INFO: \u001b[0mTrain 34 11583\n",
      "\u001b[32m[2020-06-27 20:00:48] __main__ INFO: \u001b[0mEpoch 34 Step 100/351 lr 0.000032 loss 0.2801 (0.1952) acc@1 0.9141 (0.9430) acc@5 1.0000 (0.9976)\n",
      "\u001b[32m[2020-06-27 20:02:40] __main__ INFO: \u001b[0mEpoch 34 Step 200/351 lr 0.000032 loss 0.2378 (0.1934) acc@1 0.9219 (0.9434) acc@5 1.0000 (0.9979)\n",
      "\u001b[32m[2020-06-27 20:04:31] __main__ INFO: \u001b[0mEpoch 34 Step 300/351 lr 0.000032 loss 0.1674 (0.1939) acc@1 0.9297 (0.9425) acc@5 1.0000 (0.9980)\n",
      "\u001b[32m[2020-06-27 20:05:29] __main__ INFO: \u001b[0mEpoch 34 Step 351/351 lr 0.000032 loss 0.2432 (0.1932) acc@1 0.9141 (0.9428) acc@5 0.9922 (0.9980)\n",
      "\u001b[32m[2020-06-27 20:05:29] __main__ INFO: \u001b[0mElapsed 392.81\n",
      "\u001b[32m[2020-06-27 20:05:29] __main__ INFO: \u001b[0mVal 34\n",
      "\u001b[32m[2020-06-27 20:05:42] __main__ INFO: \u001b[0mEpoch 34 loss 0.3402 acc@1 0.8922 acc@5 0.9948\n",
      "\u001b[32m[2020-06-27 20:05:42] __main__ INFO: \u001b[0mElapsed 13.22\n",
      "\u001b[32m[2020-06-27 20:05:42] __main__ INFO: \u001b[0mTrain 35 11934\n",
      "\u001b[32m[2020-06-27 20:07:34] __main__ INFO: \u001b[0mEpoch 35 Step 100/351 lr 0.000032 loss 0.1880 (0.1887) acc@1 0.9688 (0.9455) acc@5 1.0000 (0.9977)\n",
      "\u001b[32m[2020-06-27 20:09:27] __main__ INFO: \u001b[0mEpoch 35 Step 200/351 lr 0.000032 loss 0.1046 (0.1890) acc@1 0.9766 (0.9448) acc@5 1.0000 (0.9983)\n",
      "\u001b[32m[2020-06-27 20:11:19] __main__ INFO: \u001b[0mEpoch 35 Step 300/351 lr 0.000032 loss 0.1762 (0.1899) acc@1 0.9375 (0.9443) acc@5 1.0000 (0.9982)\n",
      "\u001b[32m[2020-06-27 20:12:16] __main__ INFO: \u001b[0mEpoch 35 Step 351/351 lr 0.000032 loss 0.1918 (0.1902) acc@1 0.9297 (0.9441) acc@5 1.0000 (0.9983)\n",
      "\u001b[32m[2020-06-27 20:12:16] __main__ INFO: \u001b[0mElapsed 394.52\n",
      "\u001b[32m[2020-06-27 20:12:16] __main__ INFO: \u001b[0mVal 35\n",
      "\u001b[32m[2020-06-27 20:12:30] __main__ INFO: \u001b[0mEpoch 35 loss 0.3389 acc@1 0.8892 acc@5 0.9946\n",
      "\u001b[32m[2020-06-27 20:12:30] __main__ INFO: \u001b[0mElapsed 13.27\n",
      "\u001b[32m[2020-06-27 20:12:30] __main__ INFO: \u001b[0mTrain 36 12285\n",
      "\u001b[32m[2020-06-27 20:14:22] __main__ INFO: \u001b[0mEpoch 36 Step 100/351 lr 0.000032 loss 0.2735 (0.1825) acc@1 0.9062 (0.9484) acc@5 1.0000 (0.9987)\n",
      "\u001b[32m[2020-06-27 20:16:13] __main__ INFO: \u001b[0mEpoch 36 Step 200/351 lr 0.000032 loss 0.1491 (0.1823) acc@1 0.9688 (0.9480) acc@5 1.0000 (0.9985)\n",
      "\u001b[32m[2020-06-27 20:18:05] __main__ INFO: \u001b[0mEpoch 36 Step 300/351 lr 0.000032 loss 0.1655 (0.1846) acc@1 0.9453 (0.9463) acc@5 1.0000 (0.9983)\n",
      "\u001b[32m[2020-06-27 20:19:02] __main__ INFO: \u001b[0mEpoch 36 Step 351/351 lr 0.000032 loss 0.1143 (0.1845) acc@1 0.9766 (0.9465) acc@5 1.0000 (0.9983)\n",
      "\u001b[32m[2020-06-27 20:19:02] __main__ INFO: \u001b[0mElapsed 392.87\n",
      "\u001b[32m[2020-06-27 20:19:02] __main__ INFO: \u001b[0mVal 36\n",
      "\u001b[32m[2020-06-27 20:19:16] __main__ INFO: \u001b[0mEpoch 36 loss 0.3378 acc@1 0.8924 acc@5 0.9948\n",
      "\u001b[32m[2020-06-27 20:19:16] __main__ INFO: \u001b[0mElapsed 13.29\n",
      "\u001b[32m[2020-06-27 20:19:16] __main__ INFO: \u001b[0mTrain 37 12636\n",
      "\u001b[32m[2020-06-27 20:21:08] __main__ INFO: \u001b[0mEpoch 37 Step 100/351 lr 0.000032 loss 0.2382 (0.1787) acc@1 0.9375 (0.9486) acc@5 1.0000 (0.9989)\n",
      "\u001b[32m[2020-06-27 20:23:01] __main__ INFO: \u001b[0mEpoch 37 Step 200/351 lr 0.000032 loss 0.2356 (0.1811) acc@1 0.9453 (0.9473) acc@5 1.0000 (0.9983)\n",
      "\u001b[32m[2020-06-27 20:24:53] __main__ INFO: \u001b[0mEpoch 37 Step 300/351 lr 0.000032 loss 0.1571 (0.1838) acc@1 0.9531 (0.9456) acc@5 1.0000 (0.9983)\n",
      "\u001b[32m[2020-06-27 20:25:50] __main__ INFO: \u001b[0mEpoch 37 Step 351/351 lr 0.000032 loss 0.2476 (0.1832) acc@1 0.9062 (0.9458) acc@5 1.0000 (0.9983)\n",
      "\u001b[32m[2020-06-27 20:25:50] __main__ INFO: \u001b[0mElapsed 394.45\n",
      "\u001b[32m[2020-06-27 20:25:50] __main__ INFO: \u001b[0mVal 37\n",
      "\u001b[32m[2020-06-27 20:26:03] __main__ INFO: \u001b[0mEpoch 37 loss 0.3386 acc@1 0.8912 acc@5 0.9948\n",
      "\u001b[32m[2020-06-27 20:26:03] __main__ INFO: \u001b[0mElapsed 13.25\n",
      "\u001b[32m[2020-06-27 20:26:03] __main__ INFO: \u001b[0mTrain 38 12987\n",
      "\u001b[32m[2020-06-27 20:27:55] __main__ INFO: \u001b[0mEpoch 38 Step 100/351 lr 0.000032 loss 0.2149 (0.1761) acc@1 0.9453 (0.9495) acc@5 0.9922 (0.9981)\n",
      "\u001b[32m[2020-06-27 20:29:47] __main__ INFO: \u001b[0mEpoch 38 Step 200/351 lr 0.000032 loss 0.1631 (0.1782) acc@1 0.9531 (0.9482) acc@5 1.0000 (0.9983)\n",
      "\u001b[32m[2020-06-27 20:31:39] __main__ INFO: \u001b[0mEpoch 38 Step 300/351 lr 0.000032 loss 0.1955 (0.1795) acc@1 0.9453 (0.9473) acc@5 1.0000 (0.9984)\n",
      "\u001b[32m[2020-06-27 20:32:36] __main__ INFO: \u001b[0mEpoch 38 Step 351/351 lr 0.000032 loss 0.1803 (0.1802) acc@1 0.9453 (0.9471) acc@5 0.9844 (0.9984)\n",
      "\u001b[32m[2020-06-27 20:32:36] __main__ INFO: \u001b[0mElapsed 392.90\n",
      "\u001b[32m[2020-06-27 20:32:36] __main__ INFO: \u001b[0mVal 38\n",
      "\u001b[32m[2020-06-27 20:32:50] __main__ INFO: \u001b[0mEpoch 38 loss 0.3371 acc@1 0.8910 acc@5 0.9954\n",
      "\u001b[32m[2020-06-27 20:32:50] __main__ INFO: \u001b[0mElapsed 13.26\n",
      "\u001b[32m[2020-06-27 20:32:50] __main__ INFO: \u001b[0mTrain 39 13338\n",
      "\u001b[32m[2020-06-27 20:34:42] __main__ INFO: \u001b[0mEpoch 39 Step 100/351 lr 0.000032 loss 0.1237 (0.1740) acc@1 0.9766 (0.9501) acc@5 1.0000 (0.9982)\n",
      "\u001b[32m[2020-06-27 20:36:34] __main__ INFO: \u001b[0mEpoch 39 Step 200/351 lr 0.000032 loss 0.1224 (0.1741) acc@1 0.9766 (0.9500) acc@5 1.0000 (0.9984)\n",
      "\u001b[32m[2020-06-27 20:38:27] __main__ INFO: \u001b[0mEpoch 39 Step 300/351 lr 0.000032 loss 0.1946 (0.1746) acc@1 0.9453 (0.9496) acc@5 1.0000 (0.9986)\n",
      "\u001b[32m[2020-06-27 20:39:24] __main__ INFO: \u001b[0mEpoch 39 Step 351/351 lr 0.000032 loss 0.2505 (0.1748) acc@1 0.9141 (0.9497) acc@5 0.9922 (0.9986)\n",
      "\u001b[32m[2020-06-27 20:39:24] __main__ INFO: \u001b[0mElapsed 394.47\n",
      "\u001b[32m[2020-06-27 20:39:24] __main__ INFO: \u001b[0mVal 39\n",
      "\u001b[32m[2020-06-27 20:39:37] __main__ INFO: \u001b[0mEpoch 39 loss 0.3379 acc@1 0.8912 acc@5 0.9954\n",
      "\u001b[32m[2020-06-27 20:39:37] __main__ INFO: \u001b[0mElapsed 13.25\n",
      "\u001b[32m[2020-06-27 20:39:37] __main__ INFO: \u001b[0mTrain 40 13689\n",
      "\u001b[32m[2020-06-27 20:41:29] __main__ INFO: \u001b[0mEpoch 40 Step 100/351 lr 0.000032 loss 0.1104 (0.1768) acc@1 0.9609 (0.9480) acc@5 1.0000 (0.9980)\n",
      "\u001b[32m[2020-06-27 20:43:21] __main__ INFO: \u001b[0mEpoch 40 Step 200/351 lr 0.000032 loss 0.1550 (0.1759) acc@1 0.9531 (0.9482) acc@5 1.0000 (0.9984)\n",
      "\u001b[32m[2020-06-27 20:45:13] __main__ INFO: \u001b[0mEpoch 40 Step 300/351 lr 0.000032 loss 0.1492 (0.1717) acc@1 0.9531 (0.9494) acc@5 0.9922 (0.9984)\n",
      "\u001b[32m[2020-06-27 20:46:10] __main__ INFO: \u001b[0mEpoch 40 Step 351/351 lr 0.000032 loss 0.1640 (0.1735) acc@1 0.9453 (0.9490) acc@5 1.0000 (0.9985)\n",
      "\u001b[32m[2020-06-27 20:46:10] __main__ INFO: \u001b[0mElapsed 392.92\n",
      "\u001b[32m[2020-06-27 20:46:10] __main__ INFO: \u001b[0mVal 40\n",
      "\u001b[32m[2020-06-27 20:46:23] __main__ INFO: \u001b[0mEpoch 40 loss 0.3376 acc@1 0.8914 acc@5 0.9944\n",
      "\u001b[32m[2020-06-27 20:46:23] __main__ INFO: \u001b[0mElapsed 13.24\n",
      "\u001b[32m[2020-06-27 20:46:23] __main__ INFO: \u001b[0mTrain 41 14040\n",
      "\u001b[32m[2020-06-27 20:48:16] __main__ INFO: \u001b[0mEpoch 41 Step 100/351 lr 0.000032 loss 0.1406 (0.1721) acc@1 0.9766 (0.9479) acc@5 1.0000 (0.9989)\n",
      "\u001b[32m[2020-06-27 20:50:08] __main__ INFO: \u001b[0mEpoch 41 Step 200/351 lr 0.000032 loss 0.1694 (0.1685) acc@1 0.9609 (0.9506) acc@5 1.0000 (0.9986)\n",
      "\u001b[32m[2020-06-27 20:52:01] __main__ INFO: \u001b[0mEpoch 41 Step 300/351 lr 0.000032 loss 0.1330 (0.1680) acc@1 0.9688 (0.9507) acc@5 1.0000 (0.9986)\n",
      "\u001b[32m[2020-06-27 20:52:58] __main__ INFO: \u001b[0mEpoch 41 Step 351/351 lr 0.000032 loss 0.1203 (0.1672) acc@1 0.9844 (0.9511) acc@5 1.0000 (0.9987)\n",
      "\u001b[32m[2020-06-27 20:52:58] __main__ INFO: \u001b[0mElapsed 394.53\n",
      "\u001b[32m[2020-06-27 20:52:58] __main__ INFO: \u001b[0mVal 41\n",
      "\u001b[32m[2020-06-27 20:53:11] __main__ INFO: \u001b[0mEpoch 41 loss 0.3381 acc@1 0.8932 acc@5 0.9952\n",
      "\u001b[32m[2020-06-27 20:53:11] __main__ INFO: \u001b[0mElapsed 13.26\n",
      "\u001b[32m[2020-06-27 20:53:11] __main__ INFO: \u001b[0mTrain 42 14391\n",
      "\u001b[32m[2020-06-27 20:55:03] __main__ INFO: \u001b[0mEpoch 42 Step 100/351 lr 0.000032 loss 0.1354 (0.1559) acc@1 0.9766 (0.9559) acc@5 1.0000 (0.9990)\n",
      "\u001b[32m[2020-06-27 20:56:55] __main__ INFO: \u001b[0mEpoch 42 Step 200/351 lr 0.000032 loss 0.1171 (0.1599) acc@1 0.9766 (0.9539) acc@5 1.0000 (0.9989)\n",
      "\u001b[32m[2020-06-27 20:58:47] __main__ INFO: \u001b[0mEpoch 42 Step 300/351 lr 0.000032 loss 0.1951 (0.1642) acc@1 0.9531 (0.9526) acc@5 0.9922 (0.9987)\n",
      "\u001b[32m[2020-06-27 20:59:44] __main__ INFO: \u001b[0mEpoch 42 Step 351/351 lr 0.000032 loss 0.1436 (0.1646) acc@1 0.9609 (0.9523) acc@5 0.9922 (0.9987)\n",
      "\u001b[32m[2020-06-27 20:59:44] __main__ INFO: \u001b[0mElapsed 393.08\n",
      "\u001b[32m[2020-06-27 20:59:44] __main__ INFO: \u001b[0mVal 42\n",
      "\u001b[32m[2020-06-27 20:59:58] __main__ INFO: \u001b[0mEpoch 42 loss 0.3377 acc@1 0.8914 acc@5 0.9952\n",
      "\u001b[32m[2020-06-27 20:59:58] __main__ INFO: \u001b[0mElapsed 13.27\n",
      "\u001b[32m[2020-06-27 20:59:58] __main__ INFO: \u001b[0mTrain 43 14742\n",
      "\u001b[32m[2020-06-27 21:01:50] __main__ INFO: \u001b[0mEpoch 43 Step 100/351 lr 0.000032 loss 0.1879 (0.1630) acc@1 0.9453 (0.9543) acc@5 1.0000 (0.9983)\n",
      "\u001b[32m[2020-06-27 21:03:42] __main__ INFO: \u001b[0mEpoch 43 Step 200/351 lr 0.000032 loss 0.0903 (0.1606) acc@1 0.9844 (0.9552) acc@5 1.0000 (0.9983)\n",
      "\u001b[32m[2020-06-27 21:05:35] __main__ INFO: \u001b[0mEpoch 43 Step 300/351 lr 0.000032 loss 0.1372 (0.1617) acc@1 0.9453 (0.9547) acc@5 1.0000 (0.9984)\n",
      "\u001b[32m[2020-06-27 21:06:32] __main__ INFO: \u001b[0mEpoch 43 Step 351/351 lr 0.000032 loss 0.1056 (0.1607) acc@1 0.9766 (0.9552) acc@5 1.0000 (0.9984)\n",
      "\u001b[32m[2020-06-27 21:06:32] __main__ INFO: \u001b[0mElapsed 394.58\n",
      "\u001b[32m[2020-06-27 21:06:32] __main__ INFO: \u001b[0mVal 43\n",
      "\u001b[32m[2020-06-27 21:06:45] __main__ INFO: \u001b[0mEpoch 43 loss 0.3373 acc@1 0.8918 acc@5 0.9950\n",
      "\u001b[32m[2020-06-27 21:06:45] __main__ INFO: \u001b[0mElapsed 13.21\n",
      "\u001b[32m[2020-06-27 21:06:45] __main__ INFO: \u001b[0mTrain 44 15093\n",
      "\u001b[32m[2020-06-27 21:08:37] __main__ INFO: \u001b[0mEpoch 44 Step 100/351 lr 0.000032 loss 0.1525 (0.1624) acc@1 0.9297 (0.9534) acc@5 1.0000 (0.9990)\n",
      "\u001b[32m[2020-06-27 21:10:29] __main__ INFO: \u001b[0mEpoch 44 Step 200/351 lr 0.000032 loss 0.1189 (0.1575) acc@1 0.9766 (0.9549) acc@5 1.0000 (0.9990)\n",
      "\u001b[32m[2020-06-27 21:12:21] __main__ INFO: \u001b[0mEpoch 44 Step 300/351 lr 0.000032 loss 0.1404 (0.1601) acc@1 0.9531 (0.9541) acc@5 1.0000 (0.9989)\n",
      "\u001b[32m[2020-06-27 21:13:18] __main__ INFO: \u001b[0mEpoch 44 Step 351/351 lr 0.000032 loss 0.1916 (0.1605) acc@1 0.9453 (0.9544) acc@5 1.0000 (0.9989)\n",
      "\u001b[32m[2020-06-27 21:13:19] __main__ INFO: \u001b[0mElapsed 393.09\n",
      "\u001b[32m[2020-06-27 21:13:19] __main__ INFO: \u001b[0mVal 44\n",
      "\u001b[32m[2020-06-27 21:13:32] __main__ INFO: \u001b[0mEpoch 44 loss 0.3375 acc@1 0.8928 acc@5 0.9952\n",
      "\u001b[32m[2020-06-27 21:13:32] __main__ INFO: \u001b[0mElapsed 13.27\n",
      "\u001b[32m[2020-06-27 21:13:32] __main__ INFO: \u001b[0mTrain 45 15444\n",
      "\u001b[32m[2020-06-27 21:15:24] __main__ INFO: \u001b[0mEpoch 45 Step 100/351 lr 0.000032 loss 0.1533 (0.1582) acc@1 0.9688 (0.9555) acc@5 1.0000 (0.9988)\n",
      "\u001b[32m[2020-06-27 21:17:17] __main__ INFO: \u001b[0mEpoch 45 Step 200/351 lr 0.000032 loss 0.1970 (0.1582) acc@1 0.9297 (0.9555) acc@5 1.0000 (0.9987)\n",
      "\u001b[32m[2020-06-27 21:19:09] __main__ INFO: \u001b[0mEpoch 45 Step 300/351 lr 0.000032 loss 0.1440 (0.1558) acc@1 0.9609 (0.9563) acc@5 1.0000 (0.9987)\n",
      "\u001b[32m[2020-06-27 21:20:06] __main__ INFO: \u001b[0mEpoch 45 Step 351/351 lr 0.000032 loss 0.1540 (0.1556) acc@1 0.9453 (0.9561) acc@5 1.0000 (0.9987)\n",
      "\u001b[32m[2020-06-27 21:20:06] __main__ INFO: \u001b[0mElapsed 394.66\n",
      "\u001b[32m[2020-06-27 21:20:06] __main__ INFO: \u001b[0mVal 45\n",
      "\u001b[32m[2020-06-27 21:20:20] __main__ INFO: \u001b[0mEpoch 45 loss 0.3362 acc@1 0.8934 acc@5 0.9948\n",
      "\u001b[32m[2020-06-27 21:20:20] __main__ INFO: \u001b[0mElapsed 13.28\n",
      "\u001b[32m[2020-06-27 21:20:20] __main__ INFO: \u001b[0mTrain 46 15795\n",
      "\u001b[32m[2020-06-27 21:22:12] __main__ INFO: \u001b[0mEpoch 46 Step 100/351 lr 0.000032 loss 0.1299 (0.1503) acc@1 0.9609 (0.9576) acc@5 1.0000 (0.9988)\n",
      "\u001b[32m[2020-06-27 21:24:04] __main__ INFO: \u001b[0mEpoch 46 Step 200/351 lr 0.000032 loss 0.0951 (0.1499) acc@1 0.9844 (0.9580) acc@5 1.0000 (0.9989)\n",
      "\u001b[32m[2020-06-27 21:25:56] __main__ INFO: \u001b[0mEpoch 46 Step 300/351 lr 0.000032 loss 0.1348 (0.1509) acc@1 0.9609 (0.9576) acc@5 1.0000 (0.9988)\n",
      "\u001b[32m[2020-06-27 21:26:53] __main__ INFO: \u001b[0mEpoch 46 Step 351/351 lr 0.000032 loss 0.2385 (0.1504) acc@1 0.9297 (0.9576) acc@5 0.9922 (0.9989)\n",
      "\u001b[32m[2020-06-27 21:26:53] __main__ INFO: \u001b[0mElapsed 393.17\n",
      "\u001b[32m[2020-06-27 21:26:53] __main__ INFO: \u001b[0mVal 46\n",
      "\u001b[32m[2020-06-27 21:27:06] __main__ INFO: \u001b[0mEpoch 46 loss 0.3358 acc@1 0.8934 acc@5 0.9954\n",
      "\u001b[32m[2020-06-27 21:27:06] __main__ INFO: \u001b[0mElapsed 13.24\n",
      "\u001b[32m[2020-06-27 21:27:06] __main__ INFO: \u001b[0mTrain 47 16146\n",
      "\u001b[32m[2020-06-27 21:28:59] __main__ INFO: \u001b[0mEpoch 47 Step 100/351 lr 0.000032 loss 0.1060 (0.1442) acc@1 0.9609 (0.9612) acc@5 1.0000 (0.9988)\n",
      "\u001b[32m[2020-06-27 21:30:51] __main__ INFO: \u001b[0mEpoch 47 Step 200/351 lr 0.000032 loss 0.2316 (0.1475) acc@1 0.9531 (0.9587) acc@5 1.0000 (0.9989)\n",
      "\u001b[32m[2020-06-27 21:32:43] __main__ INFO: \u001b[0mEpoch 47 Step 300/351 lr 0.000032 loss 0.1766 (0.1481) acc@1 0.9531 (0.9583) acc@5 1.0000 (0.9989)\n",
      "\u001b[32m[2020-06-27 21:33:41] __main__ INFO: \u001b[0mEpoch 47 Step 351/351 lr 0.000032 loss 0.1396 (0.1487) acc@1 0.9609 (0.9584) acc@5 1.0000 (0.9988)\n",
      "\u001b[32m[2020-06-27 21:33:41] __main__ INFO: \u001b[0mElapsed 394.60\n",
      "\u001b[32m[2020-06-27 21:33:41] __main__ INFO: \u001b[0mVal 47\n",
      "\u001b[32m[2020-06-27 21:33:54] __main__ INFO: \u001b[0mEpoch 47 loss 0.3346 acc@1 0.8942 acc@5 0.9944\n",
      "\u001b[32m[2020-06-27 21:33:54] __main__ INFO: \u001b[0mElapsed 13.23\n",
      "\u001b[32m[2020-06-27 21:33:54] __main__ INFO: \u001b[0mTrain 48 16497\n",
      "\u001b[32m[2020-06-27 21:35:46] __main__ INFO: \u001b[0mEpoch 48 Step 100/351 lr 0.000032 loss 0.0972 (0.1372) acc@1 0.9609 (0.9622) acc@5 1.0000 (0.9991)\n",
      "\u001b[32m[2020-06-27 21:37:38] __main__ INFO: \u001b[0mEpoch 48 Step 200/351 lr 0.000032 loss 0.1389 (0.1401) acc@1 0.9609 (0.9607) acc@5 1.0000 (0.9991)\n",
      "\u001b[32m[2020-06-27 21:39:30] __main__ INFO: \u001b[0mEpoch 48 Step 300/351 lr 0.000032 loss 0.1779 (0.1430) acc@1 0.9375 (0.9600) acc@5 1.0000 (0.9989)\n",
      "\u001b[32m[2020-06-27 21:40:27] __main__ INFO: \u001b[0mEpoch 48 Step 351/351 lr 0.000032 loss 0.1419 (0.1434) acc@1 0.9453 (0.9601) acc@5 0.9922 (0.9988)\n",
      "\u001b[32m[2020-06-27 21:40:27] __main__ INFO: \u001b[0mElapsed 393.13\n",
      "\u001b[32m[2020-06-27 21:40:27] __main__ INFO: \u001b[0mVal 48\n",
      "\u001b[32m[2020-06-27 21:40:40] __main__ INFO: \u001b[0mEpoch 48 loss 0.3355 acc@1 0.8940 acc@5 0.9954\n",
      "\u001b[32m[2020-06-27 21:40:40] __main__ INFO: \u001b[0mElapsed 13.23\n",
      "\u001b[32m[2020-06-27 21:40:40] __main__ INFO: \u001b[0mTrain 49 16848\n",
      "\u001b[32m[2020-06-27 21:42:33] __main__ INFO: \u001b[0mEpoch 49 Step 100/351 lr 0.000032 loss 0.1223 (0.1428) acc@1 0.9688 (0.9611) acc@5 1.0000 (0.9987)\n",
      "\u001b[32m[2020-06-27 21:44:25] __main__ INFO: \u001b[0mEpoch 49 Step 200/351 lr 0.000032 loss 0.1110 (0.1408) acc@1 0.9688 (0.9611) acc@5 1.0000 (0.9989)\n",
      "\u001b[32m[2020-06-27 21:46:18] __main__ INFO: \u001b[0mEpoch 49 Step 300/351 lr 0.000032 loss 0.1600 (0.1411) acc@1 0.9531 (0.9612) acc@5 1.0000 (0.9988)\n",
      "\u001b[32m[2020-06-27 21:47:15] __main__ INFO: \u001b[0mEpoch 49 Step 351/351 lr 0.000032 loss 0.1145 (0.1408) acc@1 0.9531 (0.9609) acc@5 1.0000 (0.9988)\n",
      "\u001b[32m[2020-06-27 21:47:15] __main__ INFO: \u001b[0mElapsed 394.70\n",
      "\u001b[32m[2020-06-27 21:47:15] __main__ INFO: \u001b[0mVal 49\n",
      "\u001b[32m[2020-06-27 21:47:28] __main__ INFO: \u001b[0mEpoch 49 loss 0.3354 acc@1 0.8934 acc@5 0.9952\n",
      "\u001b[32m[2020-06-27 21:47:28] __main__ INFO: \u001b[0mElapsed 13.25\n",
      "\u001b[32m[2020-06-27 21:47:28] __main__ INFO: \u001b[0mTrain 50 17199\n",
      "\u001b[32m[2020-06-27 21:49:20] __main__ INFO: \u001b[0mEpoch 50 Step 100/351 lr 0.000032 loss 0.1082 (0.1398) acc@1 0.9688 (0.9608) acc@5 1.0000 (0.9988)\n",
      "\u001b[32m[2020-06-27 21:51:12] __main__ INFO: \u001b[0mEpoch 50 Step 200/351 lr 0.000032 loss 0.2011 (0.1399) acc@1 0.9297 (0.9621) acc@5 0.9922 (0.9985)\n",
      "\u001b[32m[2020-06-27 21:53:04] __main__ INFO: \u001b[0mEpoch 50 Step 300/351 lr 0.000032 loss 0.0905 (0.1403) acc@1 0.9766 (0.9621) acc@5 1.0000 (0.9986)\n",
      "\u001b[32m[2020-06-27 21:54:01] __main__ INFO: \u001b[0mEpoch 50 Step 351/351 lr 0.000032 loss 0.2128 (0.1398) acc@1 0.9297 (0.9622) acc@5 1.0000 (0.9986)\n",
      "\u001b[32m[2020-06-27 21:54:01] __main__ INFO: \u001b[0mElapsed 393.03\n",
      "\u001b[32m[2020-06-27 21:54:01] __main__ INFO: \u001b[0mVal 50\n",
      "\u001b[32m[2020-06-27 21:54:15] __main__ INFO: \u001b[0mEpoch 50 loss 0.3356 acc@1 0.8948 acc@5 0.9948\n",
      "\u001b[32m[2020-06-27 21:54:15] __main__ INFO: \u001b[0mElapsed 13.26\n",
      "\u001b[32m[2020-06-27 21:54:15] fvcore.common.checkpoint INFO: \u001b[0mSaving checkpoint to /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_3_20/exp00_resume400_50/checkpoint_00050.pth\n"
     ]
    }
   ],
   "source": [
    "# Resume training with the un-augmented data\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "!python train.py --config /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_3_20/exp02/config.yaml \\\n",
    "    train.checkpoint /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_3_20/exp02/checkpoint_00400.pth \\\n",
    "    model.wrn.depth 28 \\\n",
    "    model.wrn.widening_factor 10 \\\n",
    "    train.batch_size 128 \\\n",
    "    train.base_lr 0.000032 \\\n",
    "    dataset.name CIFAR10 \\\n",
    "    train.output_dir /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_3_20/exp00_resume400_50 \\\n",
    "    scheduler.epochs 50\n",
    "\n",
    "#### Set LEARNING RATE based on ending LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume training with the un-augmented data\n",
    "# os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "# #!python train.py --config configs/cifar/resnet.yaml \\\n",
    "# !python train.py --config /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_5/exp00/config.yaml \\\n",
    "#     train.checkpoint /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_5/exp00/checkpoint_00300.pth \\\n",
    "#     dataset.name CIFAR10 \\\n",
    "#     train.base_lr .001 \\\n",
    "#     train.output_dir /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_5/exp00_resume300_150 \\\n",
    "#     scheduler.epochs 150\n",
    "\n",
    "#### Set LEARNING RATE based on ending LR\n",
    "#    train.resume True \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-27 22:07:56] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_3_20/exp00_resume400_50/checkpoint_00050.pth\n",
      "Files already downloaded and verified\n",
      "100%|| 40/40 [00:51<00:00,  1.28s/it]\n",
      "\u001b[32m[2020-06-27 22:08:48] __main__ INFO: \u001b[0mElapsed 51.32\n",
      "\u001b[32m[2020-06-27 22:08:48] __main__ INFO: \u001b[0mLoss 0.3287 Accuracy 0.9001\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/wrn.yaml \\\n",
    "    model.wrn.depth 28 \\\n",
    "    model.wrn.widening_factor 10 \\\n",
    "    test.checkpoint /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_3_20/exp00_resume400_50/checkpoint_00050.pth \\\n",
    "    test.output_dir /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_3_20/exp00_resume400_50/test_results_0050_cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-20 16:32:43] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_5/exp00_resume300_150/checkpoint_00150.pth\n",
      "Files already downloaded and verified\n",
      "100%|| 79/79 [00:02<00:00, 26.53it/s]\n",
      "\u001b[32m[2020-06-20 16:32:46] __main__ INFO: \u001b[0mElapsed 2.98\n",
      "\u001b[32m[2020-06-20 16:32:46] __main__ INFO: \u001b[0mLoss 0.4499 Accuracy 0.8795\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "# !python evaluate.py --config configs/cifar/wrn.yaml \\\n",
    "#     model.wrn.depth 28 \\\n",
    "#     model.wrn.widening_factor 10 \\\n",
    "#     train.batch_size 128 \\\n",
    "#     train.base_lr 0.00001 \\\n",
    "#     test.checkpoint /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_5/exp00_resume300_150/checkpoint_00150.pth \\\n",
    "#     test.output_dir /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_5/exp00_resume300_150/test_results_0150_cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-27 22:12:46] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_3_20//exp00_resume400_50/checkpoint_00050.pth\n",
      "CIFAR 10.1\n",
      "100%|| 8/8 [00:10<00:00,  1.33s/it]\n",
      "\u001b[32m[2020-06-27 22:12:57] __main__ INFO: \u001b[0mElapsed 10.66\n",
      "\u001b[32m[2020-06-27 22:12:57] __main__ INFO: \u001b[0mLoss 0.6707 Accuracy 0.7980\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10.1 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/wrn.yaml \\\n",
    "    model.wrn.depth 28 \\\n",
    "    model.wrn.widening_factor 10 \\\n",
    "    dataset.name CIFAR101 \\\n",
    "    test.checkpoint /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_3_20//exp00_resume400_50/checkpoint_00050.pth \\\n",
    "    test.output_dir /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_3_20/exp00_resume400_50/test_results_0050_cifar101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-20 16:33:19] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_5/exp00_resume300_150/checkpoint_00150.pth\n",
      "CIFAR 10.1\n",
      "100%|| 16/16 [00:00<00:00, 17.16it/s]\n",
      "\u001b[32m[2020-06-20 16:33:20] __main__ INFO: \u001b[0mElapsed 0.93\n",
      "\u001b[32m[2020-06-20 16:33:20] __main__ INFO: \u001b[0mLoss 0.8206 Accuracy 0.7710\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10.1 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "# !python evaluate.py --config configs/cifar/rwrn.yaml \\\n",
    "#     model.wrn.depth 28 \\\n",
    "#     model.wrn.widening_factor 10 \\\n",
    "#     dataset.name CIFAR101 \\\n",
    "#     test.checkpoint /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_5/exp00_resume300_150/checkpoint_00150.pth \\\n",
    "#     test.output_dir /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_5/exp00_resume300_150/test_results_0150_cifar101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-27 22:15:08] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_3_20/exp02/checkpoint_00400.pth\n",
      "CIFAR 10.1\n",
      "100%|| 8/8 [00:10<00:00,  1.32s/it]\n",
      "\u001b[32m[2020-06-27 22:15:19] __main__ INFO: \u001b[0mElapsed 10.60\n",
      "\u001b[32m[2020-06-27 22:15:19] __main__ INFO: \u001b[0mLoss 1.5253 Accuracy 0.6290\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/wrn.yaml \\\n",
    "    model.wrn.depth 28 \\\n",
    "    model.wrn.widening_factor 10 \\\n",
    "    dataset.name CIFAR101 \\\n",
    "    test.checkpoint /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_3_20/exp02/checkpoint_00400.pth \\\n",
    "    test.output_dir /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_3_20/exp02/test_results_0400_cifar101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-27 22:16:10] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_3_20/exp02/checkpoint_00300.pth\n",
      "Files already downloaded and verified\n",
      "100%|| 40/40 [00:51<00:00,  1.28s/it]\n",
      "\u001b[32m[2020-06-27 22:17:03] __main__ INFO: \u001b[0mElapsed 51.32\n",
      "\u001b[32m[2020-06-27 22:17:03] __main__ INFO: \u001b[0mLoss 0.9077 Accuracy 0.7647\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/wrn.yaml \\\n",
    "    model.wrn.depth 28 \\\n",
    "    model.wrn.widening_factor 10 \\\n",
    "    test.checkpoint /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_3_20/exp02/checkpoint_00300.pth \\\n",
    "    test.output_dir /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_3_20/exp02/test_results_0300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-27 22:18:06] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_3_20/exp02/checkpoint_00400.pth\n",
      "Files already downloaded and verified\n",
      "100%|| 40/40 [00:51<00:00,  1.29s/it]\n",
      "\u001b[32m[2020-06-27 22:18:58] __main__ INFO: \u001b[0mElapsed 51.46\n",
      "\u001b[32m[2020-06-27 22:18:58] __main__ INFO: \u001b[0mLoss 0.9243 Accuracy 0.7599\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10.1 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "!python evaluate.py --config configs/cifar/wrn.yaml \\\n",
    "    model.wrn.depth 28 \\\n",
    "    model.wrn.widening_factor 10 \\\n",
    "    test.checkpoint /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_3_20/exp02/checkpoint_00400.pth \\\n",
    "    test.output_dir /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_3_20/exp02/test_results_0400_cifar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-27 22:19:38] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_3_20/exp02/checkpoint_00300.pth\n",
      "CIFAR 10.1\n",
      "100%|| 8/8 [00:10<00:00,  1.34s/it]\n",
      "\u001b[32m[2020-06-27 22:19:50] __main__ INFO: \u001b[0mElapsed 10.70\n",
      "\u001b[32m[2020-06-27 22:19:50] __main__ INFO: \u001b[0mLoss 1.5326 Accuracy 0.6200\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10.1 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "!python evaluate.py --config configs/cifar/wrn.yaml \\\n",
    "    model.wrn.depth 28 \\\n",
    "    model.wrn.widening_factor 10 \\\n",
    "    test.checkpoint /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_3_20/exp02/checkpoint_00300.pth \\\n",
    "    dataset.name CIFAR101 \\\n",
    "    test.output_dir /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_3_20/exp02/test_results_0300_cifar101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Testset</th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Original_Accuracy</th>\n",
       "      <th>Original_CI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wrn_28_10</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>100</td>\n",
       "      <td>0.2299</td>\n",
       "      <td>0.9311</td>\n",
       "      <td>95.9</td>\n",
       "      <td>(95.5, 96.3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wrn_28_10</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>200</td>\n",
       "      <td>0.1760</td>\n",
       "      <td>0.9578</td>\n",
       "      <td>95.9</td>\n",
       "      <td>(95.5, 96.3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wrn_28_10</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.3896</td>\n",
       "      <td>0.8975</td>\n",
       "      <td>89.7</td>\n",
       "      <td>(88.3, 91.0)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Model    Testset  Epoch    Loss  Accuracy  Original_Accuracy  \\\n",
       "0  wrn_28_10    cifar10    100  0.2299    0.9311               95.9   \n",
       "1  wrn_28_10    cifar10    200  0.1760    0.9578               95.9   \n",
       "2  wrn_28_10  cifar10.1    200  0.3896    0.8975               89.7   \n",
       "\n",
       "    Original_CI  \n",
       "0  (95.5, 96.3)  \n",
       "1  (95.5, 96.3)  \n",
       "2  (88.3, 91.0)  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write the results to a CSV file so that we can analyze later.\n",
    "import pandas as pd\n",
    "\n",
    "results = {'Model': ['wrn_28_10', 'wrn_28_10', 'wrn_28_10'],\n",
    "           'Testset': ['cifar10', 'cifar10', 'cifar10.1'],\n",
    "           'Epoch': [100, 200, 200],\n",
    "           'Loss': [0.2299, 0.1760, 0.3896],\n",
    "           'Accuracy': [0.9311, 0.9578, 0.8975],\n",
    "           'Original_Accuracy': [95.9, 95.9, 89.7],\n",
    "           'Original_CI': [(95.5, 96.3), (95.5, 96.3), (88.3, 91.0)]\n",
    "           }\n",
    "\n",
    "df = pd.DataFrame(results, columns = ['Model', 'Testset', 'Epoch', 'Loss', 'Accuracy', \n",
    "                                      'Original_Accuracy', 'Original_CI'])\n",
    "\n",
    "df.to_csv('/home/ec2-user/SageMaker/experiments/wrn_28_10/exp00/results.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Testset</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Original_Accuracy</th>\n",
       "      <th>Original_CI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wrn_28_10_ra_3_20</td>\n",
       "      <td>400</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.9243</td>\n",
       "      <td>0.7599</td>\n",
       "      <td>95.9</td>\n",
       "      <td>(95.5, 96.3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wrn_28_10_ra_3_20</td>\n",
       "      <td>300</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.9077</td>\n",
       "      <td>0.7647</td>\n",
       "      <td>95.9</td>\n",
       "      <td>(95.5, 96.3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wrn_28_10_ra_3_20</td>\n",
       "      <td>400</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>1.5253</td>\n",
       "      <td>0.629</td>\n",
       "      <td>89.7</td>\n",
       "      <td>(88.3, 91.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wrn_28_10_ra_3_20</td>\n",
       "      <td>300</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>1.5326</td>\n",
       "      <td>0.62</td>\n",
       "      <td>89.7</td>\n",
       "      <td>(88.3, 91.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wrn_28_10_ra_3_20_refined400</td>\n",
       "      <td>50</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>0.6707</td>\n",
       "      <td>0.798</td>\n",
       "      <td>89.7</td>\n",
       "      <td>(88.3, 91.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>wrn_28_10_ra_3_20_refined400</td>\n",
       "      <td>50</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.3287</td>\n",
       "      <td>0.9001</td>\n",
       "      <td>95.9</td>\n",
       "      <td>(95.5, 96.3)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model Epoch    Testset    Loss Accuracy  \\\n",
       "0             wrn_28_10_ra_3_20   400    cifar10  0.9243   0.7599   \n",
       "1             wrn_28_10_ra_3_20   300    cifar10  0.9077   0.7647   \n",
       "2             wrn_28_10_ra_3_20   400  cifar10.1  1.5253    0.629   \n",
       "3             wrn_28_10_ra_3_20   300  cifar10.1  1.5326     0.62   \n",
       "4  wrn_28_10_ra_3_20_refined400    50  cifar10.1  0.6707    0.798   \n",
       "5  wrn_28_10_ra_3_20_refined400    50    cifar10  0.3287   0.9001   \n",
       "\n",
       "   Original_Accuracy   Original_CI  \n",
       "0               95.9  (95.5, 96.3)  \n",
       "1               95.9  (95.5, 96.3)  \n",
       "2               89.7  (88.3, 91.0)  \n",
       "3               89.7  (88.3, 91.0)  \n",
       "4               89.7  (88.3, 91.0)  \n",
       "5               95.9  (95.5, 96.3)  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "a = pd.Series(['wrn_28_10_ra_3_20', 400, 'cifar10', 0.9243, 0.7599])\n",
    "b = pd.Series(['wrn_28_10_ra_3_20', 300, 'cifar10', 0.9077, 0.7647])\n",
    "c = pd.Series(['wrn_28_10_ra_3_20', 400, 'cifar10.1', 1.5253, 0.6290])\n",
    "d = pd.Series(['wrn_28_10_ra_3_20', 300, 'cifar10.1', 1.5326, 0.6200])\n",
    "\n",
    "e = pd.Series(['wrn_28_10_ra_3_20_refined400', 50, 'cifar10.1', 0.6707, 0.7980])\n",
    "f = pd.Series(['wrn_28_10_ra_3_20_refined400', 50, 'cifar10', 0.3287, 0.9001])\n",
    "#g = pd.Series(['resnet_basic_32_ra_2_5_refined300', 150, 'cifar10', 0.4499, 0.8795])\n",
    "#h = pd.Series(['resnet_basic_32_ra_2_5_refined300', 150, 'cifar10.1', 0.8206, 0.7710])\n",
    "               \n",
    "df_results = pd.concat([a,b,c,d,e,f], axis=1).T\n",
    "df_results.columns = ['Model', 'Epoch', 'Testset', 'Loss', 'Accuracy']\n",
    "\n",
    "df_results['Original_Accuracy'] = df_results.apply((lambda row: 95.9 if row[2] == 'cifar10' else 89.7), axis=1)\n",
    "df_results['Original_CI'] = df_results.apply((lambda row: (95.5, 96.3) if row[2] == 'cifar10' else (88.3, 91.0)), axis=1)\n",
    "\n",
    "df_results.to_csv('/home/ec2-user/SageMaker/experiments/wrn_28_10_ra_3_20/results.csv')\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['preds', 'probs', 'labels', 'loss', 'acc']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ -7.153804  ,  -0.1832159 ,  -0.69570637, ...,  -0.50926757,\n",
       "         -5.526208  , -12.987257  ],\n",
       "       [  2.862379  ,   7.963458  ,  -6.603018  , ...,  -4.740323  ,\n",
       "         25.90399   ,  -0.52988565],\n",
       "       [  4.25749   ,   8.408992  ,  -4.3299227 , ...,  -2.3715498 ,\n",
       "         13.468082  ,   4.5792727 ],\n",
       "       ...,\n",
       "       [ -4.7270765 ,  -1.2400844 ,   1.3852903 , ...,  -0.51062894,\n",
       "         -3.399443  ,  -2.4969094 ],\n",
       "       [ -2.7640457 ,  14.635863  ,   6.7449965 , ...,  -1.3011913 ,\n",
       "         -3.036379  ,  -7.061736  ],\n",
       "       [ -2.6933427 ,   1.8961854 ,  -3.6396854 , ...,  18.63456   ,\n",
       "         -2.7524152 ,  -3.2204888 ]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Peak inside the output file for predictions\n",
    "import numpy as np\n",
    "output = '/home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_5/exp00/test_results_0400/predictions.npz'\n",
    "npzfile = np.load(output)\n",
    "print(npzfile.files)\n",
    "npzfile['preds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the model checkpoints, configs, and results to S3 \n",
    "bucket='sagemaker-may29'\n",
    "prefix = 'sagemaker/results/original-models/wrn_28_10_ra_3_20'\n",
    "path = '/home/ec2-user/SageMaker/experiments/wrn_28_10_ra_3_20'\n",
    "\n",
    "s3_resource = boto3.resource(\"s3\", region_name=\"us-east-2\")\n",
    "\n",
    "def uploadDirectory(local_path,bucket_name,s3_prefix):\n",
    "\n",
    "    my_bucket = s3_resource.Bucket(bucket_name)\n",
    "    \n",
    "    for path, subdirs, files in os.walk(local_path):\n",
    "        path = path.replace(\"\\\\\",\"/\")\n",
    "        directory_name = path.replace(local_path,\"\")\n",
    "        for file in files:\n",
    "            #print(\"Local File:\", os.path.join(path, file))\n",
    "            #print(\"      Dest:\", s3_prefix+directory_name+'/'+file)\n",
    "            my_bucket.upload_file(os.path.join(path, file), s3_prefix+directory_name+'/'+file)\n",
    "    \n",
    "uploadDirectory(path,bucket,prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
