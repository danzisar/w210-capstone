{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import numpy \n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESNEXT_29_4x64D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without Containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 1)) (1.18.1)\n",
      "Requirement already satisfied: torch>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 2)) (1.4.0)\n",
      "Requirement already satisfied: torchvision in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (0.5.0)\n",
      "Requirement already satisfied: fvcore in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (0.1.1.post20200608)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 5)) (4.44.1)\n",
      "Requirement already satisfied: yacs in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 6)) (0.1.7)\n",
      "Requirement already satisfied: apex from git+https://github.com/NVIDIA/apex.git#egg=apex in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 7)) (0.1)\n",
      "Requirement already satisfied: termcolor in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 8)) (1.1.0)\n",
      "Requirement already satisfied: thop<0.0.31.post2004070130 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 9)) (0.0.31.post2001170342)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (1.14.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (7.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (5.3.1)\n",
      "Requirement already satisfied: portalocker in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (1.7.0)\n",
      "Requirement already satisfied: tabulate in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (0.8.7)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Sagemaker Notebook must be of type, conda_pytorch_p36\n",
    "\n",
    "!pip install -r '/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorboard in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (2.2.2)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.14.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (2.23.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.18.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (3.2.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.6.0.post3)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (0.34.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.0.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (0.9.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (0.4.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.29.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (46.1.3.post20200330)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (3.12.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.16.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (1.25.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (2020.4.5.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (3.0.4)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard) (1.5.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (4.1.0)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (3.4.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (0.2.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard) (2.2.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.1.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard) (0.4.8)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Need to add this to requirements.txt\n",
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-25 22:39:18] __main__ INFO: \u001b[0mdevice: cuda\n",
      "cudnn:\n",
      "  benchmark: True\n",
      "  deterministic: False\n",
      "dataset:\n",
      "  name: CIFAR10_RA_2_20\n",
      "  dataset_dir: ''\n",
      "  image_size: 32\n",
      "  n_channels: 3\n",
      "  n_classes: 10\n",
      "model:\n",
      "  type: cifar\n",
      "  name: resnext\n",
      "  init_mode: kaiming_fan_out\n",
      "  vgg:\n",
      "    n_channels: [64, 128, 256, 512, 512]\n",
      "    n_layers: [2, 2, 3, 3, 3]\n",
      "    use_bn: True\n",
      "  resnet:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "  resnet_preact:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "  wrn:\n",
      "    depth: 28\n",
      "    initial_channels: 16\n",
      "    widening_factor: 10\n",
      "    drop_rate: 0.0\n",
      "  densenet:\n",
      "    depth: 100\n",
      "    n_blocks: [6, 12, 24, 16]\n",
      "    block_type: bottleneck\n",
      "    growth_rate: 12\n",
      "    drop_rate: 0.0\n",
      "    compression_rate: 0.5\n",
      "  pyramidnet:\n",
      "    depth: 272\n",
      "    n_blocks: [3, 24, 36, 3]\n",
      "    initial_channels: 16\n",
      "    block_type: bottleneck\n",
      "    alpha: 200\n",
      "  resnext:\n",
      "    depth: 29\n",
      "    n_blocks: [3, 4, 6, 3]\n",
      "    initial_channels: 64\n",
      "    cardinality: 4\n",
      "    base_channels: 64\n",
      "  shake_shake:\n",
      "    depth: 26\n",
      "    initial_channels: 96\n",
      "    shake_forward: True\n",
      "    shake_backward: True\n",
      "    shake_image: True\n",
      "  se_resnet_preact:\n",
      "    depth: 110\n",
      "    initial_channels: 16\n",
      "    se_reduction: 16\n",
      "    block_type: basic\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "train:\n",
      "  checkpoint: ''\n",
      "  resume: False\n",
      "  precision: O0\n",
      "  batch_size: 128\n",
      "  subdivision: 1\n",
      "  optimizer: sgd\n",
      "  base_lr: 0.1\n",
      "  momentum: 0.9\n",
      "  nesterov: True\n",
      "  weight_decay: 0.0005\n",
      "  no_weight_decay_on_bn: False\n",
      "  gradient_clip: 0\n",
      "  start_epoch: 0\n",
      "  seed: 0\n",
      "  val_first: True\n",
      "  val_period: 1\n",
      "  val_ratio: 0.1\n",
      "  use_test_as_val: False\n",
      "  output_dir: /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_20/exp00\n",
      "  log_period: 100\n",
      "  checkpoint_period: 100\n",
      "  use_tensorboard: True\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: True\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "  distributed: False\n",
      "  dist:\n",
      "    backend: nccl\n",
      "    init_method: env://\n",
      "    world_size: -1\n",
      "    node_rank: -1\n",
      "    local_rank: 0\n",
      "    use_sync_bn: False\n",
      "tensorboard:\n",
      "  train_images: False\n",
      "  val_images: False\n",
      "  model_params: False\n",
      "optim:\n",
      "  adam:\n",
      "    betas: (0.9, 0.999)\n",
      "  lars:\n",
      "    eps: 1e-09\n",
      "    threshold: 0.01\n",
      "  adabound:\n",
      "    betas: (0.9, 0.999)\n",
      "    final_lr: 0.1\n",
      "    gamma: 0.001\n",
      "scheduler:\n",
      "  epochs: 400\n",
      "  warmup:\n",
      "    type: none\n",
      "    epochs: 0\n",
      "    start_factor: 0.001\n",
      "    exponent: 4\n",
      "  type: multistep\n",
      "  milestones: [150, 225]\n",
      "  lr_decay: 0.1\n",
      "  lr_min_factor: 0.001\n",
      "  T0: 10\n",
      "  T_mul: 1.0\n",
      "validation:\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: False\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "augmentation:\n",
      "  use_random_crop: True\n",
      "  use_random_horizontal_flip: True\n",
      "  use_cutout: False\n",
      "  use_random_erasing: False\n",
      "  use_dual_cutout: False\n",
      "  use_mixup: False\n",
      "  use_ricap: False\n",
      "  use_cutmix: False\n",
      "  use_label_smoothing: False\n",
      "  random_crop:\n",
      "    padding: 4\n",
      "    fill: 0\n",
      "    padding_mode: constant\n",
      "  random_horizontal_flip:\n",
      "    prob: 0.5\n",
      "  cutout:\n",
      "    prob: 1.0\n",
      "    mask_size: 16\n",
      "    cut_inside: False\n",
      "    mask_color: 0\n",
      "    dual_cutout_alpha: 0.1\n",
      "  random_erasing:\n",
      "    prob: 0.5\n",
      "    area_ratio_range: [0.02, 0.4]\n",
      "    min_aspect_ratio: 0.3\n",
      "    max_attempt: 20\n",
      "  mixup:\n",
      "    alpha: 1.0\n",
      "  ricap:\n",
      "    beta: 0.3\n",
      "  cutmix:\n",
      "    alpha: 1.0\n",
      "  label_smoothing:\n",
      "    epsilon: 0.1\n",
      "tta:\n",
      "  use_resize: False\n",
      "  use_center_crop: False\n",
      "  resize: 256\n",
      "test:\n",
      "  checkpoint: ''\n",
      "  output_dir: ''\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    pin_memory: False\n",
      "\u001b[32m[2020-06-25 22:39:18] __main__ INFO: \u001b[0menv_info:\n",
      "  pytorch_version: 1.4.0\n",
      "  cuda_version: 10.1\n",
      "  cudnn_version: 7603\n",
      "  num_gpus: 1\n",
      "  gpu_name: Tesla K80\n",
      "  gpu_capability: 3.7\n",
      "(50000, 32, 32, 3)\n",
      "\u001b[32m[2020-06-25 22:39:25] __main__ INFO: \u001b[0mMACs  : 2.75G\n",
      "\u001b[32m[2020-06-25 22:39:25] __main__ INFO: \u001b[0m#params: 17.56M\n",
      "Selected optimization level O0:  Pure FP32 training.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\",)\n",
      "\u001b[32m[2020-06-25 22:39:25] __main__ INFO: \u001b[0mVal 0\n",
      "\u001b[32m[2020-06-25 22:39:51] __main__ INFO: \u001b[0mEpoch 0 loss 5.8321 acc@1 0.1026 acc@5 0.5110\n",
      "\u001b[32m[2020-06-25 22:39:51] __main__ INFO: \u001b[0mElapsed 26.32\n",
      "\u001b[32m[2020-06-25 22:39:51] __main__ INFO: \u001b[0mTrain 1 0\n",
      "\u001b[32m[2020-06-25 22:42:24] __main__ INFO: \u001b[0mEpoch 1 Step 100/351 lr 0.100000 loss 3.5427 (8.4224) acc@1 0.0703 (0.1022) acc@5 0.4688 (0.5036)\n",
      "\u001b[32m[2020-06-25 22:44:48] __main__ INFO: \u001b[0mEpoch 1 Step 200/351 lr 0.100000 loss 2.9519 (5.6803) acc@1 0.1094 (0.1046) acc@5 0.4609 (0.5026)\n",
      "\u001b[32m[2020-06-25 22:47:13] __main__ INFO: \u001b[0mEpoch 1 Step 300/351 lr 0.100000 loss 2.3660 (4.6665) acc@1 0.1016 (0.1019) acc@5 0.4531 (0.5017)\n",
      "\u001b[32m[2020-06-25 22:48:27] __main__ INFO: \u001b[0mEpoch 1 Step 351/351 lr 0.100000 loss 2.5174 (4.3463) acc@1 0.0859 (0.1023) acc@5 0.4922 (0.5034)\n",
      "\u001b[32m[2020-06-25 22:48:27] __main__ INFO: \u001b[0mElapsed 515.69\n",
      "\u001b[32m[2020-06-25 22:48:27] __main__ INFO: \u001b[0mVal 1\n",
      "\u001b[32m[2020-06-25 22:48:45] __main__ INFO: \u001b[0mEpoch 1 loss 2.3554 acc@1 0.0972 acc@5 0.5010\n",
      "\u001b[32m[2020-06-25 22:48:45] __main__ INFO: \u001b[0mElapsed 17.96\n",
      "\u001b[32m[2020-06-25 22:48:45] __main__ INFO: \u001b[0mTrain 2 351\n",
      "\u001b[32m[2020-06-25 22:51:10] __main__ INFO: \u001b[0mEpoch 2 Step 100/351 lr 0.100000 loss 2.3916 (2.4257) acc@1 0.1094 (0.1013) acc@5 0.4609 (0.5027)\n",
      "\u001b[32m[2020-06-25 22:53:34] __main__ INFO: \u001b[0mEpoch 2 Step 200/351 lr 0.100000 loss 2.2920 (2.4080) acc@1 0.0859 (0.1011) acc@5 0.4844 (0.5029)\n",
      "\u001b[32m[2020-06-25 22:55:59] __main__ INFO: \u001b[0mEpoch 2 Step 300/351 lr 0.100000 loss 2.3410 (2.3939) acc@1 0.0781 (0.1017) acc@5 0.5234 (0.5028)\n",
      "\u001b[32m[2020-06-25 22:57:13] __main__ INFO: \u001b[0mEpoch 2 Step 351/351 lr 0.100000 loss 2.3878 (2.3872) acc@1 0.1328 (0.1015) acc@5 0.4922 (0.5032)\n",
      "\u001b[32m[2020-06-25 22:57:13] __main__ INFO: \u001b[0mElapsed 507.87\n",
      "\u001b[32m[2020-06-25 22:57:13] __main__ INFO: \u001b[0mVal 2\n",
      "\u001b[32m[2020-06-25 22:57:31] __main__ INFO: \u001b[0mEpoch 2 loss 2.5361 acc@1 0.0990 acc@5 0.5056\n",
      "\u001b[32m[2020-06-25 22:57:31] __main__ INFO: \u001b[0mElapsed 17.99\n",
      "\u001b[32m[2020-06-25 22:57:31] __main__ INFO: \u001b[0mTrain 3 702\n",
      "\u001b[32m[2020-06-25 22:59:56] __main__ INFO: \u001b[0mEpoch 3 Step 100/351 lr 0.100000 loss 2.3390 (2.3335) acc@1 0.0625 (0.1044) acc@5 0.4688 (0.5036)\n",
      "\u001b[32m[2020-06-25 23:02:20] __main__ INFO: \u001b[0mEpoch 3 Step 200/351 lr 0.100000 loss 2.2948 (2.3303) acc@1 0.1094 (0.1043) acc@5 0.5234 (0.5094)\n",
      "\u001b[32m[2020-06-25 23:04:45] __main__ INFO: \u001b[0mEpoch 3 Step 300/351 lr 0.100000 loss 2.3110 (2.3272) acc@1 0.1250 (0.1045) acc@5 0.5938 (0.5070)\n",
      "\u001b[32m[2020-06-25 23:05:59] __main__ INFO: \u001b[0mEpoch 3 Step 351/351 lr 0.100000 loss 2.3383 (2.3260) acc@1 0.1016 (0.1043) acc@5 0.4531 (0.5071)\n",
      "\u001b[32m[2020-06-25 23:05:59] __main__ INFO: \u001b[0mElapsed 508.32\n",
      "\u001b[32m[2020-06-25 23:05:59] __main__ INFO: \u001b[0mVal 3\n",
      "\u001b[32m[2020-06-25 23:06:17] __main__ INFO: \u001b[0mEpoch 3 loss 2.9796 acc@1 0.1124 acc@5 0.5030\n",
      "\u001b[32m[2020-06-25 23:06:17] __main__ INFO: \u001b[0mElapsed 17.99\n",
      "\u001b[32m[2020-06-25 23:06:17] __main__ INFO: \u001b[0mTrain 4 1053\n",
      "\u001b[32m[2020-06-25 23:08:42] __main__ INFO: \u001b[0mEpoch 4 Step 100/351 lr 0.100000 loss 2.3123 (2.3170) acc@1 0.1094 (0.1049) acc@5 0.4766 (0.5088)\n",
      "\u001b[32m[2020-06-25 23:11:07] __main__ INFO: \u001b[0mEpoch 4 Step 200/351 lr 0.100000 loss 2.3185 (2.3171) acc@1 0.0625 (0.1026) acc@5 0.4766 (0.5079)\n",
      "\u001b[32m[2020-06-25 23:13:31] __main__ INFO: \u001b[0mEpoch 4 Step 300/351 lr 0.100000 loss 2.3312 (2.3145) acc@1 0.0938 (0.1035) acc@5 0.5547 (0.5085)\n",
      "\u001b[32m[2020-06-25 23:14:45] __main__ INFO: \u001b[0mEpoch 4 Step 351/351 lr 0.100000 loss 2.3198 (2.3139) acc@1 0.1172 (0.1036) acc@5 0.4844 (0.5081)\n",
      "\u001b[32m[2020-06-25 23:14:45] __main__ INFO: \u001b[0mElapsed 508.13\n",
      "\u001b[32m[2020-06-25 23:14:45] __main__ INFO: \u001b[0mVal 4\n",
      "\u001b[32m[2020-06-25 23:15:03] __main__ INFO: \u001b[0mEpoch 4 loss 2.5658 acc@1 0.1038 acc@5 0.5052\n",
      "\u001b[32m[2020-06-25 23:15:03] __main__ INFO: \u001b[0mElapsed 17.98\n",
      "\u001b[32m[2020-06-25 23:15:03] __main__ INFO: \u001b[0mTrain 5 1404\n",
      "\u001b[32m[2020-06-25 23:17:28] __main__ INFO: \u001b[0mEpoch 5 Step 100/351 lr 0.100000 loss 2.3065 (2.3097) acc@1 0.1094 (0.1042) acc@5 0.5547 (0.5141)\n",
      "\u001b[32m[2020-06-25 23:19:53] __main__ INFO: \u001b[0mEpoch 5 Step 200/351 lr 0.100000 loss 2.3026 (2.3087) acc@1 0.1172 (0.1041) acc@5 0.4922 (0.5083)\n",
      "\u001b[32m[2020-06-25 23:22:17] __main__ INFO: \u001b[0mEpoch 5 Step 300/351 lr 0.100000 loss 2.2963 (2.3075) acc@1 0.1406 (0.1050) acc@5 0.5703 (0.5109)\n",
      "\u001b[32m[2020-06-25 23:23:31] __main__ INFO: \u001b[0mEpoch 5 Step 351/351 lr 0.100000 loss 2.2864 (2.3070) acc@1 0.1406 (0.1045) acc@5 0.5234 (0.5128)\n",
      "\u001b[32m[2020-06-25 23:23:31] __main__ INFO: \u001b[0mElapsed 507.90\n",
      "\u001b[32m[2020-06-25 23:23:31] __main__ INFO: \u001b[0mVal 5\n",
      "\u001b[32m[2020-06-25 23:23:49] __main__ INFO: \u001b[0mEpoch 5 loss 2.4663 acc@1 0.1102 acc@5 0.5272\n",
      "\u001b[32m[2020-06-25 23:23:49] __main__ INFO: \u001b[0mElapsed 17.99\n",
      "\u001b[32m[2020-06-25 23:23:49] __main__ INFO: \u001b[0mTrain 6 1755\n",
      "\u001b[32m[2020-06-25 23:26:14] __main__ INFO: \u001b[0mEpoch 6 Step 100/351 lr 0.100000 loss 2.2971 (2.3005) acc@1 0.1484 (0.1139) acc@5 0.5156 (0.5352)\n",
      "\u001b[32m[2020-06-25 23:28:38] __main__ INFO: \u001b[0mEpoch 6 Step 200/351 lr 0.100000 loss 2.2680 (2.2976) acc@1 0.1953 (0.1132) acc@5 0.5938 (0.5379)\n",
      "\u001b[32m[2020-06-25 23:31:03] __main__ INFO: \u001b[0mEpoch 6 Step 300/351 lr 0.100000 loss 2.2660 (2.2946) acc@1 0.1484 (0.1152) acc@5 0.5703 (0.5414)\n",
      "\u001b[32m[2020-06-25 23:32:17] __main__ INFO: \u001b[0mEpoch 6 Step 351/351 lr 0.100000 loss 2.2659 (2.2936) acc@1 0.1719 (0.1158) acc@5 0.6016 (0.5435)\n",
      "\u001b[32m[2020-06-25 23:32:17] __main__ INFO: \u001b[0mElapsed 507.61\n",
      "\u001b[32m[2020-06-25 23:32:17] __main__ INFO: \u001b[0mVal 6\n",
      "\u001b[32m[2020-06-25 23:32:35] __main__ INFO: \u001b[0mEpoch 6 loss 2.2955 acc@1 0.1232 acc@5 0.5520\n",
      "\u001b[32m[2020-06-25 23:32:35] __main__ INFO: \u001b[0mElapsed 17.96\n",
      "\u001b[32m[2020-06-25 23:32:35] __main__ INFO: \u001b[0mTrain 7 2106\n",
      "\u001b[32m[2020-06-25 23:34:59] __main__ INFO: \u001b[0mEpoch 7 Step 100/351 lr 0.100000 loss 2.2761 (2.2805) acc@1 0.1641 (0.1295) acc@5 0.6328 (0.5655)\n",
      "\u001b[32m[2020-06-25 23:37:24] __main__ INFO: \u001b[0mEpoch 7 Step 200/351 lr 0.100000 loss 2.2454 (2.2779) acc@1 0.1719 (0.1271) acc@5 0.6172 (0.5693)\n",
      "\u001b[32m[2020-06-25 23:39:48] __main__ INFO: \u001b[0mEpoch 7 Step 300/351 lr 0.100000 loss 2.2637 (2.2745) acc@1 0.1016 (0.1290) acc@5 0.6328 (0.5721)\n",
      "\u001b[32m[2020-06-25 23:41:02] __main__ INFO: \u001b[0mEpoch 7 Step 351/351 lr 0.100000 loss 2.2303 (2.2731) acc@1 0.1797 (0.1286) acc@5 0.6406 (0.5743)\n",
      "\u001b[32m[2020-06-25 23:41:02] __main__ INFO: \u001b[0mElapsed 507.50\n",
      "\u001b[32m[2020-06-25 23:41:02] __main__ INFO: \u001b[0mVal 7\n",
      "\u001b[32m[2020-06-25 23:41:20] __main__ INFO: \u001b[0mEpoch 7 loss 2.2804 acc@1 0.1354 acc@5 0.5964\n",
      "\u001b[32m[2020-06-25 23:41:20] __main__ INFO: \u001b[0mElapsed 17.94\n",
      "\u001b[32m[2020-06-25 23:41:20] __main__ INFO: \u001b[0mTrain 8 2457\n",
      "\u001b[32m[2020-06-25 23:43:45] __main__ INFO: \u001b[0mEpoch 8 Step 100/351 lr 0.100000 loss 2.2394 (2.2590) acc@1 0.1797 (0.1391) acc@5 0.6172 (0.5913)\n",
      "\u001b[32m[2020-06-25 23:46:09] __main__ INFO: \u001b[0mEpoch 8 Step 200/351 lr 0.100000 loss 2.1989 (2.2529) acc@1 0.1953 (0.1413) acc@5 0.6562 (0.5970)\n",
      "\u001b[32m[2020-06-25 23:48:34] __main__ INFO: \u001b[0mEpoch 8 Step 300/351 lr 0.100000 loss 2.2489 (2.2466) acc@1 0.1250 (0.1424) acc@5 0.6016 (0.6016)\n",
      "\u001b[32m[2020-06-25 23:49:48] __main__ INFO: \u001b[0mEpoch 8 Step 351/351 lr 0.100000 loss 2.2394 (2.2443) acc@1 0.1562 (0.1443) acc@5 0.5469 (0.6021)\n",
      "\u001b[32m[2020-06-25 23:49:48] __main__ INFO: \u001b[0mElapsed 507.76\n",
      "\u001b[32m[2020-06-25 23:49:48] __main__ INFO: \u001b[0mVal 8\n",
      "\u001b[32m[2020-06-25 23:50:06] __main__ INFO: \u001b[0mEpoch 8 loss 2.3701 acc@1 0.1312 acc@5 0.5874\n",
      "\u001b[32m[2020-06-25 23:50:06] __main__ INFO: \u001b[0mElapsed 17.94\n",
      "\u001b[32m[2020-06-25 23:50:06] __main__ INFO: \u001b[0mTrain 9 2808\n",
      "\u001b[32m[2020-06-25 23:52:30] __main__ INFO: \u001b[0mEpoch 9 Step 100/351 lr 0.100000 loss 2.1602 (2.2223) acc@1 0.1250 (0.1514) acc@5 0.6641 (0.6233)\n",
      "\u001b[32m[2020-06-25 23:54:55] __main__ INFO: \u001b[0mEpoch 9 Step 200/351 lr 0.100000 loss 2.1850 (2.2155) acc@1 0.1719 (0.1511) acc@5 0.6172 (0.6207)\n",
      "\u001b[32m[2020-06-25 23:57:20] __main__ INFO: \u001b[0mEpoch 9 Step 300/351 lr 0.100000 loss 2.1303 (2.2096) acc@1 0.1719 (0.1545) acc@5 0.6641 (0.6263)\n",
      "\u001b[32m[2020-06-25 23:58:33] __main__ INFO: \u001b[0mEpoch 9 Step 351/351 lr 0.100000 loss 2.0769 (2.2061) acc@1 0.2578 (0.1565) acc@5 0.6875 (0.6276)\n",
      "\u001b[32m[2020-06-25 23:58:33] __main__ INFO: \u001b[0mElapsed 507.66\n",
      "\u001b[32m[2020-06-25 23:58:33] __main__ INFO: \u001b[0mVal 9\n",
      "\u001b[32m[2020-06-25 23:58:51] __main__ INFO: \u001b[0mEpoch 9 loss 2.4843 acc@1 0.1526 acc@5 0.6030\n",
      "\u001b[32m[2020-06-25 23:58:51] __main__ INFO: \u001b[0mElapsed 17.97\n",
      "\u001b[32m[2020-06-25 23:58:51] __main__ INFO: \u001b[0mTrain 10 3159\n",
      "\u001b[32m[2020-06-26 00:01:16] __main__ INFO: \u001b[0mEpoch 10 Step 100/351 lr 0.100000 loss 2.1775 (2.1840) acc@1 0.1172 (0.1713) acc@5 0.6328 (0.6379)\n",
      "\u001b[32m[2020-06-26 00:03:41] __main__ INFO: \u001b[0mEpoch 10 Step 200/351 lr 0.100000 loss 2.1783 (2.1786) acc@1 0.2031 (0.1740) acc@5 0.6250 (0.6420)\n",
      "\u001b[32m[2020-06-26 00:06:05] __main__ INFO: \u001b[0mEpoch 10 Step 300/351 lr 0.100000 loss 2.0693 (2.1704) acc@1 0.1875 (0.1797) acc@5 0.7266 (0.6465)\n",
      "\u001b[32m[2020-06-26 00:07:19] __main__ INFO: \u001b[0mEpoch 10 Step 351/351 lr 0.100000 loss 2.0766 (2.1679) acc@1 0.2188 (0.1806) acc@5 0.7188 (0.6473)\n",
      "\u001b[32m[2020-06-26 00:07:19] __main__ INFO: \u001b[0mElapsed 507.72\n",
      "\u001b[32m[2020-06-26 00:07:19] __main__ INFO: \u001b[0mVal 10\n",
      "\u001b[32m[2020-06-26 00:07:37] __main__ INFO: \u001b[0mEpoch 10 loss 2.1492 acc@1 0.1844 acc@5 0.6576\n",
      "\u001b[32m[2020-06-26 00:07:37] __main__ INFO: \u001b[0mElapsed 17.95\n",
      "\u001b[32m[2020-06-26 00:07:37] __main__ INFO: \u001b[0mTrain 11 3510\n",
      "\u001b[32m[2020-06-26 00:10:02] __main__ INFO: \u001b[0mEpoch 11 Step 100/351 lr 0.100000 loss 2.0847 (2.1319) acc@1 0.2422 (0.1992) acc@5 0.6562 (0.6696)\n",
      "\u001b[32m[2020-06-26 00:12:26] __main__ INFO: \u001b[0mEpoch 11 Step 200/351 lr 0.100000 loss 2.0669 (2.1451) acc@1 0.2656 (0.1931) acc@5 0.7422 (0.6607)\n",
      "\u001b[32m[2020-06-26 00:14:51] __main__ INFO: \u001b[0mEpoch 11 Step 300/351 lr 0.100000 loss 2.1643 (2.1410) acc@1 0.1562 (0.1943) acc@5 0.6250 (0.6623)\n",
      "\u001b[32m[2020-06-26 00:16:05] __main__ INFO: \u001b[0mEpoch 11 Step 351/351 lr 0.100000 loss 2.1713 (2.1360) acc@1 0.1953 (0.1960) acc@5 0.6406 (0.6647)\n",
      "\u001b[32m[2020-06-26 00:16:05] __main__ INFO: \u001b[0mElapsed 507.74\n",
      "\u001b[32m[2020-06-26 00:16:05] __main__ INFO: \u001b[0mVal 11\n",
      "\u001b[32m[2020-06-26 00:16:23] __main__ INFO: \u001b[0mEpoch 11 loss 2.1174 acc@1 0.2110 acc@5 0.6796\n",
      "\u001b[32m[2020-06-26 00:16:23] __main__ INFO: \u001b[0mElapsed 17.98\n",
      "\u001b[32m[2020-06-26 00:16:23] __main__ INFO: \u001b[0mTrain 12 3861\n",
      "\u001b[32m[2020-06-26 00:18:47] __main__ INFO: \u001b[0mEpoch 12 Step 100/351 lr 0.100000 loss 2.1006 (2.1098) acc@1 0.2734 (0.2082) acc@5 0.6250 (0.6795)\n",
      "\u001b[32m[2020-06-26 00:21:12] __main__ INFO: \u001b[0mEpoch 12 Step 200/351 lr 0.100000 loss 2.0553 (2.1045) acc@1 0.2344 (0.2124) acc@5 0.7344 (0.6803)\n",
      "\u001b[32m[2020-06-26 00:23:36] __main__ INFO: \u001b[0mEpoch 12 Step 300/351 lr 0.100000 loss 2.0478 (2.0952) acc@1 0.2500 (0.2142) acc@5 0.7109 (0.6836)\n",
      "\u001b[32m[2020-06-26 00:24:50] __main__ INFO: \u001b[0mEpoch 12 Step 351/351 lr 0.100000 loss 2.0979 (2.0932) acc@1 0.1406 (0.2142) acc@5 0.6406 (0.6829)\n",
      "\u001b[32m[2020-06-26 00:24:50] __main__ INFO: \u001b[0mElapsed 507.35\n",
      "\u001b[32m[2020-06-26 00:24:50] __main__ INFO: \u001b[0mVal 12\n",
      "\u001b[32m[2020-06-26 00:25:08] __main__ INFO: \u001b[0mEpoch 12 loss 2.1490 acc@1 0.2162 acc@5 0.6772\n",
      "\u001b[32m[2020-06-26 00:25:08] __main__ INFO: \u001b[0mElapsed 17.94\n",
      "\u001b[32m[2020-06-26 00:25:08] __main__ INFO: \u001b[0mTrain 13 4212\n",
      "\u001b[32m[2020-06-26 00:27:33] __main__ INFO: \u001b[0mEpoch 13 Step 100/351 lr 0.100000 loss 1.9402 (2.0502) acc@1 0.2891 (0.2323) acc@5 0.6875 (0.6903)\n",
      "\u001b[32m[2020-06-26 00:29:57] __main__ INFO: \u001b[0mEpoch 13 Step 200/351 lr 0.100000 loss 2.0866 (2.0544) acc@1 0.2188 (0.2303) acc@5 0.7109 (0.6914)\n",
      "\u001b[32m[2020-06-26 00:32:22] __main__ INFO: \u001b[0mEpoch 13 Step 300/351 lr 0.100000 loss 2.0312 (2.0475) acc@1 0.2109 (0.2331) acc@5 0.7422 (0.6945)\n",
      "\u001b[32m[2020-06-26 00:33:35] __main__ INFO: \u001b[0mEpoch 13 Step 351/351 lr 0.100000 loss 2.0506 (2.0468) acc@1 0.2109 (0.2328) acc@5 0.6953 (0.6951)\n",
      "\u001b[32m[2020-06-26 00:33:35] __main__ INFO: \u001b[0mElapsed 507.17\n",
      "\u001b[32m[2020-06-26 00:33:35] __main__ INFO: \u001b[0mVal 13\n",
      "\u001b[32m[2020-06-26 00:33:53] __main__ INFO: \u001b[0mEpoch 13 loss 2.1408 acc@1 0.2212 acc@5 0.6766\n",
      "\u001b[32m[2020-06-26 00:33:53] __main__ INFO: \u001b[0mElapsed 17.94\n",
      "\u001b[32m[2020-06-26 00:33:53] __main__ INFO: \u001b[0mTrain 14 4563\n",
      "\u001b[32m[2020-06-26 00:36:18] __main__ INFO: \u001b[0mEpoch 14 Step 100/351 lr 0.100000 loss 1.9100 (2.0159) acc@1 0.2500 (0.2491) acc@5 0.7578 (0.7083)\n",
      "\u001b[32m[2020-06-26 00:38:42] __main__ INFO: \u001b[0mEpoch 14 Step 200/351 lr 0.100000 loss 1.9780 (2.0078) acc@1 0.2031 (0.2522) acc@5 0.7422 (0.7102)\n",
      "\u001b[32m[2020-06-26 00:41:06] __main__ INFO: \u001b[0mEpoch 14 Step 300/351 lr 0.100000 loss 2.0261 (2.0068) acc@1 0.2109 (0.2499) acc@5 0.7266 (0.7090)\n",
      "\u001b[32m[2020-06-26 00:42:20] __main__ INFO: \u001b[0mEpoch 14 Step 351/351 lr 0.100000 loss 1.9857 (2.0058) acc@1 0.2422 (0.2515) acc@5 0.7500 (0.7086)\n",
      "\u001b[32m[2020-06-26 00:42:20] __main__ INFO: \u001b[0mElapsed 506.77\n",
      "\u001b[32m[2020-06-26 00:42:20] __main__ INFO: \u001b[0mVal 14\n",
      "\u001b[32m[2020-06-26 00:42:38] __main__ INFO: \u001b[0mEpoch 14 loss 2.1750 acc@1 0.2276 acc@5 0.6734\n",
      "\u001b[32m[2020-06-26 00:42:38] __main__ INFO: \u001b[0mElapsed 17.92\n",
      "\u001b[32m[2020-06-26 00:42:38] __main__ INFO: \u001b[0mTrain 15 4914\n",
      "\u001b[32m[2020-06-26 00:45:02] __main__ INFO: \u001b[0mEpoch 15 Step 100/351 lr 0.100000 loss 2.0351 (1.9850) acc@1 0.2578 (0.2620) acc@5 0.6641 (0.7123)\n",
      "\u001b[32m[2020-06-26 00:47:27] __main__ INFO: \u001b[0mEpoch 15 Step 200/351 lr 0.100000 loss 1.9298 (1.9855) acc@1 0.2578 (0.2597) acc@5 0.7266 (0.7097)\n",
      "\u001b[32m[2020-06-26 00:49:51] __main__ INFO: \u001b[0mEpoch 15 Step 300/351 lr 0.100000 loss 1.9288 (1.9772) acc@1 0.2734 (0.2624) acc@5 0.7188 (0.7131)\n",
      "\u001b[32m[2020-06-26 00:51:04] __main__ INFO: \u001b[0mEpoch 15 Step 351/351 lr 0.100000 loss 1.9326 (1.9721) acc@1 0.2578 (0.2647) acc@5 0.7031 (0.7145)\n",
      "\u001b[32m[2020-06-26 00:51:04] __main__ INFO: \u001b[0mElapsed 506.59\n",
      "\u001b[32m[2020-06-26 00:51:04] __main__ INFO: \u001b[0mVal 15\n",
      "\u001b[32m[2020-06-26 00:51:22] __main__ INFO: \u001b[0mEpoch 15 loss 2.1062 acc@1 0.2452 acc@5 0.6912\n",
      "\u001b[32m[2020-06-26 00:51:22] __main__ INFO: \u001b[0mElapsed 17.90\n",
      "\u001b[32m[2020-06-26 00:51:22] __main__ INFO: \u001b[0mTrain 16 5265\n",
      "\u001b[32m[2020-06-26 00:53:47] __main__ INFO: \u001b[0mEpoch 16 Step 100/351 lr 0.100000 loss 1.8995 (1.9450) acc@1 0.2734 (0.2727) acc@5 0.7422 (0.7193)\n",
      "\u001b[32m[2020-06-26 00:56:11] __main__ INFO: \u001b[0mEpoch 16 Step 200/351 lr 0.100000 loss 1.9516 (1.9424) acc@1 0.2422 (0.2769) acc@5 0.8047 (0.7205)\n",
      "\u001b[32m[2020-06-26 00:58:35] __main__ INFO: \u001b[0mEpoch 16 Step 300/351 lr 0.100000 loss 1.9249 (1.9447) acc@1 0.2578 (0.2766) acc@5 0.7500 (0.7215)\n",
      "\u001b[32m[2020-06-26 00:59:49] __main__ INFO: \u001b[0mEpoch 16 Step 351/351 lr 0.100000 loss 1.9495 (1.9415) acc@1 0.2812 (0.2783) acc@5 0.7344 (0.7220)\n",
      "\u001b[32m[2020-06-26 00:59:49] __main__ INFO: \u001b[0mElapsed 506.37\n",
      "\u001b[32m[2020-06-26 00:59:49] __main__ INFO: \u001b[0mVal 16\n",
      "\u001b[32m[2020-06-26 01:00:07] __main__ INFO: \u001b[0mEpoch 16 loss 2.3162 acc@1 0.2240 acc@5 0.6502\n",
      "\u001b[32m[2020-06-26 01:00:07] __main__ INFO: \u001b[0mElapsed 17.88\n",
      "\u001b[32m[2020-06-26 01:00:07] __main__ INFO: \u001b[0mTrain 17 5616\n",
      "\u001b[32m[2020-06-26 01:02:31] __main__ INFO: \u001b[0mEpoch 17 Step 100/351 lr 0.100000 loss 1.9170 (1.9203) acc@1 0.3047 (0.2872) acc@5 0.7969 (0.7306)\n",
      "\u001b[32m[2020-06-26 01:04:55] __main__ INFO: \u001b[0mEpoch 17 Step 200/351 lr 0.100000 loss 1.9775 (1.9209) acc@1 0.2578 (0.2885) acc@5 0.6797 (0.7282)\n",
      "\u001b[32m[2020-06-26 01:07:19] __main__ INFO: \u001b[0mEpoch 17 Step 300/351 lr 0.100000 loss 1.9298 (1.9156) acc@1 0.2969 (0.2897) acc@5 0.7188 (0.7293)\n",
      "\u001b[32m[2020-06-26 01:08:33] __main__ INFO: \u001b[0mEpoch 17 Step 351/351 lr 0.100000 loss 1.8221 (1.9109) acc@1 0.3672 (0.2923) acc@5 0.6875 (0.7302)\n",
      "\u001b[32m[2020-06-26 01:08:33] __main__ INFO: \u001b[0mElapsed 506.27\n",
      "\u001b[32m[2020-06-26 01:08:33] __main__ INFO: \u001b[0mVal 17\n",
      "\u001b[32m[2020-06-26 01:08:51] __main__ INFO: \u001b[0mEpoch 17 loss 1.9529 acc@1 0.2804 acc@5 0.7240\n",
      "\u001b[32m[2020-06-26 01:08:51] __main__ INFO: \u001b[0mElapsed 17.90\n",
      "\u001b[32m[2020-06-26 01:08:51] __main__ INFO: \u001b[0mTrain 18 5967\n",
      "\u001b[32m[2020-06-26 01:11:15] __main__ INFO: \u001b[0mEpoch 18 Step 100/351 lr 0.100000 loss 1.9464 (1.8842) acc@1 0.3359 (0.3047) acc@5 0.7578 (0.7328)\n",
      "\u001b[32m[2020-06-26 01:13:39] __main__ INFO: \u001b[0mEpoch 18 Step 200/351 lr 0.100000 loss 1.8907 (1.8863) acc@1 0.3125 (0.3037) acc@5 0.7500 (0.7320)\n",
      "\u001b[32m[2020-06-26 01:16:03] __main__ INFO: \u001b[0mEpoch 18 Step 300/351 lr 0.100000 loss 1.7817 (1.8821) acc@1 0.3125 (0.3030) acc@5 0.7812 (0.7343)\n",
      "\u001b[32m[2020-06-26 01:17:16] __main__ INFO: \u001b[0mEpoch 18 Step 351/351 lr 0.100000 loss 1.7976 (1.8824) acc@1 0.3359 (0.3027) acc@5 0.7500 (0.7341)\n",
      "\u001b[32m[2020-06-26 01:17:16] __main__ INFO: \u001b[0mElapsed 505.65\n",
      "\u001b[32m[2020-06-26 01:17:16] __main__ INFO: \u001b[0mVal 18\n",
      "\u001b[32m[2020-06-26 01:17:34] __main__ INFO: \u001b[0mEpoch 18 loss 2.3554 acc@1 0.2138 acc@5 0.6680\n",
      "\u001b[32m[2020-06-26 01:17:34] __main__ INFO: \u001b[0mElapsed 17.88\n",
      "\u001b[32m[2020-06-26 01:17:34] __main__ INFO: \u001b[0mTrain 19 6318\n",
      "\u001b[32m[2020-06-26 01:19:58] __main__ INFO: \u001b[0mEpoch 19 Step 100/351 lr 0.100000 loss 1.8222 (1.8724) acc@1 0.2969 (0.3059) acc@5 0.7656 (0.7418)\n",
      "\u001b[32m[2020-06-26 01:22:22] __main__ INFO: \u001b[0mEpoch 19 Step 200/351 lr 0.100000 loss 1.8207 (1.8638) acc@1 0.3438 (0.3073) acc@5 0.7422 (0.7391)\n",
      "\u001b[32m[2020-06-26 01:24:46] __main__ INFO: \u001b[0mEpoch 19 Step 300/351 lr 0.100000 loss 1.8822 (1.8643) acc@1 0.3281 (0.3068) acc@5 0.7422 (0.7390)\n",
      "\u001b[32m[2020-06-26 01:26:00] __main__ INFO: \u001b[0mEpoch 19 Step 351/351 lr 0.100000 loss 1.9269 (1.8604) acc@1 0.2969 (0.3080) acc@5 0.7500 (0.7397)\n",
      "\u001b[32m[2020-06-26 01:26:00] __main__ INFO: \u001b[0mElapsed 505.65\n",
      "\u001b[32m[2020-06-26 01:26:00] __main__ INFO: \u001b[0mVal 19\n",
      "\u001b[32m[2020-06-26 01:26:18] __main__ INFO: \u001b[0mEpoch 19 loss 2.0199 acc@1 0.2886 acc@5 0.7176\n",
      "\u001b[32m[2020-06-26 01:26:18] __main__ INFO: \u001b[0mElapsed 17.88\n",
      "\u001b[32m[2020-06-26 01:26:18] __main__ INFO: \u001b[0mTrain 20 6669\n",
      "\u001b[32m[2020-06-26 01:28:42] __main__ INFO: \u001b[0mEpoch 20 Step 100/351 lr 0.100000 loss 1.7897 (1.8512) acc@1 0.3906 (0.3123) acc@5 0.8203 (0.7463)\n",
      "\u001b[32m[2020-06-26 01:31:06] __main__ INFO: \u001b[0mEpoch 20 Step 200/351 lr 0.100000 loss 2.0081 (1.8429) acc@1 0.2188 (0.3127) acc@5 0.6875 (0.7455)\n",
      "\u001b[32m[2020-06-26 01:33:30] __main__ INFO: \u001b[0mEpoch 20 Step 300/351 lr 0.100000 loss 1.7681 (1.8437) acc@1 0.3828 (0.3120) acc@5 0.7812 (0.7456)\n",
      "\u001b[32m[2020-06-26 01:34:44] __main__ INFO: \u001b[0mEpoch 20 Step 351/351 lr 0.100000 loss 1.9456 (1.8429) acc@1 0.2969 (0.3126) acc@5 0.6875 (0.7447)\n",
      "\u001b[32m[2020-06-26 01:34:44] __main__ INFO: \u001b[0mElapsed 505.71\n",
      "\u001b[32m[2020-06-26 01:34:44] __main__ INFO: \u001b[0mVal 20\n",
      "\u001b[32m[2020-06-26 01:35:01] __main__ INFO: \u001b[0mEpoch 20 loss 1.9305 acc@1 0.3036 acc@5 0.7436\n",
      "\u001b[32m[2020-06-26 01:35:01] __main__ INFO: \u001b[0mElapsed 17.89\n",
      "\u001b[32m[2020-06-26 01:35:01] __main__ INFO: \u001b[0mTrain 21 7020\n",
      "\u001b[32m[2020-06-26 01:37:26] __main__ INFO: \u001b[0mEpoch 21 Step 100/351 lr 0.100000 loss 1.8468 (1.8260) acc@1 0.3125 (0.3214) acc@5 0.7734 (0.7462)\n",
      "\u001b[32m[2020-06-26 01:39:50] __main__ INFO: \u001b[0mEpoch 21 Step 200/351 lr 0.100000 loss 1.9201 (1.8258) acc@1 0.2891 (0.3218) acc@5 0.7031 (0.7479)\n",
      "\u001b[32m[2020-06-26 01:42:14] __main__ INFO: \u001b[0mEpoch 21 Step 300/351 lr 0.100000 loss 1.8508 (1.8257) acc@1 0.2812 (0.3204) acc@5 0.7578 (0.7476)\n",
      "\u001b[32m[2020-06-26 01:43:27] __main__ INFO: \u001b[0mEpoch 21 Step 351/351 lr 0.100000 loss 1.9793 (1.8263) acc@1 0.2422 (0.3199) acc@5 0.7031 (0.7471)\n",
      "\u001b[32m[2020-06-26 01:43:27] __main__ INFO: \u001b[0mElapsed 506.00\n",
      "\u001b[32m[2020-06-26 01:43:27] __main__ INFO: \u001b[0mVal 21\n",
      "\u001b[32m[2020-06-26 01:43:45] __main__ INFO: \u001b[0mEpoch 21 loss 2.0108 acc@1 0.2826 acc@5 0.7218\n",
      "\u001b[32m[2020-06-26 01:43:45] __main__ INFO: \u001b[0mElapsed 17.94\n",
      "\u001b[32m[2020-06-26 01:43:45] __main__ INFO: \u001b[0mTrain 22 7371\n",
      "\u001b[32m[2020-06-26 01:46:10] __main__ INFO: \u001b[0mEpoch 22 Step 100/351 lr 0.100000 loss 1.9184 (1.8115) acc@1 0.3125 (0.3274) acc@5 0.7578 (0.7551)\n",
      "\u001b[32m[2020-06-26 01:48:34] __main__ INFO: \u001b[0mEpoch 22 Step 200/351 lr 0.100000 loss 1.7301 (1.8167) acc@1 0.3672 (0.3224) acc@5 0.7734 (0.7525)\n",
      "\u001b[32m[2020-06-26 01:50:58] __main__ INFO: \u001b[0mEpoch 22 Step 300/351 lr 0.100000 loss 1.8013 (1.8152) acc@1 0.3672 (0.3245) acc@5 0.7344 (0.7527)\n",
      "\u001b[32m[2020-06-26 01:52:11] __main__ INFO: \u001b[0mEpoch 22 Step 351/351 lr 0.100000 loss 1.9889 (1.8142) acc@1 0.2422 (0.3251) acc@5 0.7109 (0.7529)\n",
      "\u001b[32m[2020-06-26 01:52:11] __main__ INFO: \u001b[0mElapsed 505.98\n",
      "\u001b[32m[2020-06-26 01:52:11] __main__ INFO: \u001b[0mVal 22\n",
      "\u001b[32m[2020-06-26 01:52:29] __main__ INFO: \u001b[0mEpoch 22 loss 2.0887 acc@1 0.2740 acc@5 0.7164\n",
      "\u001b[32m[2020-06-26 01:52:29] __main__ INFO: \u001b[0mElapsed 17.91\n",
      "\u001b[32m[2020-06-26 01:52:29] __main__ INFO: \u001b[0mTrain 23 7722\n",
      "\u001b[32m[2020-06-26 01:54:54] __main__ INFO: \u001b[0mEpoch 23 Step 100/351 lr 0.100000 loss 1.7214 (1.7915) acc@1 0.3359 (0.3370) acc@5 0.8203 (0.7500)\n",
      "\u001b[32m[2020-06-26 01:57:18] __main__ INFO: \u001b[0mEpoch 23 Step 200/351 lr 0.100000 loss 1.7146 (1.7982) acc@1 0.4141 (0.3336) acc@5 0.8125 (0.7500)\n",
      "\u001b[32m[2020-06-26 01:59:42] __main__ INFO: \u001b[0mEpoch 23 Step 300/351 lr 0.100000 loss 1.8581 (1.8004) acc@1 0.2812 (0.3307) acc@5 0.6797 (0.7491)\n",
      "\u001b[32m[2020-06-26 02:00:55] __main__ INFO: \u001b[0mEpoch 23 Step 351/351 lr 0.100000 loss 1.7569 (1.7988) acc@1 0.3125 (0.3316) acc@5 0.8047 (0.7506)\n",
      "\u001b[32m[2020-06-26 02:00:55] __main__ INFO: \u001b[0mElapsed 505.80\n",
      "\u001b[32m[2020-06-26 02:00:55] __main__ INFO: \u001b[0mVal 23\n",
      "\u001b[32m[2020-06-26 02:01:13] __main__ INFO: \u001b[0mEpoch 23 loss 1.8138 acc@1 0.3298 acc@5 0.7464\n",
      "\u001b[32m[2020-06-26 02:01:13] __main__ INFO: \u001b[0mElapsed 17.90\n",
      "\u001b[32m[2020-06-26 02:01:13] __main__ INFO: \u001b[0mTrain 24 8073\n",
      "\u001b[32m[2020-06-26 02:03:37] __main__ INFO: \u001b[0mEpoch 24 Step 100/351 lr 0.100000 loss 1.8397 (1.7714) acc@1 0.3203 (0.3405) acc@5 0.7812 (0.7535)\n",
      "\u001b[32m[2020-06-26 02:06:01] __main__ INFO: \u001b[0mEpoch 24 Step 200/351 lr 0.100000 loss 1.7722 (1.7810) acc@1 0.3750 (0.3369) acc@5 0.7656 (0.7550)\n",
      "\u001b[32m[2020-06-26 02:08:25] __main__ INFO: \u001b[0mEpoch 24 Step 300/351 lr 0.100000 loss 1.6887 (1.7872) acc@1 0.3594 (0.3347) acc@5 0.7500 (0.7542)\n",
      "\u001b[32m[2020-06-26 02:09:39] __main__ INFO: \u001b[0mEpoch 24 Step 351/351 lr 0.100000 loss 1.8617 (1.7863) acc@1 0.2969 (0.3366) acc@5 0.7266 (0.7554)\n",
      "\u001b[32m[2020-06-26 02:09:39] __main__ INFO: \u001b[0mElapsed 505.68\n",
      "\u001b[32m[2020-06-26 02:09:39] __main__ INFO: \u001b[0mVal 24\n",
      "\u001b[32m[2020-06-26 02:09:57] __main__ INFO: \u001b[0mEpoch 24 loss 2.0461 acc@1 0.2776 acc@5 0.7196\n",
      "\u001b[32m[2020-06-26 02:09:57] __main__ INFO: \u001b[0mElapsed 17.85\n",
      "\u001b[32m[2020-06-26 02:09:57] __main__ INFO: \u001b[0mTrain 25 8424\n",
      "\u001b[32m[2020-06-26 02:12:21] __main__ INFO: \u001b[0mEpoch 25 Step 100/351 lr 0.100000 loss 1.7023 (1.7770) acc@1 0.3750 (0.3368) acc@5 0.7578 (0.7552)\n",
      "\u001b[32m[2020-06-26 02:14:45] __main__ INFO: \u001b[0mEpoch 25 Step 200/351 lr 0.100000 loss 1.7135 (1.7802) acc@1 0.4297 (0.3356) acc@5 0.7422 (0.7546)\n",
      "\u001b[32m[2020-06-26 02:17:09] __main__ INFO: \u001b[0mEpoch 25 Step 300/351 lr 0.100000 loss 1.7986 (1.7757) acc@1 0.3516 (0.3376) acc@5 0.7188 (0.7574)\n",
      "\u001b[32m[2020-06-26 02:18:22] __main__ INFO: \u001b[0mEpoch 25 Step 351/351 lr 0.100000 loss 1.7854 (1.7767) acc@1 0.2969 (0.3376) acc@5 0.7109 (0.7571)\n",
      "\u001b[32m[2020-06-26 02:18:22] __main__ INFO: \u001b[0mElapsed 505.51\n",
      "\u001b[32m[2020-06-26 02:18:22] __main__ INFO: \u001b[0mVal 25\n",
      "\u001b[32m[2020-06-26 02:18:40] __main__ INFO: \u001b[0mEpoch 25 loss 1.8332 acc@1 0.3214 acc@5 0.7534\n",
      "\u001b[32m[2020-06-26 02:18:40] __main__ INFO: \u001b[0mElapsed 17.89\n",
      "\u001b[32m[2020-06-26 02:18:40] __main__ INFO: \u001b[0mTrain 26 8775\n",
      "\u001b[32m[2020-06-26 02:21:04] __main__ INFO: \u001b[0mEpoch 26 Step 100/351 lr 0.100000 loss 1.7329 (1.7553) acc@1 0.3984 (0.3427) acc@5 0.8047 (0.7517)\n",
      "\u001b[32m[2020-06-26 02:23:28] __main__ INFO: \u001b[0mEpoch 26 Step 200/351 lr 0.100000 loss 1.6932 (1.7609) acc@1 0.4219 (0.3416) acc@5 0.7266 (0.7508)\n",
      "\u001b[32m[2020-06-26 02:25:52] __main__ INFO: \u001b[0mEpoch 26 Step 300/351 lr 0.100000 loss 1.6884 (1.7622) acc@1 0.3828 (0.3428) acc@5 0.7500 (0.7531)\n",
      "\u001b[32m[2020-06-26 02:27:06] __main__ INFO: \u001b[0mEpoch 26 Step 351/351 lr 0.100000 loss 1.7562 (1.7636) acc@1 0.3750 (0.3426) acc@5 0.7578 (0.7533)\n",
      "\u001b[32m[2020-06-26 02:27:06] __main__ INFO: \u001b[0mElapsed 505.60\n",
      "\u001b[32m[2020-06-26 02:27:06] __main__ INFO: \u001b[0mVal 26\n",
      "\u001b[32m[2020-06-26 02:27:23] __main__ INFO: \u001b[0mEpoch 26 loss 1.8840 acc@1 0.3224 acc@5 0.7442\n",
      "\u001b[32m[2020-06-26 02:27:23] __main__ INFO: \u001b[0mElapsed 17.88\n",
      "\u001b[32m[2020-06-26 02:27:23] __main__ INFO: \u001b[0mTrain 27 9126\n",
      "\u001b[32m[2020-06-26 02:29:48] __main__ INFO: \u001b[0mEpoch 27 Step 100/351 lr 0.100000 loss 1.9023 (1.7561) acc@1 0.2656 (0.3498) acc@5 0.6875 (0.7488)\n",
      "\u001b[32m[2020-06-26 02:32:12] __main__ INFO: \u001b[0mEpoch 27 Step 200/351 lr 0.100000 loss 1.8705 (1.7473) acc@1 0.2969 (0.3523) acc@5 0.7031 (0.7579)\n",
      "\u001b[32m[2020-06-26 02:34:36] __main__ INFO: \u001b[0mEpoch 27 Step 300/351 lr 0.100000 loss 1.8030 (1.7512) acc@1 0.3438 (0.3508) acc@5 0.7656 (0.7580)\n",
      "\u001b[32m[2020-06-26 02:35:49] __main__ INFO: \u001b[0mEpoch 27 Step 351/351 lr 0.100000 loss 1.7540 (1.7544) acc@1 0.3359 (0.3494) acc@5 0.7500 (0.7565)\n",
      "\u001b[32m[2020-06-26 02:35:49] __main__ INFO: \u001b[0mElapsed 505.71\n",
      "\u001b[32m[2020-06-26 02:35:49] __main__ INFO: \u001b[0mVal 27\n",
      "\u001b[32m[2020-06-26 02:36:07] __main__ INFO: \u001b[0mEpoch 27 loss 1.8259 acc@1 0.3254 acc@5 0.7636\n",
      "\u001b[32m[2020-06-26 02:36:07] __main__ INFO: \u001b[0mElapsed 17.89\n",
      "\u001b[32m[2020-06-26 02:36:07] __main__ INFO: \u001b[0mTrain 28 9477\n",
      "\u001b[32m[2020-06-26 02:38:31] __main__ INFO: \u001b[0mEpoch 28 Step 100/351 lr 0.100000 loss 1.8881 (1.7345) acc@1 0.2656 (0.3541) acc@5 0.7734 (0.7681)\n",
      "\u001b[32m[2020-06-26 02:40:55] __main__ INFO: \u001b[0mEpoch 28 Step 200/351 lr 0.100000 loss 1.6944 (1.7404) acc@1 0.3359 (0.3503) acc@5 0.7734 (0.7646)\n",
      "\u001b[32m[2020-06-26 02:43:19] __main__ INFO: \u001b[0mEpoch 28 Step 300/351 lr 0.100000 loss 1.6452 (1.7421) acc@1 0.3438 (0.3483) acc@5 0.7656 (0.7621)\n",
      "\u001b[32m[2020-06-26 02:44:33] __main__ INFO: \u001b[0mEpoch 28 Step 351/351 lr 0.100000 loss 1.7371 (1.7421) acc@1 0.2812 (0.3490) acc@5 0.7578 (0.7623)\n",
      "\u001b[32m[2020-06-26 02:44:33] __main__ INFO: \u001b[0mElapsed 505.84\n",
      "\u001b[32m[2020-06-26 02:44:33] __main__ INFO: \u001b[0mVal 28\n",
      "\u001b[32m[2020-06-26 02:44:51] __main__ INFO: \u001b[0mEpoch 28 loss 1.8575 acc@1 0.3346 acc@5 0.7514\n",
      "\u001b[32m[2020-06-26 02:44:51] __main__ INFO: \u001b[0mElapsed 17.88\n",
      "\u001b[32m[2020-06-26 02:44:51] __main__ INFO: \u001b[0mTrain 29 9828\n",
      "\u001b[32m[2020-06-26 02:47:15] __main__ INFO: \u001b[0mEpoch 29 Step 100/351 lr 0.100000 loss 1.8703 (1.7332) acc@1 0.3125 (0.3569) acc@5 0.7031 (0.7669)\n",
      "\u001b[32m[2020-06-26 02:49:39] __main__ INFO: \u001b[0mEpoch 29 Step 200/351 lr 0.100000 loss 1.6677 (1.7338) acc@1 0.3750 (0.3573) acc@5 0.8203 (0.7655)\n",
      "\u001b[32m[2020-06-26 02:52:03] __main__ INFO: \u001b[0mEpoch 29 Step 300/351 lr 0.100000 loss 1.8964 (1.7323) acc@1 0.2734 (0.3570) acc@5 0.6484 (0.7650)\n",
      "\u001b[32m[2020-06-26 02:53:16] __main__ INFO: \u001b[0mEpoch 29 Step 351/351 lr 0.100000 loss 1.7203 (1.7333) acc@1 0.3125 (0.3562) acc@5 0.8125 (0.7638)\n",
      "\u001b[32m[2020-06-26 02:53:16] __main__ INFO: \u001b[0mElapsed 505.70\n",
      "\u001b[32m[2020-06-26 02:53:16] __main__ INFO: \u001b[0mVal 29\n",
      "\u001b[32m[2020-06-26 02:53:34] __main__ INFO: \u001b[0mEpoch 29 loss 2.0722 acc@1 0.2832 acc@5 0.7262\n",
      "\u001b[32m[2020-06-26 02:53:34] __main__ INFO: \u001b[0mElapsed 17.88\n",
      "\u001b[32m[2020-06-26 02:53:34] __main__ INFO: \u001b[0mTrain 30 10179\n",
      "\u001b[32m[2020-06-26 02:55:59] __main__ INFO: \u001b[0mEpoch 30 Step 100/351 lr 0.100000 loss 1.8268 (1.7133) acc@1 0.3125 (0.3645) acc@5 0.7109 (0.7653)\n",
      "\u001b[32m[2020-06-26 02:58:23] __main__ INFO: \u001b[0mEpoch 30 Step 200/351 lr 0.100000 loss 1.7529 (1.7230) acc@1 0.3438 (0.3595) acc@5 0.7344 (0.7628)\n",
      "\u001b[32m[2020-06-26 03:00:47] __main__ INFO: \u001b[0mEpoch 30 Step 300/351 lr 0.100000 loss 1.8095 (1.7241) acc@1 0.3516 (0.3580) acc@5 0.7188 (0.7646)\n",
      "\u001b[32m[2020-06-26 03:02:00] __main__ INFO: \u001b[0mEpoch 30 Step 351/351 lr 0.100000 loss 1.5933 (1.7249) acc@1 0.4062 (0.3574) acc@5 0.8594 (0.7651)\n",
      "\u001b[32m[2020-06-26 03:02:00] __main__ INFO: \u001b[0mElapsed 506.02\n",
      "\u001b[32m[2020-06-26 03:02:00] __main__ INFO: \u001b[0mVal 30\n",
      "\u001b[32m[2020-06-26 03:02:18] __main__ INFO: \u001b[0mEpoch 30 loss 1.9031 acc@1 0.3376 acc@5 0.7392\n",
      "\u001b[32m[2020-06-26 03:02:18] __main__ INFO: \u001b[0mElapsed 17.91\n",
      "\u001b[32m[2020-06-26 03:02:18] __main__ INFO: \u001b[0mTrain 31 10530\n",
      "\u001b[32m[2020-06-26 03:04:42] __main__ INFO: \u001b[0mEpoch 31 Step 100/351 lr 0.100000 loss 1.6021 (1.7164) acc@1 0.3906 (0.3621) acc@5 0.7734 (0.7624)\n",
      "\u001b[32m[2020-06-26 03:07:07] __main__ INFO: \u001b[0mEpoch 31 Step 200/351 lr 0.100000 loss 1.8323 (1.7107) acc@1 0.2500 (0.3634) acc@5 0.7812 (0.7675)\n",
      "\u001b[32m[2020-06-26 03:09:31] __main__ INFO: \u001b[0mEpoch 31 Step 300/351 lr 0.100000 loss 1.9295 (1.7120) acc@1 0.3203 (0.3626) acc@5 0.7188 (0.7661)\n",
      "\u001b[32m[2020-06-26 03:10:44] __main__ INFO: \u001b[0mEpoch 31 Step 351/351 lr 0.100000 loss 1.7231 (1.7138) acc@1 0.3672 (0.3618) acc@5 0.7578 (0.7651)\n",
      "\u001b[32m[2020-06-26 03:10:44] __main__ INFO: \u001b[0mElapsed 505.70\n",
      "\u001b[32m[2020-06-26 03:10:44] __main__ INFO: \u001b[0mVal 31\n",
      "\u001b[32m[2020-06-26 03:11:02] __main__ INFO: \u001b[0mEpoch 31 loss 1.8015 acc@1 0.3418 acc@5 0.7534\n",
      "\u001b[32m[2020-06-26 03:11:02] __main__ INFO: \u001b[0mElapsed 17.92\n",
      "\u001b[32m[2020-06-26 03:11:02] __main__ INFO: \u001b[0mTrain 32 10881\n",
      "\u001b[32m[2020-06-26 03:13:26] __main__ INFO: \u001b[0mEpoch 32 Step 100/351 lr 0.100000 loss 1.6655 (1.6938) acc@1 0.3828 (0.3697) acc@5 0.7891 (0.7684)\n",
      "\u001b[32m[2020-06-26 03:15:50] __main__ INFO: \u001b[0mEpoch 32 Step 200/351 lr 0.100000 loss 1.8047 (1.7004) acc@1 0.3281 (0.3677) acc@5 0.6953 (0.7683)\n",
      "\u001b[32m[2020-06-26 03:18:14] __main__ INFO: \u001b[0mEpoch 32 Step 300/351 lr 0.100000 loss 1.7964 (1.6996) acc@1 0.3047 (0.3667) acc@5 0.7578 (0.7680)\n",
      "\u001b[32m[2020-06-26 03:19:27] __main__ INFO: \u001b[0mEpoch 32 Step 351/351 lr 0.100000 loss 1.6215 (1.7039) acc@1 0.3672 (0.3666) acc@5 0.8203 (0.7679)\n",
      "\u001b[32m[2020-06-26 03:19:27] __main__ INFO: \u001b[0mElapsed 505.05\n",
      "\u001b[32m[2020-06-26 03:19:27] __main__ INFO: \u001b[0mVal 32\n",
      "\u001b[32m[2020-06-26 03:19:45] __main__ INFO: \u001b[0mEpoch 32 loss 1.9180 acc@1 0.3156 acc@5 0.7410\n",
      "\u001b[32m[2020-06-26 03:19:45] __main__ INFO: \u001b[0mElapsed 17.88\n",
      "\u001b[32m[2020-06-26 03:19:45] __main__ INFO: \u001b[0mTrain 33 11232\n",
      "\u001b[32m[2020-06-26 03:22:09] __main__ INFO: \u001b[0mEpoch 33 Step 100/351 lr 0.100000 loss 1.6660 (1.6799) acc@1 0.4219 (0.3775) acc@5 0.8203 (0.7699)\n",
      "\u001b[32m[2020-06-26 03:24:33] __main__ INFO: \u001b[0mEpoch 33 Step 200/351 lr 0.100000 loss 1.8877 (1.6845) acc@1 0.3359 (0.3741) acc@5 0.7188 (0.7676)\n",
      "\u001b[32m[2020-06-26 03:26:57] __main__ INFO: \u001b[0mEpoch 33 Step 300/351 lr 0.100000 loss 1.6953 (1.6859) acc@1 0.3516 (0.3748) acc@5 0.7422 (0.7686)\n",
      "\u001b[32m[2020-06-26 03:28:10] __main__ INFO: \u001b[0mEpoch 33 Step 351/351 lr 0.100000 loss 1.8286 (1.6926) acc@1 0.3125 (0.3718) acc@5 0.7734 (0.7676)\n",
      "\u001b[32m[2020-06-26 03:28:10] __main__ INFO: \u001b[0mElapsed 505.04\n",
      "\u001b[32m[2020-06-26 03:28:10] __main__ INFO: \u001b[0mVal 33\n",
      "\u001b[32m[2020-06-26 03:28:28] __main__ INFO: \u001b[0mEpoch 33 loss 1.8732 acc@1 0.3350 acc@5 0.7506\n",
      "\u001b[32m[2020-06-26 03:28:28] __main__ INFO: \u001b[0mElapsed 17.88\n",
      "\u001b[32m[2020-06-26 03:28:28] __main__ INFO: \u001b[0mTrain 34 11583\n",
      "\u001b[32m[2020-06-26 03:30:52] __main__ INFO: \u001b[0mEpoch 34 Step 100/351 lr 0.100000 loss 1.7912 (1.6789) acc@1 0.3281 (0.3762) acc@5 0.7500 (0.7662)\n",
      "\u001b[32m[2020-06-26 03:33:16] __main__ INFO: \u001b[0mEpoch 34 Step 200/351 lr 0.100000 loss 1.5489 (1.6816) acc@1 0.4453 (0.3759) acc@5 0.8047 (0.7661)\n",
      "\u001b[32m[2020-06-26 03:35:39] __main__ INFO: \u001b[0mEpoch 34 Step 300/351 lr 0.100000 loss 1.8305 (1.6820) acc@1 0.3672 (0.3767) acc@5 0.7812 (0.7676)\n",
      "\u001b[32m[2020-06-26 03:36:53] __main__ INFO: \u001b[0mEpoch 34 Step 351/351 lr 0.100000 loss 1.8794 (1.6816) acc@1 0.3047 (0.3762) acc@5 0.6797 (0.7676)\n",
      "\u001b[32m[2020-06-26 03:36:53] __main__ INFO: \u001b[0mElapsed 505.06\n",
      "\u001b[32m[2020-06-26 03:36:53] __main__ INFO: \u001b[0mVal 34\n",
      "\u001b[32m[2020-06-26 03:37:11] __main__ INFO: \u001b[0mEpoch 34 loss 1.7816 acc@1 0.3486 acc@5 0.7654\n",
      "\u001b[32m[2020-06-26 03:37:11] __main__ INFO: \u001b[0mElapsed 17.87\n",
      "\u001b[32m[2020-06-26 03:37:11] __main__ INFO: \u001b[0mTrain 35 11934\n",
      "\u001b[32m[2020-06-26 03:39:35] __main__ INFO: \u001b[0mEpoch 35 Step 100/351 lr 0.100000 loss 1.7504 (1.6748) acc@1 0.3672 (0.3770) acc@5 0.7656 (0.7680)\n",
      "\u001b[32m[2020-06-26 03:41:59] __main__ INFO: \u001b[0mEpoch 35 Step 200/351 lr 0.100000 loss 1.6150 (1.6770) acc@1 0.3906 (0.3782) acc@5 0.7891 (0.7702)\n",
      "\u001b[32m[2020-06-26 03:44:22] __main__ INFO: \u001b[0mEpoch 35 Step 300/351 lr 0.100000 loss 1.6992 (1.6798) acc@1 0.3828 (0.3796) acc@5 0.7656 (0.7701)\n",
      "\u001b[32m[2020-06-26 03:45:36] __main__ INFO: \u001b[0mEpoch 35 Step 351/351 lr 0.100000 loss 1.7182 (1.6785) acc@1 0.3438 (0.3791) acc@5 0.7656 (0.7706)\n",
      "\u001b[32m[2020-06-26 03:45:36] __main__ INFO: \u001b[0mElapsed 505.02\n",
      "\u001b[32m[2020-06-26 03:45:36] __main__ INFO: \u001b[0mVal 35\n",
      "\u001b[32m[2020-06-26 03:45:54] __main__ INFO: \u001b[0mEpoch 35 loss 2.0008 acc@1 0.3110 acc@5 0.7390\n",
      "\u001b[32m[2020-06-26 03:45:54] __main__ INFO: \u001b[0mElapsed 17.87\n",
      "\u001b[32m[2020-06-26 03:45:54] __main__ INFO: \u001b[0mTrain 36 12285\n",
      "\u001b[32m[2020-06-26 03:48:18] __main__ INFO: \u001b[0mEpoch 36 Step 100/351 lr 0.100000 loss 1.8450 (1.6683) acc@1 0.3516 (0.3804) acc@5 0.7578 (0.7708)\n",
      "\u001b[32m[2020-06-26 03:50:41] __main__ INFO: \u001b[0mEpoch 36 Step 200/351 lr 0.100000 loss 1.6150 (1.6640) acc@1 0.3672 (0.3821) acc@5 0.7656 (0.7753)\n",
      "\u001b[32m[2020-06-26 03:53:05] __main__ INFO: \u001b[0mEpoch 36 Step 300/351 lr 0.100000 loss 1.7200 (1.6720) acc@1 0.3516 (0.3799) acc@5 0.7031 (0.7725)\n",
      "\u001b[32m[2020-06-26 03:54:19] __main__ INFO: \u001b[0mEpoch 36 Step 351/351 lr 0.100000 loss 1.5343 (1.6706) acc@1 0.4375 (0.3809) acc@5 0.7969 (0.7727)\n",
      "\u001b[32m[2020-06-26 03:54:19] __main__ INFO: \u001b[0mElapsed 505.06\n",
      "\u001b[32m[2020-06-26 03:54:19] __main__ INFO: \u001b[0mVal 36\n",
      "\u001b[32m[2020-06-26 03:54:37] __main__ INFO: \u001b[0mEpoch 36 loss 1.7711 acc@1 0.3484 acc@5 0.7606\n",
      "\u001b[32m[2020-06-26 03:54:37] __main__ INFO: \u001b[0mElapsed 17.88\n",
      "\u001b[32m[2020-06-26 03:54:37] __main__ INFO: \u001b[0mTrain 37 12636\n",
      "\u001b[32m[2020-06-26 03:57:01] __main__ INFO: \u001b[0mEpoch 37 Step 100/351 lr 0.100000 loss 1.6430 (1.6558) acc@1 0.3906 (0.3878) acc@5 0.8047 (0.7788)\n",
      "\u001b[32m[2020-06-26 03:59:24] __main__ INFO: \u001b[0mEpoch 37 Step 200/351 lr 0.100000 loss 1.6440 (1.6630) acc@1 0.4062 (0.3841) acc@5 0.7734 (0.7725)\n",
      "\u001b[32m[2020-06-26 04:01:48] __main__ INFO: \u001b[0mEpoch 37 Step 300/351 lr 0.100000 loss 1.5940 (1.6617) acc@1 0.3828 (0.3854) acc@5 0.7891 (0.7740)\n",
      "\u001b[32m[2020-06-26 04:03:02] __main__ INFO: \u001b[0mEpoch 37 Step 351/351 lr 0.100000 loss 1.6533 (1.6657) acc@1 0.3516 (0.3841) acc@5 0.7109 (0.7734)\n",
      "\u001b[32m[2020-06-26 04:03:02] __main__ INFO: \u001b[0mElapsed 505.07\n",
      "\u001b[32m[2020-06-26 04:03:02] __main__ INFO: \u001b[0mVal 37\n",
      "\u001b[32m[2020-06-26 04:03:20] __main__ INFO: \u001b[0mEpoch 37 loss 1.8012 acc@1 0.3554 acc@5 0.7712\n",
      "\u001b[32m[2020-06-26 04:03:20] __main__ INFO: \u001b[0mElapsed 17.89\n",
      "\u001b[32m[2020-06-26 04:03:20] __main__ INFO: \u001b[0mTrain 38 12987\n",
      "\u001b[32m[2020-06-26 04:05:43] __main__ INFO: \u001b[0mEpoch 38 Step 100/351 lr 0.100000 loss 1.6696 (1.6423) acc@1 0.3594 (0.3923) acc@5 0.7266 (0.7738)\n",
      "\u001b[32m[2020-06-26 04:08:07] __main__ INFO: \u001b[0mEpoch 38 Step 200/351 lr 0.100000 loss 1.6372 (1.6604) acc@1 0.4688 (0.3856) acc@5 0.8281 (0.7716)\n",
      "\u001b[32m[2020-06-26 04:10:31] __main__ INFO: \u001b[0mEpoch 38 Step 300/351 lr 0.100000 loss 1.7417 (1.6592) acc@1 0.3203 (0.3854) acc@5 0.7656 (0.7707)\n",
      "\u001b[32m[2020-06-26 04:11:44] __main__ INFO: \u001b[0mEpoch 38 Step 351/351 lr 0.100000 loss 1.5990 (1.6554) acc@1 0.3984 (0.3877) acc@5 0.7344 (0.7724)\n",
      "\u001b[32m[2020-06-26 04:11:45] __main__ INFO: \u001b[0mElapsed 505.00\n",
      "\u001b[32m[2020-06-26 04:11:45] __main__ INFO: \u001b[0mVal 38\n",
      "\u001b[32m[2020-06-26 04:12:02] __main__ INFO: \u001b[0mEpoch 38 loss 1.6984 acc@1 0.3758 acc@5 0.7724\n",
      "\u001b[32m[2020-06-26 04:12:02] __main__ INFO: \u001b[0mElapsed 17.87\n",
      "\u001b[32m[2020-06-26 04:12:02] __main__ INFO: \u001b[0mTrain 39 13338\n",
      "\u001b[32m[2020-06-26 04:14:26] __main__ INFO: \u001b[0mEpoch 39 Step 100/351 lr 0.100000 loss 1.6682 (1.6435) acc@1 0.3672 (0.3897) acc@5 0.7500 (0.7759)\n",
      "\u001b[32m[2020-06-26 04:16:50] __main__ INFO: \u001b[0mEpoch 39 Step 200/351 lr 0.100000 loss 1.8665 (1.6539) acc@1 0.3125 (0.3866) acc@5 0.7344 (0.7757)\n",
      "\u001b[32m[2020-06-26 04:19:14] __main__ INFO: \u001b[0mEpoch 39 Step 300/351 lr 0.100000 loss 1.7324 (1.6544) acc@1 0.3281 (0.3855) acc@5 0.7422 (0.7748)\n",
      "\u001b[32m[2020-06-26 04:20:27] __main__ INFO: \u001b[0mEpoch 39 Step 351/351 lr 0.100000 loss 1.6198 (1.6528) acc@1 0.3906 (0.3859) acc@5 0.8203 (0.7748)\n",
      "\u001b[32m[2020-06-26 04:20:27] __main__ INFO: \u001b[0mElapsed 505.00\n",
      "\u001b[32m[2020-06-26 04:20:27] __main__ INFO: \u001b[0mVal 39\n",
      "\u001b[32m[2020-06-26 04:20:45] __main__ INFO: \u001b[0mEpoch 39 loss 1.7084 acc@1 0.3742 acc@5 0.7758\n",
      "\u001b[32m[2020-06-26 04:20:45] __main__ INFO: \u001b[0mElapsed 17.89\n",
      "\u001b[32m[2020-06-26 04:20:45] __main__ INFO: \u001b[0mTrain 40 13689\n",
      "\u001b[32m[2020-06-26 04:23:09] __main__ INFO: \u001b[0mEpoch 40 Step 100/351 lr 0.100000 loss 1.6908 (1.6511) acc@1 0.3750 (0.3863) acc@5 0.7734 (0.7768)\n",
      "\u001b[32m[2020-06-26 04:25:33] __main__ INFO: \u001b[0mEpoch 40 Step 200/351 lr 0.100000 loss 1.6465 (1.6509) acc@1 0.4141 (0.3893) acc@5 0.7500 (0.7741)\n",
      "\u001b[32m[2020-06-26 04:27:57] __main__ INFO: \u001b[0mEpoch 40 Step 300/351 lr 0.100000 loss 1.6791 (1.6495) acc@1 0.3750 (0.3899) acc@5 0.7500 (0.7754)\n",
      "\u001b[32m[2020-06-26 04:29:10] __main__ INFO: \u001b[0mEpoch 40 Step 351/351 lr 0.100000 loss 1.6116 (1.6500) acc@1 0.3906 (0.3890) acc@5 0.7656 (0.7749)\n",
      "\u001b[32m[2020-06-26 04:29:10] __main__ INFO: \u001b[0mElapsed 504.74\n",
      "\u001b[32m[2020-06-26 04:29:10] __main__ INFO: \u001b[0mVal 40\n",
      "\u001b[32m[2020-06-26 04:29:28] __main__ INFO: \u001b[0mEpoch 40 loss 1.8268 acc@1 0.3584 acc@5 0.7614\n",
      "\u001b[32m[2020-06-26 04:29:28] __main__ INFO: \u001b[0mElapsed 17.88\n",
      "\u001b[32m[2020-06-26 04:29:28] __main__ INFO: \u001b[0mTrain 41 14040\n",
      "\u001b[32m[2020-06-26 04:31:52] __main__ INFO: \u001b[0mEpoch 41 Step 100/351 lr 0.100000 loss 1.6311 (1.6312) acc@1 0.4062 (0.3955) acc@5 0.7812 (0.7778)\n",
      "\u001b[32m[2020-06-26 04:34:15] __main__ INFO: \u001b[0mEpoch 41 Step 200/351 lr 0.100000 loss 1.5885 (1.6370) acc@1 0.4453 (0.3923) acc@5 0.7422 (0.7741)\n",
      "\u001b[32m[2020-06-26 04:36:39] __main__ INFO: \u001b[0mEpoch 41 Step 300/351 lr 0.100000 loss 1.5278 (1.6397) acc@1 0.4141 (0.3914) acc@5 0.8125 (0.7733)\n",
      "\u001b[32m[2020-06-26 04:37:52] __main__ INFO: \u001b[0mEpoch 41 Step 351/351 lr 0.100000 loss 1.5965 (1.6467) acc@1 0.3516 (0.3887) acc@5 0.7500 (0.7729)\n",
      "\u001b[32m[2020-06-26 04:37:52] __main__ INFO: \u001b[0mElapsed 504.31\n",
      "\u001b[32m[2020-06-26 04:37:52] __main__ INFO: \u001b[0mVal 41\n",
      "\u001b[32m[2020-06-26 04:38:10] __main__ INFO: \u001b[0mEpoch 41 loss 1.9887 acc@1 0.3058 acc@5 0.7322\n",
      "\u001b[32m[2020-06-26 04:38:10] __main__ INFO: \u001b[0mElapsed 17.87\n",
      "\u001b[32m[2020-06-26 04:38:10] __main__ INFO: \u001b[0mTrain 42 14391\n",
      "\u001b[32m[2020-06-26 04:40:34] __main__ INFO: \u001b[0mEpoch 42 Step 100/351 lr 0.100000 loss 1.5055 (1.6310) acc@1 0.4531 (0.3962) acc@5 0.7734 (0.7743)\n",
      "\u001b[32m[2020-06-26 04:42:57] __main__ INFO: \u001b[0mEpoch 42 Step 200/351 lr 0.100000 loss 1.6738 (1.6367) acc@1 0.3750 (0.3916) acc@5 0.7266 (0.7730)\n",
      "\u001b[32m[2020-06-26 04:45:21] __main__ INFO: \u001b[0mEpoch 42 Step 300/351 lr 0.100000 loss 1.7592 (1.6413) acc@1 0.3203 (0.3911) acc@5 0.7734 (0.7715)\n",
      "\u001b[32m[2020-06-26 04:46:34] __main__ INFO: \u001b[0mEpoch 42 Step 351/351 lr 0.100000 loss 1.7473 (1.6435) acc@1 0.3594 (0.3900) acc@5 0.7578 (0.7717)\n",
      "\u001b[32m[2020-06-26 04:46:34] __main__ INFO: \u001b[0mElapsed 504.11\n",
      "\u001b[32m[2020-06-26 04:46:34] __main__ INFO: \u001b[0mVal 42\n",
      "\u001b[32m[2020-06-26 04:46:52] __main__ INFO: \u001b[0mEpoch 42 loss 1.6740 acc@1 0.3866 acc@5 0.7764\n",
      "\u001b[32m[2020-06-26 04:46:52] __main__ INFO: \u001b[0mElapsed 17.85\n",
      "\u001b[32m[2020-06-26 04:46:52] __main__ INFO: \u001b[0mTrain 43 14742\n",
      "\u001b[32m[2020-06-26 04:49:16] __main__ INFO: \u001b[0mEpoch 43 Step 100/351 lr 0.100000 loss 1.6147 (1.6184) acc@1 0.4141 (0.3968) acc@5 0.7891 (0.7786)\n",
      "\u001b[32m[2020-06-26 04:51:39] __main__ INFO: \u001b[0mEpoch 43 Step 200/351 lr 0.100000 loss 1.5305 (1.6271) acc@1 0.4297 (0.3962) acc@5 0.7891 (0.7748)\n",
      "\u001b[32m[2020-06-26 04:54:03] __main__ INFO: \u001b[0mEpoch 43 Step 300/351 lr 0.100000 loss 1.5894 (1.6285) acc@1 0.3984 (0.3954) acc@5 0.7578 (0.7755)\n",
      "\u001b[32m[2020-06-26 04:55:16] __main__ INFO: \u001b[0mEpoch 43 Step 351/351 lr 0.100000 loss 1.6894 (1.6313) acc@1 0.3828 (0.3943) acc@5 0.7969 (0.7760)\n",
      "\u001b[32m[2020-06-26 04:55:16] __main__ INFO: \u001b[0mElapsed 503.90\n",
      "\u001b[32m[2020-06-26 04:55:16] __main__ INFO: \u001b[0mVal 43\n",
      "\u001b[32m[2020-06-26 04:55:34] __main__ INFO: \u001b[0mEpoch 43 loss 1.9549 acc@1 0.3208 acc@5 0.7376\n",
      "\u001b[32m[2020-06-26 04:55:34] __main__ INFO: \u001b[0mElapsed 17.82\n",
      "\u001b[32m[2020-06-26 04:55:34] __main__ INFO: \u001b[0mTrain 44 15093\n",
      "\u001b[32m[2020-06-26 04:57:57] __main__ INFO: \u001b[0mEpoch 44 Step 100/351 lr 0.100000 loss 1.4572 (1.6179) acc@1 0.5078 (0.4070) acc@5 0.7969 (0.7751)\n",
      "\u001b[32m[2020-06-26 05:00:21] __main__ INFO: \u001b[0mEpoch 44 Step 200/351 lr 0.100000 loss 1.8004 (1.6293) acc@1 0.3203 (0.4011) acc@5 0.7188 (0.7743)\n",
      "\u001b[32m[2020-06-26 05:02:45] __main__ INFO: \u001b[0mEpoch 44 Step 300/351 lr 0.100000 loss 1.7474 (1.6292) acc@1 0.3672 (0.3984) acc@5 0.7500 (0.7749)\n",
      "\u001b[32m[2020-06-26 05:03:58] __main__ INFO: \u001b[0mEpoch 44 Step 351/351 lr 0.100000 loss 1.4203 (1.6287) acc@1 0.5078 (0.3999) acc@5 0.8359 (0.7762)\n",
      "\u001b[32m[2020-06-26 05:03:58] __main__ INFO: \u001b[0mElapsed 503.98\n",
      "\u001b[32m[2020-06-26 05:03:58] __main__ INFO: \u001b[0mVal 44\n",
      "\u001b[32m[2020-06-26 05:04:16] __main__ INFO: \u001b[0mEpoch 44 loss 1.6819 acc@1 0.3794 acc@5 0.7722\n",
      "\u001b[32m[2020-06-26 05:04:16] __main__ INFO: \u001b[0mElapsed 17.84\n",
      "\u001b[32m[2020-06-26 05:04:16] __main__ INFO: \u001b[0mTrain 45 15444\n",
      "\u001b[32m[2020-06-26 05:06:39] __main__ INFO: \u001b[0mEpoch 45 Step 100/351 lr 0.100000 loss 1.6309 (1.6092) acc@1 0.4219 (0.4028) acc@5 0.7734 (0.7802)\n",
      "\u001b[32m[2020-06-26 05:09:03] __main__ INFO: \u001b[0mEpoch 45 Step 200/351 lr 0.100000 loss 1.6868 (1.6209) acc@1 0.3516 (0.3947) acc@5 0.7969 (0.7767)\n",
      "\u001b[32m[2020-06-26 05:11:27] __main__ INFO: \u001b[0mEpoch 45 Step 300/351 lr 0.100000 loss 1.4904 (1.6224) acc@1 0.4453 (0.3950) acc@5 0.7344 (0.7754)\n",
      "\u001b[32m[2020-06-26 05:12:40] __main__ INFO: \u001b[0mEpoch 45 Step 351/351 lr 0.100000 loss 1.6608 (1.6210) acc@1 0.3594 (0.3960) acc@5 0.7656 (0.7759)\n",
      "\u001b[32m[2020-06-26 05:12:40] __main__ INFO: \u001b[0mElapsed 504.40\n",
      "\u001b[32m[2020-06-26 05:12:40] __main__ INFO: \u001b[0mVal 45\n",
      "\u001b[32m[2020-06-26 05:12:58] __main__ INFO: \u001b[0mEpoch 45 loss 1.7257 acc@1 0.3746 acc@5 0.7626\n",
      "\u001b[32m[2020-06-26 05:12:58] __main__ INFO: \u001b[0mElapsed 17.87\n",
      "\u001b[32m[2020-06-26 05:12:58] __main__ INFO: \u001b[0mTrain 46 15795\n",
      "\u001b[32m[2020-06-26 05:15:22] __main__ INFO: \u001b[0mEpoch 46 Step 100/351 lr 0.100000 loss 1.5541 (1.6040) acc@1 0.4062 (0.4074) acc@5 0.7500 (0.7806)\n",
      "\u001b[32m[2020-06-26 05:17:45] __main__ INFO: \u001b[0mEpoch 46 Step 200/351 lr 0.100000 loss 1.7404 (1.6189) acc@1 0.3047 (0.4018) acc@5 0.7031 (0.7775)\n",
      "\u001b[32m[2020-06-26 05:20:09] __main__ INFO: \u001b[0mEpoch 46 Step 300/351 lr 0.100000 loss 1.5403 (1.6184) acc@1 0.4062 (0.4019) acc@5 0.7969 (0.7779)\n",
      "\u001b[32m[2020-06-26 05:21:22] __main__ INFO: \u001b[0mEpoch 46 Step 351/351 lr 0.100000 loss 1.5229 (1.6183) acc@1 0.4531 (0.4020) acc@5 0.7812 (0.7786)\n",
      "\u001b[32m[2020-06-26 05:21:22] __main__ INFO: \u001b[0mElapsed 504.28\n",
      "\u001b[32m[2020-06-26 05:21:22] __main__ INFO: \u001b[0mVal 46\n",
      "\u001b[32m[2020-06-26 05:21:40] __main__ INFO: \u001b[0mEpoch 46 loss 1.7086 acc@1 0.3808 acc@5 0.7604\n",
      "\u001b[32m[2020-06-26 05:21:40] __main__ INFO: \u001b[0mElapsed 17.85\n",
      "\u001b[32m[2020-06-26 05:21:40] __main__ INFO: \u001b[0mTrain 47 16146\n",
      "\u001b[32m[2020-06-26 05:24:04] __main__ INFO: \u001b[0mEpoch 47 Step 100/351 lr 0.100000 loss 1.6623 (1.6190) acc@1 0.4062 (0.4006) acc@5 0.7891 (0.7802)\n",
      "\u001b[32m[2020-06-26 05:26:27] __main__ INFO: \u001b[0mEpoch 47 Step 200/351 lr 0.100000 loss 1.5590 (1.6167) acc@1 0.4062 (0.4026) acc@5 0.8203 (0.7797)\n",
      "\u001b[32m[2020-06-26 05:28:51] __main__ INFO: \u001b[0mEpoch 47 Step 300/351 lr 0.100000 loss 1.6026 (1.6177) acc@1 0.3828 (0.4006) acc@5 0.7812 (0.7798)\n",
      "\u001b[32m[2020-06-26 05:30:04] __main__ INFO: \u001b[0mEpoch 47 Step 351/351 lr 0.100000 loss 1.6027 (1.6183) acc@1 0.4062 (0.4012) acc@5 0.8203 (0.7805)\n",
      "\u001b[32m[2020-06-26 05:30:04] __main__ INFO: \u001b[0mElapsed 504.03\n",
      "\u001b[32m[2020-06-26 05:30:04] __main__ INFO: \u001b[0mVal 47\n",
      "\u001b[32m[2020-06-26 05:30:22] __main__ INFO: \u001b[0mEpoch 47 loss 2.0035 acc@1 0.3154 acc@5 0.7538\n",
      "\u001b[32m[2020-06-26 05:30:22] __main__ INFO: \u001b[0mElapsed 17.81\n",
      "\u001b[32m[2020-06-26 05:30:22] __main__ INFO: \u001b[0mTrain 48 16497\n",
      "\u001b[32m[2020-06-26 05:32:45] __main__ INFO: \u001b[0mEpoch 48 Step 100/351 lr 0.100000 loss 1.6141 (1.6181) acc@1 0.4297 (0.4017) acc@5 0.8047 (0.7763)\n",
      "\u001b[32m[2020-06-26 05:35:08] __main__ INFO: \u001b[0mEpoch 48 Step 200/351 lr 0.100000 loss 1.6304 (1.6091) acc@1 0.4141 (0.4040) acc@5 0.7891 (0.7792)\n",
      "\u001b[32m[2020-06-26 05:37:31] __main__ INFO: \u001b[0mEpoch 48 Step 300/351 lr 0.100000 loss 1.6983 (1.6127) acc@1 0.3516 (0.4030) acc@5 0.7891 (0.7807)\n",
      "\u001b[32m[2020-06-26 05:38:44] __main__ INFO: \u001b[0mEpoch 48 Step 351/351 lr 0.100000 loss 1.5411 (1.6127) acc@1 0.4844 (0.4029) acc@5 0.7969 (0.7806)\n",
      "\u001b[32m[2020-06-26 05:38:44] __main__ INFO: \u001b[0mElapsed 502.19\n",
      "\u001b[32m[2020-06-26 05:38:44] __main__ INFO: \u001b[0mVal 48\n",
      "\u001b[32m[2020-06-26 05:39:02] __main__ INFO: \u001b[0mEpoch 48 loss 1.7770 acc@1 0.3536 acc@5 0.7560\n",
      "\u001b[32m[2020-06-26 05:39:02] __main__ INFO: \u001b[0mElapsed 17.77\n",
      "\u001b[32m[2020-06-26 05:39:02] __main__ INFO: \u001b[0mTrain 49 16848\n",
      "\u001b[32m[2020-06-26 05:41:25] __main__ INFO: \u001b[0mEpoch 49 Step 100/351 lr 0.100000 loss 1.5691 (1.5870) acc@1 0.4141 (0.4104) acc@5 0.7422 (0.7837)\n",
      "\u001b[32m[2020-06-26 05:43:48] __main__ INFO: \u001b[0mEpoch 49 Step 200/351 lr 0.100000 loss 1.5940 (1.5984) acc@1 0.4297 (0.4044) acc@5 0.7969 (0.7803)\n",
      "\u001b[32m[2020-06-26 05:46:11] __main__ INFO: \u001b[0mEpoch 49 Step 300/351 lr 0.100000 loss 1.5742 (1.6074) acc@1 0.3828 (0.4011) acc@5 0.7734 (0.7802)\n",
      "\u001b[32m[2020-06-26 05:47:24] __main__ INFO: \u001b[0mEpoch 49 Step 351/351 lr 0.100000 loss 1.7748 (1.6101) acc@1 0.3125 (0.4004) acc@5 0.6797 (0.7798)\n",
      "\u001b[32m[2020-06-26 05:47:24] __main__ INFO: \u001b[0mElapsed 502.10\n",
      "\u001b[32m[2020-06-26 05:47:24] __main__ INFO: \u001b[0mVal 49\n",
      "\u001b[32m[2020-06-26 05:47:42] __main__ INFO: \u001b[0mEpoch 49 loss 1.6860 acc@1 0.3770 acc@5 0.7558\n",
      "\u001b[32m[2020-06-26 05:47:42] __main__ INFO: \u001b[0mElapsed 17.79\n",
      "\u001b[32m[2020-06-26 05:47:42] __main__ INFO: \u001b[0mTrain 50 17199\n",
      "\u001b[32m[2020-06-26 05:50:05] __main__ INFO: \u001b[0mEpoch 50 Step 100/351 lr 0.100000 loss 1.7438 (1.5886) acc@1 0.3359 (0.4060) acc@5 0.7969 (0.7839)\n",
      "\u001b[32m[2020-06-26 05:52:28] __main__ INFO: \u001b[0mEpoch 50 Step 200/351 lr 0.100000 loss 1.6535 (1.6013) acc@1 0.3906 (0.4034) acc@5 0.7422 (0.7797)\n",
      "\u001b[32m[2020-06-26 05:54:51] __main__ INFO: \u001b[0mEpoch 50 Step 300/351 lr 0.100000 loss 1.5098 (1.6049) acc@1 0.4453 (0.4030) acc@5 0.8125 (0.7808)\n",
      "\u001b[32m[2020-06-26 05:56:04] __main__ INFO: \u001b[0mEpoch 50 Step 351/351 lr 0.100000 loss 1.5077 (1.6076) acc@1 0.4531 (0.4013) acc@5 0.8047 (0.7806)\n",
      "\u001b[32m[2020-06-26 05:56:04] __main__ INFO: \u001b[0mElapsed 502.01\n",
      "\u001b[32m[2020-06-26 05:56:04] __main__ INFO: \u001b[0mVal 50\n",
      "\u001b[32m[2020-06-26 05:56:22] __main__ INFO: \u001b[0mEpoch 50 loss 1.7605 acc@1 0.3506 acc@5 0.7578\n",
      "\u001b[32m[2020-06-26 05:56:22] __main__ INFO: \u001b[0mElapsed 17.81\n",
      "\u001b[32m[2020-06-26 05:56:22] __main__ INFO: \u001b[0mTrain 51 17550\n",
      "\u001b[32m[2020-06-26 05:58:45] __main__ INFO: \u001b[0mEpoch 51 Step 100/351 lr 0.100000 loss 1.5930 (1.5840) acc@1 0.4297 (0.4115) acc@5 0.7656 (0.7805)\n",
      "\u001b[32m[2020-06-26 06:01:08] __main__ INFO: \u001b[0mEpoch 51 Step 200/351 lr 0.100000 loss 1.7100 (1.5943) acc@1 0.3750 (0.4060) acc@5 0.7734 (0.7811)\n",
      "\u001b[32m[2020-06-26 06:03:30] __main__ INFO: \u001b[0mEpoch 51 Step 300/351 lr 0.100000 loss 1.6206 (1.6048) acc@1 0.3750 (0.4018) acc@5 0.7734 (0.7783)\n",
      "\u001b[32m[2020-06-26 06:04:43] __main__ INFO: \u001b[0mEpoch 51 Step 351/351 lr 0.100000 loss 1.5743 (1.6077) acc@1 0.4141 (0.4009) acc@5 0.8125 (0.7788)\n",
      "\u001b[32m[2020-06-26 06:04:43] __main__ INFO: \u001b[0mElapsed 501.76\n",
      "\u001b[32m[2020-06-26 06:04:43] __main__ INFO: \u001b[0mVal 51\n",
      "\u001b[32m[2020-06-26 06:05:01] __main__ INFO: \u001b[0mEpoch 51 loss 1.7900 acc@1 0.3612 acc@5 0.7396\n",
      "\u001b[32m[2020-06-26 06:05:01] __main__ INFO: \u001b[0mElapsed 17.76\n",
      "\u001b[32m[2020-06-26 06:05:01] __main__ INFO: \u001b[0mTrain 52 17901\n",
      "\u001b[32m[2020-06-26 06:07:24] __main__ INFO: \u001b[0mEpoch 52 Step 100/351 lr 0.100000 loss 1.5224 (1.5778) acc@1 0.4766 (0.4157) acc@5 0.7969 (0.7832)\n",
      "\u001b[32m[2020-06-26 06:09:47] __main__ INFO: \u001b[0mEpoch 52 Step 200/351 lr 0.100000 loss 1.4797 (1.5958) acc@1 0.4375 (0.4075) acc@5 0.8359 (0.7811)\n",
      "\u001b[32m[2020-06-26 06:12:10] __main__ INFO: \u001b[0mEpoch 52 Step 300/351 lr 0.100000 loss 1.6043 (1.5997) acc@1 0.3672 (0.4069) acc@5 0.7812 (0.7810)\n",
      "\u001b[32m[2020-06-26 06:13:23] __main__ INFO: \u001b[0mEpoch 52 Step 351/351 lr 0.100000 loss 1.5072 (1.6002) acc@1 0.4062 (0.4075) acc@5 0.8047 (0.7810)\n",
      "\u001b[32m[2020-06-26 06:13:23] __main__ INFO: \u001b[0mElapsed 501.71\n",
      "\u001b[32m[2020-06-26 06:13:23] __main__ INFO: \u001b[0mVal 52\n",
      "\u001b[32m[2020-06-26 06:13:41] __main__ INFO: \u001b[0mEpoch 52 loss 1.8065 acc@1 0.3584 acc@5 0.7736\n",
      "\u001b[32m[2020-06-26 06:13:41] __main__ INFO: \u001b[0mElapsed 17.79\n",
      "\u001b[32m[2020-06-26 06:13:41] __main__ INFO: \u001b[0mTrain 53 18252\n",
      "\u001b[32m[2020-06-26 06:16:04] __main__ INFO: \u001b[0mEpoch 53 Step 100/351 lr 0.100000 loss 1.6520 (1.5881) acc@1 0.3750 (0.4136) acc@5 0.7812 (0.7788)\n",
      "\u001b[32m[2020-06-26 06:18:26] __main__ INFO: \u001b[0mEpoch 53 Step 200/351 lr 0.100000 loss 1.4064 (1.5937) acc@1 0.4688 (0.4116) acc@5 0.8672 (0.7802)\n",
      "\u001b[32m[2020-06-26 06:20:49] __main__ INFO: \u001b[0mEpoch 53 Step 300/351 lr 0.100000 loss 1.6300 (1.5965) acc@1 0.3828 (0.4106) acc@5 0.7656 (0.7811)\n",
      "\u001b[32m[2020-06-26 06:22:02] __main__ INFO: \u001b[0mEpoch 53 Step 351/351 lr 0.100000 loss 1.6615 (1.5961) acc@1 0.3984 (0.4107) acc@5 0.7500 (0.7809)\n",
      "\u001b[32m[2020-06-26 06:22:02] __main__ INFO: \u001b[0mElapsed 501.67\n",
      "\u001b[32m[2020-06-26 06:22:02] __main__ INFO: \u001b[0mVal 53\n",
      "\u001b[32m[2020-06-26 06:22:20] __main__ INFO: \u001b[0mEpoch 53 loss 1.7364 acc@1 0.3700 acc@5 0.7710\n",
      "\u001b[32m[2020-06-26 06:22:20] __main__ INFO: \u001b[0mElapsed 17.76\n",
      "\u001b[32m[2020-06-26 06:22:20] __main__ INFO: \u001b[0mTrain 54 18603\n",
      "\u001b[32m[2020-06-26 06:24:43] __main__ INFO: \u001b[0mEpoch 54 Step 100/351 lr 0.100000 loss 1.6304 (1.5844) acc@1 0.3984 (0.4115) acc@5 0.7500 (0.7824)\n",
      "\u001b[32m[2020-06-26 06:27:06] __main__ INFO: \u001b[0mEpoch 54 Step 200/351 lr 0.100000 loss 1.5495 (1.5888) acc@1 0.4219 (0.4098) acc@5 0.7578 (0.7822)\n",
      "\u001b[32m[2020-06-26 06:29:29] __main__ INFO: \u001b[0mEpoch 54 Step 300/351 lr 0.100000 loss 1.6638 (1.5911) acc@1 0.3359 (0.4102) acc@5 0.8047 (0.7812)\n",
      "\u001b[32m[2020-06-26 06:30:42] __main__ INFO: \u001b[0mEpoch 54 Step 351/351 lr 0.100000 loss 1.6214 (1.5917) acc@1 0.3594 (0.4097) acc@5 0.7578 (0.7824)\n",
      "\u001b[32m[2020-06-26 06:30:42] __main__ INFO: \u001b[0mElapsed 501.62\n",
      "\u001b[32m[2020-06-26 06:30:42] __main__ INFO: \u001b[0mVal 54\n",
      "\u001b[32m[2020-06-26 06:30:59] __main__ INFO: \u001b[0mEpoch 54 loss 2.0181 acc@1 0.3352 acc@5 0.7566\n",
      "\u001b[32m[2020-06-26 06:30:59] __main__ INFO: \u001b[0mElapsed 17.78\n",
      "\u001b[32m[2020-06-26 06:30:59] __main__ INFO: \u001b[0mTrain 55 18954\n",
      "\u001b[32m[2020-06-26 06:33:22] __main__ INFO: \u001b[0mEpoch 55 Step 100/351 lr 0.100000 loss 1.7122 (1.5737) acc@1 0.4062 (0.4156) acc@5 0.7734 (0.7859)\n",
      "\u001b[32m[2020-06-26 06:35:46] __main__ INFO: \u001b[0mEpoch 55 Step 200/351 lr 0.100000 loss 1.6171 (1.5883) acc@1 0.3750 (0.4137) acc@5 0.7891 (0.7843)\n",
      "\u001b[32m[2020-06-26 06:38:09] __main__ INFO: \u001b[0mEpoch 55 Step 300/351 lr 0.100000 loss 1.5842 (1.5880) acc@1 0.4219 (0.4129) acc@5 0.7578 (0.7837)\n",
      "\u001b[32m[2020-06-26 06:39:22] __main__ INFO: \u001b[0mEpoch 55 Step 351/351 lr 0.100000 loss 1.7081 (1.5905) acc@1 0.3750 (0.4120) acc@5 0.7656 (0.7824)\n",
      "\u001b[32m[2020-06-26 06:39:22] __main__ INFO: \u001b[0mElapsed 502.67\n",
      "\u001b[32m[2020-06-26 06:39:22] __main__ INFO: \u001b[0mVal 55\n",
      "\u001b[32m[2020-06-26 06:39:40] __main__ INFO: \u001b[0mEpoch 55 loss 1.8043 acc@1 0.3786 acc@5 0.7584\n",
      "\u001b[32m[2020-06-26 06:39:40] __main__ INFO: \u001b[0mElapsed 17.79\n",
      "\u001b[32m[2020-06-26 06:39:40] __main__ INFO: \u001b[0mTrain 56 19305\n",
      "\u001b[32m[2020-06-26 06:42:03] __main__ INFO: \u001b[0mEpoch 56 Step 100/351 lr 0.100000 loss 1.8209 (1.5708) acc@1 0.3672 (0.4184) acc@5 0.7266 (0.7834)\n",
      "\u001b[32m[2020-06-26 06:44:26] __main__ INFO: \u001b[0mEpoch 56 Step 200/351 lr 0.100000 loss 1.5749 (1.5841) acc@1 0.3359 (0.4137) acc@5 0.7734 (0.7824)\n",
      "\u001b[32m[2020-06-26 06:46:49] __main__ INFO: \u001b[0mEpoch 56 Step 300/351 lr 0.100000 loss 1.7494 (1.5828) acc@1 0.3359 (0.4137) acc@5 0.7500 (0.7845)\n",
      "\u001b[32m[2020-06-26 06:48:02] __main__ INFO: \u001b[0mEpoch 56 Step 351/351 lr 0.100000 loss 1.7289 (1.5867) acc@1 0.3594 (0.4123) acc@5 0.7734 (0.7840)\n",
      "\u001b[32m[2020-06-26 06:48:02] __main__ INFO: \u001b[0mElapsed 502.30\n",
      "\u001b[32m[2020-06-26 06:48:02] __main__ INFO: \u001b[0mVal 56\n",
      "\u001b[32m[2020-06-26 06:48:20] __main__ INFO: \u001b[0mEpoch 56 loss 1.7257 acc@1 0.3734 acc@5 0.7698\n",
      "\u001b[32m[2020-06-26 06:48:20] __main__ INFO: \u001b[0mElapsed 17.89\n",
      "\u001b[32m[2020-06-26 06:48:20] __main__ INFO: \u001b[0mTrain 57 19656\n",
      "\u001b[32m[2020-06-26 06:50:44] __main__ INFO: \u001b[0mEpoch 57 Step 100/351 lr 0.100000 loss 1.4727 (1.5668) acc@1 0.4609 (0.4172) acc@5 0.7812 (0.7822)\n",
      "\u001b[32m[2020-06-26 06:53:07] __main__ INFO: \u001b[0mEpoch 57 Step 200/351 lr 0.100000 loss 1.6248 (1.5760) acc@1 0.3984 (0.4141) acc@5 0.7500 (0.7815)\n",
      "\u001b[32m[2020-06-26 06:55:31] __main__ INFO: \u001b[0mEpoch 57 Step 300/351 lr 0.100000 loss 1.5326 (1.5852) acc@1 0.4219 (0.4101) acc@5 0.8359 (0.7796)\n",
      "\u001b[32m[2020-06-26 06:56:44] __main__ INFO: \u001b[0mEpoch 57 Step 351/351 lr 0.100000 loss 1.5947 (1.5857) acc@1 0.3750 (0.4100) acc@5 0.8125 (0.7800)\n",
      "\u001b[32m[2020-06-26 06:56:44] __main__ INFO: \u001b[0mElapsed 503.68\n",
      "\u001b[32m[2020-06-26 06:56:44] __main__ INFO: \u001b[0mVal 57\n",
      "\u001b[32m[2020-06-26 06:57:02] __main__ INFO: \u001b[0mEpoch 57 loss 1.7168 acc@1 0.3780 acc@5 0.7652\n",
      "\u001b[32m[2020-06-26 06:57:02] __main__ INFO: \u001b[0mElapsed 17.85\n",
      "\u001b[32m[2020-06-26 06:57:02] __main__ INFO: \u001b[0mTrain 58 20007\n",
      "\u001b[32m[2020-06-26 06:59:25] __main__ INFO: \u001b[0mEpoch 58 Step 100/351 lr 0.100000 loss 1.5349 (1.5711) acc@1 0.4141 (0.4175) acc@5 0.7891 (0.7848)\n",
      "\u001b[32m[2020-06-26 07:01:48] __main__ INFO: \u001b[0mEpoch 58 Step 200/351 lr 0.100000 loss 1.5228 (1.5854) acc@1 0.4219 (0.4129) acc@5 0.8047 (0.7832)\n",
      "\u001b[32m[2020-06-26 07:04:12] __main__ INFO: \u001b[0mEpoch 58 Step 300/351 lr 0.100000 loss 1.6542 (1.5833) acc@1 0.3984 (0.4133) acc@5 0.7578 (0.7848)\n",
      "\u001b[32m[2020-06-26 07:05:25] __main__ INFO: \u001b[0mEpoch 58 Step 351/351 lr 0.100000 loss 1.6832 (1.5820) acc@1 0.3359 (0.4140) acc@5 0.7266 (0.7837)\n",
      "\u001b[32m[2020-06-26 07:05:25] __main__ INFO: \u001b[0mElapsed 503.31\n",
      "\u001b[32m[2020-06-26 07:05:25] __main__ INFO: \u001b[0mVal 58\n",
      "\u001b[32m[2020-06-26 07:05:43] __main__ INFO: \u001b[0mEpoch 58 loss 1.7387 acc@1 0.3798 acc@5 0.7714\n",
      "\u001b[32m[2020-06-26 07:05:43] __main__ INFO: \u001b[0mElapsed 17.85\n",
      "\u001b[32m[2020-06-26 07:05:43] __main__ INFO: \u001b[0mTrain 59 20358\n",
      "\u001b[32m[2020-06-26 07:08:06] __main__ INFO: \u001b[0mEpoch 59 Step 100/351 lr 0.100000 loss 1.4456 (1.5809) acc@1 0.4609 (0.4191) acc@5 0.8047 (0.7857)\n",
      "\u001b[32m[2020-06-26 07:10:30] __main__ INFO: \u001b[0mEpoch 59 Step 200/351 lr 0.100000 loss 1.6451 (1.5774) acc@1 0.3672 (0.4164) acc@5 0.8047 (0.7835)\n",
      "\u001b[32m[2020-06-26 07:12:53] __main__ INFO: \u001b[0mEpoch 59 Step 300/351 lr 0.100000 loss 1.5669 (1.5790) acc@1 0.5156 (0.4163) acc@5 0.7969 (0.7835)\n",
      "\u001b[32m[2020-06-26 07:14:05] __main__ INFO: \u001b[0mEpoch 59 Step 351/351 lr 0.100000 loss 1.6859 (1.5842) acc@1 0.4141 (0.4127) acc@5 0.7812 (0.7833)\n",
      "\u001b[32m[2020-06-26 07:14:05] __main__ INFO: \u001b[0mElapsed 502.64\n",
      "\u001b[32m[2020-06-26 07:14:05] __main__ INFO: \u001b[0mVal 59\n",
      "\u001b[32m[2020-06-26 07:14:23] __main__ INFO: \u001b[0mEpoch 59 loss 1.8552 acc@1 0.3560 acc@5 0.7562\n",
      "\u001b[32m[2020-06-26 07:14:23] __main__ INFO: \u001b[0mElapsed 17.81\n",
      "\u001b[32m[2020-06-26 07:14:23] __main__ INFO: \u001b[0mTrain 60 20709\n",
      "\u001b[32m[2020-06-26 07:16:46] __main__ INFO: \u001b[0mEpoch 60 Step 100/351 lr 0.100000 loss 1.5261 (1.5696) acc@1 0.4375 (0.4165) acc@5 0.8516 (0.7856)\n",
      "\u001b[32m[2020-06-26 07:19:10] __main__ INFO: \u001b[0mEpoch 60 Step 200/351 lr 0.100000 loss 1.6194 (1.5755) acc@1 0.3594 (0.4139) acc@5 0.7500 (0.7857)\n",
      "\u001b[32m[2020-06-26 07:21:34] __main__ INFO: \u001b[0mEpoch 60 Step 300/351 lr 0.100000 loss 1.5379 (1.5828) acc@1 0.4297 (0.4109) acc@5 0.8047 (0.7841)\n",
      "\u001b[32m[2020-06-26 07:22:47] __main__ INFO: \u001b[0mEpoch 60 Step 351/351 lr 0.100000 loss 1.6541 (1.5850) acc@1 0.3594 (0.4096) acc@5 0.7656 (0.7851)\n",
      "\u001b[32m[2020-06-26 07:22:47] __main__ INFO: \u001b[0mElapsed 503.69\n",
      "\u001b[32m[2020-06-26 07:22:47] __main__ INFO: \u001b[0mVal 60\n",
      "\u001b[32m[2020-06-26 07:23:05] __main__ INFO: \u001b[0mEpoch 60 loss 1.7182 acc@1 0.3804 acc@5 0.7598\n",
      "\u001b[32m[2020-06-26 07:23:05] __main__ INFO: \u001b[0mElapsed 17.84\n",
      "\u001b[32m[2020-06-26 07:23:05] __main__ INFO: \u001b[0mTrain 61 21060\n",
      "\u001b[32m[2020-06-26 07:25:28] __main__ INFO: \u001b[0mEpoch 61 Step 100/351 lr 0.100000 loss 1.5967 (1.5801) acc@1 0.4531 (0.4184) acc@5 0.7500 (0.7822)\n",
      "\u001b[32m[2020-06-26 07:27:52] __main__ INFO: \u001b[0mEpoch 61 Step 200/351 lr 0.100000 loss 1.5776 (1.5748) acc@1 0.4219 (0.4188) acc@5 0.7656 (0.7834)\n",
      "\u001b[32m[2020-06-26 07:30:15] __main__ INFO: \u001b[0mEpoch 61 Step 300/351 lr 0.100000 loss 1.5400 (1.5801) acc@1 0.3750 (0.4146) acc@5 0.7422 (0.7821)\n",
      "\u001b[32m[2020-06-26 07:31:29] __main__ INFO: \u001b[0mEpoch 61 Step 351/351 lr 0.100000 loss 1.5053 (1.5794) acc@1 0.4141 (0.4147) acc@5 0.7344 (0.7827)\n",
      "\u001b[32m[2020-06-26 07:31:29] __main__ INFO: \u001b[0mElapsed 503.84\n",
      "\u001b[32m[2020-06-26 07:31:29] __main__ INFO: \u001b[0mVal 61\n",
      "\u001b[32m[2020-06-26 07:31:46] __main__ INFO: \u001b[0mEpoch 61 loss 1.7252 acc@1 0.3752 acc@5 0.7784\n",
      "\u001b[32m[2020-06-26 07:31:46] __main__ INFO: \u001b[0mElapsed 17.84\n",
      "\u001b[32m[2020-06-26 07:31:46] __main__ INFO: \u001b[0mTrain 62 21411\n",
      "\u001b[32m[2020-06-26 07:34:10] __main__ INFO: \u001b[0mEpoch 62 Step 100/351 lr 0.100000 loss 1.5714 (1.5738) acc@1 0.4453 (0.4141) acc@5 0.7891 (0.7769)\n",
      "\u001b[32m[2020-06-26 07:36:33] __main__ INFO: \u001b[0mEpoch 62 Step 200/351 lr 0.100000 loss 1.4891 (1.5802) acc@1 0.4766 (0.4109) acc@5 0.7891 (0.7807)\n",
      "\u001b[32m[2020-06-26 07:38:57] __main__ INFO: \u001b[0mEpoch 62 Step 300/351 lr 0.100000 loss 1.5808 (1.5751) acc@1 0.3828 (0.4150) acc@5 0.7891 (0.7810)\n",
      "\u001b[32m[2020-06-26 07:40:10] __main__ INFO: \u001b[0mEpoch 62 Step 351/351 lr 0.100000 loss 1.6444 (1.5758) acc@1 0.3984 (0.4150) acc@5 0.7344 (0.7810)\n",
      "\u001b[32m[2020-06-26 07:40:10] __main__ INFO: \u001b[0mElapsed 503.69\n",
      "\u001b[32m[2020-06-26 07:40:10] __main__ INFO: \u001b[0mVal 62\n",
      "\u001b[32m[2020-06-26 07:40:28] __main__ INFO: \u001b[0mEpoch 62 loss 1.5952 acc@1 0.4146 acc@5 0.7892\n",
      "\u001b[32m[2020-06-26 07:40:28] __main__ INFO: \u001b[0mElapsed 17.84\n",
      "\u001b[32m[2020-06-26 07:40:28] __main__ INFO: \u001b[0mTrain 63 21762\n",
      "\u001b[32m[2020-06-26 07:42:52] __main__ INFO: \u001b[0mEpoch 63 Step 100/351 lr 0.100000 loss 1.5517 (1.5502) acc@1 0.4062 (0.4241) acc@5 0.7422 (0.7856)\n",
      "\u001b[32m[2020-06-26 07:45:15] __main__ INFO: \u001b[0mEpoch 63 Step 200/351 lr 0.100000 loss 1.4388 (1.5768) acc@1 0.4844 (0.4153) acc@5 0.8047 (0.7831)\n",
      "\u001b[32m[2020-06-26 07:47:39] __main__ INFO: \u001b[0mEpoch 63 Step 300/351 lr 0.100000 loss 1.4657 (1.5766) acc@1 0.4531 (0.4153) acc@5 0.8281 (0.7841)\n",
      "\u001b[32m[2020-06-26 07:48:52] __main__ INFO: \u001b[0mEpoch 63 Step 351/351 lr 0.100000 loss 1.7421 (1.5800) acc@1 0.3125 (0.4143) acc@5 0.7422 (0.7833)\n",
      "\u001b[32m[2020-06-26 07:48:52] __main__ INFO: \u001b[0mElapsed 503.87\n",
      "\u001b[32m[2020-06-26 07:48:52] __main__ INFO: \u001b[0mVal 63\n",
      "\u001b[32m[2020-06-26 07:49:10] __main__ INFO: \u001b[0mEpoch 63 loss 1.6618 acc@1 0.4058 acc@5 0.7734\n",
      "\u001b[32m[2020-06-26 07:49:10] __main__ INFO: \u001b[0mElapsed 17.86\n",
      "\u001b[32m[2020-06-26 07:49:10] __main__ INFO: \u001b[0mTrain 64 22113\n",
      "\u001b[32m[2020-06-26 07:51:33] __main__ INFO: \u001b[0mEpoch 64 Step 100/351 lr 0.100000 loss 1.4905 (1.5710) acc@1 0.3984 (0.4149) acc@5 0.7734 (0.7814)\n",
      "\u001b[32m[2020-06-26 07:53:56] __main__ INFO: \u001b[0mEpoch 64 Step 200/351 lr 0.100000 loss 1.6617 (1.5733) acc@1 0.3359 (0.4145) acc@5 0.7969 (0.7827)\n",
      "\u001b[32m[2020-06-26 07:56:19] __main__ INFO: \u001b[0mEpoch 64 Step 300/351 lr 0.100000 loss 1.5848 (1.5736) acc@1 0.3984 (0.4152) acc@5 0.7578 (0.7830)\n",
      "\u001b[32m[2020-06-26 07:57:32] __main__ INFO: \u001b[0mEpoch 64 Step 351/351 lr 0.100000 loss 1.6322 (1.5761) acc@1 0.3984 (0.4144) acc@5 0.8047 (0.7825)\n",
      "\u001b[32m[2020-06-26 07:57:32] __main__ INFO: \u001b[0mElapsed 501.81\n",
      "\u001b[32m[2020-06-26 07:57:32] __main__ INFO: \u001b[0mVal 64\n",
      "\u001b[32m[2020-06-26 07:57:49] __main__ INFO: \u001b[0mEpoch 64 loss 1.6643 acc@1 0.3914 acc@5 0.7796\n",
      "\u001b[32m[2020-06-26 07:57:49] __main__ INFO: \u001b[0mElapsed 17.79\n",
      "\u001b[32m[2020-06-26 07:57:49] __main__ INFO: \u001b[0mTrain 65 22464\n",
      "\u001b[32m[2020-06-26 08:00:12] __main__ INFO: \u001b[0mEpoch 65 Step 100/351 lr 0.100000 loss 1.6844 (1.5549) acc@1 0.3750 (0.4197) acc@5 0.7734 (0.7877)\n",
      "\u001b[32m[2020-06-26 08:02:35] __main__ INFO: \u001b[0mEpoch 65 Step 200/351 lr 0.100000 loss 1.6254 (1.5641) acc@1 0.3594 (0.4189) acc@5 0.7344 (0.7863)\n",
      "\u001b[32m[2020-06-26 08:04:58] __main__ INFO: \u001b[0mEpoch 65 Step 300/351 lr 0.100000 loss 1.5679 (1.5666) acc@1 0.4375 (0.4185) acc@5 0.7656 (0.7842)\n",
      "\u001b[32m[2020-06-26 08:06:11] __main__ INFO: \u001b[0mEpoch 65 Step 351/351 lr 0.100000 loss 1.6575 (1.5665) acc@1 0.4141 (0.4185) acc@5 0.7812 (0.7846)\n",
      "\u001b[32m[2020-06-26 08:06:11] __main__ INFO: \u001b[0mElapsed 501.23\n",
      "\u001b[32m[2020-06-26 08:06:11] __main__ INFO: \u001b[0mVal 65\n",
      "\u001b[32m[2020-06-26 08:06:28] __main__ INFO: \u001b[0mEpoch 65 loss 1.7791 acc@1 0.3662 acc@5 0.7644\n",
      "\u001b[32m[2020-06-26 08:06:28] __main__ INFO: \u001b[0mElapsed 17.84\n",
      "\u001b[32m[2020-06-26 08:06:28] __main__ INFO: \u001b[0mTrain 66 22815\n",
      "\u001b[32m[2020-06-26 08:08:52] __main__ INFO: \u001b[0mEpoch 66 Step 100/351 lr 0.100000 loss 1.6260 (1.5485) acc@1 0.3750 (0.4260) acc@5 0.7812 (0.7884)\n",
      "\u001b[32m[2020-06-26 08:11:15] __main__ INFO: \u001b[0mEpoch 66 Step 200/351 lr 0.100000 loss 1.5295 (1.5597) acc@1 0.4453 (0.4235) acc@5 0.8281 (0.7857)\n"
     ]
    }
   ],
   "source": [
    "# Train the model per the settings specified for ResNext 29_4x64d in the original paper\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "!python train.py --config configs/cifar/resnext.yaml \\\n",
    "    model.resnext.cardinality 4 \\\n",
    "    train.batch_size 128 \\\n",
    "    dataset.name CIFAR10_RA_3_20 \\\n",
    "    train.base_lr 0.1 \\\n",
    "    train.output_dir /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_3_20/exp00 \\\n",
    "    scheduler.epochs 400\n",
    "\n",
    "# Number of epochs should be 300!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-11 01:08:55] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/resnext_29_4x64d/exp00/checkpoint_00300.pth\n",
      "Files already downloaded and verified\n",
      "100%|███████████████████████████████████████████| 79/79 [00:34<00:00,  2.29it/s]\n",
      "\u001b[32m[2020-06-11 01:09:30] __main__ INFO: \u001b[0mElapsed 34.44\n",
      "\u001b[32m[2020-06-11 01:09:30] __main__ INFO: \u001b[0mLoss 0.1517 Accuracy 0.9535\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/resnext.yaml \\\n",
    "   model.resnext.cardinality 4 \\\n",
    "   test.batch_size 128 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/resnext_29_4x64d/exp00/checkpoint_00300.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/resnext_29_4x64d/exp00/test_results_0300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-11 01:50:14] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/resnext_29_4x64d/exp00/checkpoint_00200.pth\n",
      "Files already downloaded and verified\n",
      "100%|███████████████████████████████████████████| 79/79 [00:34<00:00,  2.30it/s]\n",
      "\u001b[32m[2020-06-11 01:50:49] __main__ INFO: \u001b[0mElapsed 34.36\n",
      "\u001b[32m[2020-06-11 01:50:49] __main__ INFO: \u001b[0mLoss 0.2311 Accuracy 0.9321\n"
     ]
    }
   ],
   "source": [
    "!python evaluate.py --config configs/cifar/resnext.yaml \\\n",
    "   model.resnext.cardinality 4 \\\n",
    "   test.batch_size 128 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/resnext_29_4x64d/exp00/checkpoint_00200.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/resnext_29_4x64d/exp00/test_results_0200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-11 01:51:05] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/resnext_29_4x64d/exp00/checkpoint_00100.pth\n",
      "Files already downloaded and verified\n",
      "100%|███████████████████████████████████████████| 79/79 [00:34<00:00,  2.28it/s]\n",
      "\u001b[32m[2020-06-11 01:51:41] __main__ INFO: \u001b[0mElapsed 34.69\n",
      "\u001b[32m[2020-06-11 01:51:41] __main__ INFO: \u001b[0mLoss 0.6746 Accuracy 0.8019\n"
     ]
    }
   ],
   "source": [
    "!python evaluate.py --config configs/cifar/resnext.yaml \\\n",
    "   model.resnext.cardinality 4 \\\n",
    "   test.batch_size 128 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/resnext_29_4x64d/exp00/checkpoint_00100.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/resnext_29_4x64d/exp00/test_results_0100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-14 17:43:04] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/resnext_29_4x64d/exp00/checkpoint_00300.pth\n",
      "CIFAR 10.1\n",
      "100%|███████████████████████████████████████████| 16/16 [00:07<00:00,  2.21it/s]\n",
      "\u001b[32m[2020-06-14 17:43:12] __main__ INFO: \u001b[0mElapsed 7.25\n",
      "\u001b[32m[2020-06-14 17:43:12] __main__ INFO: \u001b[0mLoss 0.3742 Accuracy 0.8905\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10.1 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "!python evaluate.py --config configs/cifar/resnext.yaml \\\n",
    "    model.resnext.cardinality 4 \\\n",
    "    test.batch_size 128 \\\n",
    "    dataset.name CIFAR101 \\\n",
    "    test.checkpoint /home/ec2-user/SageMaker/experiments/resnext_29_4x64d/exp00/checkpoint_00300.pth \\\n",
    "    test.output_dir /home/ec2-user/SageMaker/experiments/resnext_29_4x64d/exp00/test_results_0300_CIFAR101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Testset</th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Original_Accuracy</th>\n",
       "      <th>Original_CI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>resnext_29_4x64d</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>100</td>\n",
       "      <td>0.6746</td>\n",
       "      <td>0.8019</td>\n",
       "      <td>96.4</td>\n",
       "      <td>(96.0, 96.7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>resnext_29_4x64d</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>200</td>\n",
       "      <td>0.2311</td>\n",
       "      <td>0.9321</td>\n",
       "      <td>96.4</td>\n",
       "      <td>(96.0, 96.7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>resnext_29_4x64d</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>300</td>\n",
       "      <td>0.1517</td>\n",
       "      <td>0.9535</td>\n",
       "      <td>96.4</td>\n",
       "      <td>(96.0, 96.7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>resnext_29_4x64d</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>300</td>\n",
       "      <td>0.3742</td>\n",
       "      <td>0.8905</td>\n",
       "      <td>89.6</td>\n",
       "      <td>(88.2, 90.9)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Model    Testset  Epoch    Loss  Accuracy  Original_Accuracy  \\\n",
       "0  resnext_29_4x64d    cifar10    100  0.6746    0.8019               96.4   \n",
       "1  resnext_29_4x64d    cifar10    200  0.2311    0.9321               96.4   \n",
       "2  resnext_29_4x64d    cifar10    300  0.1517    0.9535               96.4   \n",
       "3  resnext_29_4x64d  cifar10.1    300  0.3742    0.8905               89.6   \n",
       "\n",
       "    Original_CI  \n",
       "0  (96.0, 96.7)  \n",
       "1  (96.0, 96.7)  \n",
       "2  (96.0, 96.7)  \n",
       "3  (88.2, 90.9)  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write the results to a CSV file so that we can analyze later.\n",
    "import pandas as pd\n",
    "\n",
    "results = {'Model': ['resnext_29_4x64d', 'resnext_29_4x64d', 'resnext_29_4x64d', 'resnext_29_4x64d'],\n",
    "           'Testset': ['cifar10', 'cifar10', 'cifar10', 'cifar10.1'],\n",
    "           'Epoch': [100, 200, 300, 300],\n",
    "           'Loss': [0.6746, 0.2311, 0.1517, 0.3742],\n",
    "           'Accuracy': [0.8019, 0.9321, 0.9535, 0.8905],\n",
    "           'Original_Accuracy': [96.4, 96.4, 96.4, 89.6],\n",
    "           'Original_CI': [(96.0, 96.7), (96.0, 96.7), (96.0, 96.7), (88.2, 90.9)]\n",
    "           }\n",
    "\n",
    "df = pd.DataFrame(results, columns = ['Model', 'Testset', 'Epoch', 'Loss', 'Accuracy', \n",
    "                                      'Original_Accuracy', 'Original_CI'])\n",
    "\n",
    "\n",
    "df.to_csv('/home/ec2-user/SageMaker/experiments/resnext_29_4x64d/exp00/results.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['preds', 'probs', 'labels', 'loss', 'acc']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-1.2041907 , -1.8999833 , -0.24285015, ..., -1.5008752 ,\n",
       "        -1.8426697 , -2.8560946 ],\n",
       "       [ 0.5460079 ,  2.220384  , -1.9393705 , ..., -2.6070693 ,\n",
       "        11.327686  , -1.2085156 ],\n",
       "       [-1.3446747 ,  2.1730833 , -1.1615647 , ..., -2.2299995 ,\n",
       "        10.984515  , -0.75660706],\n",
       "       ...,\n",
       "       [-2.4790986 , -1.3337001 ,  0.61669415, ..., -0.83421385,\n",
       "        -1.8529658 , -1.7280097 ],\n",
       "       [-0.90489024,  9.350766  ,  1.0618937 , ..., -2.3210623 ,\n",
       "        -0.9061641 , -1.8115013 ],\n",
       "       [-1.4560711 , -1.0518838 , -1.4613396 , ..., 12.668192  ,\n",
       "        -2.1191459 , -0.8881919 ]], dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Peak inside the output file for predictions\n",
    "import numpy as np\n",
    "output = '/home/ec2-user/SageMaker/experiments/resnext_29_4x64d/exp00/test_results_0300/predictions.npz'\n",
    "npzfile = np.load(output)\n",
    "print(npzfile.files)\n",
    "npzfile['preds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the model checkpoints, configs, and results to S3 \n",
    "bucket='sagemaker-may29'\n",
    "prefix = 'sagemaker/results/original-models/'\n",
    "path = '/home/ec2-user/SageMaker/experiments/'\n",
    "\n",
    "s3_resource = boto3.resource(\"s3\", region_name=\"us-east-2\")\n",
    "\n",
    "def uploadDirectory(local_path,bucket_name,s3_prefix):\n",
    "\n",
    "    my_bucket = s3_resource.Bucket(bucket_name)\n",
    "    \n",
    "    for path, subdirs, files in os.walk(local_path):\n",
    "        path = path.replace(\"\\\\\",\"/\")\n",
    "        directory_name = path.replace(local_path,\"\")\n",
    "        for file in files:\n",
    "            #print(\"Local File:\", os.path.join(path, file))\n",
    "            #print(\"      Dest:\", s3_prefix+directory_name+'/'+file)\n",
    "            my_bucket.upload_file(os.path.join(path, file), s3_prefix+directory_name+'/'+file)\n",
    "    \n",
    "uploadDirectory(path,bucket,prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Everything Below Is In-Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SageMaker Way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 Variable Definitions \n",
    "bucket='sagemaker-may29'\n",
    "prefix = '/sagemaker-may29/sagemaker/cifar102/data/cifar10_10k'\n",
    "model_path = '/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/'\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "#bucket = sagemaker.Session().default_bucket(\n",
    "#sagemaker_session = sagemaker.Session()\n",
    "bucket='sagemaker-may29'\n",
    "sagemaker_session = sagemaker.Session(default_bucket=bucket)\n",
    "\n",
    "\n",
    "# Set S3 dataset path \n",
    "#inputs = 's3://' + bucket + '/sagemaker/{}'.format(args.train_data)\n",
    "inputs = 's3://' + bucket + '/sagemaker-may29/sagemaker/cifar102/data/cifar10_10k/cifar10_10k_test'\n",
    "inputs = 's3://' + bucket + '/sagemaker-may29/sagemaker/cifar102/data/cifar10_10k/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/'\n",
    "container_data_dir = '/opt/ml/input/data/training'\n",
    "container_model_dir = '/opt/ml/model'\n",
    "\n",
    "parameters = {\n",
    "    'config': 'resnext.yaml',\n",
    "    'resnext.depth': 29,\n",
    "    'train.batch_size': 128,\n",
    "    'train.base_lr': 0.1,\n",
    "    #'data_dir': container_data_dir,\n",
    "    #'dataset.dataset_dir': container_data_dir\n",
    "    #'output_dir': container_model_dir,\n",
    "    #'num_train_epochs': 3,\n",
    "    #'per_gpu_train_batch_size': 64,\n",
    "    #'per_gpu_eval_batch_size': 64,\n",
    "    #'save_steps': 150,\n",
    "    #'logging_steps': 150\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the PyTorch class that enables the model script to run as a \n",
    "# training job on the SageMaker distributed, managed training infrastructure\n",
    "estimator = PyTorch(entry_point= model_path + 'train.py',\n",
    "                    role=role,\n",
    "                    framework_version='1.4.0',\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type='ml.p2.xlarge',\n",
    "                    hyperparameters=parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n",
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-08 14:49:34 Starting - Starting the training job...\n",
      "2020-06-08 14:49:36 Starting - Launching requested ML instances.........\n",
      "2020-06-08 14:51:06 Starting - Preparing the instances for training......\n",
      "2020-06-08 14:52:21 Downloading - Downloading input data...\n",
      "2020-06-08 14:53:01 Training - Downloading the training image...........\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-06-08 14:54:49,375 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-06-08 14:54:49,397 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-06-08 14:54:49,403 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-06-08 14:54:49,684 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-06-08 14:54:49,684 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-06-08 14:54:49,684 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-06-08 14:54:49,685 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m pip install . \u001b[0m\n",
      "\u001b[34mProcessing /tmp/tmprotlgjc8/module_dir\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: default-user-module-name\n",
      "  Building wheel for default-user-module-name (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for default-user-module-name (setup.py): finished with status 'done'\n",
      "  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=10386 sha256=4f15af3dee30a7116f4f4bdfc068a488b47ab4d7d874d41ddf7a13c58d82ad7a\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-hdo2srdp/wheels/c9/42/19/42704d4efcc75760cfda35a2b5e353ab766536836839b671fb\u001b[0m\n",
      "\u001b[34mSuccessfully built default-user-module-name\u001b[0m\n",
      "\u001b[34mInstalling collected packages: default-user-module-name\u001b[0m\n",
      "\u001b[34mSuccessfully installed default-user-module-name-1.0.0\u001b[0m\n",
      "\u001b[34m2020-06-08 14:54:51,994 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"resnext.depth\": 29,\n",
      "        \"train.base_lr\": 0.1,\n",
      "        \"train.batch_size\": 128,\n",
      "        \"config\": \"resnext.yaml\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2020-06-08-14-49-34-014\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-2-681306256852/pytorch-training-2020-06-08-14-49-34-014/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"config\":\"resnext.yaml\",\"resnext.depth\":29,\"train.base_lr\":0.1,\"train.batch_size\":128}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-2-681306256852/pytorch-training-2020-06-08-14-49-34-014/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"config\":\"resnext.yaml\",\"resnext.depth\":29,\"train.base_lr\":0.1,\"train.batch_size\":128},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2020-06-08-14-49-34-014\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-681306256852/pytorch-training-2020-06-08-14-49-34-014/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--config\",\"resnext.yaml\",\"--resnext.depth\",\"29\",\"--train.base_lr\",\"0.1\",\"--train.batch_size\",\"128\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_RESNEXT.DEPTH=29\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN.BASE_LR=0.1\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN.BATCH_SIZE=128\u001b[0m\n",
      "\u001b[34mSM_HP_CONFIG=resnext.yaml\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python train.py --config resnext.yaml --resnext.depth 29 --train.base_lr 0.1 --train.batch_size 128\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m2020-06-08 14:54:52,052 sagemaker-containers ERROR    ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[34mCommand \"/opt/conda/bin/python train.py --config resnext.yaml --resnext.depth 29 --train.base_lr 0.1 --train.batch_size 128\"\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"train.py\", line 7, in <module>\n",
      "    import apex\u001b[0m\n",
      "\u001b[34mModuleNotFoundError: No module named 'apex'\u001b[0m\n",
      "\n",
      "2020-06-08 14:55:00 Uploading - Uploading generated training model\n",
      "2020-06-08 14:55:00 Failed - Training job failed\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job pytorch-training-2020-06-08-14-49-34-014: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"/opt/conda/bin/python train.py --config resnext.yaml --resnext.depth 29 --train.base_lr 0.1 --train.batch_size 128\"\nTraceback (most recent call last):\n  File \"train.py\", line 7, in <module>\n    import apex\nModuleNotFoundError: No module named 'apex'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b5cd4c918894>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m's3://'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbucket\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/sagemaker-may29/sagemaker/cifar102/data/cifar10_10k/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#estimator.fit({'training': inputs})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compilation_job_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0;31m# If logs are requested, call logs_for_jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"None\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1109\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1110\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type)\u001b[0m\n\u001b[1;32m   3066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3067\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3068\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TrainingJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3069\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3070\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   2660\u001b[0m                 ),\n\u001b[1;32m   2661\u001b[0m                 \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Stopped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2662\u001b[0;31m                 \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2663\u001b[0m             )\n\u001b[1;32m   2664\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job pytorch-training-2020-06-08-14-49-34-014: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"/opt/conda/bin/python train.py --config resnext.yaml --resnext.depth 29 --train.base_lr 0.1 --train.batch_size 128\"\nTraceback (most recent call last):\n  File \"train.py\", line 7, in <module>\n    import apex\nModuleNotFoundError: No module named 'apex'"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "#inputs = 's3://' + bucket + '/sagemaker-may29/sagemaker/cifar102/data/cifar10_10k/cifar10_10k_test'\n",
    "inputs = 's3://' + bucket + '/sagemaker-may29/sagemaker/cifar102/data/cifar10_10k/'\n",
    "#estimator.fit({'training': inputs})\n",
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ec2-user/SageMaker'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Augmentation \n",
    "https://imgaug.readthedocs.io/en/latest/source/overview/collections.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imgaug in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (0.4.0)\n",
      "Requirement already satisfied: imageio in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from imgaug) (2.3.0)\n",
      "Requirement already satisfied: opencv-python in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from imgaug) (4.1.1.26)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from imgaug) (1.11.0)\n",
      "Requirement already satisfied: matplotlib in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from imgaug) (3.0.3)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from imgaug) (1.1.0)\n",
      "Requirement already satisfied: Pillow in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from imgaug) (5.4.1)\n",
      "Requirement already satisfied: Shapely in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from imgaug) (1.7.0)\n",
      "Requirement already satisfied: scikit-image>=0.14.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from imgaug) (0.17.2)\n",
      "Requirement already satisfied: numpy>=1.15 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from imgaug) (1.15.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from matplotlib->imgaug) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from matplotlib->imgaug) (1.0.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from matplotlib->imgaug) (2.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from matplotlib->imgaug) (2.7.3)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from scikit-image>=0.14.2->imgaug) (1.1.1)\n",
      "Requirement already satisfied: networkx>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from scikit-image>=0.14.2->imgaug) (2.1)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from scikit-image>=0.14.2->imgaug) (2020.6.3)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib->imgaug) (47.1.1)\n",
      "Requirement already satisfied: decorator>=4.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from networkx>=2.0->scikit-image>=0.14.2->imgaug) (4.3.0)\n",
      "\u001b[31mfastai 1.0.60 requires nvidia-ml-py3, which is not installed.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.2b1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install imgaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imgaug.augmenters as iaa\n",
    "aug = iaa.RandAugment(n=2, m=(0, 9))  \n",
    "  # n is the number of transformations to apply per image\n",
    "  # m is magnitude -> specifying a tuple will randomly select values between the min and max (max is 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3 \n",
    "from PIL import Image\n",
    "\n",
    "cifar10 = \"sagemaker/cifar-10/cifar/\"\n",
    "#cifar10_test_rec = \"s3://sagemaker-may29/sagemaker/cifar-10/cifar/test.rec\"\n",
    "\n",
    "def download_all_objects_in_folder(b, p):\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    my_bucket = s3_resource.Bucket(b)\n",
    "    objects = my_bucket.objects.filter(Prefix=p)\n",
    "    for obj in objects:\n",
    "        path, filename = os.path.split(obj.key)\n",
    "        my_bucket.download_file(obj.key, filename)    \n",
    "\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "# Download the unzipped data from the cifar10 folder\n",
    "download_all_objects_in_folder(bucket, cifar10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.rec: data\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandAugment \n",
    "https://arxiv.org/abs/1909.13719\n",
    "https://pypi.org/project/randaugment/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting randaugment\n",
      "  Using cached https://files.pythonhosted.org/packages/fb/ea/e24549f459800dc3bed21cd4e9c0d49d5b8deed65214b2444bd3e5a49f30/randaugment-1.0.2-py3-none-any.whl\n",
      "\u001b[31mfastai 1.0.60 requires nvidia-ml-py3, which is not installed.\u001b[0m\n",
      "Installing collected packages: randaugment\n",
      "Successfully installed randaugment-1.0.2\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.2b1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install randaugment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ImageFolder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-114a2729ff3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrandaugment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandAugment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageNetPolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m data = ImageFolder('/home/ec2-user/SageMaker/w210-capstone/data/cifar10/subset300', transform=transforms.Compose(\n\u001b[0m\u001b[1;32m      3\u001b[0m                         [\n\u001b[1;32m      4\u001b[0m                             \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomCrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# fill parameter needs torchvision installed from source\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                             \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomHorizontalFlip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ImageFolder' is not defined"
     ]
    }
   ],
   "source": [
    "from randaugment import RandAugment, ImageNetPolicy\n",
    "data = ImageFolder('/home/ec2-user/SageMaker/w210-capstone/data/cifar10/subset300', transform=transforms.Compose(\n",
    "                        [\n",
    "                            transforms.RandomCrop(32, padding=4, fill=128), # fill parameter needs torchvision installed from source\n",
    "                            transforms.RandomHorizontalFlip(), \n",
    "                            RandAugment(),\n",
    "                            #ImageNetPolicy(),\n",
    "                            transforms.ToTensor(), \n",
    "                            Cutout(size=16), # (https://github.com/uoguelph-mlrg/Cutout/blob/master/util/cutout.py)\n",
    "                            transforms.Normalize(...)\n",
    "                        ])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Augment\n",
    "https://blog.insightdatascience.com/automl-for-data-augmentation-e87cf692c366\n",
    "https://colab.research.google.com/drive/1KCAv2i_F3E3m_PKh56nbbZY8WnaASvgl#scrollTo=SuhR6Q3AMFy3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deepaugment\n",
      "  Downloading https://files.pythonhosted.org/packages/99/f9/40211d827039df475091639c6aded9a1786849f898b9c619e24c15efc82a/deepaugment-1.1.2-py2.py3-none-any.whl\n",
      "Collecting keras-applications==1.0.6 (from deepaugment)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/c4/2ff40221029f7098d58f8d7fb99b97e8100f3293f9856f0fb5834bef100b/Keras_Applications-1.0.6-py2.py3-none-any.whl (44kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 4.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pandas==0.23.4 (from deepaugment)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/d8/feeb346d41f181e83fba45224ab14a8d8af019b48af742e047f3845d8cff/pandas-0.23.4-cp36-cp36m-manylinux1_x86_64.whl (8.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 8.9MB 5.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scikit-optimize==0.5.2 (from deepaugment)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/44/60f82c97d1caa98752c7da2c1681cab5c7a390a0fdd3a55fac672b321cac/scikit_optimize-0.5.2-py2.py3-none-any.whl (74kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 34.7MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting click==7.0 (from deepaugment)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fa/37/45185cb5abbc30d7257104c434fe0b07e5a195a6847506c074527aa599ec/Click-7.0-py2.py3-none-any.whl (81kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 33.4MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting numpy==1.15.4 (from deepaugment)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/7f/9d804d2348471c67a7d8b5f84f9bc59fd1cefa148986f2b74552f8573555/numpy-1.15.4-cp36-cp36m-manylinux1_x86_64.whl (13.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 13.9MB 3.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opencv-contrib-python (from deepaugment)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c6/db/790dbc6bcfea87fc6f790c6306509c2691ce31c96d82e5b826545d90ea52/opencv_contrib_python-4.2.0.34-cp36-cp36m-manylinux1_x86_64.whl (34.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 34.2MB 1.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting imgaug==0.2.7 (from deepaugment)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/fc/c56a7da8c23122b7c5325b941850013880a7a93c21dc95e2b1ecd4750108/imgaug-0.2.7-py3-none-any.whl (644kB)\n",
      "\u001b[K    100% |████████████████████████████████| 645kB 34.7MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow==1.12.0 (from deepaugment)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/cc/ca70b78087015d21c5f3f93694107f34ebccb3be9624385a911d4b52ecef/tensorflow-1.12.0-cp36-cp36m-manylinux1_x86_64.whl (83.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 83.1MB 574kB/s eta 0:00:01    62% |███████████████████▉            | 51.6MB 52.9MB/s eta 0:00:01    76% |████████████████████████▋       | 63.8MB 62.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting matplotlib==3.0.2 (from deepaugment)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/07/16d781df15be30df4acfd536c479268f1208b2dfbc91e9ca5d92c9caf673/matplotlib-3.0.2-cp36-cp36m-manylinux1_x86_64.whl (12.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 12.9MB 4.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting setuptools==40.6.3 (from deepaugment)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/37/06/754589caf971b0d2d48f151c2586f62902d93dc908e2fd9b9b9f6aa3c9dd/setuptools-40.6.3-py2.py3-none-any.whl (573kB)\n",
      "\u001b[K    100% |████████████████████████████████| 573kB 31.6MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting keras==2.2.4 (from deepaugment)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl (312kB)\n",
      "\u001b[K    100% |████████████████████████████████| 317kB 37.5MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: h5py in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from keras-applications==1.0.6->deepaugment) (2.8.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from pandas==0.23.4->deepaugment) (2.7.3)\n",
      "Requirement already satisfied: pytz>=2011k in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from pandas==0.23.4->deepaugment) (2018.4)\n",
      "Requirement already satisfied: scipy>=0.14.0 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from scikit-optimize==0.5.2->deepaugment) (1.1.0)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from scikit-optimize==0.5.2->deepaugment) (0.20.3)\n",
      "Requirement already satisfied: scikit-image>=0.11.0 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from imgaug==0.2.7->deepaugment) (0.13.1)\n",
      "Collecting Shapely (from imgaug==0.2.7->deepaugment)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/fa/c96d3461fda99ed8e82ff0b219ac2c8384694b4e640a611a1a8390ecd415/Shapely-1.7.0-cp36-cp36m-manylinux1_x86_64.whl (1.8MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.8MB 18.5MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: Pillow in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from imgaug==0.2.7->deepaugment) (5.1.0)\n",
      "Requirement already satisfied: imageio in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from imgaug==0.2.7->deepaugment) (2.3.0)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from imgaug==0.2.7->deepaugment) (1.11.0)\n",
      "Collecting absl-py>=0.1.6 (from tensorflow==1.12.0->deepaugment)\n",
      "Collecting keras-preprocessing>=1.0.5 (from tensorflow==1.12.0->deepaugment)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/4c/7c3275a01e12ef9368a892926ab932b33bb13d55794881e3573482b378a7/Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 16.3MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting protobuf>=3.6.1 (from tensorflow==1.12.0->deepaugment)\n",
      "  Using cached https://files.pythonhosted.org/packages/28/05/9867ef8eafd12265267bee138fa2c46ebf34a276ea4cbe184cba4c606e8b/protobuf-3.12.2-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting grpcio>=1.8.6 (from tensorflow==1.12.0->deepaugment)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow==1.12.0->deepaugment)\n",
      "Collecting astor>=0.6.0 (from tensorflow==1.12.0->deepaugment)\n",
      "  Downloading https://files.pythonhosted.org/packages/c3/88/97eef84f48fa04fbd6750e62dcceafba6c63c81b7ac1420856c8dcc0a3f9/astor-0.8.1-py2.py3-none-any.whl\n",
      "Collecting tensorboard<1.13.0,>=1.12.0 (from tensorflow==1.12.0->deepaugment)\n",
      "\u001b[33m  Cache entry deserialization failed, entry ignored\u001b[0m\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/53/8d32ce9471c18f8d99028b7cef2e5b39ea8765bd7ef250ca05b490880971/tensorboard-1.12.2-py3-none-any.whl (3.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.1MB 20.0MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from tensorflow==1.12.0->deepaugment) (0.31.1)\n",
      "Collecting gast>=0.2.0 (from tensorflow==1.12.0->deepaugment)\n",
      "  Downloading https://files.pythonhosted.org/packages/d6/84/759f5dd23fec8ba71952d97bcc7e2c9d7d63bdc582421f3cd4be845f0c98/gast-0.3.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from matplotlib==3.0.2->deepaugment) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from matplotlib==3.0.2->deepaugment) (2.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from matplotlib==3.0.2->deepaugment) (1.0.1)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from keras==2.2.4->deepaugment) (5.3.1)\n",
      "Requirement already satisfied: networkx>=1.8 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from scikit-image>=0.11.0->imgaug==0.2.7->deepaugment) (2.1)\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from scikit-image>=0.11.0->imgaug==0.2.7->deepaugment) (0.5.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.10 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.12.0->deepaugment) (0.14.1)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.12.0->deepaugment)\n",
      "  Using cached https://files.pythonhosted.org/packages/a4/63/eaec2bd025ab48c754b55e8819af0f6a69e2b1e187611dd40cbbe101ee7f/Markdown-3.2.2-py3-none-any.whl\n",
      "Requirement already satisfied: decorator>=4.1.0 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from networkx>=1.8->scikit-image>=0.11.0->imgaug==0.2.7->deepaugment) (4.3.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard<1.13.0,>=1.12.0->tensorflow==1.12.0->deepaugment) (1.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.13.0,>=1.12.0->tensorflow==1.12.0->deepaugment) (3.0.0)\n",
      "\u001b[31mamazonei-mxnet 1.5.1 has requirement numpy<1.15.0,>=1.8.2, but you'll have numpy 1.15.4 which is incompatible.\u001b[0m\n",
      "Installing collected packages: numpy, keras-applications, pandas, scikit-optimize, click, opencv-contrib-python, Shapely, matplotlib, imgaug, absl-py, keras-preprocessing, setuptools, protobuf, grpcio, termcolor, astor, markdown, tensorboard, gast, tensorflow, keras, deepaugment\n",
      "  Found existing installation: numpy 1.14.6\n",
      "    Uninstalling numpy-1.14.6:\n",
      "      Successfully uninstalled numpy-1.14.6\n",
      "  Found existing installation: pandas 0.24.2\n",
      "    Uninstalling pandas-0.24.2:\n",
      "      Successfully uninstalled pandas-0.24.2\n",
      "  Found existing installation: click 6.7\n",
      "    Uninstalling click-6.7:\n",
      "      Successfully uninstalled click-6.7\n",
      "  Found existing installation: matplotlib 3.0.3\n",
      "    Uninstalling matplotlib-3.0.3:\n",
      "      Successfully uninstalled matplotlib-3.0.3\n",
      "  Found existing installation: setuptools 39.1.0\n",
      "    Uninstalling setuptools-39.1.0:\n",
      "      Successfully uninstalled setuptools-39.1.0\n",
      "  Found existing installation: protobuf 3.5.2\n",
      "    Uninstalling protobuf-3.5.2:\n",
      "      Successfully uninstalled protobuf-3.5.2\n",
      "Successfully installed Shapely-1.7.0 absl-py-0.9.0 astor-0.8.1 click-7.0 deepaugment-1.1.2 gast-0.3.3 grpcio-1.29.0 imgaug-0.2.7 keras-2.2.4 keras-applications-1.0.6 keras-preprocessing-1.1.2 markdown-3.2.2 matplotlib-3.0.2 numpy-1.15.4 opencv-contrib-python-4.2.0.34 pandas-0.23.4 protobuf-3.12.2 scikit-optimize-0.5.2 setuptools-40.6.3 tensorboard-1.12.2 tensorflow-1.12.0 termcolor-1.1.0\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.2b1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install deepaugment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__new__() got an unexpected keyword argument 'serialized_options'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-952c53067624>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdeepaugment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepaugment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDeepAugment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m deepaug = DeepAugment('/home/ec2-user/SageMaker/w210-capstone/data/cifar10/subset300', \n\u001b[1;32m      4\u001b[0m                       '/home/ec2-user/SageMaker/w210-capstone/data/cifar10/trainLabels.csv')\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/deepaugment/deepaugment.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# (C) 2019 Baris Ozmen <hbaristr@gmail.com>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# pylint: disable=g-bad-import-order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m  \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;31m# Protocol buffers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_pb2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_def_pb2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary_pb2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/tensorflow/core/framework/graph_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnode_def_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_node__def__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunction_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_function__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mversions_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_versions__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/tensorflow/core/framework/node_def_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mattr_value_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_attr__value__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/tensorflow/core/framework/attr_value_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_tensor__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_shape_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypes_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_types__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/tensorflow/core/framework/tensor_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mresource_handle_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_resource__handle__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_shape_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypes_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_types__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/tensorflow/core/framework/resource_handle_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0msyntax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'proto3'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0mserialized_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\030org.tensorflow.frameworkB\\016ResourceHandleP\\001Z=github.com/tensorflow/tensorflow/tensorflow/go/core/framework\\370\\001\\001'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m   \u001b[0mserialized_pb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n/tensorflow/core/framework/resource_handle.proto\\x12\\ntensorflow\\\"r\\n\\x13ResourceHandleProto\\x12\\x0e\\n\\x06\\x64\\x65vice\\x18\\x01 \\x01(\\t\\x12\\x11\\n\\tcontainer\\x18\\x02 \\x01(\\t\\x12\\x0c\\n\\x04name\\x18\\x03 \\x01(\\t\\x12\\x11\\n\\thash_code\\x18\\x04 \\x01(\\x04\\x12\\x17\\n\\x0fmaybe_type_name\\x18\\x05 \\x01(\\tBn\\n\\x18org.tensorflow.frameworkB\\x0eResourceHandleP\\x01Z=github.com/tensorflow/tensorflow/tensorflow/go/core/framework\\xf8\\x01\\x01\\x62\\x06proto3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __new__() got an unexpected keyword argument 'serialized_options'"
     ]
    }
   ],
   "source": [
    "from deepaugment.deepaugment import DeepAugment\n",
    "\n",
    "deepaug = DeepAugment('/home/ec2-user/SageMaker/w210-capstone/data/cifar10/subset300', \n",
    "                      '/home/ec2-user/SageMaker/w210-capstone/data/cifar10/trainLabels.csv')\n",
    "\n",
    "best_policies = deepaug.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker\n"
     ]
    }
   ],
   "source": [
    "!ls /home/ec2-user/SageMaker/w210-capstone/data/cifar10/subset300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must be conda_pytroch_p36 notebook\n",
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import sagemaker\n",
    "import torch\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.pytorch import PyTorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r ./w210-capstone/models/pytorch_imageclass/requirements.txt (line 1)) (1.15.4)\n",
      "Requirement already satisfied: torch>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r ./w210-capstone/models/pytorch_imageclass/requirements.txt (line 2)) (1.4.0)\n",
      "Requirement already satisfied: torchvision in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r ./w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (0.5.0)\n",
      "Requirement already satisfied: fvcore in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r ./w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (0.1.1.post20200603)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r ./w210-capstone/models/pytorch_imageclass/requirements.txt (line 5)) (4.42.1)\n",
      "Requirement already satisfied: yacs in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r ./w210-capstone/models/pytorch_imageclass/requirements.txt (line 6)) (0.1.7)\n",
      "Requirement already satisfied: apex from git+https://github.com/NVIDIA/apex.git#egg=apex in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r ./w210-capstone/models/pytorch_imageclass/requirements.txt (line 7)) (0.1)\n",
      "Requirement already satisfied: termcolor in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r ./w210-capstone/models/pytorch_imageclass/requirements.txt (line 8)) (1.1.0)\n",
      "Requirement already satisfied: thop<0.0.31.post2004070130 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r ./w210-capstone/models/pytorch_imageclass/requirements.txt (line 9)) (0.0.31.post2001170342)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision->-r ./w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (1.11.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision->-r ./w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (5.4.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r ./w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (5.3.1)\n",
      "Requirement already satisfied: tabulate in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r ./w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (0.8.7)\n",
      "Requirement already satisfied: portalocker in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r ./w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (1.7.0)\n",
      "\u001b[31mfastai 1.0.60 requires nvidia-ml-py3, which is not installed.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.2b1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r ./w210-capstone/models/pytorch_imageclass/requirements.txt\n",
    "\n",
    "# Do we need NVIDIA installed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-04 00:41:31] __main__ INFO: \u001b[0mdevice: cpu\n",
      "cudnn:\n",
      "  benchmark: True\n",
      "  deterministic: False\n",
      "dataset:\n",
      "  name: CIFAR10\n",
      "  dataset_dir: ~/.torch/datasets/CIFAR10\n",
      "  image_size: 32\n",
      "  n_channels: 3\n",
      "  n_classes: 10\n",
      "model:\n",
      "  type: cifar\n",
      "  name: resnext\n",
      "  init_mode: kaiming_fan_out\n",
      "  vgg:\n",
      "    n_channels: [64, 128, 256, 512, 512]\n",
      "    n_layers: [2, 2, 3, 3, 3]\n",
      "    use_bn: True\n",
      "  resnet:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "  resnet_preact:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "  wrn:\n",
      "    depth: 28\n",
      "    initial_channels: 16\n",
      "    widening_factor: 10\n",
      "    drop_rate: 0.0\n",
      "  densenet:\n",
      "    depth: 100\n",
      "    n_blocks: [6, 12, 24, 16]\n",
      "    block_type: bottleneck\n",
      "    growth_rate: 12\n",
      "    drop_rate: 0.0\n",
      "    compression_rate: 0.5\n",
      "  pyramidnet:\n",
      "    depth: 272\n",
      "    n_blocks: [3, 24, 36, 3]\n",
      "    initial_channels: 16\n",
      "    block_type: bottleneck\n",
      "    alpha: 200\n",
      "  resnext:\n",
      "    depth: 29\n",
      "    n_blocks: [3, 4, 6, 3]\n",
      "    initial_channels: 64\n",
      "    cardinality: 4\n",
      "    base_channels: 64\n",
      "  shake_shake:\n",
      "    depth: 26\n",
      "    initial_channels: 96\n",
      "    shake_forward: True\n",
      "    shake_backward: True\n",
      "    shake_image: True\n",
      "  se_resnet_preact:\n",
      "    depth: 110\n",
      "    initial_channels: 16\n",
      "    se_reduction: 16\n",
      "    block_type: basic\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "train:\n",
      "  checkpoint: ''\n",
      "  resume: False\n",
      "  precision: O0\n",
      "  batch_size: 128\n",
      "  subdivision: 1\n",
      "  optimizer: sgd\n",
      "  base_lr: 0.1\n",
      "  momentum: 0.9\n",
      "  nesterov: True\n",
      "  weight_decay: 0.0005\n",
      "  no_weight_decay_on_bn: False\n",
      "  gradient_clip: 0\n",
      "  start_epoch: 0\n",
      "  seed: 0\n",
      "  val_first: True\n",
      "  val_period: 1\n",
      "  val_ratio: 0.1\n",
      "  use_test_as_val: False\n",
      "  output_dir: experiments/resnext_29_4x64d/exp00\n",
      "  log_period: 100\n",
      "  checkpoint_period: 100\n",
      "  use_tensorboard: True\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: True\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "  distributed: False\n",
      "  dist:\n",
      "    backend: nccl\n",
      "    init_method: env://\n",
      "    world_size: -1\n",
      "    node_rank: -1\n",
      "    local_rank: 0\n",
      "    use_sync_bn: False\n",
      "tensorboard:\n",
      "  train_images: False\n",
      "  val_images: False\n",
      "  model_params: False\n",
      "optim:\n",
      "  adam:\n",
      "    betas: (0.9, 0.999)\n",
      "  lars:\n",
      "    eps: 1e-09\n",
      "    threshold: 0.01\n",
      "  adabound:\n",
      "    betas: (0.9, 0.999)\n",
      "    final_lr: 0.1\n",
      "    gamma: 0.001\n",
      "scheduler:\n",
      "  epochs: 300\n",
      "  warmup:\n",
      "    type: none\n",
      "    epochs: 0\n",
      "    start_factor: 0.001\n",
      "    exponent: 4\n",
      "  type: multistep\n",
      "  milestones: [150, 225]\n",
      "  lr_decay: 0.1\n",
      "  lr_min_factor: 0.001\n",
      "  T0: 10\n",
      "  T_mul: 1.0\n",
      "validation:\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: False\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "augmentation:\n",
      "  use_random_crop: True\n",
      "  use_random_horizontal_flip: True\n",
      "  use_cutout: False\n",
      "  use_random_erasing: False\n",
      "  use_dual_cutout: False\n",
      "  use_mixup: False\n",
      "  use_ricap: False\n",
      "  use_cutmix: False\n",
      "  use_label_smoothing: False\n",
      "  random_crop:\n",
      "    padding: 4\n",
      "    fill: 0\n",
      "    padding_mode: constant\n",
      "  random_horizontal_flip:\n",
      "    prob: 0.5\n",
      "  cutout:\n",
      "    prob: 1.0\n",
      "    mask_size: 16\n",
      "    cut_inside: False\n",
      "    mask_color: 0\n",
      "    dual_cutout_alpha: 0.1\n",
      "  random_erasing:\n",
      "    prob: 0.5\n",
      "    area_ratio_range: [0.02, 0.4]\n",
      "    min_aspect_ratio: 0.3\n",
      "    max_attempt: 20\n",
      "  mixup:\n",
      "    alpha: 1.0\n",
      "  ricap:\n",
      "    beta: 0.3\n",
      "  cutmix:\n",
      "    alpha: 1.0\n",
      "  label_smoothing:\n",
      "    epsilon: 0.1\n",
      "tta:\n",
      "  use_resize: False\n",
      "  use_center_crop: False\n",
      "  resize: 256\n",
      "test:\n",
      "  checkpoint: ''\n",
      "  output_dir: ''\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    pin_memory: False\n",
      "\u001b[32m[2020-06-04 00:41:31] __main__ INFO: \u001b[0menv_info:\n",
      "  pytorch_version: 1.4.0\n",
      "  cuda_version: 10.1\n",
      "  cudnn_version: 7603\n",
      "Files already downloaded and verified\n",
      "\u001b[32m[2020-06-04 00:41:33] __main__ INFO: \u001b[0mMACs  : 2.75G\n",
      "\u001b[32m[2020-06-04 00:41:33] __main__ INFO: \u001b[0m#params: 17.56M\n",
      "Selected optimization level O0:  Pure FP32 training.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Traceback (most recent call last):\n",
      "  File \"w210-capstone/models/pytorch_imageclass/train.py\", line 436, in <module>\n",
      "    main()\n",
      "  File \"w210-capstone/models/pytorch_imageclass/train.py\", line 362, in main\n",
      "    opt_level=config.train.precision)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/apex/amp/frontend.py\", line 358, in initialize\n",
      "    return _initialize(models, optimizers, _amp_state.opt_properties, num_losses, cast_model_outputs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/apex/amp/_initialize.py\", line 171, in _initialize\n",
      "    check_params_fp32(models)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/apex/amp/_initialize.py\", line 93, in check_params_fp32\n",
      "    name, param.type()))\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/apex/amp/_amp_state.py\", line 32, in warn_or_err\n",
      "    raise RuntimeError(msg)\n",
      "RuntimeError: Found param conv.weight with type torch.FloatTensor, expected torch.cuda.FloatTensor.\n",
      "When using amp.initialize, you need to provide a model with parameters\n",
      "located on a CUDA device before passing it no matter what optimization level\n",
      "you chose. Use model.to('cuda') to use the default device.\n"
     ]
    }
   ],
   "source": [
    "# Must be conda_pytroch_p36 notebook\n",
    "!python w210-capstone/models/pytorch_imageclass/train.py --config w210-capstone/models/pytorch_imageclass/configs/cifar/resnext.yaml \\\n",
    "   model.resnext.depth 29 \\\n",
    "   model.resnext.cardinality 4 \\\n",
    "   train.batch_size 128 \\\n",
    "   train.base_lr 0.1 \\\n",
    "   train.output_dir experiments/resnext_29_4x64d/exp00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCRATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Data and model checkpoints directories\n",
    "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                        help='input batch size for training (default: 64)')\n",
    "    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                        help='input batch size for testing (default: 1000)')\n",
    "    parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
    "                        help='number of epochs to train (default: 10)')\n",
    "    parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n",
    "                        help='learning rate (default: 0.01)')\n",
    "    parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n",
    "                        help='SGD momentum (default: 0.5)')\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    parser.add_argument('--log-interval', type=int, default=100, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "    parser.add_argument('--backend', type=str, default=None,\n",
    "                        help='backend for distributed training (tcp, gloo on cpu and gloo, nccl on gpu)')\n",
    "\n",
    "    \n",
    "    parser.add_argument('--epochs', type=int, default=10)\n",
    "    parser.add_argument('--batch-size', type=int, default=128)\n",
    "    parser.add_argument('--learning-rate', type=float, default=0.1)\n",
    "    \n",
    "    parser.add_argument('--model_name', dest='model_name', type=str, help='model to train')\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'],\n",
    "                        help='directory to save trained model to')\n",
    "    parser.add_argument('--train_data', dest='train_data', type=str, help='dataset for model training') \n",
    "    parser.add_argument('--workers', dest='workers', type=int, help='number of V100 worker instances; 1 indicates non-distributed training')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Container environment\n",
    "    env = sagemaker_containers.training_env()\n",
    "    parser.add_argument('--hosts', type=list, default=env.hosts)\n",
    "    parser.add_argument('--current-host', type=str, default=env.current_host)\n",
    "    parser.add_argument('--model-dir', type=str, default=env.model_dir)\n",
    "    parser.add_argument('--data-dir', type=str,\n",
    "                        default=env.channel_input_dirs['training'])\n",
    "    parser.add_argument('--num-gpus', type=int, default=env.num_gpus)\n",
    "\n",
    "    train(parser.parse_args())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-3319f1f978a5>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-3319f1f978a5>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    echo $CUDA_PATH\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#torch.cuda.get_device_name(0)\n",
    "echo $CUDA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mCould not open requirements file: [Errno 2] No such file or directory: './w210-capstone/models/pytorch/requirements.txt'\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.2b1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r ./w210-capstone/models/pytorch_/requirements.txt\n",
    "\n",
    "# Do we need NVIDIA installed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance type = local\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "instance_type = 'local'\n",
    "\n",
    "if subprocess.call('nvidia-smi') == 0:\n",
    "    ## Set type to GPU if one is present\n",
    "    instance_type = 'local_gpu'\n",
    "    \n",
    "print(\"Instance type = \" + instance_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-03 20:54:56] __main__ INFO: \u001b[0mdevice: cpu\n",
      "cudnn:\n",
      "  benchmark: True\n",
      "  deterministic: False\n",
      "dataset:\n",
      "  name: CIFAR10\n",
      "  dataset_dir: ~/.torch/datasets/CIFAR10\n",
      "  image_size: 32\n",
      "  n_channels: 3\n",
      "  n_classes: 10\n",
      "model:\n",
      "  type: cifar\n",
      "  name: resnext\n",
      "  init_mode: kaiming_fan_out\n",
      "  vgg:\n",
      "    n_channels: [64, 128, 256, 512, 512]\n",
      "    n_layers: [2, 2, 3, 3, 3]\n",
      "    use_bn: True\n",
      "  resnet:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "  resnet_preact:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "  wrn:\n",
      "    depth: 28\n",
      "    initial_channels: 16\n",
      "    widening_factor: 10\n",
      "    drop_rate: 0.0\n",
      "  densenet:\n",
      "    depth: 100\n",
      "    n_blocks: [6, 12, 24, 16]\n",
      "    block_type: bottleneck\n",
      "    growth_rate: 12\n",
      "    drop_rate: 0.0\n",
      "    compression_rate: 0.5\n",
      "  pyramidnet:\n",
      "    depth: 272\n",
      "    n_blocks: [3, 24, 36, 3]\n",
      "    initial_channels: 16\n",
      "    block_type: bottleneck\n",
      "    alpha: 200\n",
      "  resnext:\n",
      "    depth: 29\n",
      "    n_blocks: [3, 4, 6, 3]\n",
      "    initial_channels: 64\n",
      "    cardinality: 4\n",
      "    base_channels: 64\n",
      "  shake_shake:\n",
      "    depth: 26\n",
      "    initial_channels: 96\n",
      "    shake_forward: True\n",
      "    shake_backward: True\n",
      "    shake_image: True\n",
      "  se_resnet_preact:\n",
      "    depth: 110\n",
      "    initial_channels: 16\n",
      "    se_reduction: 16\n",
      "    block_type: basic\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "train:\n",
      "  checkpoint: ''\n",
      "  resume: False\n",
      "  precision: O0\n",
      "  batch_size: 128\n",
      "  subdivision: 1\n",
      "  optimizer: sgd\n",
      "  base_lr: 0.1\n",
      "  momentum: 0.9\n",
      "  nesterov: True\n",
      "  weight_decay: 0.0005\n",
      "  no_weight_decay_on_bn: False\n",
      "  gradient_clip: 0\n",
      "  start_epoch: 0\n",
      "  seed: 0\n",
      "  val_first: True\n",
      "  val_period: 1\n",
      "  val_ratio: 0.1\n",
      "  use_test_as_val: False\n",
      "  output_dir: experiments/resnext_29_4x64d/exp00\n",
      "  log_period: 100\n",
      "  checkpoint_period: 100\n",
      "  use_tensorboard: True\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: True\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "  distributed: False\n",
      "  dist:\n",
      "    backend: nccl\n",
      "    init_method: env://\n",
      "    world_size: -1\n",
      "    node_rank: -1\n",
      "    local_rank: 0\n",
      "    use_sync_bn: False\n",
      "tensorboard:\n",
      "  train_images: False\n",
      "  val_images: False\n",
      "  model_params: False\n",
      "optim:\n",
      "  adam:\n",
      "    betas: (0.9, 0.999)\n",
      "  lars:\n",
      "    eps: 1e-09\n",
      "    threshold: 0.01\n",
      "  adabound:\n",
      "    betas: (0.9, 0.999)\n",
      "    final_lr: 0.1\n",
      "    gamma: 0.001\n",
      "scheduler:\n",
      "  epochs: 300\n",
      "  warmup:\n",
      "    type: none\n",
      "    epochs: 0\n",
      "    start_factor: 0.001\n",
      "    exponent: 4\n",
      "  type: multistep\n",
      "  milestones: [150, 225]\n",
      "  lr_decay: 0.1\n",
      "  lr_min_factor: 0.001\n",
      "  T0: 10\n",
      "  T_mul: 1.0\n",
      "validation:\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: False\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "augmentation:\n",
      "  use_random_crop: True\n",
      "  use_random_horizontal_flip: True\n",
      "  use_cutout: False\n",
      "  use_random_erasing: False\n",
      "  use_dual_cutout: False\n",
      "  use_mixup: False\n",
      "  use_ricap: False\n",
      "  use_cutmix: False\n",
      "  use_label_smoothing: False\n",
      "  random_crop:\n",
      "    padding: 4\n",
      "    fill: 0\n",
      "    padding_mode: constant\n",
      "  random_horizontal_flip:\n",
      "    prob: 0.5\n",
      "  cutout:\n",
      "    prob: 1.0\n",
      "    mask_size: 16\n",
      "    cut_inside: False\n",
      "    mask_color: 0\n",
      "    dual_cutout_alpha: 0.1\n",
      "  random_erasing:\n",
      "    prob: 0.5\n",
      "    area_ratio_range: [0.02, 0.4]\n",
      "    min_aspect_ratio: 0.3\n",
      "    max_attempt: 20\n",
      "  mixup:\n",
      "    alpha: 1.0\n",
      "  ricap:\n",
      "    beta: 0.3\n",
      "  cutmix:\n",
      "    alpha: 1.0\n",
      "  label_smoothing:\n",
      "    epsilon: 0.1\n",
      "tta:\n",
      "  use_resize: False\n",
      "  use_center_crop: False\n",
      "  resize: 256\n",
      "test:\n",
      "  checkpoint: ''\n",
      "  output_dir: ''\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    pin_memory: False\n",
      "\u001b[32m[2020-06-03 20:54:56] __main__ INFO: \u001b[0menv_info:\n",
      "  pytorch_version: 1.4.0\n",
      "  cuda_version: 10.1\n",
      "  cudnn_version: 7603\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /home/ec2-user/.torch/datasets/CIFAR10/cifar-10-python.tar.gz\n",
      "170500096it [00:02, 67641451.46it/s]                                            \n",
      "Extracting /home/ec2-user/.torch/datasets/CIFAR10/cifar-10-python.tar.gz to /home/ec2-user/.torch/datasets/CIFAR10\n",
      "\u001b[32m[2020-06-03 20:55:02] __main__ INFO: \u001b[0mMACs  : 2.75G\n",
      "\u001b[32m[2020-06-03 20:55:02] __main__ INFO: \u001b[0m#params: 17.56M\n",
      "Selected optimization level O0:  Pure FP32 training.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Traceback (most recent call last):\n",
      "  File \"w210-capstone/models/pytorch/train.py\", line 436, in <module>\n",
      "    main()\n",
      "  File \"w210-capstone/models/pytorch/train.py\", line 362, in main\n",
      "    opt_level=config.train.precision)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/apex/amp/frontend.py\", line 358, in initialize\n",
      "    return _initialize(models, optimizers, _amp_state.opt_properties, num_losses, cast_model_outputs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/apex/amp/_initialize.py\", line 171, in _initialize\n",
      "    check_params_fp32(models)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/apex/amp/_initialize.py\", line 93, in check_params_fp32\n",
      "    name, param.type()))\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/apex/amp/_amp_state.py\", line 32, in warn_or_err\n",
      "    raise RuntimeError(msg)\n",
      "RuntimeError: Found param conv.weight with type torch.FloatTensor, expected torch.cuda.FloatTensor.\n",
      "When using amp.initialize, you need to provide a model with parameters\n",
      "located on a CUDA device before passing it no matter what optimization level\n",
      "you chose. Use model.to('cuda') to use the default device.\n"
     ]
    }
   ],
   "source": [
    "# ResNext-29_4x64\n",
    "!python w210-capstone/models/pytorch/train.py --config w210-capstone/models/pytorch/configs/cifar/resnext.yaml \\\n",
    "   model.resnext.depth 29 \\\n",
    "   model.resnext.cardinality 4 \\\n",
    "   train.batch_size 128 \\\n",
    "   train.base_lr 0.1 \\\n",
    "   train.output_dir experiments/resnext_29_4x64d/exp00\n",
    "\n",
    "# ResNeXt-29 4x64d with a single GPU, batch size 32 and initial learning rate 0.025 \n",
    "# (8 GPUs, batch size 128 and initial learning rate 0.1 in paper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# initialize command line argument parsing\n",
    "if __name__=='__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--epochs', type=int, default=10)\n",
    "    parser.add_argument('--batch-size', type=int, default=128)\n",
    "    parser.add_argument('--learning-rate', type=float, default=0.1)\n",
    "    \n",
    "    parser.add_argument('--model_name', dest='model_name', type=str, help='model to train')\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'],\n",
    "                        help='directory to save trained model to')\n",
    "    parser.add_argument('--train_data', dest='train_data', type=str, help='dataset for model training') \n",
    "    parser.add_argument('--workers', dest='workers', type=int, help='number of V100 worker instances; 1 indicates non-distributed training')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Check quality of arguments\n",
    "    valid_args = {'datasets': ['cifar10', 'cifar10_10k', 'cifar10_30k', 'cifar102', 'cifar102_30k'],\n",
    "                  'model_names': ['wrn', 'shake_shake_32', 'shake_shake_96', 'shake_shake_112', 'pyramid_net']}\n",
    "\n",
    "    if args.train_data not in valid_args['datasets']:\n",
    "        parser.error('Invalid train_data parameter')\n",
    "\n",
    "    if args.model_name not in valid_args['model_names']:\n",
    "        parser.error('Invalid model_name parameter')\n",
    "    \n",
    "    if args.workers < 1:\n",
    "        parser.error('Invalid number of workers')\n",
    "\n",
    "    if not args.model_name:\n",
    "        parser.error('--model_name parameter is required')\n",
    "    elif not args.train_data:\n",
    "        parser.error('--train_data parameter is required')\n",
    "    elif not args.workers:\n",
    "        parser.error('--workers parameter is required')\n",
    "\n",
    "    # Set SageMaker session & execution role\n",
    "    bucket='sagemaker-may29'\n",
    "    sagemaker_session = sagemaker.Session(default_bucket=bucket)\n",
    "    role = get_execution_role()\n",
    "\n",
    "\n",
    "    # Set S3 path for data batches\n",
    "    inputs = 's3://' + bucket + '/sagemaker/{}'.format(args.train_data)\n",
    "\n",
    "    # Create the sagemaker estimator\n",
    "    pytorch_estimator = PyTorch('./w210-capstone/models/pytorch/train.py',\n",
    "                                train_instance_type='ml.p3.2xlarge',\n",
    "                                train_instance_count=1,\n",
    "                                framework_version='1.0.0',\n",
    "                                hyperparameters = {'epochs': 20, 'batch-size': 64, 'learning-rate': 0.1})\n",
    "\n",
    "    --config w210-capstone/models/pytorch/configs/cifar/resnext.yaml \\\n",
    "   model.resnext.depth 29 \\\n",
    "   model.resnext.cardinality 4 \\\n",
    "   train.batch_size 128 \\\n",
    "   train.base_lr 0.1 \\\n",
    "   train.output_dir experiments/resnext_29_4x64d/exp00\n",
    "    \n",
    "    \n",
    "    # Train the Model\n",
    "    pytorch_estimator.fit({'train': 's3://my-data-bucket/path/to/my/training/data',\n",
    "                           'test': 's3://my-data-bucket/path/to/my/test/data'})\n",
    "\n",
    "\n",
    "    # After training, save the model to `model_dir`\n",
    "    with open(os.path.join(args.model_dir, 'model.pth'), 'wb') as f:\n",
    "        torch.save(model.state_dict(), f)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
