{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNext 29 4x64\n",
    "\n",
    " - Training Dataset:  RandAugment, N=2, M=5\n",
    "   Validation with Unaugmented Data\n",
    " - Sagemaker Notebook must be of type, conda_pytorch_p36\n",
    " \n",
    "#### Install Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import numpy \n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 1)) (1.18.1)\n",
      "Requirement already satisfied: torch>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 2)) (1.4.0)\n",
      "Requirement already satisfied: torchvision in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (0.5.0)\n",
      "Requirement already satisfied: fvcore in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (0.1.1.post20200711)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 5)) (4.44.1)\n",
      "Requirement already satisfied: yacs in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 6)) (0.1.7)\n",
      "Requirement already satisfied: apex from git+https://github.com/NVIDIA/apex.git#egg=apex in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 7)) (0.1)\n",
      "Requirement already satisfied: termcolor in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 8)) (1.1.0)\n",
      "Requirement already satisfied: thop<0.0.31.post2004070130 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 9)) (0.0.31.post2001170342)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (1.14.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (7.0.0)\n",
      "Requirement already satisfied: portalocker in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (1.7.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (5.3.1)\n",
      "Requirement already satisfied: tabulate in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (0.8.7)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: tensorboard in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (2.2.2)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (0.34.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (0.4.1)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (3.12.2)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.18.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (46.1.3.post20200330)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.14.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (0.9.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.7.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.30.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.18.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (2.23.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (3.2.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (3.4.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (4.1.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (1.25.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (2020.4.5.2)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard) (1.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.1.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard) (0.4.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard) (2.2.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r '/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt'\n",
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-17 12:25:43] __main__ INFO: \u001b[0mdevice: cuda\n",
      "cudnn:\n",
      "  benchmark: True\n",
      "  deterministic: False\n",
      "dataset:\n",
      "  name: CIFAR10_RA_2_5\n",
      "  dataset_dir: ''\n",
      "  image_size: 32\n",
      "  n_channels: 3\n",
      "  n_classes: 10\n",
      "model:\n",
      "  type: cifar\n",
      "  name: resnext\n",
      "  init_mode: kaiming_fan_out\n",
      "  vgg:\n",
      "    n_channels: [64, 128, 256, 512, 512]\n",
      "    n_layers: [2, 2, 3, 3, 3]\n",
      "    use_bn: True\n",
      "  resnet:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "  resnet_preact:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "  wrn:\n",
      "    depth: 28\n",
      "    initial_channels: 16\n",
      "    widening_factor: 10\n",
      "    drop_rate: 0.0\n",
      "  densenet:\n",
      "    depth: 100\n",
      "    n_blocks: [6, 12, 24, 16]\n",
      "    block_type: bottleneck\n",
      "    growth_rate: 12\n",
      "    drop_rate: 0.0\n",
      "    compression_rate: 0.5\n",
      "  pyramidnet:\n",
      "    depth: 272\n",
      "    n_blocks: [3, 24, 36, 3]\n",
      "    initial_channels: 16\n",
      "    block_type: bottleneck\n",
      "    alpha: 200\n",
      "  resnext:\n",
      "    depth: 29\n",
      "    n_blocks: [3, 4, 6, 3]\n",
      "    initial_channels: 64\n",
      "    cardinality: 4\n",
      "    base_channels: 64\n",
      "  shake_shake:\n",
      "    depth: 26\n",
      "    initial_channels: 96\n",
      "    shake_forward: True\n",
      "    shake_backward: True\n",
      "    shake_image: True\n",
      "  se_resnet_preact:\n",
      "    depth: 110\n",
      "    initial_channels: 16\n",
      "    se_reduction: 16\n",
      "    block_type: basic\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "train:\n",
      "  checkpoint: ''\n",
      "  resume: False\n",
      "  precision: O0\n",
      "  batch_size: 128\n",
      "  subdivision: 1\n",
      "  optimizer: sgd\n",
      "  base_lr: 0.1\n",
      "  momentum: 0.9\n",
      "  nesterov: True\n",
      "  weight_decay: 0.0005\n",
      "  no_weight_decay_on_bn: False\n",
      "  gradient_clip: 0\n",
      "  start_epoch: 0\n",
      "  seed: 0\n",
      "  val_first: True\n",
      "  val_period: 1\n",
      "  val_ratio: 0.1\n",
      "  use_test_as_val: False\n",
      "  output_dir: /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_5_c10val/exp00\n",
      "  log_period: 100\n",
      "  checkpoint_period: 100\n",
      "  use_tensorboard: True\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: True\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "  distributed: False\n",
      "  dist:\n",
      "    backend: nccl\n",
      "    init_method: env://\n",
      "    world_size: -1\n",
      "    node_rank: -1\n",
      "    local_rank: 0\n",
      "    use_sync_bn: False\n",
      "tensorboard:\n",
      "  train_images: False\n",
      "  val_images: False\n",
      "  model_params: False\n",
      "optim:\n",
      "  adam:\n",
      "    betas: (0.9, 0.999)\n",
      "  lars:\n",
      "    eps: 1e-09\n",
      "    threshold: 0.01\n",
      "  adabound:\n",
      "    betas: (0.9, 0.999)\n",
      "    final_lr: 0.1\n",
      "    gamma: 0.001\n",
      "scheduler:\n",
      "  epochs: 400\n",
      "  warmup:\n",
      "    type: none\n",
      "    epochs: 0\n",
      "    start_factor: 0.001\n",
      "    exponent: 4\n",
      "  type: multistep\n",
      "  milestones: [150, 225]\n",
      "  lr_decay: 0.1\n",
      "  lr_min_factor: 0.001\n",
      "  T0: 10\n",
      "  T_mul: 1.0\n",
      "validation:\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: False\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "augmentation:\n",
      "  use_random_crop: True\n",
      "  use_random_horizontal_flip: True\n",
      "  use_cutout: False\n",
      "  use_random_erasing: False\n",
      "  use_dual_cutout: False\n",
      "  use_mixup: False\n",
      "  use_ricap: False\n",
      "  use_cutmix: False\n",
      "  use_label_smoothing: False\n",
      "  random_crop:\n",
      "    padding: 4\n",
      "    fill: 0\n",
      "    padding_mode: constant\n",
      "  random_horizontal_flip:\n",
      "    prob: 0.5\n",
      "  cutout:\n",
      "    prob: 1.0\n",
      "    mask_size: 16\n",
      "    cut_inside: False\n",
      "    mask_color: 0\n",
      "    dual_cutout_alpha: 0.1\n",
      "  random_erasing:\n",
      "    prob: 0.5\n",
      "    area_ratio_range: [0.02, 0.4]\n",
      "    min_aspect_ratio: 0.3\n",
      "    max_attempt: 20\n",
      "  mixup:\n",
      "    alpha: 1.0\n",
      "  ricap:\n",
      "    beta: 0.3\n",
      "  cutmix:\n",
      "    alpha: 1.0\n",
      "  label_smoothing:\n",
      "    epsilon: 0.1\n",
      "tta:\n",
      "  use_resize: False\n",
      "  use_center_crop: False\n",
      "  resize: 256\n",
      "test:\n",
      "  checkpoint: ''\n",
      "  output_dir: ''\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    pin_memory: False\n",
      "\u001b[32m[2020-07-17 12:25:43] __main__ INFO: \u001b[0menv_info:\n",
      "  pytorch_version: 1.4.0\n",
      "  cuda_version: 10.1\n",
      "  cudnn_version: 7603\n",
      "  num_gpus: 1\n",
      "  gpu_name: Tesla K80\n",
      "  gpu_capability: 3.7\n",
      "Files already downloaded and verified\n",
      "\u001b[32m[2020-07-17 12:25:50] __main__ INFO: \u001b[0mMACs  : 2.75G\n",
      "\u001b[32m[2020-07-17 12:25:50] __main__ INFO: \u001b[0m#params: 17.56M\n",
      "Selected optimization level O0:  Pure FP32 training.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\",)\n",
      "\u001b[32m[2020-07-17 12:25:50] __main__ INFO: \u001b[0mVal 0\n",
      "\u001b[32m[2020-07-17 12:26:16] __main__ INFO: \u001b[0mEpoch 0 loss 10.1558 acc@1 0.1032 acc@5 0.4996\n",
      "\u001b[32m[2020-07-17 12:26:16] __main__ INFO: \u001b[0mElapsed 26.17\n",
      "\u001b[32m[2020-07-17 12:26:16] __main__ INFO: \u001b[0mTrain 1 0\n",
      "\u001b[32m[2020-07-17 12:28:47] __main__ INFO: \u001b[0mEpoch 1 Step 100/351 lr 0.100000 loss 3.3797 (8.0245) acc@1 0.1016 (0.1087) acc@5 0.5000 (0.5228)\n",
      "\u001b[32m[2020-07-17 12:31:11] __main__ INFO: \u001b[0mEpoch 1 Step 200/351 lr 0.100000 loss 2.4969 (5.3343) acc@1 0.1172 (0.1104) acc@5 0.5469 (0.5271)\n",
      "\u001b[32m[2020-07-17 12:33:34] __main__ INFO: \u001b[0mEpoch 1 Step 300/351 lr 0.100000 loss 2.5741 (4.3769) acc@1 0.1484 (0.1134) acc@5 0.6172 (0.5317)\n",
      "\u001b[32m[2020-07-17 12:34:48] __main__ INFO: \u001b[0mEpoch 1 Step 351/351 lr 0.100000 loss 2.3728 (4.0904) acc@1 0.1250 (0.1157) acc@5 0.5703 (0.5359)\n",
      "\u001b[32m[2020-07-17 12:34:48] __main__ INFO: \u001b[0mElapsed 511.55\n",
      "\u001b[32m[2020-07-17 12:34:48] __main__ INFO: \u001b[0mVal 1\n",
      "\u001b[32m[2020-07-17 12:35:06] __main__ INFO: \u001b[0mEpoch 1 loss 2.4652 acc@1 0.1500 acc@5 0.6396\n",
      "\u001b[32m[2020-07-17 12:35:06] __main__ INFO: \u001b[0mElapsed 18.05\n",
      "\u001b[32m[2020-07-17 12:35:06] __main__ INFO: \u001b[0mTrain 2 351\n",
      "\u001b[32m[2020-07-17 12:37:29] __main__ INFO: \u001b[0mEpoch 2 Step 100/351 lr 0.100000 loss 2.3303 (2.3723) acc@1 0.1328 (0.1182) acc@5 0.5547 (0.5617)\n",
      "\u001b[32m[2020-07-17 12:39:53] __main__ INFO: \u001b[0mEpoch 2 Step 200/351 lr 0.100000 loss 2.2953 (2.3596) acc@1 0.1328 (0.1214) acc@5 0.5938 (0.5630)\n",
      "\u001b[32m[2020-07-17 12:42:16] __main__ INFO: \u001b[0mEpoch 2 Step 300/351 lr 0.100000 loss 2.3374 (2.3481) acc@1 0.1328 (0.1233) acc@5 0.5547 (0.5654)\n",
      "\u001b[32m[2020-07-17 12:43:29] __main__ INFO: \u001b[0mEpoch 2 Step 351/351 lr 0.100000 loss 2.3735 (2.3433) acc@1 0.1641 (0.1243) acc@5 0.5078 (0.5656)\n",
      "\u001b[32m[2020-07-17 12:43:29] __main__ INFO: \u001b[0mElapsed 503.58\n",
      "\u001b[32m[2020-07-17 12:43:29] __main__ INFO: \u001b[0mVal 2\n",
      "\u001b[32m[2020-07-17 12:43:47] __main__ INFO: \u001b[0mEpoch 2 loss 2.2251 acc@1 0.1590 acc@5 0.6546\n",
      "\u001b[32m[2020-07-17 12:43:47] __main__ INFO: \u001b[0mElapsed 18.05\n",
      "\u001b[32m[2020-07-17 12:43:47] __main__ INFO: \u001b[0mTrain 3 702\n",
      "\u001b[32m[2020-07-17 12:46:11] __main__ INFO: \u001b[0mEpoch 3 Step 100/351 lr 0.100000 loss 2.3874 (2.3013) acc@1 0.2109 (0.1332) acc@5 0.6094 (0.5772)\n",
      "\u001b[32m[2020-07-17 12:48:34] __main__ INFO: \u001b[0mEpoch 3 Step 200/351 lr 0.100000 loss 2.2751 (2.2835) acc@1 0.1328 (0.1393) acc@5 0.5781 (0.5913)\n",
      "\u001b[32m[2020-07-17 12:50:57] __main__ INFO: \u001b[0mEpoch 3 Step 300/351 lr 0.100000 loss 2.2388 (2.2619) acc@1 0.1875 (0.1493) acc@5 0.5859 (0.6049)\n",
      "\u001b[32m[2020-07-17 12:52:11] __main__ INFO: \u001b[0mEpoch 3 Step 351/351 lr 0.100000 loss 2.1595 (2.2522) acc@1 0.1797 (0.1522) acc@5 0.6406 (0.6108)\n",
      "\u001b[32m[2020-07-17 12:52:11] __main__ INFO: \u001b[0mElapsed 503.34\n",
      "\u001b[32m[2020-07-17 12:52:11] __main__ INFO: \u001b[0mVal 3\n",
      "\u001b[32m[2020-07-17 12:52:29] __main__ INFO: \u001b[0mEpoch 3 loss 2.0287 acc@1 0.2446 acc@5 0.7726\n",
      "\u001b[32m[2020-07-17 12:52:29] __main__ INFO: \u001b[0mElapsed 18.02\n",
      "\u001b[32m[2020-07-17 12:52:29] __main__ INFO: \u001b[0mTrain 4 1053\n",
      "\u001b[32m[2020-07-17 12:54:52] __main__ INFO: \u001b[0mEpoch 4 Step 100/351 lr 0.100000 loss 2.2166 (2.1732) acc@1 0.1797 (0.1830) acc@5 0.6172 (0.6644)\n",
      "\u001b[32m[2020-07-17 12:57:16] __main__ INFO: \u001b[0mEpoch 4 Step 200/351 lr 0.100000 loss 2.0869 (2.1609) acc@1 0.1953 (0.1859) acc@5 0.6562 (0.6675)\n",
      "\u001b[32m[2020-07-17 12:59:39] __main__ INFO: \u001b[0mEpoch 4 Step 300/351 lr 0.100000 loss 1.9997 (2.1504) acc@1 0.2734 (0.1874) acc@5 0.7734 (0.6707)\n",
      "\u001b[32m[2020-07-17 13:00:52] __main__ INFO: \u001b[0mEpoch 4 Step 351/351 lr 0.100000 loss 2.0904 (2.1440) acc@1 0.2344 (0.1895) acc@5 0.7109 (0.6737)\n",
      "\u001b[32m[2020-07-17 13:00:52] __main__ INFO: \u001b[0mElapsed 503.61\n",
      "\u001b[32m[2020-07-17 13:00:52] __main__ INFO: \u001b[0mVal 4\n",
      "\u001b[32m[2020-07-17 13:01:10] __main__ INFO: \u001b[0mEpoch 4 loss 1.8277 acc@1 0.3146 acc@5 0.8456\n",
      "\u001b[32m[2020-07-17 13:01:10] __main__ INFO: \u001b[0mElapsed 18.03\n",
      "\u001b[32m[2020-07-17 13:01:10] __main__ INFO: \u001b[0mTrain 5 1404\n",
      "\u001b[32m[2020-07-17 13:03:34] __main__ INFO: \u001b[0mEpoch 5 Step 100/351 lr 0.100000 loss 2.0332 (2.0989) acc@1 0.1719 (0.2067) acc@5 0.6953 (0.6907)\n",
      "\u001b[32m[2020-07-17 13:05:57] __main__ INFO: \u001b[0mEpoch 5 Step 200/351 lr 0.100000 loss 2.0930 (2.0899) acc@1 0.2344 (0.2090) acc@5 0.7109 (0.6978)\n",
      "\u001b[32m[2020-07-17 13:08:21] __main__ INFO: \u001b[0mEpoch 5 Step 300/351 lr 0.100000 loss 1.9824 (2.0837) acc@1 0.2656 (0.2112) acc@5 0.7422 (0.7019)\n",
      "\u001b[32m[2020-07-17 13:09:34] __main__ INFO: \u001b[0mEpoch 5 Step 351/351 lr 0.100000 loss 2.0018 (2.0790) acc@1 0.2422 (0.2130) acc@5 0.7422 (0.7034)\n",
      "\u001b[32m[2020-07-17 13:09:34] __main__ INFO: \u001b[0mElapsed 503.91\n",
      "\u001b[32m[2020-07-17 13:09:34] __main__ INFO: \u001b[0mVal 5\n",
      "\u001b[32m[2020-07-17 13:09:52] __main__ INFO: \u001b[0mEpoch 5 loss 1.7569 acc@1 0.3476 acc@5 0.8632\n",
      "\u001b[32m[2020-07-17 13:09:52] __main__ INFO: \u001b[0mElapsed 18.03\n",
      "\u001b[32m[2020-07-17 13:09:52] __main__ INFO: \u001b[0mTrain 6 1755\n",
      "\u001b[32m[2020-07-17 13:12:16] __main__ INFO: \u001b[0mEpoch 6 Step 100/351 lr 0.100000 loss 1.9221 (2.0280) acc@1 0.2500 (0.2383) acc@5 0.7578 (0.7184)\n",
      "\u001b[32m[2020-07-17 13:14:39] __main__ INFO: \u001b[0mEpoch 6 Step 200/351 lr 0.100000 loss 1.9706 (2.0146) acc@1 0.2812 (0.2436) acc@5 0.7422 (0.7236)\n",
      "\u001b[32m[2020-07-17 13:17:03] __main__ INFO: \u001b[0mEpoch 6 Step 300/351 lr 0.100000 loss 2.0087 (2.0056) acc@1 0.2734 (0.2465) acc@5 0.7422 (0.7265)\n",
      "\u001b[32m[2020-07-17 13:18:16] __main__ INFO: \u001b[0mEpoch 6 Step 351/351 lr 0.100000 loss 2.0335 (1.9998) acc@1 0.2656 (0.2496) acc@5 0.7188 (0.7290)\n",
      "\u001b[32m[2020-07-17 13:18:16] __main__ INFO: \u001b[0mElapsed 504.08\n",
      "\u001b[32m[2020-07-17 13:18:16] __main__ INFO: \u001b[0mVal 6\n",
      "\u001b[32m[2020-07-17 13:18:34] __main__ INFO: \u001b[0mEpoch 6 loss 1.6403 acc@1 0.3744 acc@5 0.8970\n",
      "\u001b[32m[2020-07-17 13:18:34] __main__ INFO: \u001b[0mElapsed 18.03\n",
      "\u001b[32m[2020-07-17 13:18:34] __main__ INFO: \u001b[0mTrain 7 2106\n",
      "\u001b[32m[2020-07-17 13:20:58] __main__ INFO: \u001b[0mEpoch 7 Step 100/351 lr 0.100000 loss 1.9644 (1.9279) acc@1 0.3047 (0.2780) acc@5 0.7266 (0.7515)\n",
      "\u001b[32m[2020-07-17 13:23:22] __main__ INFO: \u001b[0mEpoch 7 Step 200/351 lr 0.100000 loss 1.8918 (1.9139) acc@1 0.3438 (0.2849) acc@5 0.7578 (0.7551)\n",
      "\u001b[32m[2020-07-17 13:25:45] __main__ INFO: \u001b[0mEpoch 7 Step 300/351 lr 0.100000 loss 1.9856 (1.9010) acc@1 0.2422 (0.2901) acc@5 0.6953 (0.7581)\n",
      "\u001b[32m[2020-07-17 13:26:58] __main__ INFO: \u001b[0mEpoch 7 Step 351/351 lr 0.100000 loss 1.9230 (1.8930) acc@1 0.2578 (0.2940) acc@5 0.7266 (0.7593)\n",
      "\u001b[32m[2020-07-17 13:26:58] __main__ INFO: \u001b[0mElapsed 504.10\n",
      "\u001b[32m[2020-07-17 13:26:58] __main__ INFO: \u001b[0mVal 7\n",
      "\u001b[32m[2020-07-17 13:27:17] __main__ INFO: \u001b[0mEpoch 7 loss 1.4080 acc@1 0.4868 acc@5 0.9280\n",
      "\u001b[32m[2020-07-17 13:27:17] __main__ INFO: \u001b[0mElapsed 18.04\n",
      "\u001b[32m[2020-07-17 13:27:17] __main__ INFO: \u001b[0mTrain 8 2457\n",
      "\u001b[32m[2020-07-17 13:29:40] __main__ INFO: \u001b[0mEpoch 8 Step 100/351 lr 0.100000 loss 1.9265 (1.8023) acc@1 0.2578 (0.3342) acc@5 0.7734 (0.7830)\n",
      "\u001b[32m[2020-07-17 13:32:04] __main__ INFO: \u001b[0mEpoch 8 Step 200/351 lr 0.100000 loss 1.7275 (1.7943) acc@1 0.3672 (0.3345) acc@5 0.8281 (0.7841)\n",
      "\u001b[32m[2020-07-17 13:34:27] __main__ INFO: \u001b[0mEpoch 8 Step 300/351 lr 0.100000 loss 1.7680 (1.7846) acc@1 0.3359 (0.3384) acc@5 0.7500 (0.7836)\n",
      "\u001b[32m[2020-07-17 13:35:41] __main__ INFO: \u001b[0mEpoch 8 Step 351/351 lr 0.100000 loss 1.7527 (1.7777) acc@1 0.2812 (0.3412) acc@5 0.7969 (0.7857)\n",
      "\u001b[32m[2020-07-17 13:35:41] __main__ INFO: \u001b[0mElapsed 504.10\n",
      "\u001b[32m[2020-07-17 13:35:41] __main__ INFO: \u001b[0mVal 8\n",
      "\u001b[32m[2020-07-17 13:35:59] __main__ INFO: \u001b[0mEpoch 8 loss 1.3317 acc@1 0.5252 acc@5 0.9366\n",
      "\u001b[32m[2020-07-17 13:35:59] __main__ INFO: \u001b[0mElapsed 18.02\n",
      "\u001b[32m[2020-07-17 13:35:59] __main__ INFO: \u001b[0mTrain 9 2808\n",
      "\u001b[32m[2020-07-17 13:38:22] __main__ INFO: \u001b[0mEpoch 9 Step 100/351 lr 0.100000 loss 1.7021 (1.7101) acc@1 0.3594 (0.3685) acc@5 0.7812 (0.7892)\n",
      "\u001b[32m[2020-07-17 13:40:46] __main__ INFO: \u001b[0mEpoch 9 Step 200/351 lr 0.100000 loss 1.6577 (1.7027) acc@1 0.4062 (0.3728) acc@5 0.7656 (0.7923)\n",
      "\u001b[32m[2020-07-17 13:43:10] __main__ INFO: \u001b[0mEpoch 9 Step 300/351 lr 0.100000 loss 1.8773 (1.6917) acc@1 0.3203 (0.3771) acc@5 0.7578 (0.7966)\n",
      "\u001b[32m[2020-07-17 13:44:23] __main__ INFO: \u001b[0mEpoch 9 Step 351/351 lr 0.100000 loss 1.8004 (1.6864) acc@1 0.3516 (0.3791) acc@5 0.7734 (0.7973)\n",
      "\u001b[32m[2020-07-17 13:44:23] __main__ INFO: \u001b[0mElapsed 504.22\n",
      "\u001b[32m[2020-07-17 13:44:23] __main__ INFO: \u001b[0mVal 9\n",
      "\u001b[32m[2020-07-17 13:44:41] __main__ INFO: \u001b[0mEpoch 9 loss 1.2460 acc@1 0.5572 acc@5 0.9494\n",
      "\u001b[32m[2020-07-17 13:44:41] __main__ INFO: \u001b[0mElapsed 18.06\n",
      "\u001b[32m[2020-07-17 13:44:41] __main__ INFO: \u001b[0mTrain 10 3159\n",
      "\u001b[32m[2020-07-17 13:47:05] __main__ INFO: \u001b[0mEpoch 10 Step 100/351 lr 0.100000 loss 1.6437 (1.6317) acc@1 0.4062 (0.3992) acc@5 0.7891 (0.8044)\n",
      "\u001b[32m[2020-07-17 13:49:28] __main__ INFO: \u001b[0mEpoch 10 Step 200/351 lr 0.100000 loss 1.5809 (1.6327) acc@1 0.3984 (0.3963) acc@5 0.8203 (0.8063)\n",
      "\u001b[32m[2020-07-17 13:51:52] __main__ INFO: \u001b[0mEpoch 10 Step 300/351 lr 0.100000 loss 1.5311 (1.6282) acc@1 0.4453 (0.4000) acc@5 0.8125 (0.8070)\n",
      "\u001b[32m[2020-07-17 13:53:05] __main__ INFO: \u001b[0mEpoch 10 Step 351/351 lr 0.100000 loss 1.5225 (1.6232) acc@1 0.4609 (0.4019) acc@5 0.7812 (0.8071)\n",
      "\u001b[32m[2020-07-17 13:53:05] __main__ INFO: \u001b[0mElapsed 504.12\n",
      "\u001b[32m[2020-07-17 13:53:05] __main__ INFO: \u001b[0mVal 10\n",
      "\u001b[32m[2020-07-17 13:53:23] __main__ INFO: \u001b[0mEpoch 10 loss 1.1389 acc@1 0.6020 acc@5 0.9546\n",
      "\u001b[32m[2020-07-17 13:53:23] __main__ INFO: \u001b[0mElapsed 18.04\n",
      "\u001b[32m[2020-07-17 13:53:23] __main__ INFO: \u001b[0mTrain 11 3510\n",
      "\u001b[32m[2020-07-17 13:55:47] __main__ INFO: \u001b[0mEpoch 11 Step 100/351 lr 0.100000 loss 1.6033 (1.5755) acc@1 0.4141 (0.4166) acc@5 0.8281 (0.8140)\n",
      "\u001b[32m[2020-07-17 13:58:10] __main__ INFO: \u001b[0mEpoch 11 Step 200/351 lr 0.100000 loss 1.4683 (1.5789) acc@1 0.4688 (0.4159) acc@5 0.7891 (0.8107)\n",
      "\u001b[32m[2020-07-17 14:00:34] __main__ INFO: \u001b[0mEpoch 11 Step 300/351 lr 0.100000 loss 1.4959 (1.5753) acc@1 0.5156 (0.4188) acc@5 0.8438 (0.8112)\n",
      "\u001b[32m[2020-07-17 14:01:47] __main__ INFO: \u001b[0mEpoch 11 Step 351/351 lr 0.100000 loss 1.3306 (1.5734) acc@1 0.4922 (0.4186) acc@5 0.8594 (0.8114)\n",
      "\u001b[32m[2020-07-17 14:01:47] __main__ INFO: \u001b[0mElapsed 504.21\n",
      "\u001b[32m[2020-07-17 14:01:47] __main__ INFO: \u001b[0mVal 11\n",
      "\u001b[32m[2020-07-17 14:02:05] __main__ INFO: \u001b[0mEpoch 11 loss 1.0303 acc@1 0.6288 acc@5 0.9658\n",
      "\u001b[32m[2020-07-17 14:02:05] __main__ INFO: \u001b[0mElapsed 18.04\n",
      "\u001b[32m[2020-07-17 14:02:05] __main__ INFO: \u001b[0mTrain 12 3861\n",
      "\u001b[32m[2020-07-17 14:04:29] __main__ INFO: \u001b[0mEpoch 12 Step 100/351 lr 0.100000 loss 1.4730 (1.5382) acc@1 0.4688 (0.4366) acc@5 0.8203 (0.8181)\n",
      "\u001b[32m[2020-07-17 14:06:53] __main__ INFO: \u001b[0mEpoch 12 Step 200/351 lr 0.100000 loss 1.7034 (1.5381) acc@1 0.3594 (0.4336) acc@5 0.7109 (0.8210)\n",
      "\u001b[32m[2020-07-17 14:09:17] __main__ INFO: \u001b[0mEpoch 12 Step 300/351 lr 0.100000 loss 1.5992 (1.5430) acc@1 0.4219 (0.4328) acc@5 0.7891 (0.8189)\n",
      "\u001b[32m[2020-07-17 14:10:30] __main__ INFO: \u001b[0mEpoch 12 Step 351/351 lr 0.100000 loss 1.5235 (1.5419) acc@1 0.4531 (0.4325) acc@5 0.8203 (0.8183)\n",
      "\u001b[32m[2020-07-17 14:10:30] __main__ INFO: \u001b[0mElapsed 504.52\n",
      "\u001b[32m[2020-07-17 14:10:30] __main__ INFO: \u001b[0mVal 12\n",
      "\u001b[32m[2020-07-17 14:10:48] __main__ INFO: \u001b[0mEpoch 12 loss 1.1637 acc@1 0.5954 acc@5 0.9502\n",
      "\u001b[32m[2020-07-17 14:10:48] __main__ INFO: \u001b[0mElapsed 18.04\n",
      "\u001b[32m[2020-07-17 14:10:48] __main__ INFO: \u001b[0mTrain 13 4212\n",
      "\u001b[32m[2020-07-17 14:13:12] __main__ INFO: \u001b[0mEpoch 13 Step 100/351 lr 0.100000 loss 1.4895 (1.4999) acc@1 0.4453 (0.4465) acc@5 0.8281 (0.8241)\n",
      "\u001b[32m[2020-07-17 14:15:35] __main__ INFO: \u001b[0mEpoch 13 Step 200/351 lr 0.100000 loss 1.4746 (1.5016) acc@1 0.4688 (0.4466) acc@5 0.8516 (0.8244)\n",
      "\u001b[32m[2020-07-17 14:17:59] __main__ INFO: \u001b[0mEpoch 13 Step 300/351 lr 0.100000 loss 1.4504 (1.5061) acc@1 0.4609 (0.4454) acc@5 0.7812 (0.8239)\n",
      "\u001b[32m[2020-07-17 14:19:12] __main__ INFO: \u001b[0mEpoch 13 Step 351/351 lr 0.100000 loss 1.6595 (1.5048) acc@1 0.3594 (0.4456) acc@5 0.8047 (0.8237)\n",
      "\u001b[32m[2020-07-17 14:19:12] __main__ INFO: \u001b[0mElapsed 504.07\n",
      "\u001b[32m[2020-07-17 14:19:12] __main__ INFO: \u001b[0mVal 13\n",
      "\u001b[32m[2020-07-17 14:19:30] __main__ INFO: \u001b[0mEpoch 13 loss 1.3647 acc@1 0.5470 acc@5 0.9366\n",
      "\u001b[32m[2020-07-17 14:19:30] __main__ INFO: \u001b[0mElapsed 18.06\n",
      "\u001b[32m[2020-07-17 14:19:30] __main__ INFO: \u001b[0mTrain 14 4563\n",
      "\u001b[32m[2020-07-17 14:21:54] __main__ INFO: \u001b[0mEpoch 14 Step 100/351 lr 0.100000 loss 1.5994 (1.4579) acc@1 0.4219 (0.4640) acc@5 0.7891 (0.8276)\n",
      "\u001b[32m[2020-07-17 14:24:17] __main__ INFO: \u001b[0mEpoch 14 Step 200/351 lr 0.100000 loss 1.5820 (1.4683) acc@1 0.3984 (0.4590) acc@5 0.8203 (0.8253)\n",
      "\u001b[32m[2020-07-17 14:26:41] __main__ INFO: \u001b[0mEpoch 14 Step 300/351 lr 0.100000 loss 1.5641 (1.4728) acc@1 0.3984 (0.4567) acc@5 0.8281 (0.8244)\n",
      "\u001b[32m[2020-07-17 14:27:54] __main__ INFO: \u001b[0mEpoch 14 Step 351/351 lr 0.100000 loss 1.4611 (1.4740) acc@1 0.4766 (0.4558) acc@5 0.8125 (0.8236)\n",
      "\u001b[32m[2020-07-17 14:27:54] __main__ INFO: \u001b[0mElapsed 504.02\n",
      "\u001b[32m[2020-07-17 14:27:54] __main__ INFO: \u001b[0mVal 14\n",
      "\u001b[32m[2020-07-17 14:28:12] __main__ INFO: \u001b[0mEpoch 14 loss 1.3330 acc@1 0.5816 acc@5 0.9580\n",
      "\u001b[32m[2020-07-17 14:28:12] __main__ INFO: \u001b[0mElapsed 18.05\n",
      "\u001b[32m[2020-07-17 14:28:12] __main__ INFO: \u001b[0mTrain 15 4914\n",
      "\u001b[32m[2020-07-17 14:30:36] __main__ INFO: \u001b[0mEpoch 15 Step 100/351 lr 0.100000 loss 1.4867 (1.4403) acc@1 0.4688 (0.4712) acc@5 0.8047 (0.8252)\n",
      "\u001b[32m[2020-07-17 14:32:59] __main__ INFO: \u001b[0mEpoch 15 Step 200/351 lr 0.100000 loss 1.5306 (1.4383) acc@1 0.4531 (0.4724) acc@5 0.7969 (0.8277)\n",
      "\u001b[32m[2020-07-17 14:35:23] __main__ INFO: \u001b[0mEpoch 15 Step 300/351 lr 0.100000 loss 1.3756 (1.4399) acc@1 0.4844 (0.4704) acc@5 0.7734 (0.8273)\n",
      "\u001b[32m[2020-07-17 14:36:36] __main__ INFO: \u001b[0mEpoch 15 Step 351/351 lr 0.100000 loss 1.3574 (1.4399) acc@1 0.5469 (0.4710) acc@5 0.8281 (0.8278)\n",
      "\u001b[32m[2020-07-17 14:36:36] __main__ INFO: \u001b[0mElapsed 503.99\n",
      "\u001b[32m[2020-07-17 14:36:36] __main__ INFO: \u001b[0mVal 15\n",
      "\u001b[32m[2020-07-17 14:36:54] __main__ INFO: \u001b[0mEpoch 15 loss 1.1004 acc@1 0.6304 acc@5 0.9650\n",
      "\u001b[32m[2020-07-17 14:36:54] __main__ INFO: \u001b[0mElapsed 18.08\n",
      "\u001b[32m[2020-07-17 14:36:54] __main__ INFO: \u001b[0mTrain 16 5265\n",
      "\u001b[32m[2020-07-17 14:39:18] __main__ INFO: \u001b[0mEpoch 16 Step 100/351 lr 0.100000 loss 1.3793 (1.4110) acc@1 0.5000 (0.4839) acc@5 0.8672 (0.8332)\n",
      "\u001b[32m[2020-07-17 14:41:41] __main__ INFO: \u001b[0mEpoch 16 Step 200/351 lr 0.100000 loss 1.3609 (1.4244) acc@1 0.5469 (0.4765) acc@5 0.7812 (0.8286)\n",
      "\u001b[32m[2020-07-17 14:44:05] __main__ INFO: \u001b[0mEpoch 16 Step 300/351 lr 0.100000 loss 1.2703 (1.4202) acc@1 0.5547 (0.4790) acc@5 0.8281 (0.8307)\n",
      "\u001b[32m[2020-07-17 14:45:18] __main__ INFO: \u001b[0mEpoch 16 Step 351/351 lr 0.100000 loss 1.4489 (1.4202) acc@1 0.4609 (0.4792) acc@5 0.8125 (0.8304)\n",
      "\u001b[32m[2020-07-17 14:45:18] __main__ INFO: \u001b[0mElapsed 504.02\n",
      "\u001b[32m[2020-07-17 14:45:18] __main__ INFO: \u001b[0mVal 16\n",
      "\u001b[32m[2020-07-17 14:45:36] __main__ INFO: \u001b[0mEpoch 16 loss 0.8506 acc@1 0.7056 acc@5 0.9764\n",
      "\u001b[32m[2020-07-17 14:45:36] __main__ INFO: \u001b[0mElapsed 18.03\n",
      "\u001b[32m[2020-07-17 14:45:36] __main__ INFO: \u001b[0mTrain 17 5616\n",
      "\u001b[32m[2020-07-17 14:48:00] __main__ INFO: \u001b[0mEpoch 17 Step 100/351 lr 0.100000 loss 1.3091 (1.3929) acc@1 0.5859 (0.4863) acc@5 0.8438 (0.8349)\n",
      "\u001b[32m[2020-07-17 14:50:23] __main__ INFO: \u001b[0mEpoch 17 Step 200/351 lr 0.100000 loss 1.4271 (1.3977) acc@1 0.4609 (0.4846) acc@5 0.8125 (0.8329)\n",
      "\u001b[32m[2020-07-17 14:52:47] __main__ INFO: \u001b[0mEpoch 17 Step 300/351 lr 0.100000 loss 1.3262 (1.3944) acc@1 0.4844 (0.4880) acc@5 0.8203 (0.8329)\n",
      "\u001b[32m[2020-07-17 14:54:00] __main__ INFO: \u001b[0mEpoch 17 Step 351/351 lr 0.100000 loss 1.4122 (1.3949) acc@1 0.4375 (0.4877) acc@5 0.8906 (0.8332)\n",
      "\u001b[32m[2020-07-17 14:54:00] __main__ INFO: \u001b[0mElapsed 503.87\n",
      "\u001b[32m[2020-07-17 14:54:00] __main__ INFO: \u001b[0mVal 17\n",
      "\u001b[32m[2020-07-17 14:54:18] __main__ INFO: \u001b[0mEpoch 17 loss 0.9431 acc@1 0.6826 acc@5 0.9710\n",
      "\u001b[32m[2020-07-17 14:54:18] __main__ INFO: \u001b[0mElapsed 18.06\n",
      "\u001b[32m[2020-07-17 14:54:18] __main__ INFO: \u001b[0mTrain 18 5967\n",
      "\u001b[32m[2020-07-17 14:56:42] __main__ INFO: \u001b[0mEpoch 18 Step 100/351 lr 0.100000 loss 1.3374 (1.3557) acc@1 0.5312 (0.4989) acc@5 0.8828 (0.8420)\n",
      "\u001b[32m[2020-07-17 14:59:05] __main__ INFO: \u001b[0mEpoch 18 Step 200/351 lr 0.100000 loss 1.2898 (1.3659) acc@1 0.5234 (0.4964) acc@5 0.8281 (0.8402)\n",
      "\u001b[32m[2020-07-17 15:01:29] __main__ INFO: \u001b[0mEpoch 18 Step 300/351 lr 0.100000 loss 1.2938 (1.3744) acc@1 0.5000 (0.4935) acc@5 0.8125 (0.8361)\n",
      "\u001b[32m[2020-07-17 15:02:42] __main__ INFO: \u001b[0mEpoch 18 Step 351/351 lr 0.100000 loss 1.1352 (1.3746) acc@1 0.6406 (0.4940) acc@5 0.9062 (0.8349)\n",
      "\u001b[32m[2020-07-17 15:02:42] __main__ INFO: \u001b[0mElapsed 503.86\n",
      "\u001b[32m[2020-07-17 15:02:42] __main__ INFO: \u001b[0mVal 18\n",
      "\u001b[32m[2020-07-17 15:03:00] __main__ INFO: \u001b[0mEpoch 18 loss 1.0240 acc@1 0.6334 acc@5 0.9734\n",
      "\u001b[32m[2020-07-17 15:03:00] __main__ INFO: \u001b[0mElapsed 18.06\n",
      "\u001b[32m[2020-07-17 15:03:00] __main__ INFO: \u001b[0mTrain 19 6318\n",
      "\u001b[32m[2020-07-17 15:05:24] __main__ INFO: \u001b[0mEpoch 19 Step 100/351 lr 0.100000 loss 1.4848 (1.3440) acc@1 0.4297 (0.5020) acc@5 0.8125 (0.8357)\n",
      "\u001b[32m[2020-07-17 15:07:47] __main__ INFO: \u001b[0mEpoch 19 Step 200/351 lr 0.100000 loss 1.4954 (1.3533) acc@1 0.4375 (0.4988) acc@5 0.8359 (0.8358)\n",
      "\u001b[32m[2020-07-17 15:10:11] __main__ INFO: \u001b[0mEpoch 19 Step 300/351 lr 0.100000 loss 1.5274 (1.3544) acc@1 0.3594 (0.5001) acc@5 0.7812 (0.8353)\n",
      "\u001b[32m[2020-07-17 15:11:24] __main__ INFO: \u001b[0mEpoch 19 Step 351/351 lr 0.100000 loss 1.3120 (1.3581) acc@1 0.5391 (0.4989) acc@5 0.8359 (0.8346)\n",
      "\u001b[32m[2020-07-17 15:11:24] __main__ INFO: \u001b[0mElapsed 503.87\n",
      "\u001b[32m[2020-07-17 15:11:24] __main__ INFO: \u001b[0mVal 19\n",
      "\u001b[32m[2020-07-17 15:11:42] __main__ INFO: \u001b[0mEpoch 19 loss 0.9388 acc@1 0.6956 acc@5 0.9690\n",
      "\u001b[32m[2020-07-17 15:11:42] __main__ INFO: \u001b[0mElapsed 18.05\n",
      "\u001b[32m[2020-07-17 15:11:42] __main__ INFO: \u001b[0mTrain 20 6669\n",
      "\u001b[32m[2020-07-17 15:14:06] __main__ INFO: \u001b[0mEpoch 20 Step 100/351 lr 0.100000 loss 1.5323 (1.3381) acc@1 0.4453 (0.5105) acc@5 0.7891 (0.8380)\n",
      "\u001b[32m[2020-07-17 15:16:29] __main__ INFO: \u001b[0mEpoch 20 Step 200/351 lr 0.100000 loss 1.5409 (1.3428) acc@1 0.4219 (0.5075) acc@5 0.8438 (0.8367)\n",
      "\u001b[32m[2020-07-17 15:18:53] __main__ INFO: \u001b[0mEpoch 20 Step 300/351 lr 0.100000 loss 1.4657 (1.3446) acc@1 0.4766 (0.5059) acc@5 0.8359 (0.8355)\n",
      "\u001b[32m[2020-07-17 15:20:06] __main__ INFO: \u001b[0mEpoch 20 Step 351/351 lr 0.100000 loss 1.2974 (1.3441) acc@1 0.5234 (0.5052) acc@5 0.8047 (0.8358)\n",
      "\u001b[32m[2020-07-17 15:20:06] __main__ INFO: \u001b[0mElapsed 503.91\n",
      "\u001b[32m[2020-07-17 15:20:06] __main__ INFO: \u001b[0mVal 20\n",
      "\u001b[32m[2020-07-17 15:20:24] __main__ INFO: \u001b[0mEpoch 20 loss 0.8286 acc@1 0.7234 acc@5 0.9750\n",
      "\u001b[32m[2020-07-17 15:20:24] __main__ INFO: \u001b[0mElapsed 18.03\n",
      "\u001b[32m[2020-07-17 15:20:24] __main__ INFO: \u001b[0mTrain 21 7020\n",
      "\u001b[32m[2020-07-17 15:22:47] __main__ INFO: \u001b[0mEpoch 21 Step 100/351 lr 0.100000 loss 1.2452 (1.3131) acc@1 0.5469 (0.5172) acc@5 0.8359 (0.8363)\n",
      "\u001b[32m[2020-07-17 15:25:11] __main__ INFO: \u001b[0mEpoch 21 Step 200/351 lr 0.100000 loss 1.4441 (1.3327) acc@1 0.4844 (0.5100) acc@5 0.8516 (0.8359)\n",
      "\u001b[32m[2020-07-17 15:27:35] __main__ INFO: \u001b[0mEpoch 21 Step 300/351 lr 0.100000 loss 1.2230 (1.3356) acc@1 0.5312 (0.5083) acc@5 0.8594 (0.8372)\n",
      "\u001b[32m[2020-07-17 15:28:48] __main__ INFO: \u001b[0mEpoch 21 Step 351/351 lr 0.100000 loss 1.5197 (1.3351) acc@1 0.4688 (0.5083) acc@5 0.8203 (0.8358)\n",
      "\u001b[32m[2020-07-17 15:28:48] __main__ INFO: \u001b[0mElapsed 503.78\n",
      "\u001b[32m[2020-07-17 15:28:48] __main__ INFO: \u001b[0mVal 21\n",
      "\u001b[32m[2020-07-17 15:29:06] __main__ INFO: \u001b[0mEpoch 21 loss 1.2004 acc@1 0.6236 acc@5 0.9716\n",
      "\u001b[32m[2020-07-17 15:29:06] __main__ INFO: \u001b[0mElapsed 18.04\n",
      "\u001b[32m[2020-07-17 15:29:06] __main__ INFO: \u001b[0mTrain 22 7371\n",
      "\u001b[32m[2020-07-17 15:31:29] __main__ INFO: \u001b[0mEpoch 22 Step 100/351 lr 0.100000 loss 1.2658 (1.3093) acc@1 0.5703 (0.5184) acc@5 0.8594 (0.8427)\n",
      "\u001b[32m[2020-07-17 15:33:53] __main__ INFO: \u001b[0mEpoch 22 Step 200/351 lr 0.100000 loss 1.3579 (1.3181) acc@1 0.4688 (0.5148) acc@5 0.8359 (0.8391)\n",
      "\u001b[32m[2020-07-17 15:36:16] __main__ INFO: \u001b[0mEpoch 22 Step 300/351 lr 0.100000 loss 1.3432 (1.3164) acc@1 0.4844 (0.5154) acc@5 0.8750 (0.8407)\n",
      "\u001b[32m[2020-07-17 15:37:29] __main__ INFO: \u001b[0mEpoch 22 Step 351/351 lr 0.100000 loss 1.2359 (1.3193) acc@1 0.5781 (0.5138) acc@5 0.8750 (0.8400)\n",
      "\u001b[32m[2020-07-17 15:37:29] __main__ INFO: \u001b[0mElapsed 503.62\n",
      "\u001b[32m[2020-07-17 15:37:29] __main__ INFO: \u001b[0mVal 22\n",
      "\u001b[32m[2020-07-17 15:37:47] __main__ INFO: \u001b[0mEpoch 22 loss 1.0053 acc@1 0.6562 acc@5 0.9610\n",
      "\u001b[32m[2020-07-17 15:37:47] __main__ INFO: \u001b[0mElapsed 18.07\n",
      "\u001b[32m[2020-07-17 15:37:47] __main__ INFO: \u001b[0mTrain 23 7722\n",
      "\u001b[32m[2020-07-17 15:40:11] __main__ INFO: \u001b[0mEpoch 23 Step 100/351 lr 0.100000 loss 1.1622 (1.3174) acc@1 0.5547 (0.5164) acc@5 0.9141 (0.8401)\n",
      "\u001b[32m[2020-07-17 15:42:35] __main__ INFO: \u001b[0mEpoch 23 Step 200/351 lr 0.100000 loss 1.4006 (1.3112) acc@1 0.4688 (0.5188) acc@5 0.8047 (0.8406)\n",
      "\u001b[32m[2020-07-17 15:44:58] __main__ INFO: \u001b[0mEpoch 23 Step 300/351 lr 0.100000 loss 1.3860 (1.3118) acc@1 0.4922 (0.5181) acc@5 0.7812 (0.8400)\n",
      "\u001b[32m[2020-07-17 15:46:11] __main__ INFO: \u001b[0mEpoch 23 Step 351/351 lr 0.100000 loss 1.3946 (1.3111) acc@1 0.5234 (0.5173) acc@5 0.8359 (0.8401)\n",
      "\u001b[32m[2020-07-17 15:46:11] __main__ INFO: \u001b[0mElapsed 503.79\n",
      "\u001b[32m[2020-07-17 15:46:11] __main__ INFO: \u001b[0mVal 23\n",
      "\u001b[32m[2020-07-17 15:46:29] __main__ INFO: \u001b[0mEpoch 23 loss 0.9397 acc@1 0.6968 acc@5 0.9760\n",
      "\u001b[32m[2020-07-17 15:46:29] __main__ INFO: \u001b[0mElapsed 18.04\n",
      "\u001b[32m[2020-07-17 15:46:29] __main__ INFO: \u001b[0mTrain 24 8073\n",
      "\u001b[32m[2020-07-17 15:48:53] __main__ INFO: \u001b[0mEpoch 24 Step 100/351 lr 0.100000 loss 1.2851 (1.2903) acc@1 0.5234 (0.5243) acc@5 0.8750 (0.8436)\n",
      "\u001b[32m[2020-07-17 15:51:16] __main__ INFO: \u001b[0mEpoch 24 Step 200/351 lr 0.100000 loss 1.4702 (1.2957) acc@1 0.4844 (0.5212) acc@5 0.8828 (0.8438)\n",
      "\u001b[32m[2020-07-17 15:53:40] __main__ INFO: \u001b[0mEpoch 24 Step 300/351 lr 0.100000 loss 1.1609 (1.3019) acc@1 0.5938 (0.5181) acc@5 0.8203 (0.8425)\n",
      "\u001b[32m[2020-07-17 15:54:53] __main__ INFO: \u001b[0mEpoch 24 Step 351/351 lr 0.100000 loss 1.3675 (1.3023) acc@1 0.4922 (0.5180) acc@5 0.8203 (0.8419)\n",
      "\u001b[32m[2020-07-17 15:54:53] __main__ INFO: \u001b[0mElapsed 503.76\n",
      "\u001b[32m[2020-07-17 15:54:53] __main__ INFO: \u001b[0mVal 24\n",
      "\u001b[32m[2020-07-17 15:55:11] __main__ INFO: \u001b[0mEpoch 24 loss 0.7684 acc@1 0.7254 acc@5 0.9808\n",
      "\u001b[32m[2020-07-17 15:55:11] __main__ INFO: \u001b[0mElapsed 18.04\n",
      "\u001b[32m[2020-07-17 15:55:11] __main__ INFO: \u001b[0mTrain 25 8424\n",
      "\u001b[32m[2020-07-17 15:57:35] __main__ INFO: \u001b[0mEpoch 25 Step 100/351 lr 0.100000 loss 1.3945 (1.2926) acc@1 0.4766 (0.5243) acc@5 0.8359 (0.8405)\n",
      "\u001b[32m[2020-07-17 15:59:58] __main__ INFO: \u001b[0mEpoch 25 Step 200/351 lr 0.100000 loss 1.3445 (1.2970) acc@1 0.5156 (0.5228) acc@5 0.8281 (0.8389)\n",
      "\u001b[32m[2020-07-17 16:02:22] __main__ INFO: \u001b[0mEpoch 25 Step 300/351 lr 0.100000 loss 1.0846 (1.2943) acc@1 0.6016 (0.5253) acc@5 0.8828 (0.8408)\n",
      "\u001b[32m[2020-07-17 16:03:35] __main__ INFO: \u001b[0mEpoch 25 Step 351/351 lr 0.100000 loss 1.2276 (1.2999) acc@1 0.5391 (0.5221) acc@5 0.8281 (0.8395)\n",
      "\u001b[32m[2020-07-17 16:03:35] __main__ INFO: \u001b[0mElapsed 503.68\n",
      "\u001b[32m[2020-07-17 16:03:35] __main__ INFO: \u001b[0mVal 25\n",
      "\u001b[32m[2020-07-17 16:03:53] __main__ INFO: \u001b[0mEpoch 25 loss 0.7741 acc@1 0.7378 acc@5 0.9816\n",
      "\u001b[32m[2020-07-17 16:03:53] __main__ INFO: \u001b[0mElapsed 18.06\n",
      "\u001b[32m[2020-07-17 16:03:53] __main__ INFO: \u001b[0mTrain 26 8775\n",
      "\u001b[32m[2020-07-17 16:06:16] __main__ INFO: \u001b[0mEpoch 26 Step 100/351 lr 0.100000 loss 1.2314 (1.2789) acc@1 0.5781 (0.5268) acc@5 0.8984 (0.8405)\n"
     ]
    }
   ],
   "source": [
    "# Train the model per the settings specified in the original paper\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "!python train.py --config configs/cifar/resnext.yaml \\\n",
    "    model.resnext.cardinality 4 \\\n",
    "    train.batch_size 128 \\\n",
    "    train.base_lr 0.1 \\\n",
    "    dataset.name CIFAR10_RA_2_5 \\\n",
    "    train.base_lr 0.1 \\\n",
    "    train.output_dir /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_5_c10val/exp00 \\\n",
    "    scheduler.epochs 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Refine the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-19 22:40:59] __main__ INFO: \u001b[0mdevice: cuda\n",
      "cudnn:\n",
      "  benchmark: True\n",
      "  deterministic: False\n",
      "dataset:\n",
      "  name: CIFAR10\n",
      "  dataset_dir: ~/.torch/datasets/CIFAR10\n",
      "  image_size: 32\n",
      "  n_channels: 3\n",
      "  n_classes: 10\n",
      "model:\n",
      "  type: cifar\n",
      "  name: resnext\n",
      "  init_mode: kaiming_fan_out\n",
      "  vgg:\n",
      "    n_channels: [64, 128, 256, 512, 512]\n",
      "    n_layers: [2, 2, 3, 3, 3]\n",
      "    use_bn: True\n",
      "  resnet:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "  resnet_preact:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "  wrn:\n",
      "    depth: 28\n",
      "    initial_channels: 16\n",
      "    widening_factor: 10\n",
      "    drop_rate: 0.0\n",
      "  densenet:\n",
      "    depth: 100\n",
      "    n_blocks: [6, 12, 24, 16]\n",
      "    block_type: bottleneck\n",
      "    growth_rate: 12\n",
      "    drop_rate: 0.0\n",
      "    compression_rate: 0.5\n",
      "  pyramidnet:\n",
      "    depth: 272\n",
      "    n_blocks: [3, 24, 36, 3]\n",
      "    initial_channels: 16\n",
      "    block_type: bottleneck\n",
      "    alpha: 200\n",
      "  resnext:\n",
      "    depth: 29\n",
      "    n_blocks: [3, 4, 6, 3]\n",
      "    initial_channels: 64\n",
      "    cardinality: 4\n",
      "    base_channels: 64\n",
      "  shake_shake:\n",
      "    depth: 26\n",
      "    initial_channels: 96\n",
      "    shake_forward: True\n",
      "    shake_backward: True\n",
      "    shake_image: True\n",
      "  se_resnet_preact:\n",
      "    depth: 110\n",
      "    initial_channels: 16\n",
      "    se_reduction: 16\n",
      "    block_type: basic\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "train:\n",
      "  checkpoint: /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_5_c10val/exp00/checkpoint_00400.pth\n",
      "  resume: False\n",
      "  precision: O0\n",
      "  batch_size: 128\n",
      "  subdivision: 1\n",
      "  optimizer: sgd\n",
      "  base_lr: 0.001\n",
      "  momentum: 0.9\n",
      "  nesterov: True\n",
      "  weight_decay: 0.0005\n",
      "  no_weight_decay_on_bn: False\n",
      "  gradient_clip: 0\n",
      "  start_epoch: 0\n",
      "  seed: 0\n",
      "  val_first: True\n",
      "  val_period: 1\n",
      "  val_ratio: 0.1\n",
      "  use_test_as_val: False\n",
      "  output_dir: /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_5_c10val/exp00_resume400_50\n",
      "  log_period: 100\n",
      "  checkpoint_period: 100\n",
      "  use_tensorboard: True\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: True\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "  distributed: False\n",
      "  dist:\n",
      "    backend: nccl\n",
      "    init_method: env://\n",
      "    world_size: -1\n",
      "    node_rank: -1\n",
      "    local_rank: 0\n",
      "    use_sync_bn: False\n",
      "tensorboard:\n",
      "  train_images: False\n",
      "  val_images: False\n",
      "  model_params: False\n",
      "optim:\n",
      "  adam:\n",
      "    betas: (0.9, 0.999)\n",
      "  lars:\n",
      "    eps: 1e-09\n",
      "    threshold: 0.01\n",
      "  adabound:\n",
      "    betas: (0.9, 0.999)\n",
      "    final_lr: 0.1\n",
      "    gamma: 0.001\n",
      "scheduler:\n",
      "  epochs: 50\n",
      "  warmup:\n",
      "    type: none\n",
      "    epochs: 0\n",
      "    start_factor: 0.001\n",
      "    exponent: 4\n",
      "  type: multistep\n",
      "  milestones: [150, 225]\n",
      "  lr_decay: 0.1\n",
      "  lr_min_factor: 0.001\n",
      "  T0: 10\n",
      "  T_mul: 1.0\n",
      "validation:\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: False\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "augmentation:\n",
      "  use_random_crop: True\n",
      "  use_random_horizontal_flip: True\n",
      "  use_cutout: False\n",
      "  use_random_erasing: False\n",
      "  use_dual_cutout: False\n",
      "  use_mixup: False\n",
      "  use_ricap: False\n",
      "  use_cutmix: False\n",
      "  use_label_smoothing: False\n",
      "  random_crop:\n",
      "    padding: 4\n",
      "    fill: 0\n",
      "    padding_mode: constant\n",
      "  random_horizontal_flip:\n",
      "    prob: 0.5\n",
      "  cutout:\n",
      "    prob: 1.0\n",
      "    mask_size: 16\n",
      "    cut_inside: False\n",
      "    mask_color: 0\n",
      "    dual_cutout_alpha: 0.1\n",
      "  random_erasing:\n",
      "    prob: 0.5\n",
      "    area_ratio_range: [0.02, 0.4]\n",
      "    min_aspect_ratio: 0.3\n",
      "    max_attempt: 20\n",
      "  mixup:\n",
      "    alpha: 1.0\n",
      "  ricap:\n",
      "    beta: 0.3\n",
      "  cutmix:\n",
      "    alpha: 1.0\n",
      "  label_smoothing:\n",
      "    epsilon: 0.1\n",
      "tta:\n",
      "  use_resize: False\n",
      "  use_center_crop: False\n",
      "  resize: 256\n",
      "test:\n",
      "  checkpoint: ''\n",
      "  output_dir: ''\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    pin_memory: False\n",
      "\u001b[32m[2020-07-19 22:40:59] __main__ INFO: \u001b[0menv_info:\n",
      "  pytorch_version: 1.4.0\n",
      "  cuda_version: 10.1\n",
      "  cudnn_version: 7603\n",
      "  num_gpus: 1\n",
      "  gpu_name: Tesla K80\n",
      "  gpu_capability: 3.7\n",
      "Files already downloaded and verified\n",
      "\u001b[32m[2020-07-19 22:41:04] __main__ INFO: \u001b[0mMACs  : 2.75G\n",
      "\u001b[32m[2020-07-19 22:41:04] __main__ INFO: \u001b[0m#params: 17.56M\n",
      "Selected optimization level O0:  Pure FP32 training.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\",)\n",
      "\u001b[32m[2020-07-19 22:41:04] __main__ INFO: \u001b[0mVal 0\n",
      "\u001b[32m[2020-07-19 22:41:30] __main__ INFO: \u001b[0mEpoch 0 loss 0.4305 acc@1 0.8764 acc@5 0.9946\n",
      "\u001b[32m[2020-07-19 22:41:30] __main__ INFO: \u001b[0mElapsed 26.30\n",
      "\u001b[32m[2020-07-19 22:41:30] __main__ INFO: \u001b[0mTrain 1 0\n",
      "\u001b[32m[2020-07-19 22:44:02] __main__ INFO: \u001b[0mEpoch 1 Step 100/351 lr 0.001000 loss 0.0923 (0.1568) acc@1 0.9766 (0.9504) acc@5 1.0000 (0.9983)\n",
      "\u001b[32m[2020-07-19 22:46:26] __main__ INFO: \u001b[0mEpoch 1 Step 200/351 lr 0.001000 loss 0.1691 (0.1484) acc@1 0.9609 (0.9528) acc@5 0.9922 (0.9984)\n"
     ]
    }
   ],
   "source": [
    "# Resume training with the un-augmented data\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "!python train.py --config /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_5_c10val/exp00/config.yaml \\\n",
    "    train.checkpoint /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_5_c10val/exp00/checkpoint_00400.pth \\\n",
    "    dataset.name CIFAR10 \\\n",
    "    model.resnext.cardinality 4 \\\n",
    "    train.batch_size 128 \\\n",
    "    train.base_lr 0.001 \\\n",
    "    train.output_dir /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_5_c10val/exp00_resume400_50 \\\n",
    "    scheduler.epochs 50\n",
    "\n",
    "#### Set LEARNING RATE based on ending LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-20 23:31:06] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_5_c10val/exp00_resume400_50/checkpoint_00050.pth\n",
      "Files already downloaded and verified\n",
      "100%|| 79/79 [00:36<00:00,  2.16it/s]\n",
      "\u001b[32m[2020-07-20 23:31:43] __main__ INFO: \u001b[0mElapsed 36.49\n",
      "\u001b[32m[2020-07-20 23:31:43] __main__ INFO: \u001b[0mLoss 0.2288 Accuracy 0.9314\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/resnext.yaml \\\n",
    "   model.resnext.cardinality 4 \\\n",
    "   test.batch_size 128 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_5_c10val/exp00_resume400_50/checkpoint_00050.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_5_c10val/exp00_resume400_50/test_results_0050_cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-20 23:32:25] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_5_c10val/exp00_resume400_50/checkpoint_00050.pth\n",
      "CIFAR 10.1\n",
      "100%|| 16/16 [00:07<00:00,  2.12it/s]\n",
      "\u001b[32m[2020-07-20 23:32:33] __main__ INFO: \u001b[0mElapsed 7.56\n",
      "\u001b[32m[2020-07-20 23:32:33] __main__ INFO: \u001b[0mLoss 0.5403 Accuracy 0.8540\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10.1 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/resnext.yaml \\\n",
    "   model.resnext.cardinality 4 \\\n",
    "   test.batch_size 128 \\\n",
    "   dataset.name CIFAR101 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_5_c10val/exp00_resume400_50/checkpoint_00050.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_5_c10val/exp00_resume400_50/test_results_0050_cifar101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-20 23:33:06] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_5_c10val/exp00/checkpoint_00400.pth\n",
      "Files already downloaded and verified\n",
      "100%|| 79/79 [00:36<00:00,  2.16it/s]\n",
      "\u001b[32m[2020-07-20 23:33:43] __main__ INFO: \u001b[0mElapsed 36.59\n",
      "\u001b[32m[2020-07-20 23:33:43] __main__ INFO: \u001b[0mLoss 0.4064 Accuracy 0.8837\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/resnext.yaml \\\n",
    "   model.resnext.cardinality 4 \\\n",
    "   test.batch_size 128 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_5_c10val/exp00/checkpoint_00400.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_5_c10val/exp00/test_results_0400_cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-20 23:33:58] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_5_c10val/exp00/checkpoint_00400.pth\n",
      "CIFAR 10.1\n",
      "100%|| 16/16 [00:07<00:00,  2.11it/s]\n",
      "\u001b[32m[2020-07-20 23:34:06] __main__ INFO: \u001b[0mElapsed 7.60\n",
      "\u001b[32m[2020-07-20 23:34:06] __main__ INFO: \u001b[0mLoss 0.8153 Accuracy 0.7810\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10.1 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "!python evaluate.py --config configs/cifar/resnext.yaml \\\n",
    "   model.resnext.cardinality 4 \\\n",
    "   test.batch_size 128 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_5_c10val/exp00/checkpoint_00400.pth \\\n",
    "   dataset.name CIFAR101 \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_5_c10val/exp00/test_results_0400_cifar101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Record the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Testset</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Original_Accuracy</th>\n",
       "      <th>Original_CI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>resnext_29_4x64d_ra_2_5_c10val</td>\n",
       "      <td>400</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.4064</td>\n",
       "      <td>0.8837</td>\n",
       "      <td>96.4</td>\n",
       "      <td>(96.0, 96.7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>resnext_29_4x64d_ra_2_5_c10val</td>\n",
       "      <td>400</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>0.8153</td>\n",
       "      <td>0.781</td>\n",
       "      <td>89.6</td>\n",
       "      <td>(88.2, 90.9)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>resnext_29_4x64d_ra_2_5_c10val_refined400</td>\n",
       "      <td>50</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>0.5403</td>\n",
       "      <td>0.854</td>\n",
       "      <td>89.6</td>\n",
       "      <td>(88.2, 90.9)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>resnext_29_4x64d_ra_2_5_c10val_refined400</td>\n",
       "      <td>50</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.2288</td>\n",
       "      <td>0.9314</td>\n",
       "      <td>96.4</td>\n",
       "      <td>(96.0, 96.7)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Model Epoch    Testset    Loss  \\\n",
       "0             resnext_29_4x64d_ra_2_5_c10val   400    cifar10  0.4064   \n",
       "1             resnext_29_4x64d_ra_2_5_c10val   400  cifar10.1  0.8153   \n",
       "2  resnext_29_4x64d_ra_2_5_c10val_refined400    50  cifar10.1  0.5403   \n",
       "3  resnext_29_4x64d_ra_2_5_c10val_refined400    50    cifar10  0.2288   \n",
       "\n",
       "  Accuracy  Original_Accuracy   Original_CI  \n",
       "0   0.8837               96.4  (96.0, 96.7)  \n",
       "1    0.781               89.6  (88.2, 90.9)  \n",
       "2    0.854               89.6  (88.2, 90.9)  \n",
       "3   0.9314               96.4  (96.0, 96.7)  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "model = 'resnext_29_4x64d_ra_2_5_c10val'\n",
    "model_refined = model + '_refined400'\n",
    "\n",
    "a = pd.Series([model, 400, 'cifar10', 0.4064, 0.8837])\n",
    "c = pd.Series([model, 400, 'cifar10.1', 0.8153, 0.7810])\n",
    "\n",
    "e = pd.Series([model_refined, 50, 'cifar10.1', 0.5403, 0.8540])\n",
    "f = pd.Series([model_refined, 50, 'cifar10', 0.2288, 0.9314])\n",
    "               \n",
    "df_results = pd.concat([a,c,e,f], axis=1).T\n",
    "df_results.columns = ['Model', 'Epoch', 'Testset', 'Loss', 'Accuracy']\n",
    "\n",
    "df_results['Original_Accuracy'] = df_results.apply((lambda row: 96.4 if row[2] == 'cifar10' else 89.6), axis=1)\n",
    "df_results['Original_CI'] = df_results.apply((lambda row: (96.0, 96.7) if row[2] == 'cifar10' else (88.2, 90.9)), axis=1)\n",
    "\n",
    "df_results.to_csv('/home/ec2-user/SageMaker/experiments/' + model + '/results.csv')\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the model checkpoints, configs, and results to S3 \n",
    "bucket='sagemaker-june29'\n",
    "prefix = 'sagemaker/results/original-models/resnext_29_4x64d_ra_2_5_c10val'\n",
    "path = '/home/ec2-user/SageMaker/experiments/resnext_29_4x64d_ra_2_5_c10val'\n",
    "\n",
    "s3_resource = boto3.resource(\"s3\", region_name=\"us-east-1\")\n",
    "\n",
    "def uploadDirectory(local_path,bucket_name,s3_prefix):\n",
    "\n",
    "    my_bucket = s3_resource.Bucket(bucket_name)\n",
    "    \n",
    "    for path, subdirs, files in os.walk(local_path):\n",
    "        path = path.replace(\"\\\\\",\"/\")\n",
    "        directory_name = path.replace(local_path,\"\")\n",
    "        for file in files:\n",
    "            #print(\"Local File:\", os.path.join(path, file))\n",
    "            #print(\"      Dest:\", s3_prefix+directory_name+'/'+file)\n",
    "            my_bucket.upload_file(os.path.join(path, file), s3_prefix+directory_name+'/'+file)\n",
    "    \n",
    "uploadDirectory(path,bucket,prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
