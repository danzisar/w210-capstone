{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import numpy \n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 1)) (1.15.4)\n",
      "Requirement already satisfied: torch>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 2)) (1.4.0)\n",
      "Requirement already satisfied: torchvision in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (0.5.0)\n",
      "Collecting fvcore (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4))\n",
      "  Downloading https://files.pythonhosted.org/packages/b5/df/ddfd507b0ea628eac743da5a0af4be8eb5b4ae84c27471bf63ef0add3b91/fvcore-0.1.1.post20200630.tar.gz\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 5)) (4.42.1)\n",
      "Collecting yacs (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 6))\n",
      "  Downloading https://files.pythonhosted.org/packages/81/3b/40e876afde9f5ffa1cfdce10565aba85b0dc2e067ed551dfb566cfee6d4d/yacs-0.1.7-py3-none-any.whl\n",
      "Collecting apex from git+https://github.com/NVIDIA/apex.git#egg=apex (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 7))\n",
      "  Cloning https://github.com/NVIDIA/apex.git to /tmp/pip-install-3qoyz7j5/apex\n",
      "Collecting termcolor (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 8))\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Collecting thop<0.0.31.post2004070130 (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 9))\n",
      "  Downloading https://files.pythonhosted.org/packages/de/dc/8ca7381a90a79fecc85f9c9e3e3914865067561c6b1b56f201f97842255e/thop-0.0.31.post2001170342-py3-none-any.whl\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (1.11.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (5.4.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (5.3.1)\n",
      "Collecting portalocker (from fvcore->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4))\n",
      "  Downloading https://files.pythonhosted.org/packages/53/84/7b3146ec6378d28abc73ab484f09f47dfa008ad6f03f33d90a369f880e25/portalocker-1.7.0-py2.py3-none-any.whl\n",
      "Collecting tabulate (from fvcore->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4))\n",
      "  Downloading https://files.pythonhosted.org/packages/c4/f4/770ae9385990f5a19a91431163d262182d3203662ea2b5739d0fcfc080f1/tabulate-0.8.7-py3-none-any.whl\n",
      "Building wheels for collected packages: fvcore, apex, termcolor\n",
      "  Running setup.py bdist_wheel for fvcore ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/80/eb/49/83b9d20a804f1b4b163d1c1451c670a2067a00175662516f01\n",
      "  Running setup.py bdist_wheel for apex ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-tovzmrhz/wheels/20/ef/9d/1967e1ee0ae20e7dc8e41ab7208017893b0a026243189508a3\n",
      "  Running setup.py bdist_wheel for termcolor ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "Successfully built fvcore apex termcolor\n",
      "\u001b[31mfastai 1.0.60 requires nvidia-ml-py3, which is not installed.\u001b[0m\n",
      "Installing collected packages: yacs, portalocker, termcolor, tabulate, fvcore, apex, thop\n",
      "Successfully installed apex-0.1 fvcore-0.1.1.post20200630 portalocker-1.7.0 tabulate-0.8.7 termcolor-1.1.0 thop-0.0.31.post2001170342 yacs-0.1.7\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.2b1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Sagemaker Notebook must be of type, conda_pytorch_p36\n",
    "\n",
    "!pip install -r '/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorboard\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/74/0a6fcb206dcc72a6da9a62dd81784bfdbff5fedb099982861dc2219014fb/tensorboard-2.2.2-py3-none-any.whl (3.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.0MB 11.2MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting grpcio>=1.24.3 (from tensorboard)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/29/1bd649737e427a6bb850174293b4f2b72ab80dd49462142db9b81e1e5c7b/grpcio-1.30.0.tar.gz (19.7MB)\n",
      "\u001b[K    100% |████████████████████████████████| 19.7MB 2.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard)\n",
      "  Downloading https://files.pythonhosted.org/packages/7b/b8/88def36e74bee9fce511c9519571f4e485e890093ab7442284f4ffaef60b/google_auth_oauthlib-0.4.1-py2.py3-none-any.whl\n",
      "Collecting google-auth<2,>=1.6.3 (from tensorboard)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/21/57/d706964a7e4056f3f2244e16705388c11631fbb53d3e2d2a2d0fbc24d470/google_auth-1.18.0-py2.py3-none-any.whl (90kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 29.6MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (0.14.1)\n",
      "Collecting absl-py>=0.4 (from tensorboard)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/53/9243c600e047bd4c3df9e69cfabc1e8004a82cac2e0c484580a78a94ba2a/absl-py-0.9.0.tar.gz (104kB)\n",
      "\u001b[K    100% |████████████████████████████████| 112kB 34.6MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting requests<3,>=2.21.0 (from tensorboard)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/1e/0c169c6a5381e241ba7404532c16a21d86ab872c9bed8bdcd4c423954103/requests-2.24.0-py2.py3-none-any.whl (61kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 31.1MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.11.0)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b6/85/5c5ac0a8c5efdfab916e9c6bc18963f6a6996a8a1e19ec4ad8c9ac9c623c/tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779kB)\n",
      "\u001b[K    100% |████████████████████████████████| 788kB 28.1MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting setuptools>=41.0.0 (from tensorboard)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/93/4860cebd5ad3ff2664ad3c966490ccb46e3b88458b2095145bca11727ca4/setuptools-47.3.1-py3-none-any.whl (582kB)\n",
      "\u001b[K    100% |████████████████████████████████| 583kB 30.9MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.12.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.15.4)\n",
      "Collecting markdown>=2.6.8 (from tensorboard)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/63/eaec2bd025ab48c754b55e8819af0f6a69e2b1e187611dd40cbbe101ee7f/Markdown-3.2.2-py3-none-any.whl (88kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 30.8MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting protobuf>=3.6.0 (from tensorboard)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/05/9867ef8eafd12265267bee138fa2c46ebf34a276ea4cbe184cba4c606e8b/protobuf-3.12.2-cp36-cp36m-manylinux1_x86_64.whl (1.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.3MB 23.5MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (0.31.1)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard)\n",
      "  Downloading https://files.pythonhosted.org/packages/a3/12/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379/requests_oauthlib-1.3.0-py2.py3-none-any.whl\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/de/214830a981892a3e286c3794f41ae67a4495df1108c3da8a9f62159b9a9d/pyasn1_modules-0.2.8-py2.py3-none-any.whl (155kB)\n",
      "\u001b[K    100% |████████████████████████████████| 163kB 37.2MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (3.4.2)\n",
      "Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard)\n",
      "  Downloading https://files.pythonhosted.org/packages/cd/5c/f3aa86b6d5482f3051b433c7616668a9b96fbe49a622210e2c9781938a5c/cachetools-4.1.1-py3-none-any.whl\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (2.6)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (1.23)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard) (1.5.0)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/05/57/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704/oauthlib-3.1.0-py2.py3-none-any.whl (147kB)\n",
      "\u001b[K    100% |████████████████████████████████| 153kB 36.5MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard) (0.4.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard) (3.0.0)\n",
      "Building wheels for collected packages: grpcio, absl-py\n",
      "  Running setup.py bdist_wheel for grpcio ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/1e/e9/13/2ce6c99171a977bad2f9a4bccfa596a6e7ea060b9fbff51bc3\n",
      "  Running setup.py bdist_wheel for absl-py ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/8e/28/49/fad4e7f0b9a1227708cbbee4487ac8558a7334849cb81c813d\n",
      "Successfully built grpcio absl-py\n",
      "\u001b[31mfastai 1.0.60 requires nvidia-ml-py3, which is not installed.\u001b[0m\n",
      "Installing collected packages: grpcio, pyasn1-modules, setuptools, cachetools, google-auth, requests, oauthlib, requests-oauthlib, google-auth-oauthlib, absl-py, tensorboard-plugin-wit, markdown, protobuf, tensorboard\n",
      "  Found existing installation: setuptools 39.1.0\n",
      "    Uninstalling setuptools-39.1.0:\n",
      "      Successfully uninstalled setuptools-39.1.0\n",
      "  Found existing installation: requests 2.20.0\n",
      "    Uninstalling requests-2.20.0:\n",
      "      Successfully uninstalled requests-2.20.0\n",
      "  Found existing installation: protobuf 3.5.2\n",
      "    Uninstalling protobuf-3.5.2:\n",
      "      Successfully uninstalled protobuf-3.5.2\n",
      "Successfully installed absl-py-0.9.0 cachetools-4.1.1 google-auth-1.18.0 google-auth-oauthlib-0.4.1 grpcio-1.30.0 markdown-3.2.2 oauthlib-3.1.0 protobuf-3.12.2 pyasn1-modules-0.2.8 requests-2.24.0 requests-oauthlib-1.3.0 setuptools-47.3.1 tensorboard-2.2.2 tensorboard-plugin-wit-1.7.0\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.2b1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Need to add this to requirements.txt\n",
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "end basic config, begin training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "models to be trained for RA_2_20:\n",
    "\n",
    "350 augmented, 100 unaugmented\n",
    "250 augmented, 200 unaugmented\n",
    "150 augmented, 300 unaugmented\n",
    "\n",
    "If these look promising, perhaps we break into 50s. This would require changing the yml files in order to have checkpoints to be every 50 instead of every 100 epochs\n",
    "\n",
    "first pipeline: 350 augmented, 100 unaugmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-02 19:25:38] __main__ INFO: \u001b[0mdevice: cuda\n",
      "cudnn:\n",
      "  benchmark: True\n",
      "  deterministic: False\n",
      "dataset:\n",
      "  name: CIFAR10_RA_2_20\n",
      "  dataset_dir: ''\n",
      "  image_size: 32\n",
      "  n_channels: 3\n",
      "  n_classes: 10\n",
      "model:\n",
      "  type: cifar\n",
      "  name: resnet\n",
      "  init_mode: kaiming_fan_out\n",
      "  vgg:\n",
      "    n_channels: [64, 128, 256, 512, 512]\n",
      "    n_layers: [2, 2, 3, 3, 3]\n",
      "    use_bn: True\n",
      "  resnet:\n",
      "    depth: 32\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "  resnet_preact:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "  wrn:\n",
      "    depth: 28\n",
      "    initial_channels: 16\n",
      "    widening_factor: 10\n",
      "    drop_rate: 0.0\n",
      "  densenet:\n",
      "    depth: 100\n",
      "    n_blocks: [6, 12, 24, 16]\n",
      "    block_type: bottleneck\n",
      "    growth_rate: 12\n",
      "    drop_rate: 0.0\n",
      "    compression_rate: 0.5\n",
      "  pyramidnet:\n",
      "    depth: 272\n",
      "    n_blocks: [3, 24, 36, 3]\n",
      "    initial_channels: 16\n",
      "    block_type: bottleneck\n",
      "    alpha: 200\n",
      "  resnext:\n",
      "    depth: 29\n",
      "    n_blocks: [3, 4, 6, 3]\n",
      "    initial_channels: 64\n",
      "    cardinality: 8\n",
      "    base_channels: 4\n",
      "  shake_shake:\n",
      "    depth: 26\n",
      "    initial_channels: 96\n",
      "    shake_forward: True\n",
      "    shake_backward: True\n",
      "    shake_image: True\n",
      "  se_resnet_preact:\n",
      "    depth: 110\n",
      "    initial_channels: 16\n",
      "    se_reduction: 16\n",
      "    block_type: basic\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "train:\n",
      "  checkpoint: ''\n",
      "  resume: False\n",
      "  precision: O0\n",
      "  batch_size: 128\n",
      "  subdivision: 1\n",
      "  optimizer: sgd\n",
      "  base_lr: 0.1\n",
      "  momentum: 0.9\n",
      "  nesterov: True\n",
      "  weight_decay: 0.0001\n",
      "  no_weight_decay_on_bn: False\n",
      "  gradient_clip: 0\n",
      "  start_epoch: 0\n",
      "  seed: 0\n",
      "  val_first: True\n",
      "  val_period: 1\n",
      "  val_ratio: 0.1\n",
      "  use_test_as_val: False\n",
      "  output_dir: /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/july2/exp00\n",
      "  log_period: 50\n",
      "  checkpoint_period: 50\n",
      "  use_tensorboard: True\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: True\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "  distributed: False\n",
      "  dist:\n",
      "    backend: nccl\n",
      "    init_method: env://\n",
      "    world_size: -1\n",
      "    node_rank: -1\n",
      "    local_rank: 0\n",
      "    use_sync_bn: False\n",
      "tensorboard:\n",
      "  train_images: False\n",
      "  val_images: False\n",
      "  model_params: False\n",
      "optim:\n",
      "  adam:\n",
      "    betas: (0.9, 0.999)\n",
      "  lars:\n",
      "    eps: 1e-09\n",
      "    threshold: 0.01\n",
      "  adabound:\n",
      "    betas: (0.9, 0.999)\n",
      "    final_lr: 0.1\n",
      "    gamma: 0.001\n",
      "scheduler:\n",
      "  epochs: 350\n",
      "  warmup:\n",
      "    type: none\n",
      "    epochs: 0\n",
      "    start_factor: 0.001\n",
      "    exponent: 4\n",
      "  type: multistep\n",
      "  milestones: [80, 120]\n",
      "  lr_decay: 0.1\n",
      "  lr_min_factor: 0.001\n",
      "  T0: 10\n",
      "  T_mul: 1.0\n",
      "validation:\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: False\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "augmentation:\n",
      "  use_random_crop: True\n",
      "  use_random_horizontal_flip: True\n",
      "  use_cutout: False\n",
      "  use_random_erasing: False\n",
      "  use_dual_cutout: False\n",
      "  use_mixup: False\n",
      "  use_ricap: False\n",
      "  use_cutmix: False\n",
      "  use_label_smoothing: False\n",
      "  random_crop:\n",
      "    padding: 4\n",
      "    fill: 0\n",
      "    padding_mode: constant\n",
      "  random_horizontal_flip:\n",
      "    prob: 0.5\n",
      "  cutout:\n",
      "    prob: 1.0\n",
      "    mask_size: 16\n",
      "    cut_inside: False\n",
      "    mask_color: 0\n",
      "    dual_cutout_alpha: 0.1\n",
      "  random_erasing:\n",
      "    prob: 0.5\n",
      "    area_ratio_range: [0.02, 0.4]\n",
      "    min_aspect_ratio: 0.3\n",
      "    max_attempt: 20\n",
      "  mixup:\n",
      "    alpha: 1.0\n",
      "  ricap:\n",
      "    beta: 0.3\n",
      "  cutmix:\n",
      "    alpha: 1.0\n",
      "  label_smoothing:\n",
      "    epsilon: 0.1\n",
      "tta:\n",
      "  use_resize: False\n",
      "  use_center_crop: False\n",
      "  resize: 256\n",
      "test:\n",
      "  checkpoint: ''\n",
      "  output_dir: ''\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    pin_memory: False\n",
      "\u001b[32m[2020-07-02 19:25:38] __main__ INFO: \u001b[0menv_info:\n",
      "  pytorch_version: 1.4.0\n",
      "  cuda_version: 10.1\n",
      "  cudnn_version: 7603\n",
      "  num_gpus: 1\n",
      "  gpu_name: Tesla K80\n",
      "  gpu_capability: 3.7\n",
      "(50000, 32, 32, 3)\n",
      "\u001b[32m[2020-07-02 19:26:07] __main__ INFO: \u001b[0mMACs  : 69.76M\n",
      "\u001b[32m[2020-07-02 19:26:07] __main__ INFO: \u001b[0m#params: 466.91K\n",
      "Selected optimization level O0:  Pure FP32 training.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\",)\n",
      "\u001b[32m[2020-07-02 19:26:07] __main__ INFO: \u001b[0mVal 0\n",
      "\u001b[32m[2020-07-02 19:26:09] __main__ INFO: \u001b[0mEpoch 0 loss 3472.9998 acc@1 0.1008 acc@5 0.5044\n",
      "\u001b[32m[2020-07-02 19:26:09] __main__ INFO: \u001b[0mElapsed 1.55\n",
      "\u001b[32m[2020-07-02 19:26:09] __main__ INFO: \u001b[0mTrain 1 0\n",
      "\u001b[32m[2020-07-02 19:26:14] __main__ INFO: \u001b[0mEpoch 1 Step 50/351 lr 0.100000 loss 2.3290 (3.1816) acc@1 0.1484 (0.1014) acc@5 0.5391 (0.5039)\n",
      "\u001b[32m[2020-07-02 19:26:18] __main__ INFO: \u001b[0mEpoch 1 Step 100/351 lr 0.100000 loss 2.3168 (2.7576) acc@1 0.0859 (0.1007) acc@5 0.5625 (0.5018)\n",
      "\u001b[32m[2020-07-02 19:26:23] __main__ INFO: \u001b[0mEpoch 1 Step 150/351 lr 0.100000 loss 2.3037 (2.6114) acc@1 0.1172 (0.1003) acc@5 0.5078 (0.5008)\n",
      "\u001b[32m[2020-07-02 19:26:28] __main__ INFO: \u001b[0mEpoch 1 Step 200/351 lr 0.100000 loss 2.3065 (2.5372) acc@1 0.0781 (0.1011) acc@5 0.4609 (0.4986)\n",
      "\u001b[32m[2020-07-02 19:26:32] __main__ INFO: \u001b[0mEpoch 1 Step 250/351 lr 0.100000 loss 2.3151 (2.4926) acc@1 0.1094 (0.0995) acc@5 0.5234 (0.4970)\n",
      "\u001b[32m[2020-07-02 19:26:37] __main__ INFO: \u001b[0mEpoch 1 Step 300/351 lr 0.100000 loss 2.3608 (2.4622) acc@1 0.0938 (0.0998) acc@5 0.4531 (0.4974)\n",
      "\u001b[32m[2020-07-02 19:26:42] __main__ INFO: \u001b[0mEpoch 1 Step 350/351 lr 0.100000 loss 2.3065 (2.4406) acc@1 0.1172 (0.0998) acc@5 0.5938 (0.4987)\n",
      "\u001b[32m[2020-07-02 19:26:42] __main__ INFO: \u001b[0mEpoch 1 Step 351/351 lr 0.100000 loss 2.3049 (2.4402) acc@1 0.0703 (0.0997) acc@5 0.5234 (0.4988)\n",
      "\u001b[32m[2020-07-02 19:26:42] __main__ INFO: \u001b[0mElapsed 33.16\n",
      "\u001b[32m[2020-07-02 19:26:42] __main__ INFO: \u001b[0mVal 1\n",
      "\u001b[32m[2020-07-02 19:26:43] __main__ INFO: \u001b[0mEpoch 1 loss 2.3059 acc@1 0.0940 acc@5 0.4956\n",
      "\u001b[32m[2020-07-02 19:26:43] __main__ INFO: \u001b[0mElapsed 1.05\n",
      "\u001b[32m[2020-07-02 19:26:43] __main__ INFO: \u001b[0mTrain 2 351\n",
      "\u001b[32m[2020-07-02 19:26:48] __main__ INFO: \u001b[0mEpoch 2 Step 50/351 lr 0.100000 loss 2.3206 (2.3090) acc@1 0.1094 (0.0991) acc@5 0.4922 (0.5048)\n",
      "\u001b[32m[2020-07-02 19:26:53] __main__ INFO: \u001b[0mEpoch 2 Step 100/351 lr 0.100000 loss 2.3094 (2.3080) acc@1 0.0547 (0.1027) acc@5 0.5312 (0.5088)\n",
      "\u001b[32m[2020-07-02 19:26:57] __main__ INFO: \u001b[0mEpoch 2 Step 150/351 lr 0.100000 loss 2.2962 (2.3081) acc@1 0.0781 (0.1034) acc@5 0.4844 (0.5099)\n",
      "\u001b[32m[2020-07-02 19:27:02] __main__ INFO: \u001b[0mEpoch 2 Step 200/351 lr 0.100000 loss 2.3099 (2.3078) acc@1 0.0781 (0.1016) acc@5 0.4609 (0.5071)\n",
      "\u001b[32m[2020-07-02 19:27:07] __main__ INFO: \u001b[0mEpoch 2 Step 250/351 lr 0.100000 loss 2.3116 (2.3078) acc@1 0.1016 (0.1012) acc@5 0.5078 (0.5073)\n",
      "\u001b[32m[2020-07-02 19:27:11] __main__ INFO: \u001b[0mEpoch 2 Step 300/351 lr 0.100000 loss 2.3121 (2.3076) acc@1 0.0391 (0.1007) acc@5 0.3984 (0.5063)\n",
      "\u001b[32m[2020-07-02 19:27:16] __main__ INFO: \u001b[0mEpoch 2 Step 350/351 lr 0.100000 loss 2.3106 (2.3074) acc@1 0.1172 (0.1008) acc@5 0.5078 (0.5069)\n",
      "\u001b[32m[2020-07-02 19:27:16] __main__ INFO: \u001b[0mEpoch 2 Step 351/351 lr 0.100000 loss 2.3125 (2.3074) acc@1 0.0859 (0.1007) acc@5 0.4922 (0.5068)\n",
      "\u001b[32m[2020-07-02 19:27:16] __main__ INFO: \u001b[0mElapsed 33.08\n",
      "\u001b[32m[2020-07-02 19:27:16] __main__ INFO: \u001b[0mVal 2\n",
      "\u001b[32m[2020-07-02 19:27:17] __main__ INFO: \u001b[0mEpoch 2 loss 2.3068 acc@1 0.1028 acc@5 0.4976\n",
      "\u001b[32m[2020-07-02 19:27:17] __main__ INFO: \u001b[0mElapsed 1.04\n",
      "\u001b[32m[2020-07-02 19:27:17] __main__ INFO: \u001b[0mTrain 3 702\n",
      "\u001b[32m[2020-07-02 19:27:22] __main__ INFO: \u001b[0mEpoch 3 Step 50/351 lr 0.100000 loss 2.3083 (2.3055) acc@1 0.0781 (0.1050) acc@5 0.5391 (0.5050)\n",
      "\u001b[32m[2020-07-02 19:27:27] __main__ INFO: \u001b[0mEpoch 3 Step 100/351 lr 0.100000 loss 2.3188 (2.3057) acc@1 0.0703 (0.1023) acc@5 0.4688 (0.5002)\n",
      "\u001b[32m[2020-07-02 19:27:31] __main__ INFO: \u001b[0mEpoch 3 Step 150/351 lr 0.100000 loss 2.3039 (2.3058) acc@1 0.1562 (0.1013) acc@5 0.4453 (0.5009)\n",
      "\u001b[32m[2020-07-02 19:27:36] __main__ INFO: \u001b[0mEpoch 3 Step 200/351 lr 0.100000 loss 2.3042 (2.3055) acc@1 0.1406 (0.1015) acc@5 0.5312 (0.5017)\n",
      "\u001b[32m[2020-07-02 19:27:41] __main__ INFO: \u001b[0mEpoch 3 Step 250/351 lr 0.100000 loss 2.3042 (2.3056) acc@1 0.1328 (0.1007) acc@5 0.5156 (0.5018)\n",
      "\u001b[32m[2020-07-02 19:27:46] __main__ INFO: \u001b[0mEpoch 3 Step 300/351 lr 0.100000 loss 2.3108 (2.3055) acc@1 0.0469 (0.1009) acc@5 0.4453 (0.5026)\n",
      "\u001b[32m[2020-07-02 19:27:50] __main__ INFO: \u001b[0mEpoch 3 Step 350/351 lr 0.100000 loss 2.3029 (2.3056) acc@1 0.0703 (0.1003) acc@5 0.4375 (0.5017)\n",
      "\u001b[32m[2020-07-02 19:27:50] __main__ INFO: \u001b[0mEpoch 3 Step 351/351 lr 0.100000 loss 2.3076 (2.3056) acc@1 0.1016 (0.1003) acc@5 0.5078 (0.5017)\n",
      "\u001b[32m[2020-07-02 19:27:50] __main__ INFO: \u001b[0mElapsed 33.23\n",
      "\u001b[32m[2020-07-02 19:27:50] __main__ INFO: \u001b[0mVal 3\n",
      "\u001b[32m[2020-07-02 19:27:51] __main__ INFO: \u001b[0mEpoch 3 loss 2.3038 acc@1 0.1026 acc@5 0.5006\n",
      "\u001b[32m[2020-07-02 19:27:51] __main__ INFO: \u001b[0mElapsed 1.04\n",
      "\u001b[32m[2020-07-02 19:27:51] __main__ INFO: \u001b[0mTrain 4 1053\n",
      "\u001b[32m[2020-07-02 19:27:56] __main__ INFO: \u001b[0mEpoch 4 Step 50/351 lr 0.100000 loss 2.2964 (2.3045) acc@1 0.1250 (0.0938) acc@5 0.5781 (0.5011)\n",
      "\u001b[32m[2020-07-02 19:28:01] __main__ INFO: \u001b[0mEpoch 4 Step 100/351 lr 0.100000 loss 2.3082 (2.3038) acc@1 0.1016 (0.0998) acc@5 0.5156 (0.5053)\n",
      "\u001b[32m[2020-07-02 19:28:06] __main__ INFO: \u001b[0mEpoch 4 Step 150/351 lr 0.100000 loss 2.3122 (2.3041) acc@1 0.0625 (0.0997) acc@5 0.4766 (0.5018)\n",
      "\u001b[32m[2020-07-02 19:28:10] __main__ INFO: \u001b[0mEpoch 4 Step 200/351 lr 0.100000 loss 2.3054 (2.3040) acc@1 0.1172 (0.1011) acc@5 0.5312 (0.5017)\n",
      "\u001b[32m[2020-07-02 19:28:15] __main__ INFO: \u001b[0mEpoch 4 Step 250/351 lr 0.100000 loss 2.2916 (2.3038) acc@1 0.0859 (0.1020) acc@5 0.5078 (0.5024)\n",
      "\u001b[32m[2020-07-02 19:28:20] __main__ INFO: \u001b[0mEpoch 4 Step 300/351 lr 0.100000 loss 2.3127 (2.3040) acc@1 0.1094 (0.1012) acc@5 0.4531 (0.5019)\n",
      "\u001b[32m[2020-07-02 19:28:25] __main__ INFO: \u001b[0mEpoch 4 Step 350/351 lr 0.100000 loss 2.3017 (2.3041) acc@1 0.0859 (0.1019) acc@5 0.5547 (0.5016)\n",
      "\u001b[32m[2020-07-02 19:28:25] __main__ INFO: \u001b[0mEpoch 4 Step 351/351 lr 0.100000 loss 2.2996 (2.3041) acc@1 0.0781 (0.1018) acc@5 0.4922 (0.5016)\n",
      "\u001b[32m[2020-07-02 19:28:25] __main__ INFO: \u001b[0mElapsed 33.22\n",
      "\u001b[32m[2020-07-02 19:28:25] __main__ INFO: \u001b[0mVal 4\n",
      "\u001b[32m[2020-07-02 19:28:26] __main__ INFO: \u001b[0mEpoch 4 loss 2.3059 acc@1 0.1032 acc@5 0.4982\n",
      "\u001b[32m[2020-07-02 19:28:26] __main__ INFO: \u001b[0mElapsed 1.07\n",
      "\u001b[32m[2020-07-02 19:28:26] __main__ INFO: \u001b[0mTrain 5 1404\n",
      "\u001b[32m[2020-07-02 19:28:31] __main__ INFO: \u001b[0mEpoch 5 Step 50/351 lr 0.100000 loss 2.3137 (2.3058) acc@1 0.1172 (0.1045) acc@5 0.4844 (0.5161)\n",
      "\u001b[32m[2020-07-02 19:28:35] __main__ INFO: \u001b[0mEpoch 5 Step 100/351 lr 0.100000 loss 2.2959 (2.3044) acc@1 0.1016 (0.1026) acc@5 0.5625 (0.5138)\n",
      "\u001b[32m[2020-07-02 19:28:40] __main__ INFO: \u001b[0mEpoch 5 Step 150/351 lr 0.100000 loss 2.3052 (2.3042) acc@1 0.0781 (0.1022) acc@5 0.5156 (0.5123)\n",
      "\u001b[32m[2020-07-02 19:28:45] __main__ INFO: \u001b[0mEpoch 5 Step 200/351 lr 0.100000 loss 2.3134 (2.3042) acc@1 0.1094 (0.1009) acc@5 0.5000 (0.5080)\n",
      "\u001b[32m[2020-07-02 19:28:49] __main__ INFO: \u001b[0mEpoch 5 Step 250/351 lr 0.100000 loss 2.3079 (2.3039) acc@1 0.0859 (0.1007) acc@5 0.4141 (0.5063)\n",
      "\u001b[32m[2020-07-02 19:28:54] __main__ INFO: \u001b[0mEpoch 5 Step 300/351 lr 0.100000 loss 2.3002 (2.3037) acc@1 0.1094 (0.1007) acc@5 0.4688 (0.5059)\n",
      "\u001b[32m[2020-07-02 19:28:59] __main__ INFO: \u001b[0mEpoch 5 Step 350/351 lr 0.100000 loss 2.3032 (2.3034) acc@1 0.1016 (0.1015) acc@5 0.5156 (0.5076)\n",
      "\u001b[32m[2020-07-02 19:28:59] __main__ INFO: \u001b[0mEpoch 5 Step 351/351 lr 0.100000 loss 2.3114 (2.3034) acc@1 0.0938 (0.1015) acc@5 0.5391 (0.5077)\n",
      "\u001b[32m[2020-07-02 19:28:59] __main__ INFO: \u001b[0mElapsed 33.28\n",
      "\u001b[32m[2020-07-02 19:28:59] __main__ INFO: \u001b[0mVal 5\n",
      "\u001b[32m[2020-07-02 19:29:00] __main__ INFO: \u001b[0mEpoch 5 loss 2.3125 acc@1 0.1014 acc@5 0.5056\n",
      "\u001b[32m[2020-07-02 19:29:00] __main__ INFO: \u001b[0mElapsed 1.05\n",
      "\u001b[32m[2020-07-02 19:29:00] __main__ INFO: \u001b[0mTrain 6 1755\n",
      "\u001b[32m[2020-07-02 19:29:05] __main__ INFO: \u001b[0mEpoch 6 Step 50/351 lr 0.100000 loss 2.3006 (2.3016) acc@1 0.0703 (0.0994) acc@5 0.5625 (0.5119)\n",
      "\u001b[32m[2020-07-02 19:29:10] __main__ INFO: \u001b[0mEpoch 6 Step 100/351 lr 0.100000 loss 2.2978 (2.3018) acc@1 0.1172 (0.0993) acc@5 0.5078 (0.5109)\n",
      "\u001b[32m[2020-07-02 19:29:14] __main__ INFO: \u001b[0mEpoch 6 Step 150/351 lr 0.100000 loss 2.2907 (2.3021) acc@1 0.1328 (0.1014) acc@5 0.5156 (0.5072)\n",
      "\u001b[32m[2020-07-02 19:29:19] __main__ INFO: \u001b[0mEpoch 6 Step 200/351 lr 0.100000 loss 2.3063 (2.3021) acc@1 0.1094 (0.1029) acc@5 0.4219 (0.5062)\n",
      "\u001b[32m[2020-07-02 19:29:24] __main__ INFO: \u001b[0mEpoch 6 Step 250/351 lr 0.100000 loss 2.2986 (2.3020) acc@1 0.0859 (0.1030) acc@5 0.5234 (0.5069)\n",
      "\u001b[32m[2020-07-02 19:29:29] __main__ INFO: \u001b[0mEpoch 6 Step 300/351 lr 0.100000 loss 2.2902 (2.3019) acc@1 0.1094 (0.1030) acc@5 0.5703 (0.5077)\n",
      "\u001b[32m[2020-07-02 19:29:33] __main__ INFO: \u001b[0mEpoch 6 Step 350/351 lr 0.100000 loss 2.2976 (2.3020) acc@1 0.1016 (0.1038) acc@5 0.5078 (0.5091)\n",
      "\u001b[32m[2020-07-02 19:29:33] __main__ INFO: \u001b[0mEpoch 6 Step 351/351 lr 0.100000 loss 2.3002 (2.3020) acc@1 0.1016 (0.1038) acc@5 0.5234 (0.5092)\n",
      "\u001b[32m[2020-07-02 19:29:33] __main__ INFO: \u001b[0mElapsed 33.39\n",
      "\u001b[32m[2020-07-02 19:29:33] __main__ INFO: \u001b[0mVal 6\n",
      "\u001b[32m[2020-07-02 19:29:34] __main__ INFO: \u001b[0mEpoch 6 loss 2.3017 acc@1 0.0974 acc@5 0.4986\n",
      "\u001b[32m[2020-07-02 19:29:34] __main__ INFO: \u001b[0mElapsed 1.05\n",
      "\u001b[32m[2020-07-02 19:29:34] __main__ INFO: \u001b[0mTrain 7 2106\n",
      "\u001b[32m[2020-07-02 19:29:39] __main__ INFO: \u001b[0mEpoch 7 Step 50/351 lr 0.100000 loss 2.3027 (2.3019) acc@1 0.1562 (0.1031) acc@5 0.4609 (0.5070)\n",
      "\u001b[32m[2020-07-02 19:29:44] __main__ INFO: \u001b[0mEpoch 7 Step 100/351 lr 0.100000 loss 2.2867 (2.3015) acc@1 0.1719 (0.1071) acc@5 0.5234 (0.5130)\n",
      "\u001b[32m[2020-07-02 19:29:49] __main__ INFO: \u001b[0mEpoch 7 Step 150/351 lr 0.100000 loss 2.2912 (2.3014) acc@1 0.1328 (0.1092) acc@5 0.5859 (0.5131)\n",
      "\u001b[32m[2020-07-02 19:29:54] __main__ INFO: \u001b[0mEpoch 7 Step 200/351 lr 0.100000 loss 2.2981 (2.3014) acc@1 0.0781 (0.1071) acc@5 0.5234 (0.5127)\n",
      "\u001b[32m[2020-07-02 19:29:58] __main__ INFO: \u001b[0mEpoch 7 Step 250/351 lr 0.100000 loss 2.2992 (2.3014) acc@1 0.0781 (0.1070) acc@5 0.5156 (0.5104)\n",
      "\u001b[32m[2020-07-02 19:30:03] __main__ INFO: \u001b[0mEpoch 7 Step 300/351 lr 0.100000 loss 2.3083 (2.3012) acc@1 0.0938 (0.1071) acc@5 0.5078 (0.5108)\n",
      "\u001b[32m[2020-07-02 19:30:08] __main__ INFO: \u001b[0mEpoch 7 Step 350/351 lr 0.100000 loss 2.3045 (2.3011) acc@1 0.0859 (0.1067) acc@5 0.5078 (0.5111)\n",
      "\u001b[32m[2020-07-02 19:30:08] __main__ INFO: \u001b[0mEpoch 7 Step 351/351 lr 0.100000 loss 2.2937 (2.3010) acc@1 0.0938 (0.1067) acc@5 0.4844 (0.5110)\n",
      "\u001b[32m[2020-07-02 19:30:08] __main__ INFO: \u001b[0mElapsed 33.35\n",
      "\u001b[32m[2020-07-02 19:30:08] __main__ INFO: \u001b[0mVal 7\n",
      "\u001b[32m[2020-07-02 19:30:09] __main__ INFO: \u001b[0mEpoch 7 loss 2.2998 acc@1 0.1126 acc@5 0.5150\n",
      "\u001b[32m[2020-07-02 19:30:09] __main__ INFO: \u001b[0mElapsed 1.07\n",
      "\u001b[32m[2020-07-02 19:30:09] __main__ INFO: \u001b[0mTrain 8 2457\n",
      "\u001b[32m[2020-07-02 19:30:14] __main__ INFO: \u001b[0mEpoch 8 Step 50/351 lr 0.100000 loss 2.2970 (2.2995) acc@1 0.0938 (0.1075) acc@5 0.5078 (0.5239)\n",
      "\u001b[32m[2020-07-02 19:30:19] __main__ INFO: \u001b[0mEpoch 8 Step 100/351 lr 0.100000 loss 2.2965 (2.3000) acc@1 0.1172 (0.1084) acc@5 0.5547 (0.5233)\n",
      "\u001b[32m[2020-07-02 19:30:23] __main__ INFO: \u001b[0mEpoch 8 Step 150/351 lr 0.100000 loss 2.2854 (2.3004) acc@1 0.0938 (0.1054) acc@5 0.5469 (0.5196)\n",
      "\u001b[32m[2020-07-02 19:30:28] __main__ INFO: \u001b[0mEpoch 8 Step 200/351 lr 0.100000 loss 2.2946 (2.3005) acc@1 0.1016 (0.1052) acc@5 0.5078 (0.5182)\n",
      "\u001b[32m[2020-07-02 19:30:33] __main__ INFO: \u001b[0mEpoch 8 Step 250/351 lr 0.100000 loss 2.2921 (2.3005) acc@1 0.1172 (0.1068) acc@5 0.5234 (0.5173)\n",
      "\u001b[32m[2020-07-02 19:30:37] __main__ INFO: \u001b[0mEpoch 8 Step 300/351 lr 0.100000 loss 2.3000 (2.3002) acc@1 0.1562 (0.1064) acc@5 0.5469 (0.5176)\n",
      "\u001b[32m[2020-07-02 19:30:42] __main__ INFO: \u001b[0mEpoch 8 Step 350/351 lr 0.100000 loss 2.2846 (2.2997) acc@1 0.1250 (0.1070) acc@5 0.5938 (0.5177)\n",
      "\u001b[32m[2020-07-02 19:30:42] __main__ INFO: \u001b[0mEpoch 8 Step 351/351 lr 0.100000 loss 2.2991 (2.2997) acc@1 0.1016 (0.1070) acc@5 0.4531 (0.5176)\n",
      "\u001b[32m[2020-07-02 19:30:42] __main__ INFO: \u001b[0mElapsed 33.37\n",
      "\u001b[32m[2020-07-02 19:30:42] __main__ INFO: \u001b[0mVal 8\n",
      "\u001b[32m[2020-07-02 19:30:43] __main__ INFO: \u001b[0mEpoch 8 loss 2.3055 acc@1 0.1050 acc@5 0.5086\n",
      "\u001b[32m[2020-07-02 19:30:43] __main__ INFO: \u001b[0mElapsed 1.06\n",
      "\u001b[32m[2020-07-02 19:30:43] __main__ INFO: \u001b[0mTrain 9 2808\n",
      "\u001b[32m[2020-07-02 19:30:48] __main__ INFO: \u001b[0mEpoch 9 Step 50/351 lr 0.100000 loss 2.3170 (2.3002) acc@1 0.0781 (0.1073) acc@5 0.4453 (0.5123)\n",
      "\u001b[32m[2020-07-02 19:30:53] __main__ INFO: \u001b[0mEpoch 9 Step 100/351 lr 0.100000 loss 2.2896 (2.2997) acc@1 0.1719 (0.1098) acc@5 0.5156 (0.5148)\n",
      "\u001b[32m[2020-07-02 19:30:58] __main__ INFO: \u001b[0mEpoch 9 Step 150/351 lr 0.100000 loss 2.3220 (2.3001) acc@1 0.1094 (0.1090) acc@5 0.4219 (0.5136)\n",
      "\u001b[32m[2020-07-02 19:31:02] __main__ INFO: \u001b[0mEpoch 9 Step 200/351 lr 0.100000 loss 2.2856 (2.2994) acc@1 0.1016 (0.1102) acc@5 0.5703 (0.5158)\n",
      "\u001b[32m[2020-07-02 19:31:07] __main__ INFO: \u001b[0mEpoch 9 Step 250/351 lr 0.100000 loss 2.2961 (2.2991) acc@1 0.0938 (0.1092) acc@5 0.4609 (0.5164)\n",
      "\u001b[32m[2020-07-02 19:31:12] __main__ INFO: \u001b[0mEpoch 9 Step 300/351 lr 0.100000 loss 2.2964 (2.2982) acc@1 0.1328 (0.1109) acc@5 0.5781 (0.5188)\n",
      "\u001b[32m[2020-07-02 19:31:17] __main__ INFO: \u001b[0mEpoch 9 Step 350/351 lr 0.100000 loss 2.3119 (2.2984) acc@1 0.1250 (0.1099) acc@5 0.4688 (0.5190)\n",
      "\u001b[32m[2020-07-02 19:31:17] __main__ INFO: \u001b[0mEpoch 9 Step 351/351 lr 0.100000 loss 2.2922 (2.2984) acc@1 0.1328 (0.1100) acc@5 0.5156 (0.5189)\n",
      "\u001b[32m[2020-07-02 19:31:17] __main__ INFO: \u001b[0mElapsed 33.45\n",
      "\u001b[32m[2020-07-02 19:31:17] __main__ INFO: \u001b[0mVal 9\n",
      "\u001b[32m[2020-07-02 19:31:18] __main__ INFO: \u001b[0mEpoch 9 loss 2.3026 acc@1 0.1058 acc@5 0.5162\n",
      "\u001b[32m[2020-07-02 19:31:18] __main__ INFO: \u001b[0mElapsed 1.05\n",
      "\u001b[32m[2020-07-02 19:31:18] __main__ INFO: \u001b[0mTrain 10 3159\n",
      "\u001b[32m[2020-07-02 19:31:23] __main__ INFO: \u001b[0mEpoch 10 Step 50/351 lr 0.100000 loss 2.3047 (2.2999) acc@1 0.1406 (0.1075) acc@5 0.4766 (0.5127)\n",
      "\u001b[32m[2020-07-02 19:31:27] __main__ INFO: \u001b[0mEpoch 10 Step 100/351 lr 0.100000 loss 2.3012 (2.3000) acc@1 0.1406 (0.1078) acc@5 0.4922 (0.5164)\n",
      "\u001b[32m[2020-07-02 19:31:32] __main__ INFO: \u001b[0mEpoch 10 Step 150/351 lr 0.100000 loss 2.2895 (2.2979) acc@1 0.0859 (0.1112) acc@5 0.5312 (0.5191)\n",
      "\u001b[32m[2020-07-02 19:31:37] __main__ INFO: \u001b[0mEpoch 10 Step 200/351 lr 0.100000 loss 2.3097 (2.2974) acc@1 0.0938 (0.1106) acc@5 0.4766 (0.5168)\n",
      "\u001b[32m[2020-07-02 19:31:42] __main__ INFO: \u001b[0mEpoch 10 Step 250/351 lr 0.100000 loss 2.2971 (2.2977) acc@1 0.1016 (0.1098) acc@5 0.5469 (0.5194)\n",
      "\u001b[32m[2020-07-02 19:31:46] __main__ INFO: \u001b[0mEpoch 10 Step 300/351 lr 0.100000 loss 2.3054 (2.2974) acc@1 0.0859 (0.1107) acc@5 0.5469 (0.5198)\n",
      "\u001b[32m[2020-07-02 19:31:51] __main__ INFO: \u001b[0mEpoch 10 Step 350/351 lr 0.100000 loss 2.3068 (2.2967) acc@1 0.0859 (0.1108) acc@5 0.5078 (0.5228)\n",
      "\u001b[32m[2020-07-02 19:31:51] __main__ INFO: \u001b[0mEpoch 10 Step 351/351 lr 0.100000 loss 2.2690 (2.2966) acc@1 0.1406 (0.1108) acc@5 0.6406 (0.5232)\n",
      "\u001b[32m[2020-07-02 19:31:51] __main__ INFO: \u001b[0mElapsed 33.43\n",
      "\u001b[32m[2020-07-02 19:31:51] __main__ INFO: \u001b[0mVal 10\n",
      "\u001b[32m[2020-07-02 19:31:52] __main__ INFO: \u001b[0mEpoch 10 loss 2.2948 acc@1 0.1152 acc@5 0.5340\n",
      "\u001b[32m[2020-07-02 19:31:52] __main__ INFO: \u001b[0mElapsed 1.05\n",
      "\u001b[32m[2020-07-02 19:31:52] __main__ INFO: \u001b[0mTrain 11 3510\n",
      "\u001b[32m[2020-07-02 19:31:57] __main__ INFO: \u001b[0mEpoch 11 Step 50/351 lr 0.100000 loss 2.3050 (2.2929) acc@1 0.1094 (0.1184) acc@5 0.4844 (0.5341)\n",
      "\u001b[32m[2020-07-02 19:32:02] __main__ INFO: \u001b[0mEpoch 11 Step 100/351 lr 0.100000 loss 2.2655 (2.2921) acc@1 0.1328 (0.1184) acc@5 0.5625 (0.5363)\n",
      "\u001b[32m[2020-07-02 19:32:07] __main__ INFO: \u001b[0mEpoch 11 Step 150/351 lr 0.100000 loss 2.2604 (2.2905) acc@1 0.1484 (0.1172) acc@5 0.5859 (0.5398)\n",
      "\u001b[32m[2020-07-02 19:32:12] __main__ INFO: \u001b[0mEpoch 11 Step 200/351 lr 0.100000 loss 2.3127 (2.2885) acc@1 0.1250 (0.1184) acc@5 0.4922 (0.5432)\n",
      "\u001b[32m[2020-07-02 19:32:16] __main__ INFO: \u001b[0mEpoch 11 Step 250/351 lr 0.100000 loss 2.2713 (2.2867) acc@1 0.1797 (0.1200) acc@5 0.5703 (0.5448)\n",
      "\u001b[32m[2020-07-02 19:32:21] __main__ INFO: \u001b[0mEpoch 11 Step 300/351 lr 0.100000 loss 2.2443 (2.2854) acc@1 0.1562 (0.1205) acc@5 0.5781 (0.5466)\n",
      "\u001b[32m[2020-07-02 19:32:26] __main__ INFO: \u001b[0mEpoch 11 Step 350/351 lr 0.100000 loss 2.2415 (2.2850) acc@1 0.1328 (0.1212) acc@5 0.6016 (0.5485)\n",
      "\u001b[32m[2020-07-02 19:32:26] __main__ INFO: \u001b[0mEpoch 11 Step 351/351 lr 0.100000 loss 2.2877 (2.2850) acc@1 0.0781 (0.1210) acc@5 0.5703 (0.5485)\n",
      "\u001b[32m[2020-07-02 19:32:26] __main__ INFO: \u001b[0mElapsed 33.60\n",
      "\u001b[32m[2020-07-02 19:32:26] __main__ INFO: \u001b[0mVal 11\n",
      "\u001b[32m[2020-07-02 19:32:27] __main__ INFO: \u001b[0mEpoch 11 loss 2.2793 acc@1 0.1222 acc@5 0.5496\n",
      "\u001b[32m[2020-07-02 19:32:27] __main__ INFO: \u001b[0mElapsed 1.05\n",
      "\u001b[32m[2020-07-02 19:32:27] __main__ INFO: \u001b[0mTrain 12 3861\n",
      "\u001b[32m[2020-07-02 19:32:32] __main__ INFO: \u001b[0mEpoch 12 Step 50/351 lr 0.100000 loss 2.2719 (2.2759) acc@1 0.1016 (0.1244) acc@5 0.6250 (0.5600)\n",
      "\u001b[32m[2020-07-02 19:32:37] __main__ INFO: \u001b[0mEpoch 12 Step 100/351 lr 0.100000 loss 2.2966 (2.2747) acc@1 0.1328 (0.1238) acc@5 0.5391 (0.5566)\n",
      "\u001b[32m[2020-07-02 19:32:41] __main__ INFO: \u001b[0mEpoch 12 Step 150/351 lr 0.100000 loss 2.2460 (2.2712) acc@1 0.1172 (0.1247) acc@5 0.6250 (0.5597)\n",
      "\u001b[32m[2020-07-02 19:32:46] __main__ INFO: \u001b[0mEpoch 12 Step 200/351 lr 0.100000 loss 2.2953 (2.2720) acc@1 0.1094 (0.1267) acc@5 0.5703 (0.5600)\n",
      "\u001b[32m[2020-07-02 19:32:51] __main__ INFO: \u001b[0mEpoch 12 Step 250/351 lr 0.100000 loss 2.2923 (2.2716) acc@1 0.0859 (0.1268) acc@5 0.5547 (0.5613)\n",
      "\u001b[32m[2020-07-02 19:32:56] __main__ INFO: \u001b[0mEpoch 12 Step 300/351 lr 0.100000 loss 2.2079 (2.2704) acc@1 0.2344 (0.1268) acc@5 0.5859 (0.5648)\n",
      "\u001b[32m[2020-07-02 19:33:00] __main__ INFO: \u001b[0mEpoch 12 Step 350/351 lr 0.100000 loss 2.2406 (2.2683) acc@1 0.1250 (0.1277) acc@5 0.5938 (0.5673)\n",
      "\u001b[32m[2020-07-02 19:33:00] __main__ INFO: \u001b[0mEpoch 12 Step 351/351 lr 0.100000 loss 2.2609 (2.2683) acc@1 0.1172 (0.1276) acc@5 0.5781 (0.5674)\n",
      "\u001b[32m[2020-07-02 19:33:00] __main__ INFO: \u001b[0mElapsed 33.47\n",
      "\u001b[32m[2020-07-02 19:33:00] __main__ INFO: \u001b[0mVal 12\n",
      "\u001b[32m[2020-07-02 19:33:01] __main__ INFO: \u001b[0mEpoch 12 loss 2.2664 acc@1 0.1288 acc@5 0.5742\n",
      "\u001b[32m[2020-07-02 19:33:01] __main__ INFO: \u001b[0mElapsed 1.05\n",
      "\u001b[32m[2020-07-02 19:33:01] __main__ INFO: \u001b[0mTrain 13 4212\n",
      "\u001b[32m[2020-07-02 19:33:06] __main__ INFO: \u001b[0mEpoch 13 Step 50/351 lr 0.100000 loss 2.2599 (2.2596) acc@1 0.1094 (0.1327) acc@5 0.6094 (0.5805)\n",
      "\u001b[32m[2020-07-02 19:33:11] __main__ INFO: \u001b[0mEpoch 13 Step 100/351 lr 0.100000 loss 2.2873 (2.2618) acc@1 0.1094 (0.1288) acc@5 0.5625 (0.5739)\n",
      "\u001b[32m[2020-07-02 19:33:16] __main__ INFO: \u001b[0mEpoch 13 Step 150/351 lr 0.100000 loss 2.2670 (2.2581) acc@1 0.0859 (0.1294) acc@5 0.5781 (0.5759)\n",
      "\u001b[32m[2020-07-02 19:33:21] __main__ INFO: \u001b[0mEpoch 13 Step 200/351 lr 0.100000 loss 2.2053 (2.2538) acc@1 0.1953 (0.1323) acc@5 0.6484 (0.5801)\n",
      "\u001b[32m[2020-07-02 19:33:25] __main__ INFO: \u001b[0mEpoch 13 Step 250/351 lr 0.100000 loss 2.2280 (2.2510) acc@1 0.1328 (0.1341) acc@5 0.6484 (0.5821)\n",
      "\u001b[32m[2020-07-02 19:33:30] __main__ INFO: \u001b[0mEpoch 13 Step 300/351 lr 0.100000 loss 2.1784 (2.2474) acc@1 0.1328 (0.1369) acc@5 0.6562 (0.5868)\n",
      "\u001b[32m[2020-07-02 19:33:35] __main__ INFO: \u001b[0mEpoch 13 Step 350/351 lr 0.100000 loss 2.2694 (2.2443) acc@1 0.1016 (0.1390) acc@5 0.6016 (0.5892)\n",
      "\u001b[32m[2020-07-02 19:33:35] __main__ INFO: \u001b[0mEpoch 13 Step 351/351 lr 0.100000 loss 2.2351 (2.2442) acc@1 0.0703 (0.1388) acc@5 0.6484 (0.5893)\n",
      "\u001b[32m[2020-07-02 19:33:35] __main__ INFO: \u001b[0mElapsed 33.55\n",
      "\u001b[32m[2020-07-02 19:33:35] __main__ INFO: \u001b[0mVal 13\n",
      "\u001b[32m[2020-07-02 19:33:36] __main__ INFO: \u001b[0mEpoch 13 loss 2.2078 acc@1 0.1556 acc@5 0.6224\n",
      "\u001b[32m[2020-07-02 19:33:36] __main__ INFO: \u001b[0mElapsed 1.08\n",
      "\u001b[32m[2020-07-02 19:33:36] __main__ INFO: \u001b[0mTrain 14 4563\n",
      "\u001b[32m[2020-07-02 19:33:41] __main__ INFO: \u001b[0mEpoch 14 Step 50/351 lr 0.100000 loss 2.2140 (2.2093) acc@1 0.1406 (0.1492) acc@5 0.6250 (0.6194)\n",
      "\u001b[32m[2020-07-02 19:33:46] __main__ INFO: \u001b[0mEpoch 14 Step 100/351 lr 0.100000 loss 2.2371 (2.2117) acc@1 0.1406 (0.1523) acc@5 0.6641 (0.6195)\n",
      "\u001b[32m[2020-07-02 19:33:51] __main__ INFO: \u001b[0mEpoch 14 Step 150/351 lr 0.100000 loss 2.3062 (2.2089) acc@1 0.0781 (0.1545) acc@5 0.5625 (0.6228)\n",
      "\u001b[32m[2020-07-02 19:33:55] __main__ INFO: \u001b[0mEpoch 14 Step 200/351 lr 0.100000 loss 2.2080 (2.2134) acc@1 0.1641 (0.1538) acc@5 0.6016 (0.6195)\n",
      "\u001b[32m[2020-07-02 19:34:00] __main__ INFO: \u001b[0mEpoch 14 Step 250/351 lr 0.100000 loss 2.2055 (2.2125) acc@1 0.1484 (0.1542) acc@5 0.6719 (0.6192)\n",
      "\u001b[32m[2020-07-02 19:34:05] __main__ INFO: \u001b[0mEpoch 14 Step 300/351 lr 0.100000 loss 2.1837 (2.2103) acc@1 0.2344 (0.1549) acc@5 0.6719 (0.6207)\n",
      "\u001b[32m[2020-07-02 19:34:10] __main__ INFO: \u001b[0mEpoch 14 Step 350/351 lr 0.100000 loss 2.1758 (2.2078) acc@1 0.1641 (0.1565) acc@5 0.5781 (0.6236)\n",
      "\u001b[32m[2020-07-02 19:34:10] __main__ INFO: \u001b[0mEpoch 14 Step 351/351 lr 0.100000 loss 2.2363 (2.2079) acc@1 0.1641 (0.1565) acc@5 0.6250 (0.6236)\n",
      "\u001b[32m[2020-07-02 19:34:10] __main__ INFO: \u001b[0mElapsed 33.60\n",
      "\u001b[32m[2020-07-02 19:34:10] __main__ INFO: \u001b[0mVal 14\n",
      "\u001b[32m[2020-07-02 19:34:11] __main__ INFO: \u001b[0mEpoch 14 loss 2.2304 acc@1 0.1578 acc@5 0.6208\n",
      "\u001b[32m[2020-07-02 19:34:11] __main__ INFO: \u001b[0mElapsed 1.05\n",
      "\u001b[32m[2020-07-02 19:34:11] __main__ INFO: \u001b[0mTrain 15 4914\n",
      "\u001b[32m[2020-07-02 19:34:16] __main__ INFO: \u001b[0mEpoch 15 Step 50/351 lr 0.100000 loss 2.1422 (2.1880) acc@1 0.2266 (0.1634) acc@5 0.6562 (0.6314)\n",
      "\u001b[32m[2020-07-02 19:34:20] __main__ INFO: \u001b[0mEpoch 15 Step 100/351 lr 0.100000 loss 2.1506 (2.1825) acc@1 0.1484 (0.1723) acc@5 0.6406 (0.6364)\n",
      "\u001b[32m[2020-07-02 19:34:25] __main__ INFO: \u001b[0mEpoch 15 Step 150/351 lr 0.100000 loss 2.1561 (2.1787) acc@1 0.1641 (0.1748) acc@5 0.6953 (0.6422)\n",
      "\u001b[32m[2020-07-02 19:34:30] __main__ INFO: \u001b[0mEpoch 15 Step 200/351 lr 0.100000 loss 2.1405 (2.1724) acc@1 0.1875 (0.1784) acc@5 0.6484 (0.6469)\n",
      "\u001b[32m[2020-07-02 19:34:35] __main__ INFO: \u001b[0mEpoch 15 Step 250/351 lr 0.100000 loss 2.1959 (2.1704) acc@1 0.1641 (0.1783) acc@5 0.6953 (0.6478)\n",
      "\u001b[32m[2020-07-02 19:34:39] __main__ INFO: \u001b[0mEpoch 15 Step 300/351 lr 0.100000 loss 2.1382 (2.1678) acc@1 0.1797 (0.1804) acc@5 0.6875 (0.6507)\n",
      "\u001b[32m[2020-07-02 19:34:44] __main__ INFO: \u001b[0mEpoch 15 Step 350/351 lr 0.100000 loss 2.1577 (2.1656) acc@1 0.1562 (0.1815) acc@5 0.6172 (0.6520)\n",
      "\u001b[32m[2020-07-02 19:34:44] __main__ INFO: \u001b[0mEpoch 15 Step 351/351 lr 0.100000 loss 2.1842 (2.1656) acc@1 0.2109 (0.1816) acc@5 0.6562 (0.6520)\n",
      "\u001b[32m[2020-07-02 19:34:44] __main__ INFO: \u001b[0mElapsed 33.57\n",
      "\u001b[32m[2020-07-02 19:34:44] __main__ INFO: \u001b[0mVal 15\n",
      "\u001b[32m[2020-07-02 19:34:45] __main__ INFO: \u001b[0mEpoch 15 loss 2.1307 acc@1 0.2026 acc@5 0.6718\n",
      "\u001b[32m[2020-07-02 19:34:45] __main__ INFO: \u001b[0mElapsed 1.07\n",
      "\u001b[32m[2020-07-02 19:34:45] __main__ INFO: \u001b[0mTrain 16 5265\n",
      "\u001b[32m[2020-07-02 19:34:50] __main__ INFO: \u001b[0mEpoch 16 Step 50/351 lr 0.100000 loss 2.1494 (2.1426) acc@1 0.2109 (0.1947) acc@5 0.6719 (0.6659)\n",
      "\u001b[32m[2020-07-02 19:34:55] __main__ INFO: \u001b[0mEpoch 16 Step 100/351 lr 0.100000 loss 2.1648 (2.1402) acc@1 0.1875 (0.1959) acc@5 0.6406 (0.6662)\n",
      "\u001b[32m[2020-07-02 19:35:00] __main__ INFO: \u001b[0mEpoch 16 Step 150/351 lr 0.100000 loss 2.2226 (2.1376) acc@1 0.1953 (0.1964) acc@5 0.6016 (0.6644)\n",
      "\u001b[32m[2020-07-02 19:35:05] __main__ INFO: \u001b[0mEpoch 16 Step 200/351 lr 0.100000 loss 2.1699 (2.1351) acc@1 0.1641 (0.1993) acc@5 0.5938 (0.6667)\n",
      "\u001b[32m[2020-07-02 19:35:09] __main__ INFO: \u001b[0mEpoch 16 Step 250/351 lr 0.100000 loss 2.2009 (2.1338) acc@1 0.1719 (0.1992) acc@5 0.6641 (0.6678)\n",
      "\u001b[32m[2020-07-02 19:35:14] __main__ INFO: \u001b[0mEpoch 16 Step 300/351 lr 0.100000 loss 2.0990 (2.1325) acc@1 0.2734 (0.1995) acc@5 0.6406 (0.6664)\n",
      "\u001b[32m[2020-07-02 19:35:19] __main__ INFO: \u001b[0mEpoch 16 Step 350/351 lr 0.100000 loss 2.1246 (2.1295) acc@1 0.2109 (0.1996) acc@5 0.5938 (0.6661)\n",
      "\u001b[32m[2020-07-02 19:35:19] __main__ INFO: \u001b[0mEpoch 16 Step 351/351 lr 0.100000 loss 2.1057 (2.1294) acc@1 0.1562 (0.1995) acc@5 0.6875 (0.6662)\n",
      "\u001b[32m[2020-07-02 19:35:19] __main__ INFO: \u001b[0mElapsed 33.61\n",
      "\u001b[32m[2020-07-02 19:35:19] __main__ INFO: \u001b[0mVal 16\n",
      "\u001b[32m[2020-07-02 19:35:20] __main__ INFO: \u001b[0mEpoch 16 loss 2.1368 acc@1 0.1976 acc@5 0.6878\n",
      "\u001b[32m[2020-07-02 19:35:20] __main__ INFO: \u001b[0mElapsed 1.05\n",
      "\u001b[32m[2020-07-02 19:35:20] __main__ INFO: \u001b[0mTrain 17 5616\n",
      "\u001b[32m[2020-07-02 19:35:25] __main__ INFO: \u001b[0mEpoch 17 Step 50/351 lr 0.100000 loss 2.1135 (2.1139) acc@1 0.1328 (0.2036) acc@5 0.7031 (0.6731)\n",
      "\u001b[32m[2020-07-02 19:35:30] __main__ INFO: \u001b[0mEpoch 17 Step 100/351 lr 0.100000 loss 2.1214 (2.1048) acc@1 0.1875 (0.2082) acc@5 0.7969 (0.6761)\n",
      "\u001b[32m[2020-07-02 19:35:35] __main__ INFO: \u001b[0mEpoch 17 Step 150/351 lr 0.100000 loss 2.1708 (2.1019) acc@1 0.0938 (0.2095) acc@5 0.6016 (0.6737)\n",
      "\u001b[32m[2020-07-02 19:35:39] __main__ INFO: \u001b[0mEpoch 17 Step 200/351 lr 0.100000 loss 2.0629 (2.0960) acc@1 0.2188 (0.2126) acc@5 0.6562 (0.6761)\n",
      "\u001b[32m[2020-07-02 19:35:44] __main__ INFO: \u001b[0mEpoch 17 Step 250/351 lr 0.100000 loss 2.1243 (2.0918) acc@1 0.1719 (0.2140) acc@5 0.5938 (0.6787)\n",
      "\u001b[32m[2020-07-02 19:35:49] __main__ INFO: \u001b[0mEpoch 17 Step 300/351 lr 0.100000 loss 2.0504 (2.0873) acc@1 0.2578 (0.2169) acc@5 0.6797 (0.6804)\n",
      "\u001b[32m[2020-07-02 19:35:54] __main__ INFO: \u001b[0mEpoch 17 Step 350/351 lr 0.100000 loss 1.9720 (2.0832) acc@1 0.2500 (0.2188) acc@5 0.7109 (0.6823)\n",
      "\u001b[32m[2020-07-02 19:35:54] __main__ INFO: \u001b[0mEpoch 17 Step 351/351 lr 0.100000 loss 1.9837 (2.0829) acc@1 0.2344 (0.2189) acc@5 0.7031 (0.6824)\n",
      "\u001b[32m[2020-07-02 19:35:54] __main__ INFO: \u001b[0mElapsed 33.62\n",
      "\u001b[32m[2020-07-02 19:35:54] __main__ INFO: \u001b[0mVal 17\n",
      "\u001b[32m[2020-07-02 19:35:55] __main__ INFO: \u001b[0mEpoch 17 loss 2.1474 acc@1 0.2228 acc@5 0.6802\n",
      "\u001b[32m[2020-07-02 19:35:55] __main__ INFO: \u001b[0mElapsed 1.05\n",
      "\u001b[32m[2020-07-02 19:35:55] __main__ INFO: \u001b[0mTrain 18 5967\n",
      "\u001b[32m[2020-07-02 19:36:00] __main__ INFO: \u001b[0mEpoch 18 Step 50/351 lr 0.100000 loss 1.9926 (2.0441) acc@1 0.2031 (0.2383) acc@5 0.7109 (0.7006)\n",
      "\u001b[32m[2020-07-02 19:36:04] __main__ INFO: \u001b[0mEpoch 18 Step 100/351 lr 0.100000 loss 2.0317 (2.0447) acc@1 0.2266 (0.2362) acc@5 0.6719 (0.6968)\n",
      "\u001b[32m[2020-07-02 19:36:09] __main__ INFO: \u001b[0mEpoch 18 Step 150/351 lr 0.100000 loss 2.0184 (2.0477) acc@1 0.2656 (0.2377) acc@5 0.7500 (0.6948)\n",
      "\u001b[32m[2020-07-02 19:36:14] __main__ INFO: \u001b[0mEpoch 18 Step 200/351 lr 0.100000 loss 2.2546 (2.0451) acc@1 0.1328 (0.2362) acc@5 0.5938 (0.6933)\n",
      "\u001b[32m[2020-07-02 19:36:19] __main__ INFO: \u001b[0mEpoch 18 Step 250/351 lr 0.100000 loss 1.9729 (2.0449) acc@1 0.2734 (0.2358) acc@5 0.7344 (0.6948)\n",
      "\u001b[32m[2020-07-02 19:36:24] __main__ INFO: \u001b[0mEpoch 18 Step 300/351 lr 0.100000 loss 2.1632 (2.0423) acc@1 0.2031 (0.2380) acc@5 0.6250 (0.6946)\n",
      "\u001b[32m[2020-07-02 19:36:28] __main__ INFO: \u001b[0mEpoch 18 Step 350/351 lr 0.100000 loss 2.0549 (2.0375) acc@1 0.2109 (0.2402) acc@5 0.6406 (0.6955)\n",
      "\u001b[32m[2020-07-02 19:36:28] __main__ INFO: \u001b[0mEpoch 18 Step 351/351 lr 0.100000 loss 2.1212 (2.0378) acc@1 0.2578 (0.2403) acc@5 0.6953 (0.6955)\n",
      "\u001b[32m[2020-07-02 19:36:28] __main__ INFO: \u001b[0mElapsed 33.64\n",
      "\u001b[32m[2020-07-02 19:36:28] __main__ INFO: \u001b[0mVal 18\n",
      "\u001b[32m[2020-07-02 19:36:29] __main__ INFO: \u001b[0mEpoch 18 loss 2.0061 acc@1 0.2492 acc@5 0.7030\n",
      "\u001b[32m[2020-07-02 19:36:29] __main__ INFO: \u001b[0mElapsed 1.07\n",
      "\u001b[32m[2020-07-02 19:36:29] __main__ INFO: \u001b[0mTrain 19 6318\n",
      "\u001b[32m[2020-07-02 19:36:34] __main__ INFO: \u001b[0mEpoch 19 Step 50/351 lr 0.100000 loss 2.0523 (2.0350) acc@1 0.2422 (0.2452) acc@5 0.7031 (0.6984)\n",
      "\u001b[32m[2020-07-02 19:36:39] __main__ INFO: \u001b[0mEpoch 19 Step 100/351 lr 0.100000 loss 2.0473 (2.0133) acc@1 0.2500 (0.2502) acc@5 0.6797 (0.7038)\n",
      "\u001b[32m[2020-07-02 19:36:44] __main__ INFO: \u001b[0mEpoch 19 Step 150/351 lr 0.100000 loss 1.9154 (2.0060) acc@1 0.2656 (0.2542) acc@5 0.7578 (0.7043)\n",
      "\u001b[32m[2020-07-02 19:36:49] __main__ INFO: \u001b[0mEpoch 19 Step 200/351 lr 0.100000 loss 2.0930 (2.0019) acc@1 0.2344 (0.2547) acc@5 0.6328 (0.7046)\n",
      "\u001b[32m[2020-07-02 19:36:53] __main__ INFO: \u001b[0mEpoch 19 Step 250/351 lr 0.100000 loss 1.9229 (1.9962) acc@1 0.2500 (0.2568) acc@5 0.8203 (0.7064)\n",
      "\u001b[32m[2020-07-02 19:36:58] __main__ INFO: \u001b[0mEpoch 19 Step 300/351 lr 0.100000 loss 1.9224 (1.9930) acc@1 0.2891 (0.2583) acc@5 0.7422 (0.7081)\n",
      "\u001b[32m[2020-07-02 19:37:03] __main__ INFO: \u001b[0mEpoch 19 Step 350/351 lr 0.100000 loss 1.9890 (1.9877) acc@1 0.2734 (0.2598) acc@5 0.6719 (0.7094)\n",
      "\u001b[32m[2020-07-02 19:37:03] __main__ INFO: \u001b[0mEpoch 19 Step 351/351 lr 0.100000 loss 1.9083 (1.9875) acc@1 0.2344 (0.2598) acc@5 0.7500 (0.7095)\n",
      "\u001b[32m[2020-07-02 19:37:03] __main__ INFO: \u001b[0mElapsed 33.59\n",
      "\u001b[32m[2020-07-02 19:37:03] __main__ INFO: \u001b[0mVal 19\n",
      "\u001b[32m[2020-07-02 19:37:04] __main__ INFO: \u001b[0mEpoch 19 loss 2.0373 acc@1 0.2490 acc@5 0.7028\n",
      "\u001b[32m[2020-07-02 19:37:04] __main__ INFO: \u001b[0mElapsed 1.06\n",
      "\u001b[32m[2020-07-02 19:37:04] __main__ INFO: \u001b[0mTrain 20 6669\n",
      "\u001b[32m[2020-07-02 19:37:09] __main__ INFO: \u001b[0mEpoch 20 Step 50/351 lr 0.100000 loss 1.9399 (1.9709) acc@1 0.2578 (0.2675) acc@5 0.7188 (0.7120)\n",
      "\u001b[32m[2020-07-02 19:37:14] __main__ INFO: \u001b[0mEpoch 20 Step 100/351 lr 0.100000 loss 2.0308 (1.9538) acc@1 0.2734 (0.2720) acc@5 0.6641 (0.7162)\n",
      "\u001b[32m[2020-07-02 19:37:19] __main__ INFO: \u001b[0mEpoch 20 Step 150/351 lr 0.100000 loss 1.9325 (1.9556) acc@1 0.3203 (0.2746) acc@5 0.7422 (0.7179)\n",
      "\u001b[32m[2020-07-02 19:37:23] __main__ INFO: \u001b[0mEpoch 20 Step 200/351 lr 0.100000 loss 1.9989 (1.9485) acc@1 0.2422 (0.2763) acc@5 0.7188 (0.7191)\n",
      "\u001b[32m[2020-07-02 19:37:28] __main__ INFO: \u001b[0mEpoch 20 Step 250/351 lr 0.100000 loss 1.9048 (1.9526) acc@1 0.3047 (0.2750) acc@5 0.7734 (0.7167)\n",
      "\u001b[32m[2020-07-02 19:37:33] __main__ INFO: \u001b[0mEpoch 20 Step 300/351 lr 0.100000 loss 1.8531 (1.9477) acc@1 0.2812 (0.2778) acc@5 0.7266 (0.7183)\n",
      "\u001b[32m[2020-07-02 19:37:38] __main__ INFO: \u001b[0mEpoch 20 Step 350/351 lr 0.100000 loss 1.8357 (1.9452) acc@1 0.3047 (0.2791) acc@5 0.7578 (0.7200)\n",
      "\u001b[32m[2020-07-02 19:37:38] __main__ INFO: \u001b[0mEpoch 20 Step 351/351 lr 0.100000 loss 1.8037 (1.9448) acc@1 0.3281 (0.2792) acc@5 0.7422 (0.7200)\n",
      "\u001b[32m[2020-07-02 19:37:38] __main__ INFO: \u001b[0mElapsed 33.64\n",
      "\u001b[32m[2020-07-02 19:37:38] __main__ INFO: \u001b[0mVal 20\n",
      "\u001b[32m[2020-07-02 19:37:39] __main__ INFO: \u001b[0mEpoch 20 loss 1.9116 acc@1 0.2884 acc@5 0.7304\n",
      "\u001b[32m[2020-07-02 19:37:39] __main__ INFO: \u001b[0mElapsed 1.08\n",
      "\u001b[32m[2020-07-02 19:37:39] __main__ INFO: \u001b[0mTrain 21 7020\n",
      "\u001b[32m[2020-07-02 19:37:44] __main__ INFO: \u001b[0mEpoch 21 Step 50/351 lr 0.100000 loss 1.8725 (1.9081) acc@1 0.3281 (0.2972) acc@5 0.7344 (0.7291)\n",
      "\u001b[32m[2020-07-02 19:37:49] __main__ INFO: \u001b[0mEpoch 21 Step 100/351 lr 0.100000 loss 2.0171 (1.9071) acc@1 0.3047 (0.2942) acc@5 0.6641 (0.7289)\n",
      "\u001b[32m[2020-07-02 19:37:53] __main__ INFO: \u001b[0mEpoch 21 Step 150/351 lr 0.100000 loss 1.9085 (1.9089) acc@1 0.2891 (0.2911) acc@5 0.7109 (0.7283)\n",
      "\u001b[32m[2020-07-02 19:37:58] __main__ INFO: \u001b[0mEpoch 21 Step 200/351 lr 0.100000 loss 1.9025 (1.9079) acc@1 0.3281 (0.2911) acc@5 0.7031 (0.7288)\n",
      "\u001b[32m[2020-07-02 19:38:03] __main__ INFO: \u001b[0mEpoch 21 Step 250/351 lr 0.100000 loss 1.8189 (1.9056) acc@1 0.3594 (0.2925) acc@5 0.7891 (0.7288)\n",
      "\u001b[32m[2020-07-02 19:38:08] __main__ INFO: \u001b[0mEpoch 21 Step 300/351 lr 0.100000 loss 1.7427 (1.9051) acc@1 0.3281 (0.2933) acc@5 0.7266 (0.7290)\n",
      "\u001b[32m[2020-07-02 19:38:12] __main__ INFO: \u001b[0mEpoch 21 Step 350/351 lr 0.100000 loss 1.8779 (1.9032) acc@1 0.2891 (0.2936) acc@5 0.7188 (0.7295)\n",
      "\u001b[32m[2020-07-02 19:38:12] __main__ INFO: \u001b[0mEpoch 21 Step 351/351 lr 0.100000 loss 1.9191 (1.9033) acc@1 0.2891 (0.2936) acc@5 0.7578 (0.7296)\n",
      "\u001b[32m[2020-07-02 19:38:13] __main__ INFO: \u001b[0mElapsed 33.64\n",
      "\u001b[32m[2020-07-02 19:38:13] __main__ INFO: \u001b[0mVal 21\n",
      "\u001b[32m[2020-07-02 19:38:14] __main__ INFO: \u001b[0mEpoch 21 loss 1.8485 acc@1 0.3128 acc@5 0.7364\n",
      "\u001b[32m[2020-07-02 19:38:14] __main__ INFO: \u001b[0mElapsed 1.07\n",
      "\u001b[32m[2020-07-02 19:38:14] __main__ INFO: \u001b[0mTrain 22 7371\n",
      "\u001b[32m[2020-07-02 19:38:18] __main__ INFO: \u001b[0mEpoch 22 Step 50/351 lr 0.100000 loss 1.8677 (1.8716) acc@1 0.3359 (0.3033) acc@5 0.7734 (0.7486)\n",
      "\u001b[32m[2020-07-02 19:38:23] __main__ INFO: \u001b[0mEpoch 22 Step 100/351 lr 0.100000 loss 1.9632 (1.8715) acc@1 0.2266 (0.3057) acc@5 0.7109 (0.7420)\n",
      "\u001b[32m[2020-07-02 19:38:28] __main__ INFO: \u001b[0mEpoch 22 Step 150/351 lr 0.100000 loss 1.8180 (1.8741) acc@1 0.3203 (0.3038) acc@5 0.7734 (0.7383)\n",
      "\u001b[32m[2020-07-02 19:38:33] __main__ INFO: \u001b[0mEpoch 22 Step 200/351 lr 0.100000 loss 1.9503 (1.8730) acc@1 0.2734 (0.3034) acc@5 0.6953 (0.7393)\n",
      "\u001b[32m[2020-07-02 19:38:38] __main__ INFO: \u001b[0mEpoch 22 Step 250/351 lr 0.100000 loss 1.9322 (1.8686) acc@1 0.2812 (0.3061) acc@5 0.7344 (0.7407)\n",
      "\u001b[32m[2020-07-02 19:38:42] __main__ INFO: \u001b[0mEpoch 22 Step 300/351 lr 0.100000 loss 1.8318 (1.8632) acc@1 0.3203 (0.3085) acc@5 0.7188 (0.7410)\n",
      "\u001b[32m[2020-07-02 19:38:47] __main__ INFO: \u001b[0mEpoch 22 Step 350/351 lr 0.100000 loss 1.9624 (1.8631) acc@1 0.2656 (0.3076) acc@5 0.7500 (0.7392)\n",
      "\u001b[32m[2020-07-02 19:38:47] __main__ INFO: \u001b[0mEpoch 22 Step 351/351 lr 0.100000 loss 1.9294 (1.8633) acc@1 0.2812 (0.3075) acc@5 0.7109 (0.7391)\n",
      "\u001b[32m[2020-07-02 19:38:47] __main__ INFO: \u001b[0mElapsed 33.64\n",
      "\u001b[32m[2020-07-02 19:38:47] __main__ INFO: \u001b[0mVal 22\n",
      "\u001b[32m[2020-07-02 19:38:48] __main__ INFO: \u001b[0mEpoch 22 loss 1.9184 acc@1 0.3040 acc@5 0.7316\n",
      "\u001b[32m[2020-07-02 19:38:48] __main__ INFO: \u001b[0mElapsed 1.06\n",
      "\u001b[32m[2020-07-02 19:38:48] __main__ INFO: \u001b[0mTrain 23 7722\n",
      "\u001b[32m[2020-07-02 19:38:53] __main__ INFO: \u001b[0mEpoch 23 Step 50/351 lr 0.100000 loss 1.8200 (1.8365) acc@1 0.2891 (0.3242) acc@5 0.7344 (0.7459)\n",
      "\u001b[32m[2020-07-02 19:38:58] __main__ INFO: \u001b[0mEpoch 23 Step 100/351 lr 0.100000 loss 1.7784 (1.8387) acc@1 0.3281 (0.3184) acc@5 0.7734 (0.7451)\n",
      "\u001b[32m[2020-07-02 19:39:03] __main__ INFO: \u001b[0mEpoch 23 Step 150/351 lr 0.100000 loss 1.8432 (1.8271) acc@1 0.2891 (0.3222) acc@5 0.7266 (0.7451)\n",
      "\u001b[32m[2020-07-02 19:39:07] __main__ INFO: \u001b[0mEpoch 23 Step 200/351 lr 0.100000 loss 1.9251 (1.8310) acc@1 0.2969 (0.3234) acc@5 0.8047 (0.7445)\n",
      "\u001b[32m[2020-07-02 19:39:12] __main__ INFO: \u001b[0mEpoch 23 Step 250/351 lr 0.100000 loss 1.7735 (1.8345) acc@1 0.3281 (0.3223) acc@5 0.7578 (0.7430)\n",
      "\u001b[32m[2020-07-02 19:39:17] __main__ INFO: \u001b[0mEpoch 23 Step 300/351 lr 0.100000 loss 1.8138 (1.8335) acc@1 0.3281 (0.3228) acc@5 0.7266 (0.7422)\n",
      "\u001b[32m[2020-07-02 19:39:22] __main__ INFO: \u001b[0mEpoch 23 Step 350/351 lr 0.100000 loss 1.9262 (1.8343) acc@1 0.2812 (0.3221) acc@5 0.7812 (0.7426)\n",
      "\u001b[32m[2020-07-02 19:39:22] __main__ INFO: \u001b[0mEpoch 23 Step 351/351 lr 0.100000 loss 1.8587 (1.8344) acc@1 0.3438 (0.3222) acc@5 0.7656 (0.7427)\n",
      "\u001b[32m[2020-07-02 19:39:22] __main__ INFO: \u001b[0mElapsed 33.62\n",
      "\u001b[32m[2020-07-02 19:39:22] __main__ INFO: \u001b[0mVal 23\n",
      "\u001b[32m[2020-07-02 19:39:23] __main__ INFO: \u001b[0mEpoch 23 loss 1.8624 acc@1 0.3222 acc@5 0.7458\n",
      "\u001b[32m[2020-07-02 19:39:23] __main__ INFO: \u001b[0mElapsed 1.08\n",
      "\u001b[32m[2020-07-02 19:39:23] __main__ INFO: \u001b[0mTrain 24 8073\n",
      "\u001b[32m[2020-07-02 19:39:28] __main__ INFO: \u001b[0mEpoch 24 Step 50/351 lr 0.100000 loss 1.6035 (1.7874) acc@1 0.3750 (0.3403) acc@5 0.7656 (0.7566)\n",
      "\u001b[32m[2020-07-02 19:39:33] __main__ INFO: \u001b[0mEpoch 24 Step 100/351 lr 0.100000 loss 1.7865 (1.7987) acc@1 0.3125 (0.3331) acc@5 0.7422 (0.7573)\n",
      "\u001b[32m[2020-07-02 19:39:37] __main__ INFO: \u001b[0mEpoch 24 Step 150/351 lr 0.100000 loss 1.7431 (1.8011) acc@1 0.3672 (0.3332) acc@5 0.7578 (0.7538)\n",
      "\u001b[32m[2020-07-02 19:39:42] __main__ INFO: \u001b[0mEpoch 24 Step 200/351 lr 0.100000 loss 1.8602 (1.8052) acc@1 0.3516 (0.3312) acc@5 0.8047 (0.7530)\n",
      "\u001b[32m[2020-07-02 19:39:47] __main__ INFO: \u001b[0mEpoch 24 Step 250/351 lr 0.100000 loss 1.8127 (1.8094) acc@1 0.3438 (0.3292) acc@5 0.7578 (0.7509)\n",
      "\u001b[32m[2020-07-02 19:39:52] __main__ INFO: \u001b[0mEpoch 24 Step 300/351 lr 0.100000 loss 1.9269 (1.8082) acc@1 0.2578 (0.3295) acc@5 0.7344 (0.7512)\n",
      "\u001b[32m[2020-07-02 19:39:56] __main__ INFO: \u001b[0mEpoch 24 Step 350/351 lr 0.100000 loss 1.8191 (1.8055) acc@1 0.3438 (0.3303) acc@5 0.7969 (0.7517)\n",
      "\u001b[32m[2020-07-02 19:39:57] __main__ INFO: \u001b[0mEpoch 24 Step 351/351 lr 0.100000 loss 1.7549 (1.8054) acc@1 0.3984 (0.3305) acc@5 0.7500 (0.7517)\n",
      "\u001b[32m[2020-07-02 19:39:57] __main__ INFO: \u001b[0mElapsed 33.60\n",
      "\u001b[32m[2020-07-02 19:39:57] __main__ INFO: \u001b[0mVal 24\n",
      "\u001b[32m[2020-07-02 19:39:58] __main__ INFO: \u001b[0mEpoch 24 loss 1.8613 acc@1 0.3114 acc@5 0.7358\n",
      "\u001b[32m[2020-07-02 19:39:58] __main__ INFO: \u001b[0mElapsed 1.07\n",
      "\u001b[32m[2020-07-02 19:39:58] __main__ INFO: \u001b[0mTrain 25 8424\n",
      "\u001b[32m[2020-07-02 19:40:03] __main__ INFO: \u001b[0mEpoch 25 Step 50/351 lr 0.100000 loss 1.6292 (1.8122) acc@1 0.3828 (0.3255) acc@5 0.7891 (0.7458)\n",
      "\u001b[32m[2020-07-02 19:40:07] __main__ INFO: \u001b[0mEpoch 25 Step 100/351 lr 0.100000 loss 1.6692 (1.8020) acc@1 0.3594 (0.3296) acc@5 0.7500 (0.7461)\n",
      "\u001b[32m[2020-07-02 19:40:12] __main__ INFO: \u001b[0mEpoch 25 Step 150/351 lr 0.100000 loss 1.8466 (1.8012) acc@1 0.2969 (0.3307) acc@5 0.7578 (0.7457)\n",
      "\u001b[32m[2020-07-02 19:40:17] __main__ INFO: \u001b[0mEpoch 25 Step 200/351 lr 0.100000 loss 1.8293 (1.8013) acc@1 0.2891 (0.3312) acc@5 0.7344 (0.7466)\n",
      "\u001b[32m[2020-07-02 19:40:22] __main__ INFO: \u001b[0mEpoch 25 Step 250/351 lr 0.100000 loss 1.6672 (1.7960) acc@1 0.3984 (0.3334) acc@5 0.8047 (0.7499)\n",
      "\u001b[32m[2020-07-02 19:40:26] __main__ INFO: \u001b[0mEpoch 25 Step 300/351 lr 0.100000 loss 1.6475 (1.7918) acc@1 0.3672 (0.3335) acc@5 0.7812 (0.7512)\n",
      "\u001b[32m[2020-07-02 19:40:31] __main__ INFO: \u001b[0mEpoch 25 Step 350/351 lr 0.100000 loss 1.7370 (1.7920) acc@1 0.3750 (0.3337) acc@5 0.7188 (0.7521)\n",
      "\u001b[32m[2020-07-02 19:40:31] __main__ INFO: \u001b[0mEpoch 25 Step 351/351 lr 0.100000 loss 1.7949 (1.7920) acc@1 0.3125 (0.3337) acc@5 0.7266 (0.7520)\n",
      "\u001b[32m[2020-07-02 19:40:31] __main__ INFO: \u001b[0mElapsed 33.66\n",
      "\u001b[32m[2020-07-02 19:40:31] __main__ INFO: \u001b[0mVal 25\n",
      "\u001b[32m[2020-07-02 19:40:32] __main__ INFO: \u001b[0mEpoch 25 loss 1.9935 acc@1 0.2900 acc@5 0.7006\n",
      "\u001b[32m[2020-07-02 19:40:32] __main__ INFO: \u001b[0mElapsed 1.09\n",
      "\u001b[32m[2020-07-02 19:40:32] __main__ INFO: \u001b[0mTrain 26 8775\n",
      "\u001b[32m[2020-07-02 19:40:37] __main__ INFO: \u001b[0mEpoch 26 Step 50/351 lr 0.100000 loss 1.7507 (1.7683) acc@1 0.3438 (0.3447) acc@5 0.7812 (0.7544)\n",
      "\u001b[32m[2020-07-02 19:40:42] __main__ INFO: \u001b[0mEpoch 26 Step 100/351 lr 0.100000 loss 1.8547 (1.7781) acc@1 0.3203 (0.3402) acc@5 0.7266 (0.7541)\n",
      "\u001b[32m[2020-07-02 19:40:47] __main__ INFO: \u001b[0mEpoch 26 Step 150/351 lr 0.100000 loss 1.7449 (1.7731) acc@1 0.3516 (0.3421) acc@5 0.7812 (0.7547)\n",
      "\u001b[32m[2020-07-02 19:40:52] __main__ INFO: \u001b[0mEpoch 26 Step 200/351 lr 0.100000 loss 1.8003 (1.7723) acc@1 0.3516 (0.3423) acc@5 0.7344 (0.7552)\n",
      "\u001b[32m[2020-07-02 19:40:56] __main__ INFO: \u001b[0mEpoch 26 Step 250/351 lr 0.100000 loss 1.7008 (1.7701) acc@1 0.4062 (0.3437) acc@5 0.7578 (0.7548)\n",
      "\u001b[32m[2020-07-02 19:41:01] __main__ INFO: \u001b[0mEpoch 26 Step 300/351 lr 0.100000 loss 1.8094 (1.7695) acc@1 0.2734 (0.3439) acc@5 0.7422 (0.7558)\n",
      "\u001b[32m[2020-07-02 19:41:06] __main__ INFO: \u001b[0mEpoch 26 Step 350/351 lr 0.100000 loss 1.9691 (1.7677) acc@1 0.2578 (0.3443) acc@5 0.7422 (0.7569)\n",
      "\u001b[32m[2020-07-02 19:41:06] __main__ INFO: \u001b[0mEpoch 26 Step 351/351 lr 0.100000 loss 1.6072 (1.7672) acc@1 0.3906 (0.3445) acc@5 0.7266 (0.7568)\n",
      "\u001b[32m[2020-07-02 19:41:06] __main__ INFO: \u001b[0mElapsed 33.63\n",
      "\u001b[32m[2020-07-02 19:41:06] __main__ INFO: \u001b[0mVal 26\n",
      "\u001b[32m[2020-07-02 19:41:07] __main__ INFO: \u001b[0mEpoch 26 loss 1.7898 acc@1 0.3460 acc@5 0.7488\n",
      "\u001b[32m[2020-07-02 19:41:07] __main__ INFO: \u001b[0mElapsed 1.06\n",
      "\u001b[32m[2020-07-02 19:41:07] __main__ INFO: \u001b[0mTrain 27 9126\n",
      "\u001b[32m[2020-07-02 19:41:12] __main__ INFO: \u001b[0mEpoch 27 Step 50/351 lr 0.100000 loss 1.8315 (1.7647) acc@1 0.3594 (0.3503) acc@5 0.7500 (0.7598)\n",
      "\u001b[32m[2020-07-02 19:41:17] __main__ INFO: \u001b[0mEpoch 27 Step 100/351 lr 0.100000 loss 1.7840 (1.7636) acc@1 0.3281 (0.3472) acc@5 0.7891 (0.7565)\n",
      "\u001b[32m[2020-07-02 19:41:21] __main__ INFO: \u001b[0mEpoch 27 Step 150/351 lr 0.100000 loss 1.7250 (1.7638) acc@1 0.4062 (0.3492) acc@5 0.8125 (0.7556)\n",
      "\u001b[32m[2020-07-02 19:41:26] __main__ INFO: \u001b[0mEpoch 27 Step 200/351 lr 0.100000 loss 1.7056 (1.7584) acc@1 0.3672 (0.3505) acc@5 0.8047 (0.7567)\n",
      "\u001b[32m[2020-07-02 19:41:31] __main__ INFO: \u001b[0mEpoch 27 Step 250/351 lr 0.100000 loss 1.7251 (1.7552) acc@1 0.3438 (0.3507) acc@5 0.7891 (0.7571)\n",
      "\u001b[32m[2020-07-02 19:41:36] __main__ INFO: \u001b[0mEpoch 27 Step 300/351 lr 0.100000 loss 1.6609 (1.7502) acc@1 0.3594 (0.3531) acc@5 0.7969 (0.7578)\n",
      "\u001b[32m[2020-07-02 19:41:41] __main__ INFO: \u001b[0mEpoch 27 Step 350/351 lr 0.100000 loss 1.9046 (1.7498) acc@1 0.3047 (0.3524) acc@5 0.6953 (0.7578)\n",
      "\u001b[32m[2020-07-02 19:41:41] __main__ INFO: \u001b[0mEpoch 27 Step 351/351 lr 0.100000 loss 1.7901 (1.7499) acc@1 0.4062 (0.3526) acc@5 0.7969 (0.7579)\n",
      "\u001b[32m[2020-07-02 19:41:41] __main__ INFO: \u001b[0mElapsed 33.63\n",
      "\u001b[32m[2020-07-02 19:41:41] __main__ INFO: \u001b[0mVal 27\n",
      "\u001b[32m[2020-07-02 19:41:42] __main__ INFO: \u001b[0mEpoch 27 loss 1.8421 acc@1 0.3462 acc@5 0.7558\n",
      "\u001b[32m[2020-07-02 19:41:42] __main__ INFO: \u001b[0mElapsed 1.06\n",
      "\u001b[32m[2020-07-02 19:41:42] __main__ INFO: \u001b[0mTrain 28 9477\n",
      "\u001b[32m[2020-07-02 19:41:47] __main__ INFO: \u001b[0mEpoch 28 Step 50/351 lr 0.100000 loss 1.7361 (1.7222) acc@1 0.3359 (0.3659) acc@5 0.7344 (0.7706)\n",
      "\u001b[32m[2020-07-02 19:41:51] __main__ INFO: \u001b[0mEpoch 28 Step 100/351 lr 0.100000 loss 1.6593 (1.7207) acc@1 0.3672 (0.3628) acc@5 0.7969 (0.7662)\n",
      "\u001b[32m[2020-07-02 19:41:56] __main__ INFO: \u001b[0mEpoch 28 Step 150/351 lr 0.100000 loss 1.7297 (1.7247) acc@1 0.3594 (0.3615) acc@5 0.7578 (0.7647)\n",
      "\u001b[32m[2020-07-02 19:42:01] __main__ INFO: \u001b[0mEpoch 28 Step 200/351 lr 0.100000 loss 1.8197 (1.7249) acc@1 0.3594 (0.3620) acc@5 0.7109 (0.7651)\n",
      "\u001b[32m[2020-07-02 19:42:06] __main__ INFO: \u001b[0mEpoch 28 Step 250/351 lr 0.100000 loss 1.6649 (1.7257) acc@1 0.3906 (0.3619) acc@5 0.7578 (0.7638)\n",
      "\u001b[32m[2020-07-02 19:42:11] __main__ INFO: \u001b[0mEpoch 28 Step 300/351 lr 0.100000 loss 1.8043 (1.7271) acc@1 0.3047 (0.3608) acc@5 0.7188 (0.7645)\n",
      "\u001b[32m[2020-07-02 19:42:15] __main__ INFO: \u001b[0mEpoch 28 Step 350/351 lr 0.100000 loss 1.7168 (1.7302) acc@1 0.3594 (0.3595) acc@5 0.7578 (0.7638)\n",
      "\u001b[32m[2020-07-02 19:42:15] __main__ INFO: \u001b[0mEpoch 28 Step 351/351 lr 0.100000 loss 1.9628 (1.7309) acc@1 0.3047 (0.3594) acc@5 0.7266 (0.7636)\n",
      "\u001b[32m[2020-07-02 19:42:15] __main__ INFO: \u001b[0mElapsed 33.61\n",
      "\u001b[32m[2020-07-02 19:42:15] __main__ INFO: \u001b[0mVal 28\n",
      "\u001b[32m[2020-07-02 19:42:16] __main__ INFO: \u001b[0mEpoch 28 loss 1.7653 acc@1 0.3524 acc@5 0.7540\n",
      "\u001b[32m[2020-07-02 19:42:16] __main__ INFO: \u001b[0mElapsed 1.09\n",
      "\u001b[32m[2020-07-02 19:42:16] __main__ INFO: \u001b[0mTrain 29 9828\n",
      "\u001b[32m[2020-07-02 19:42:21] __main__ INFO: \u001b[0mEpoch 29 Step 50/351 lr 0.100000 loss 1.8164 (1.7266) acc@1 0.4062 (0.3641) acc@5 0.7344 (0.7578)\n",
      "\u001b[32m[2020-07-02 19:42:26] __main__ INFO: \u001b[0mEpoch 29 Step 100/351 lr 0.100000 loss 1.6316 (1.7149) acc@1 0.4141 (0.3653) acc@5 0.8516 (0.7685)\n",
      "\u001b[32m[2020-07-02 19:42:31] __main__ INFO: \u001b[0mEpoch 29 Step 150/351 lr 0.100000 loss 1.6909 (1.7224) acc@1 0.3672 (0.3624) acc@5 0.7891 (0.7643)\n",
      "\u001b[32m[2020-07-02 19:42:36] __main__ INFO: \u001b[0mEpoch 29 Step 200/351 lr 0.100000 loss 1.7000 (1.7230) acc@1 0.3594 (0.3605) acc@5 0.7578 (0.7653)\n",
      "\u001b[32m[2020-07-02 19:42:40] __main__ INFO: \u001b[0mEpoch 29 Step 250/351 lr 0.100000 loss 1.5934 (1.7243) acc@1 0.3906 (0.3593) acc@5 0.7812 (0.7627)\n",
      "\u001b[32m[2020-07-02 19:42:45] __main__ INFO: \u001b[0mEpoch 29 Step 300/351 lr 0.100000 loss 1.7875 (1.7246) acc@1 0.3359 (0.3594) acc@5 0.7266 (0.7632)\n",
      "\u001b[32m[2020-07-02 19:42:50] __main__ INFO: \u001b[0mEpoch 29 Step 350/351 lr 0.100000 loss 1.7787 (1.7228) acc@1 0.3281 (0.3607) acc@5 0.7656 (0.7637)\n",
      "\u001b[32m[2020-07-02 19:42:50] __main__ INFO: \u001b[0mEpoch 29 Step 351/351 lr 0.100000 loss 1.7016 (1.7227) acc@1 0.3594 (0.3607) acc@5 0.7500 (0.7636)\n",
      "\u001b[32m[2020-07-02 19:42:50] __main__ INFO: \u001b[0mElapsed 33.63\n",
      "\u001b[32m[2020-07-02 19:42:50] __main__ INFO: \u001b[0mVal 29\n",
      "\u001b[32m[2020-07-02 19:42:51] __main__ INFO: \u001b[0mEpoch 29 loss 1.8554 acc@1 0.3344 acc@5 0.7570\n",
      "\u001b[32m[2020-07-02 19:42:51] __main__ INFO: \u001b[0mElapsed 1.05\n",
      "\u001b[32m[2020-07-02 19:42:51] __main__ INFO: \u001b[0mTrain 30 10179\n",
      "\u001b[32m[2020-07-02 19:42:56] __main__ INFO: \u001b[0mEpoch 30 Step 50/351 lr 0.100000 loss 1.6749 (1.7283) acc@1 0.3828 (0.3622) acc@5 0.8594 (0.7683)\n",
      "\u001b[32m[2020-07-02 19:43:01] __main__ INFO: \u001b[0mEpoch 30 Step 100/351 lr 0.100000 loss 1.6737 (1.7128) acc@1 0.3672 (0.3658) acc@5 0.7422 (0.7670)\n",
      "\u001b[32m[2020-07-02 19:43:06] __main__ INFO: \u001b[0mEpoch 30 Step 150/351 lr 0.100000 loss 1.5821 (1.7043) acc@1 0.4062 (0.3692) acc@5 0.7891 (0.7657)\n",
      "\u001b[32m[2020-07-02 19:43:10] __main__ INFO: \u001b[0mEpoch 30 Step 200/351 lr 0.100000 loss 1.8652 (1.7057) acc@1 0.3672 (0.3680) acc@5 0.7578 (0.7645)\n",
      "\u001b[32m[2020-07-02 19:43:15] __main__ INFO: \u001b[0mEpoch 30 Step 250/351 lr 0.100000 loss 1.6202 (1.7049) acc@1 0.3828 (0.3687) acc@5 0.8047 (0.7639)\n",
      "\u001b[32m[2020-07-02 19:43:20] __main__ INFO: \u001b[0mEpoch 30 Step 300/351 lr 0.100000 loss 1.7166 (1.7040) acc@1 0.3438 (0.3693) acc@5 0.7812 (0.7653)\n",
      "\u001b[32m[2020-07-02 19:43:25] __main__ INFO: \u001b[0mEpoch 30 Step 350/351 lr 0.100000 loss 1.6455 (1.7018) acc@1 0.3281 (0.3700) acc@5 0.7891 (0.7653)\n",
      "\u001b[32m[2020-07-02 19:43:25] __main__ INFO: \u001b[0mEpoch 30 Step 351/351 lr 0.100000 loss 1.7971 (1.7021) acc@1 0.3125 (0.3698) acc@5 0.8359 (0.7655)\n",
      "\u001b[32m[2020-07-02 19:43:25] __main__ INFO: \u001b[0mElapsed 33.68\n",
      "\u001b[32m[2020-07-02 19:43:25] __main__ INFO: \u001b[0mVal 30\n",
      "\u001b[32m[2020-07-02 19:43:26] __main__ INFO: \u001b[0mEpoch 30 loss 1.7799 acc@1 0.3498 acc@5 0.7642\n",
      "\u001b[32m[2020-07-02 19:43:26] __main__ INFO: \u001b[0mElapsed 1.06\n",
      "\u001b[32m[2020-07-02 19:43:26] __main__ INFO: \u001b[0mTrain 31 10530\n",
      "\u001b[32m[2020-07-02 19:43:31] __main__ INFO: \u001b[0mEpoch 31 Step 50/351 lr 0.100000 loss 1.7028 (1.6907) acc@1 0.3672 (0.3711) acc@5 0.7656 (0.7683)\n",
      "\u001b[32m[2020-07-02 19:43:36] __main__ INFO: \u001b[0mEpoch 31 Step 100/351 lr 0.100000 loss 1.7736 (1.6955) acc@1 0.3203 (0.3727) acc@5 0.6641 (0.7695)\n",
      "\u001b[32m[2020-07-02 19:43:40] __main__ INFO: \u001b[0mEpoch 31 Step 150/351 lr 0.100000 loss 1.5816 (1.6942) acc@1 0.3828 (0.3714) acc@5 0.7812 (0.7692)\n",
      "\u001b[32m[2020-07-02 19:43:45] __main__ INFO: \u001b[0mEpoch 31 Step 200/351 lr 0.100000 loss 1.6121 (1.6933) acc@1 0.3984 (0.3706) acc@5 0.7188 (0.7679)\n",
      "\u001b[32m[2020-07-02 19:43:50] __main__ INFO: \u001b[0mEpoch 31 Step 250/351 lr 0.100000 loss 1.7378 (1.6928) acc@1 0.3516 (0.3711) acc@5 0.7266 (0.7690)\n",
      "\u001b[32m[2020-07-02 19:43:55] __main__ INFO: \u001b[0mEpoch 31 Step 300/351 lr 0.100000 loss 1.6802 (1.6906) acc@1 0.3828 (0.3718) acc@5 0.7734 (0.7703)\n",
      "\u001b[32m[2020-07-02 19:43:59] __main__ INFO: \u001b[0mEpoch 31 Step 350/351 lr 0.100000 loss 1.6645 (1.6926) acc@1 0.3594 (0.3711) acc@5 0.7578 (0.7694)\n",
      "\u001b[32m[2020-07-02 19:43:59] __main__ INFO: \u001b[0mEpoch 31 Step 351/351 lr 0.100000 loss 1.8352 (1.6930) acc@1 0.3750 (0.3711) acc@5 0.7812 (0.7695)\n",
      "\u001b[32m[2020-07-02 19:43:59] __main__ INFO: \u001b[0mElapsed 33.54\n",
      "\u001b[32m[2020-07-02 19:43:59] __main__ INFO: \u001b[0mVal 31\n",
      "\u001b[32m[2020-07-02 19:44:01] __main__ INFO: \u001b[0mEpoch 31 loss 1.7848 acc@1 0.3586 acc@5 0.7630\n",
      "\u001b[32m[2020-07-02 19:44:01] __main__ INFO: \u001b[0mElapsed 1.07\n",
      "\u001b[32m[2020-07-02 19:44:01] __main__ INFO: \u001b[0mTrain 32 10881\n",
      "\u001b[32m[2020-07-02 19:44:05] __main__ INFO: \u001b[0mEpoch 32 Step 50/351 lr 0.100000 loss 1.7512 (1.6821) acc@1 0.3359 (0.3733) acc@5 0.7188 (0.7664)\n",
      "\u001b[32m[2020-07-02 19:44:10] __main__ INFO: \u001b[0mEpoch 32 Step 100/351 lr 0.100000 loss 1.7669 (1.6857) acc@1 0.2891 (0.3698) acc@5 0.7812 (0.7669)\n",
      "\u001b[32m[2020-07-02 19:44:15] __main__ INFO: \u001b[0mEpoch 32 Step 150/351 lr 0.100000 loss 1.6759 (1.6854) acc@1 0.3672 (0.3720) acc@5 0.7891 (0.7695)\n",
      "\u001b[32m[2020-07-02 19:44:20] __main__ INFO: \u001b[0mEpoch 32 Step 200/351 lr 0.100000 loss 1.6952 (1.6822) acc@1 0.3828 (0.3733) acc@5 0.7578 (0.7702)\n",
      "\u001b[32m[2020-07-02 19:44:25] __main__ INFO: \u001b[0mEpoch 32 Step 250/351 lr 0.100000 loss 1.7121 (1.6826) acc@1 0.3672 (0.3741) acc@5 0.7734 (0.7704)\n",
      "\u001b[32m[2020-07-02 19:44:29] __main__ INFO: \u001b[0mEpoch 32 Step 300/351 lr 0.100000 loss 1.6736 (1.6831) acc@1 0.3984 (0.3746) acc@5 0.7500 (0.7698)\n",
      "\u001b[32m[2020-07-02 19:44:34] __main__ INFO: \u001b[0mEpoch 32 Step 350/351 lr 0.100000 loss 1.6914 (1.6803) acc@1 0.3984 (0.3759) acc@5 0.7891 (0.7704)\n",
      "\u001b[32m[2020-07-02 19:44:34] __main__ INFO: \u001b[0mEpoch 32 Step 351/351 lr 0.100000 loss 1.7006 (1.6803) acc@1 0.3672 (0.3759) acc@5 0.7578 (0.7703)\n",
      "\u001b[32m[2020-07-02 19:44:34] __main__ INFO: \u001b[0mElapsed 33.66\n",
      "\u001b[32m[2020-07-02 19:44:34] __main__ INFO: \u001b[0mVal 32\n",
      "\u001b[32m[2020-07-02 19:44:35] __main__ INFO: \u001b[0mEpoch 32 loss 1.8506 acc@1 0.3562 acc@5 0.7442\n",
      "\u001b[32m[2020-07-02 19:44:35] __main__ INFO: \u001b[0mElapsed 1.09\n",
      "\u001b[32m[2020-07-02 19:44:35] __main__ INFO: \u001b[0mTrain 33 11232\n",
      "\u001b[32m[2020-07-02 19:44:40] __main__ INFO: \u001b[0mEpoch 33 Step 50/351 lr 0.100000 loss 1.6542 (1.6468) acc@1 0.3906 (0.3933) acc@5 0.7578 (0.7783)\n",
      "\u001b[32m[2020-07-02 19:44:45] __main__ INFO: \u001b[0mEpoch 33 Step 100/351 lr 0.100000 loss 1.6432 (1.6668) acc@1 0.3984 (0.3833) acc@5 0.7891 (0.7748)\n",
      "\u001b[32m[2020-07-02 19:44:50] __main__ INFO: \u001b[0mEpoch 33 Step 150/351 lr 0.100000 loss 1.6913 (1.6639) acc@1 0.3594 (0.3845) acc@5 0.7891 (0.7751)\n",
      "\u001b[32m[2020-07-02 19:44:54] __main__ INFO: \u001b[0mEpoch 33 Step 200/351 lr 0.100000 loss 1.6317 (1.6677) acc@1 0.4297 (0.3830) acc@5 0.7656 (0.7731)\n",
      "\u001b[32m[2020-07-02 19:44:59] __main__ INFO: \u001b[0mEpoch 33 Step 250/351 lr 0.100000 loss 1.7072 (1.6687) acc@1 0.3750 (0.3828) acc@5 0.7266 (0.7718)\n",
      "\u001b[32m[2020-07-02 19:45:04] __main__ INFO: \u001b[0mEpoch 33 Step 300/351 lr 0.100000 loss 1.6784 (1.6681) acc@1 0.3672 (0.3816) acc@5 0.7344 (0.7724)\n",
      "\u001b[32m[2020-07-02 19:45:09] __main__ INFO: \u001b[0mEpoch 33 Step 350/351 lr 0.100000 loss 1.5950 (1.6669) acc@1 0.3984 (0.3828) acc@5 0.7969 (0.7719)\n",
      "\u001b[32m[2020-07-02 19:45:09] __main__ INFO: \u001b[0mEpoch 33 Step 351/351 lr 0.100000 loss 1.6635 (1.6669) acc@1 0.3672 (0.3828) acc@5 0.7891 (0.7719)\n",
      "\u001b[32m[2020-07-02 19:45:09] __main__ INFO: \u001b[0mElapsed 33.59\n",
      "\u001b[32m[2020-07-02 19:45:09] __main__ INFO: \u001b[0mVal 33\n",
      "\u001b[32m[2020-07-02 19:45:10] __main__ INFO: \u001b[0mEpoch 33 loss 1.7090 acc@1 0.3748 acc@5 0.7636\n",
      "\u001b[32m[2020-07-02 19:45:10] __main__ INFO: \u001b[0mElapsed 1.08\n",
      "\u001b[32m[2020-07-02 19:45:10] __main__ INFO: \u001b[0mTrain 34 11583\n",
      "\u001b[32m[2020-07-02 19:45:15] __main__ INFO: \u001b[0mEpoch 34 Step 50/351 lr 0.100000 loss 1.4578 (1.6511) acc@1 0.4531 (0.3919) acc@5 0.8047 (0.7759)\n",
      "\u001b[32m[2020-07-02 19:45:20] __main__ INFO: \u001b[0mEpoch 34 Step 100/351 lr 0.100000 loss 1.5624 (1.6542) acc@1 0.4141 (0.3859) acc@5 0.7656 (0.7723)\n",
      "\u001b[32m[2020-07-02 19:45:24] __main__ INFO: \u001b[0mEpoch 34 Step 150/351 lr 0.100000 loss 1.6131 (1.6494) acc@1 0.3906 (0.3886) acc@5 0.7969 (0.7770)\n",
      "\u001b[32m[2020-07-02 19:45:29] __main__ INFO: \u001b[0mEpoch 34 Step 200/351 lr 0.100000 loss 1.7099 (1.6523) acc@1 0.3594 (0.3862) acc@5 0.7344 (0.7741)\n",
      "\u001b[32m[2020-07-02 19:45:34] __main__ INFO: \u001b[0mEpoch 34 Step 250/351 lr 0.100000 loss 1.5933 (1.6516) acc@1 0.3984 (0.3867) acc@5 0.7578 (0.7737)\n",
      "\u001b[32m[2020-07-02 19:45:39] __main__ INFO: \u001b[0mEpoch 34 Step 300/351 lr 0.100000 loss 1.8105 (1.6568) acc@1 0.3125 (0.3844) acc@5 0.7188 (0.7733)\n",
      "\u001b[32m[2020-07-02 19:45:43] __main__ INFO: \u001b[0mEpoch 34 Step 350/351 lr 0.100000 loss 1.5621 (1.6586) acc@1 0.4609 (0.3840) acc@5 0.8203 (0.7731)\n",
      "\u001b[32m[2020-07-02 19:45:44] __main__ INFO: \u001b[0mEpoch 34 Step 351/351 lr 0.100000 loss 1.7869 (1.6589) acc@1 0.3516 (0.3839) acc@5 0.7500 (0.7730)\n",
      "\u001b[32m[2020-07-02 19:45:44] __main__ INFO: \u001b[0mElapsed 33.62\n",
      "\u001b[32m[2020-07-02 19:45:44] __main__ INFO: \u001b[0mVal 34\n",
      "\u001b[32m[2020-07-02 19:45:45] __main__ INFO: \u001b[0mEpoch 34 loss 1.7619 acc@1 0.3604 acc@5 0.7574\n",
      "\u001b[32m[2020-07-02 19:45:45] __main__ INFO: \u001b[0mElapsed 1.05\n",
      "\u001b[32m[2020-07-02 19:45:45] __main__ INFO: \u001b[0mTrain 35 11934\n",
      "\u001b[32m[2020-07-02 19:45:50] __main__ INFO: \u001b[0mEpoch 35 Step 50/351 lr 0.100000 loss 1.6389 (1.6406) acc@1 0.4375 (0.3931) acc@5 0.7812 (0.7802)\n",
      "\u001b[32m[2020-07-02 19:45:54] __main__ INFO: \u001b[0mEpoch 35 Step 100/351 lr 0.100000 loss 1.4615 (1.6411) acc@1 0.4453 (0.3905) acc@5 0.7891 (0.7757)\n",
      "\u001b[32m[2020-07-02 19:45:59] __main__ INFO: \u001b[0mEpoch 35 Step 150/351 lr 0.100000 loss 1.7375 (1.6409) acc@1 0.3203 (0.3915) acc@5 0.7734 (0.7738)\n",
      "\u001b[32m[2020-07-02 19:46:04] __main__ INFO: \u001b[0mEpoch 35 Step 200/351 lr 0.100000 loss 1.7756 (1.6416) acc@1 0.3828 (0.3922) acc@5 0.7656 (0.7723)\n",
      "\u001b[32m[2020-07-02 19:46:09] __main__ INFO: \u001b[0mEpoch 35 Step 250/351 lr 0.100000 loss 1.9320 (1.6455) acc@1 0.3047 (0.3901) acc@5 0.7578 (0.7721)\n",
      "\u001b[32m[2020-07-02 19:46:13] __main__ INFO: \u001b[0mEpoch 35 Step 300/351 lr 0.100000 loss 1.7570 (1.6484) acc@1 0.3750 (0.3887) acc@5 0.8047 (0.7731)\n",
      "\u001b[32m[2020-07-02 19:46:18] __main__ INFO: \u001b[0mEpoch 35 Step 350/351 lr 0.100000 loss 1.7492 (1.6474) acc@1 0.3438 (0.3896) acc@5 0.7500 (0.7736)\n",
      "\u001b[32m[2020-07-02 19:46:18] __main__ INFO: \u001b[0mEpoch 35 Step 351/351 lr 0.100000 loss 1.7584 (1.6478) acc@1 0.3359 (0.3894) acc@5 0.7578 (0.7735)\n",
      "\u001b[32m[2020-07-02 19:46:18] __main__ INFO: \u001b[0mElapsed 33.53\n",
      "\u001b[32m[2020-07-02 19:46:18] __main__ INFO: \u001b[0mVal 35\n",
      "\u001b[32m[2020-07-02 19:46:19] __main__ INFO: \u001b[0mEpoch 35 loss 1.6667 acc@1 0.3832 acc@5 0.7674\n",
      "\u001b[32m[2020-07-02 19:46:19] __main__ INFO: \u001b[0mElapsed 1.05\n",
      "\u001b[32m[2020-07-02 19:46:19] __main__ INFO: \u001b[0mTrain 36 12285\n",
      "\u001b[32m[2020-07-02 19:46:24] __main__ INFO: \u001b[0mEpoch 36 Step 50/351 lr 0.100000 loss 1.5214 (1.6153) acc@1 0.4688 (0.4036) acc@5 0.8516 (0.7811)\n",
      "\u001b[32m[2020-07-02 19:46:29] __main__ INFO: \u001b[0mEpoch 36 Step 100/351 lr 0.100000 loss 1.5541 (1.6201) acc@1 0.4375 (0.4002) acc@5 0.7891 (0.7798)\n",
      "\u001b[32m[2020-07-02 19:46:34] __main__ INFO: \u001b[0mEpoch 36 Step 150/351 lr 0.100000 loss 1.5128 (1.6172) acc@1 0.4688 (0.4005) acc@5 0.8281 (0.7777)\n",
      "\u001b[32m[2020-07-02 19:46:38] __main__ INFO: \u001b[0mEpoch 36 Step 200/351 lr 0.100000 loss 1.6889 (1.6286) acc@1 0.3516 (0.3969) acc@5 0.7812 (0.7739)\n",
      "\u001b[32m[2020-07-02 19:46:43] __main__ INFO: \u001b[0mEpoch 36 Step 250/351 lr 0.100000 loss 1.7526 (1.6342) acc@1 0.3516 (0.3957) acc@5 0.7422 (0.7742)\n",
      "\u001b[32m[2020-07-02 19:46:48] __main__ INFO: \u001b[0mEpoch 36 Step 300/351 lr 0.100000 loss 1.6415 (1.6366) acc@1 0.3672 (0.3957) acc@5 0.8203 (0.7743)\n",
      "\u001b[32m[2020-07-02 19:46:53] __main__ INFO: \u001b[0mEpoch 36 Step 350/351 lr 0.100000 loss 1.5314 (1.6403) acc@1 0.4375 (0.3936) acc@5 0.8125 (0.7741)\n",
      "\u001b[32m[2020-07-02 19:46:53] __main__ INFO: \u001b[0mEpoch 36 Step 351/351 lr 0.100000 loss 1.5786 (1.6401) acc@1 0.3984 (0.3937) acc@5 0.7891 (0.7741)\n",
      "\u001b[32m[2020-07-02 19:46:53] __main__ INFO: \u001b[0mElapsed 33.48\n",
      "\u001b[32m[2020-07-02 19:46:53] __main__ INFO: \u001b[0mVal 36\n",
      "\u001b[32m[2020-07-02 19:46:54] __main__ INFO: \u001b[0mEpoch 36 loss 1.7302 acc@1 0.3684 acc@5 0.7704\n",
      "\u001b[32m[2020-07-02 19:46:54] __main__ INFO: \u001b[0mElapsed 1.07\n",
      "\u001b[32m[2020-07-02 19:46:54] __main__ INFO: \u001b[0mTrain 37 12636\n",
      "\u001b[32m[2020-07-02 19:46:59] __main__ INFO: \u001b[0mEpoch 37 Step 50/351 lr 0.100000 loss 1.5145 (1.6312) acc@1 0.4688 (0.3980) acc@5 0.7734 (0.7733)\n",
      "\u001b[32m[2020-07-02 19:47:03] __main__ INFO: \u001b[0mEpoch 37 Step 100/351 lr 0.100000 loss 1.5308 (1.6349) acc@1 0.4219 (0.3962) acc@5 0.8672 (0.7741)\n",
      "\u001b[32m[2020-07-02 19:47:08] __main__ INFO: \u001b[0mEpoch 37 Step 150/351 lr 0.100000 loss 1.6964 (1.6336) acc@1 0.3750 (0.3953) acc@5 0.7500 (0.7742)\n",
      "\u001b[32m[2020-07-02 19:47:13] __main__ INFO: \u001b[0mEpoch 37 Step 200/351 lr 0.100000 loss 1.7981 (1.6331) acc@1 0.3125 (0.3955) acc@5 0.7734 (0.7741)\n",
      "\u001b[32m[2020-07-02 19:47:18] __main__ INFO: \u001b[0mEpoch 37 Step 250/351 lr 0.100000 loss 1.6545 (1.6293) acc@1 0.3984 (0.3970) acc@5 0.7969 (0.7751)\n",
      "\u001b[32m[2020-07-02 19:47:22] __main__ INFO: \u001b[0mEpoch 37 Step 300/351 lr 0.100000 loss 1.4781 (1.6304) acc@1 0.4844 (0.3965) acc@5 0.8047 (0.7743)\n",
      "\u001b[32m[2020-07-02 19:47:27] __main__ INFO: \u001b[0mEpoch 37 Step 350/351 lr 0.100000 loss 1.6014 (1.6295) acc@1 0.3828 (0.3966) acc@5 0.7969 (0.7760)\n",
      "\u001b[32m[2020-07-02 19:47:27] __main__ INFO: \u001b[0mEpoch 37 Step 351/351 lr 0.100000 loss 1.6686 (1.6296) acc@1 0.3984 (0.3966) acc@5 0.7734 (0.7760)\n",
      "\u001b[32m[2020-07-02 19:47:27] __main__ INFO: \u001b[0mElapsed 33.45\n",
      "\u001b[32m[2020-07-02 19:47:27] __main__ INFO: \u001b[0mVal 37\n",
      "\u001b[32m[2020-07-02 19:47:28] __main__ INFO: \u001b[0mEpoch 37 loss 1.6949 acc@1 0.3810 acc@5 0.7672\n",
      "\u001b[32m[2020-07-02 19:47:28] __main__ INFO: \u001b[0mElapsed 1.05\n",
      "\u001b[32m[2020-07-02 19:47:28] __main__ INFO: \u001b[0mTrain 38 12987\n",
      "\u001b[32m[2020-07-02 19:47:33] __main__ INFO: \u001b[0mEpoch 38 Step 50/351 lr 0.100000 loss 1.7220 (1.6360) acc@1 0.3125 (0.3953) acc@5 0.8047 (0.7772)\n",
      "\u001b[32m[2020-07-02 19:47:38] __main__ INFO: \u001b[0mEpoch 38 Step 100/351 lr 0.100000 loss 1.4745 (1.6203) acc@1 0.4844 (0.4024) acc@5 0.8047 (0.7837)\n",
      "\u001b[32m[2020-07-02 19:47:43] __main__ INFO: \u001b[0mEpoch 38 Step 150/351 lr 0.100000 loss 1.5749 (1.6131) acc@1 0.4141 (0.4016) acc@5 0.7734 (0.7828)\n",
      "\u001b[32m[2020-07-02 19:47:47] __main__ INFO: \u001b[0mEpoch 38 Step 200/351 lr 0.100000 loss 1.5237 (1.6173) acc@1 0.3828 (0.3991) acc@5 0.7578 (0.7807)\n",
      "\u001b[32m[2020-07-02 19:47:52] __main__ INFO: \u001b[0mEpoch 38 Step 250/351 lr 0.100000 loss 1.5132 (1.6159) acc@1 0.4375 (0.4008) acc@5 0.8047 (0.7835)\n",
      "\u001b[32m[2020-07-02 19:47:57] __main__ INFO: \u001b[0mEpoch 38 Step 300/351 lr 0.100000 loss 1.4801 (1.6189) acc@1 0.4531 (0.4000) acc@5 0.8203 (0.7811)\n",
      "\u001b[32m[2020-07-02 19:48:02] __main__ INFO: \u001b[0mEpoch 38 Step 350/351 lr 0.100000 loss 1.7148 (1.6230) acc@1 0.3438 (0.3983) acc@5 0.7656 (0.7793)\n",
      "\u001b[32m[2020-07-02 19:48:02] __main__ INFO: \u001b[0mEpoch 38 Step 351/351 lr 0.100000 loss 1.6570 (1.6231) acc@1 0.4297 (0.3983) acc@5 0.7656 (0.7793)\n",
      "\u001b[32m[2020-07-02 19:48:02] __main__ INFO: \u001b[0mElapsed 33.48\n",
      "\u001b[32m[2020-07-02 19:48:02] __main__ INFO: \u001b[0mVal 38\n",
      "\u001b[32m[2020-07-02 19:48:03] __main__ INFO: \u001b[0mEpoch 38 loss 1.7494 acc@1 0.3658 acc@5 0.7546\n",
      "\u001b[32m[2020-07-02 19:48:03] __main__ INFO: \u001b[0mElapsed 1.08\n",
      "\u001b[32m[2020-07-02 19:48:03] __main__ INFO: \u001b[0mTrain 39 13338\n",
      "\u001b[32m[2020-07-02 19:48:08] __main__ INFO: \u001b[0mEpoch 39 Step 50/351 lr 0.100000 loss 1.6063 (1.6103) acc@1 0.3984 (0.4100) acc@5 0.7500 (0.7792)\n",
      "\u001b[32m[2020-07-02 19:48:12] __main__ INFO: \u001b[0mEpoch 39 Step 100/351 lr 0.100000 loss 1.5019 (1.6047) acc@1 0.3984 (0.4064) acc@5 0.7969 (0.7778)\n",
      "\u001b[32m[2020-07-02 19:48:17] __main__ INFO: \u001b[0mEpoch 39 Step 150/351 lr 0.100000 loss 1.4945 (1.6008) acc@1 0.4609 (0.4092) acc@5 0.8359 (0.7771)\n",
      "\u001b[32m[2020-07-02 19:48:22] __main__ INFO: \u001b[0mEpoch 39 Step 200/351 lr 0.100000 loss 1.5245 (1.6060) acc@1 0.4453 (0.4052) acc@5 0.8203 (0.7771)\n",
      "\u001b[32m[2020-07-02 19:48:27] __main__ INFO: \u001b[0mEpoch 39 Step 250/351 lr 0.100000 loss 1.6630 (1.6050) acc@1 0.3828 (0.4060) acc@5 0.7500 (0.7788)\n",
      "\u001b[32m[2020-07-02 19:48:32] __main__ INFO: \u001b[0mEpoch 39 Step 300/351 lr 0.100000 loss 1.6196 (1.6064) acc@1 0.3516 (0.4055) acc@5 0.7656 (0.7778)\n",
      "\u001b[32m[2020-07-02 19:48:36] __main__ INFO: \u001b[0mEpoch 39 Step 350/351 lr 0.100000 loss 1.5628 (1.6112) acc@1 0.3984 (0.4033) acc@5 0.8281 (0.7779)\n",
      "\u001b[32m[2020-07-02 19:48:36] __main__ INFO: \u001b[0mEpoch 39 Step 351/351 lr 0.100000 loss 1.6651 (1.6113) acc@1 0.4062 (0.4033) acc@5 0.7266 (0.7777)\n",
      "\u001b[32m[2020-07-02 19:48:36] __main__ INFO: \u001b[0mElapsed 33.57\n",
      "\u001b[32m[2020-07-02 19:48:36] __main__ INFO: \u001b[0mVal 39\n",
      "\u001b[32m[2020-07-02 19:48:38] __main__ INFO: \u001b[0mEpoch 39 loss 1.7210 acc@1 0.3826 acc@5 0.7634\n",
      "\u001b[32m[2020-07-02 19:48:38] __main__ INFO: \u001b[0mElapsed 1.08\n",
      "\u001b[32m[2020-07-02 19:48:38] __main__ INFO: \u001b[0mTrain 40 13689\n",
      "\u001b[32m[2020-07-02 19:48:42] __main__ INFO: \u001b[0mEpoch 40 Step 50/351 lr 0.100000 loss 1.6529 (1.5877) acc@1 0.3828 (0.4138) acc@5 0.7344 (0.7822)\n",
      "\u001b[32m[2020-07-02 19:48:47] __main__ INFO: \u001b[0mEpoch 40 Step 100/351 lr 0.100000 loss 1.5274 (1.5977) acc@1 0.4141 (0.4073) acc@5 0.7734 (0.7821)\n",
      "\u001b[32m[2020-07-02 19:48:52] __main__ INFO: \u001b[0mEpoch 40 Step 150/351 lr 0.100000 loss 1.6825 (1.5992) acc@1 0.3672 (0.4055) acc@5 0.7500 (0.7810)\n",
      "\u001b[32m[2020-07-02 19:48:57] __main__ INFO: \u001b[0mEpoch 40 Step 200/351 lr 0.100000 loss 1.6081 (1.5985) acc@1 0.3984 (0.4053) acc@5 0.7578 (0.7818)\n",
      "\u001b[32m[2020-07-02 19:49:01] __main__ INFO: \u001b[0mEpoch 40 Step 250/351 lr 0.100000 loss 1.5167 (1.5947) acc@1 0.3828 (0.4076) acc@5 0.7422 (0.7818)\n",
      "\u001b[32m[2020-07-02 19:49:06] __main__ INFO: \u001b[0mEpoch 40 Step 300/351 lr 0.100000 loss 1.5986 (1.5958) acc@1 0.3984 (0.4068) acc@5 0.7891 (0.7815)\n",
      "\u001b[32m[2020-07-02 19:49:11] __main__ INFO: \u001b[0mEpoch 40 Step 350/351 lr 0.100000 loss 1.7111 (1.6020) acc@1 0.3281 (0.4042) acc@5 0.7656 (0.7796)\n",
      "\u001b[32m[2020-07-02 19:49:11] __main__ INFO: \u001b[0mEpoch 40 Step 351/351 lr 0.100000 loss 1.6646 (1.6021) acc@1 0.3906 (0.4041) acc@5 0.7812 (0.7796)\n",
      "\u001b[32m[2020-07-02 19:49:11] __main__ INFO: \u001b[0mElapsed 33.46\n",
      "\u001b[32m[2020-07-02 19:49:11] __main__ INFO: \u001b[0mVal 40\n",
      "\u001b[32m[2020-07-02 19:49:12] __main__ INFO: \u001b[0mEpoch 40 loss 1.7513 acc@1 0.3680 acc@5 0.7614\n",
      "\u001b[32m[2020-07-02 19:49:12] __main__ INFO: \u001b[0mElapsed 1.06\n",
      "\u001b[32m[2020-07-02 19:49:12] __main__ INFO: \u001b[0mTrain 41 14040\n",
      "\u001b[32m[2020-07-02 19:49:17] __main__ INFO: \u001b[0mEpoch 41 Step 50/351 lr 0.100000 loss 1.5973 (1.5808) acc@1 0.3672 (0.4156) acc@5 0.8203 (0.7747)\n",
      "\u001b[32m[2020-07-02 19:49:22] __main__ INFO: \u001b[0mEpoch 41 Step 100/351 lr 0.100000 loss 1.6802 (1.5913) acc@1 0.4141 (0.4112) acc@5 0.7656 (0.7747)\n",
      "\u001b[32m[2020-07-02 19:49:26] __main__ INFO: \u001b[0mEpoch 41 Step 150/351 lr 0.100000 loss 1.5467 (1.5885) acc@1 0.4375 (0.4120) acc@5 0.7734 (0.7745)\n",
      "\u001b[32m[2020-07-02 19:49:31] __main__ INFO: \u001b[0mEpoch 41 Step 200/351 lr 0.100000 loss 1.4869 (1.5876) acc@1 0.4219 (0.4125) acc@5 0.8750 (0.7751)\n",
      "\u001b[32m[2020-07-02 19:49:36] __main__ INFO: \u001b[0mEpoch 41 Step 250/351 lr 0.100000 loss 1.5467 (1.5899) acc@1 0.3984 (0.4118) acc@5 0.7344 (0.7768)\n",
      "\u001b[32m[2020-07-02 19:49:41] __main__ INFO: \u001b[0mEpoch 41 Step 300/351 lr 0.100000 loss 1.4788 (1.5917) acc@1 0.4297 (0.4113) acc@5 0.7891 (0.7776)\n",
      "\u001b[32m[2020-07-02 19:49:45] __main__ INFO: \u001b[0mEpoch 41 Step 350/351 lr 0.100000 loss 1.5008 (1.5936) acc@1 0.4375 (0.4095) acc@5 0.7734 (0.7772)\n",
      "\u001b[32m[2020-07-02 19:49:45] __main__ INFO: \u001b[0mEpoch 41 Step 351/351 lr 0.100000 loss 1.4756 (1.5933) acc@1 0.4141 (0.4095) acc@5 0.8516 (0.7774)\n",
      "\u001b[32m[2020-07-02 19:49:46] __main__ INFO: \u001b[0mElapsed 33.49\n",
      "\u001b[32m[2020-07-02 19:49:46] __main__ INFO: \u001b[0mVal 41\n",
      "\u001b[32m[2020-07-02 19:49:47] __main__ INFO: \u001b[0mEpoch 41 loss 1.7982 acc@1 0.3584 acc@5 0.7562\n",
      "\u001b[32m[2020-07-02 19:49:47] __main__ INFO: \u001b[0mElapsed 1.07\n",
      "\u001b[32m[2020-07-02 19:49:47] __main__ INFO: \u001b[0mTrain 42 14391\n",
      "\u001b[32m[2020-07-02 19:49:51] __main__ INFO: \u001b[0mEpoch 42 Step 50/351 lr 0.100000 loss 1.5739 (1.5710) acc@1 0.5234 (0.4148) acc@5 0.7812 (0.7794)\n",
      "\u001b[32m[2020-07-02 19:49:56] __main__ INFO: \u001b[0mEpoch 42 Step 100/351 lr 0.100000 loss 1.7140 (1.5776) acc@1 0.3594 (0.4139) acc@5 0.7500 (0.7842)\n",
      "\u001b[32m[2020-07-02 19:50:01] __main__ INFO: \u001b[0mEpoch 42 Step 150/351 lr 0.100000 loss 1.5943 (1.5764) acc@1 0.3516 (0.4143) acc@5 0.7734 (0.7866)\n",
      "\u001b[32m[2020-07-02 19:50:06] __main__ INFO: \u001b[0mEpoch 42 Step 200/351 lr 0.100000 loss 1.5802 (1.5826) acc@1 0.3984 (0.4117) acc@5 0.7656 (0.7862)\n",
      "\u001b[32m[2020-07-02 19:50:10] __main__ INFO: \u001b[0mEpoch 42 Step 250/351 lr 0.100000 loss 1.4831 (1.5833) acc@1 0.4688 (0.4132) acc@5 0.8047 (0.7854)\n",
      "\u001b[32m[2020-07-02 19:50:15] __main__ INFO: \u001b[0mEpoch 42 Step 300/351 lr 0.100000 loss 1.6759 (1.5852) acc@1 0.3906 (0.4128) acc@5 0.7969 (0.7843)\n",
      "\u001b[32m[2020-07-02 19:50:20] __main__ INFO: \u001b[0mEpoch 42 Step 350/351 lr 0.100000 loss 1.4092 (1.5870) acc@1 0.4219 (0.4110) acc@5 0.8203 (0.7837)\n",
      "\u001b[32m[2020-07-02 19:50:20] __main__ INFO: \u001b[0mEpoch 42 Step 351/351 lr 0.100000 loss 1.7632 (1.5875) acc@1 0.3594 (0.4109) acc@5 0.6797 (0.7834)\n",
      "\u001b[32m[2020-07-02 19:50:20] __main__ INFO: \u001b[0mElapsed 33.51\n",
      "\u001b[32m[2020-07-02 19:50:20] __main__ INFO: \u001b[0mVal 42\n",
      "\u001b[32m[2020-07-02 19:50:21] __main__ INFO: \u001b[0mEpoch 42 loss 1.6594 acc@1 0.3964 acc@5 0.7788\n",
      "\u001b[32m[2020-07-02 19:50:21] __main__ INFO: \u001b[0mElapsed 1.07\n",
      "\u001b[32m[2020-07-02 19:50:21] __main__ INFO: \u001b[0mTrain 43 14742\n",
      "\u001b[32m[2020-07-02 19:50:26] __main__ INFO: \u001b[0mEpoch 43 Step 50/351 lr 0.100000 loss 1.4208 (1.5862) acc@1 0.4766 (0.4113) acc@5 0.7656 (0.7777)\n",
      "\u001b[32m[2020-07-02 19:50:31] __main__ INFO: \u001b[0mEpoch 43 Step 100/351 lr 0.100000 loss 1.5398 (1.5774) acc@1 0.3906 (0.4118) acc@5 0.8047 (0.7791)\n",
      "\u001b[32m[2020-07-02 19:50:36] __main__ INFO: \u001b[0mEpoch 43 Step 150/351 lr 0.100000 loss 1.4192 (1.5691) acc@1 0.4219 (0.4158) acc@5 0.8047 (0.7816)\n",
      "\u001b[32m[2020-07-02 19:50:40] __main__ INFO: \u001b[0mEpoch 43 Step 200/351 lr 0.100000 loss 1.5510 (1.5712) acc@1 0.4375 (0.4143) acc@5 0.7812 (0.7796)\n",
      "\u001b[32m[2020-07-02 19:50:45] __main__ INFO: \u001b[0mEpoch 43 Step 250/351 lr 0.100000 loss 1.6448 (1.5757) acc@1 0.4062 (0.4122) acc@5 0.7578 (0.7786)\n",
      "\u001b[32m[2020-07-02 19:50:50] __main__ INFO: \u001b[0mEpoch 43 Step 300/351 lr 0.100000 loss 1.7640 (1.5808) acc@1 0.3672 (0.4109) acc@5 0.7188 (0.7782)\n",
      "\u001b[32m[2020-07-02 19:50:55] __main__ INFO: \u001b[0mEpoch 43 Step 350/351 lr 0.100000 loss 1.7401 (1.5799) acc@1 0.3828 (0.4120) acc@5 0.7422 (0.7788)\n",
      "\u001b[32m[2020-07-02 19:50:55] __main__ INFO: \u001b[0mEpoch 43 Step 351/351 lr 0.100000 loss 1.5724 (1.5799) acc@1 0.4141 (0.4120) acc@5 0.7422 (0.7787)\n",
      "\u001b[32m[2020-07-02 19:50:55] __main__ INFO: \u001b[0mElapsed 33.50\n",
      "\u001b[32m[2020-07-02 19:50:55] __main__ INFO: \u001b[0mVal 43\n",
      "\u001b[32m[2020-07-02 19:50:56] __main__ INFO: \u001b[0mEpoch 43 loss 1.6834 acc@1 0.3926 acc@5 0.7760\n",
      "\u001b[32m[2020-07-02 19:50:56] __main__ INFO: \u001b[0mElapsed 1.07\n",
      "\u001b[32m[2020-07-02 19:50:56] __main__ INFO: \u001b[0mTrain 44 15093\n",
      "\u001b[32m[2020-07-02 19:51:01] __main__ INFO: \u001b[0mEpoch 44 Step 50/351 lr 0.100000 loss 1.6313 (1.5551) acc@1 0.4453 (0.4259) acc@5 0.7734 (0.7820)\n",
      "\u001b[32m[2020-07-02 19:51:05] __main__ INFO: \u001b[0mEpoch 44 Step 100/351 lr 0.100000 loss 1.8864 (1.5717) acc@1 0.2969 (0.4180) acc@5 0.7266 (0.7832)\n",
      "\u001b[32m[2020-07-02 19:51:10] __main__ INFO: \u001b[0mEpoch 44 Step 150/351 lr 0.100000 loss 1.5886 (1.5657) acc@1 0.3984 (0.4189) acc@5 0.7891 (0.7855)\n",
      "\u001b[32m[2020-07-02 19:51:15] __main__ INFO: \u001b[0mEpoch 44 Step 200/351 lr 0.100000 loss 1.6000 (1.5711) acc@1 0.3750 (0.4175) acc@5 0.8203 (0.7848)\n",
      "\u001b[32m[2020-07-02 19:51:20] __main__ INFO: \u001b[0mEpoch 44 Step 250/351 lr 0.100000 loss 1.4707 (1.5732) acc@1 0.4609 (0.4173) acc@5 0.7812 (0.7843)\n",
      "\u001b[32m[2020-07-02 19:51:24] __main__ INFO: \u001b[0mEpoch 44 Step 300/351 lr 0.100000 loss 1.6404 (1.5758) acc@1 0.3906 (0.4154) acc@5 0.7734 (0.7844)\n",
      "\u001b[32m[2020-07-02 19:51:29] __main__ INFO: \u001b[0mEpoch 44 Step 350/351 lr 0.100000 loss 1.5670 (1.5761) acc@1 0.4453 (0.4147) acc@5 0.8047 (0.7846)\n",
      "\u001b[32m[2020-07-02 19:51:29] __main__ INFO: \u001b[0mEpoch 44 Step 351/351 lr 0.100000 loss 1.5361 (1.5760) acc@1 0.4453 (0.4148) acc@5 0.7734 (0.7846)\n",
      "\u001b[32m[2020-07-02 19:51:29] __main__ INFO: \u001b[0mElapsed 33.49\n",
      "\u001b[32m[2020-07-02 19:51:29] __main__ INFO: \u001b[0mVal 44\n",
      "\u001b[32m[2020-07-02 19:51:30] __main__ INFO: \u001b[0mEpoch 44 loss 1.7106 acc@1 0.3884 acc@5 0.7696\n",
      "\u001b[32m[2020-07-02 19:51:30] __main__ INFO: \u001b[0mElapsed 1.05\n",
      "\u001b[32m[2020-07-02 19:51:30] __main__ INFO: \u001b[0mTrain 45 15444\n",
      "\u001b[32m[2020-07-02 19:51:35] __main__ INFO: \u001b[0mEpoch 45 Step 50/351 lr 0.100000 loss 1.5593 (1.5609) acc@1 0.4297 (0.4189) acc@5 0.7344 (0.7836)\n",
      "\u001b[32m[2020-07-02 19:51:40] __main__ INFO: \u001b[0mEpoch 45 Step 100/351 lr 0.100000 loss 1.5887 (1.5610) acc@1 0.3828 (0.4180) acc@5 0.7891 (0.7865)\n",
      "\u001b[32m[2020-07-02 19:51:45] __main__ INFO: \u001b[0mEpoch 45 Step 150/351 lr 0.100000 loss 1.5649 (1.5551) acc@1 0.4297 (0.4216) acc@5 0.7266 (0.7878)\n",
      "\u001b[32m[2020-07-02 19:51:49] __main__ INFO: \u001b[0mEpoch 45 Step 200/351 lr 0.100000 loss 1.5935 (1.5547) acc@1 0.4453 (0.4216) acc@5 0.7812 (0.7889)\n",
      "\u001b[32m[2020-07-02 19:51:54] __main__ INFO: \u001b[0mEpoch 45 Step 250/351 lr 0.100000 loss 1.5257 (1.5584) acc@1 0.4688 (0.4209) acc@5 0.7578 (0.7882)\n",
      "\u001b[32m[2020-07-02 19:51:59] __main__ INFO: \u001b[0mEpoch 45 Step 300/351 lr 0.100000 loss 1.6092 (1.5637) acc@1 0.4297 (0.4189) acc@5 0.7969 (0.7864)\n",
      "\u001b[32m[2020-07-02 19:52:04] __main__ INFO: \u001b[0mEpoch 45 Step 350/351 lr 0.100000 loss 1.6518 (1.5667) acc@1 0.3828 (0.4176) acc@5 0.7578 (0.7855)\n",
      "\u001b[32m[2020-07-02 19:52:04] __main__ INFO: \u001b[0mEpoch 45 Step 351/351 lr 0.100000 loss 1.5260 (1.5666) acc@1 0.4453 (0.4176) acc@5 0.8203 (0.7856)\n",
      "\u001b[32m[2020-07-02 19:52:04] __main__ INFO: \u001b[0mElapsed 33.43\n",
      "\u001b[32m[2020-07-02 19:52:04] __main__ INFO: \u001b[0mVal 45\n",
      "\u001b[32m[2020-07-02 19:52:05] __main__ INFO: \u001b[0mEpoch 45 loss 1.7083 acc@1 0.3892 acc@5 0.7672\n",
      "\u001b[32m[2020-07-02 19:52:05] __main__ INFO: \u001b[0mElapsed 1.06\n",
      "\u001b[32m[2020-07-02 19:52:05] __main__ INFO: \u001b[0mTrain 46 15795\n",
      "\u001b[32m[2020-07-02 19:52:10] __main__ INFO: \u001b[0mEpoch 46 Step 50/351 lr 0.100000 loss 1.6517 (1.5772) acc@1 0.3906 (0.4134) acc@5 0.7734 (0.7802)\n",
      "\u001b[32m[2020-07-02 19:52:14] __main__ INFO: \u001b[0mEpoch 46 Step 100/351 lr 0.100000 loss 1.5253 (1.5624) acc@1 0.4609 (0.4216) acc@5 0.7812 (0.7820)\n",
      "\u001b[32m[2020-07-02 19:52:19] __main__ INFO: \u001b[0mEpoch 46 Step 150/351 lr 0.100000 loss 1.5832 (1.5581) acc@1 0.4062 (0.4243) acc@5 0.7891 (0.7814)\n",
      "\u001b[32m[2020-07-02 19:52:24] __main__ INFO: \u001b[0mEpoch 46 Step 200/351 lr 0.100000 loss 1.6453 (1.5558) acc@1 0.4219 (0.4246) acc@5 0.7812 (0.7834)\n",
      "\u001b[32m[2020-07-02 19:52:29] __main__ INFO: \u001b[0mEpoch 46 Step 250/351 lr 0.100000 loss 1.5809 (1.5562) acc@1 0.4219 (0.4229) acc@5 0.8125 (0.7845)\n",
      "\u001b[32m[2020-07-02 19:52:33] __main__ INFO: \u001b[0mEpoch 46 Step 300/351 lr 0.100000 loss 1.6814 (1.5571) acc@1 0.3516 (0.4227) acc@5 0.7812 (0.7838)\n",
      "\u001b[32m[2020-07-02 19:52:38] __main__ INFO: \u001b[0mEpoch 46 Step 350/351 lr 0.100000 loss 1.6891 (1.5588) acc@1 0.3281 (0.4218) acc@5 0.7734 (0.7842)\n",
      "\u001b[32m[2020-07-02 19:52:38] __main__ INFO: \u001b[0mEpoch 46 Step 351/351 lr 0.100000 loss 1.6420 (1.5590) acc@1 0.4297 (0.4218) acc@5 0.7578 (0.7841)\n",
      "\u001b[32m[2020-07-02 19:52:38] __main__ INFO: \u001b[0mElapsed 33.52\n",
      "\u001b[32m[2020-07-02 19:52:38] __main__ INFO: \u001b[0mVal 46\n",
      "\u001b[32m[2020-07-02 19:52:39] __main__ INFO: \u001b[0mEpoch 46 loss 1.7611 acc@1 0.3688 acc@5 0.7554\n",
      "\u001b[32m[2020-07-02 19:52:39] __main__ INFO: \u001b[0mElapsed 1.09\n",
      "\u001b[32m[2020-07-02 19:52:39] __main__ INFO: \u001b[0mTrain 47 16146\n",
      "\u001b[32m[2020-07-02 19:52:44] __main__ INFO: \u001b[0mEpoch 47 Step 50/351 lr 0.100000 loss 1.4687 (1.5559) acc@1 0.4062 (0.4252) acc@5 0.8359 (0.7830)\n",
      "\u001b[32m[2020-07-02 19:52:49] __main__ INFO: \u001b[0mEpoch 47 Step 100/351 lr 0.100000 loss 1.6722 (1.5515) acc@1 0.3438 (0.4273) acc@5 0.7188 (0.7820)\n",
      "\u001b[32m[2020-07-02 19:52:54] __main__ INFO: \u001b[0mEpoch 47 Step 150/351 lr 0.100000 loss 1.6581 (1.5504) acc@1 0.3750 (0.4258) acc@5 0.7578 (0.7839)\n",
      "\u001b[32m[2020-07-02 19:52:59] __main__ INFO: \u001b[0mEpoch 47 Step 200/351 lr 0.100000 loss 1.6556 (1.5508) acc@1 0.3750 (0.4275) acc@5 0.7969 (0.7838)\n",
      "\u001b[32m[2020-07-02 19:53:03] __main__ INFO: \u001b[0mEpoch 47 Step 250/351 lr 0.100000 loss 1.5524 (1.5482) acc@1 0.4062 (0.4275) acc@5 0.8047 (0.7851)\n",
      "\u001b[32m[2020-07-02 19:53:08] __main__ INFO: \u001b[0mEpoch 47 Step 300/351 lr 0.100000 loss 1.5290 (1.5527) acc@1 0.4375 (0.4255) acc@5 0.7734 (0.7848)\n",
      "\u001b[32m[2020-07-02 19:53:13] __main__ INFO: \u001b[0mEpoch 47 Step 350/351 lr 0.100000 loss 1.6498 (1.5550) acc@1 0.4141 (0.4246) acc@5 0.7812 (0.7840)\n",
      "\u001b[32m[2020-07-02 19:53:13] __main__ INFO: \u001b[0mEpoch 47 Step 351/351 lr 0.100000 loss 1.3938 (1.5545) acc@1 0.4688 (0.4247) acc@5 0.7812 (0.7839)\n",
      "\u001b[32m[2020-07-02 19:53:13] __main__ INFO: \u001b[0mElapsed 33.50\n",
      "\u001b[32m[2020-07-02 19:53:13] __main__ INFO: \u001b[0mVal 47\n",
      "\u001b[32m[2020-07-02 19:53:14] __main__ INFO: \u001b[0mEpoch 47 loss 1.8670 acc@1 0.3646 acc@5 0.7646\n",
      "\u001b[32m[2020-07-02 19:53:14] __main__ INFO: \u001b[0mElapsed 1.05\n",
      "\u001b[32m[2020-07-02 19:53:14] __main__ INFO: \u001b[0mTrain 48 16497\n",
      "\u001b[32m[2020-07-02 19:53:19] __main__ INFO: \u001b[0mEpoch 48 Step 50/351 lr 0.100000 loss 1.5923 (1.5309) acc@1 0.3750 (0.4286) acc@5 0.7969 (0.7895)\n",
      "\u001b[32m[2020-07-02 19:53:24] __main__ INFO: \u001b[0mEpoch 48 Step 100/351 lr 0.100000 loss 1.4384 (1.5380) acc@1 0.4219 (0.4274) acc@5 0.7812 (0.7852)\n",
      "\u001b[32m[2020-07-02 19:53:28] __main__ INFO: \u001b[0mEpoch 48 Step 150/351 lr 0.100000 loss 1.5884 (1.5396) acc@1 0.4062 (0.4277) acc@5 0.7891 (0.7864)\n",
      "\u001b[32m[2020-07-02 19:53:33] __main__ INFO: \u001b[0mEpoch 48 Step 200/351 lr 0.100000 loss 1.4785 (1.5397) acc@1 0.4688 (0.4278) acc@5 0.8203 (0.7879)\n",
      "\u001b[32m[2020-07-02 19:53:38] __main__ INFO: \u001b[0mEpoch 48 Step 250/351 lr 0.100000 loss 1.5428 (1.5433) acc@1 0.4297 (0.4265) acc@5 0.7422 (0.7868)\n",
      "\u001b[32m[2020-07-02 19:53:43] __main__ INFO: \u001b[0mEpoch 48 Step 300/351 lr 0.100000 loss 1.6024 (1.5480) acc@1 0.3438 (0.4256) acc@5 0.8047 (0.7853)\n",
      "\u001b[32m[2020-07-02 19:53:47] __main__ INFO: \u001b[0mEpoch 48 Step 350/351 lr 0.100000 loss 1.4867 (1.5515) acc@1 0.4531 (0.4247) acc@5 0.8281 (0.7856)\n",
      "\u001b[32m[2020-07-02 19:53:47] __main__ INFO: \u001b[0mEpoch 48 Step 351/351 lr 0.100000 loss 1.6410 (1.5517) acc@1 0.3984 (0.4246) acc@5 0.7500 (0.7855)\n",
      "\u001b[32m[2020-07-02 19:53:47] __main__ INFO: \u001b[0mElapsed 33.52\n",
      "\u001b[32m[2020-07-02 19:53:47] __main__ INFO: \u001b[0mVal 48\n",
      "\u001b[32m[2020-07-02 19:53:49] __main__ INFO: \u001b[0mEpoch 48 loss 1.7194 acc@1 0.3822 acc@5 0.7604\n",
      "\u001b[32m[2020-07-02 19:53:49] __main__ INFO: \u001b[0mElapsed 1.07\n",
      "\u001b[32m[2020-07-02 19:53:49] __main__ INFO: \u001b[0mTrain 49 16848\n",
      "\u001b[32m[2020-07-02 19:53:53] __main__ INFO: \u001b[0mEpoch 49 Step 50/351 lr 0.100000 loss 1.5142 (1.5432) acc@1 0.4297 (0.4288) acc@5 0.7422 (0.7844)\n",
      "\u001b[32m[2020-07-02 19:53:58] __main__ INFO: \u001b[0mEpoch 49 Step 100/351 lr 0.100000 loss 1.5950 (1.5367) acc@1 0.3984 (0.4296) acc@5 0.7812 (0.7866)\n",
      "\u001b[32m[2020-07-02 19:54:03] __main__ INFO: \u001b[0mEpoch 49 Step 150/351 lr 0.100000 loss 1.6358 (1.5408) acc@1 0.3750 (0.4269) acc@5 0.8047 (0.7856)\n",
      "\u001b[32m[2020-07-02 19:54:08] __main__ INFO: \u001b[0mEpoch 49 Step 200/351 lr 0.100000 loss 1.7068 (1.5406) acc@1 0.3516 (0.4280) acc@5 0.7422 (0.7866)\n",
      "\u001b[32m[2020-07-02 19:54:12] __main__ INFO: \u001b[0mEpoch 49 Step 250/351 lr 0.100000 loss 1.6131 (1.5433) acc@1 0.4219 (0.4261) acc@5 0.8359 (0.7861)\n",
      "\u001b[32m[2020-07-02 19:54:17] __main__ INFO: \u001b[0mEpoch 49 Step 300/351 lr 0.100000 loss 1.4419 (1.5451) acc@1 0.4375 (0.4253) acc@5 0.8125 (0.7855)\n",
      "\u001b[32m[2020-07-02 19:54:22] __main__ INFO: \u001b[0mEpoch 49 Step 350/351 lr 0.100000 loss 1.4364 (1.5474) acc@1 0.4688 (0.4246) acc@5 0.8516 (0.7860)\n",
      "\u001b[32m[2020-07-02 19:54:22] __main__ INFO: \u001b[0mEpoch 49 Step 351/351 lr 0.100000 loss 1.4061 (1.5470) acc@1 0.5000 (0.4248) acc@5 0.7734 (0.7859)\n",
      "\u001b[32m[2020-07-02 19:54:22] __main__ INFO: \u001b[0mElapsed 33.53\n",
      "\u001b[32m[2020-07-02 19:54:22] __main__ INFO: \u001b[0mVal 49\n",
      "\u001b[32m[2020-07-02 19:54:23] __main__ INFO: \u001b[0mEpoch 49 loss 1.6982 acc@1 0.3840 acc@5 0.7660\n",
      "\u001b[32m[2020-07-02 19:54:23] __main__ INFO: \u001b[0mElapsed 1.08\n",
      "\u001b[32m[2020-07-02 19:54:23] __main__ INFO: \u001b[0mTrain 50 17199\n",
      "\u001b[32m[2020-07-02 19:54:28] __main__ INFO: \u001b[0mEpoch 50 Step 50/351 lr 0.100000 loss 1.4717 (1.5171) acc@1 0.4297 (0.4348) acc@5 0.7969 (0.7839)\n",
      "\u001b[32m[2020-07-02 19:54:33] __main__ INFO: \u001b[0mEpoch 50 Step 100/351 lr 0.100000 loss 1.5783 (1.5291) acc@1 0.4219 (0.4297) acc@5 0.7500 (0.7873)\n",
      "\u001b[32m[2020-07-02 19:54:38] __main__ INFO: \u001b[0mEpoch 50 Step 150/351 lr 0.100000 loss 1.5109 (1.5338) acc@1 0.4062 (0.4274) acc@5 0.7969 (0.7859)\n",
      "\u001b[32m[2020-07-02 19:54:42] __main__ INFO: \u001b[0mEpoch 50 Step 200/351 lr 0.100000 loss 1.6264 (1.5333) acc@1 0.4453 (0.4290) acc@5 0.7969 (0.7859)\n",
      "\u001b[32m[2020-07-02 19:54:47] __main__ INFO: \u001b[0mEpoch 50 Step 250/351 lr 0.100000 loss 1.5264 (1.5362) acc@1 0.4688 (0.4288) acc@5 0.7891 (0.7851)\n",
      "\u001b[32m[2020-07-02 19:54:52] __main__ INFO: \u001b[0mEpoch 50 Step 300/351 lr 0.100000 loss 1.6001 (1.5392) acc@1 0.3984 (0.4281) acc@5 0.7969 (0.7851)\n",
      "\u001b[32m[2020-07-02 19:54:57] __main__ INFO: \u001b[0mEpoch 50 Step 350/351 lr 0.100000 loss 1.4466 (1.5402) acc@1 0.4531 (0.4278) acc@5 0.8203 (0.7852)\n",
      "\u001b[32m[2020-07-02 19:54:57] __main__ INFO: \u001b[0mEpoch 50 Step 351/351 lr 0.100000 loss 1.4977 (1.5401) acc@1 0.4375 (0.4278) acc@5 0.7891 (0.7852)\n",
      "\u001b[32m[2020-07-02 19:54:57] __main__ INFO: \u001b[0mElapsed 33.53\n",
      "\u001b[32m[2020-07-02 19:54:57] __main__ INFO: \u001b[0mVal 50\n",
      "\u001b[32m[2020-07-02 19:54:58] __main__ INFO: \u001b[0mEpoch 50 loss 1.7472 acc@1 0.3812 acc@5 0.7692\n",
      "\u001b[32m[2020-07-02 19:54:58] __main__ INFO: \u001b[0mElapsed 1.08\n",
      "\u001b[32m[2020-07-02 19:54:58] fvcore.common.checkpoint INFO: \u001b[0mSaving checkpoint to /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/july2/exp00/checkpoint_00050.pth\n",
      "\u001b[32m[2020-07-02 19:54:58] __main__ INFO: \u001b[0mTrain 51 17550\n",
      "\u001b[32m[2020-07-02 19:55:03] __main__ INFO: \u001b[0mEpoch 51 Step 50/351 lr 0.100000 loss 1.4727 (1.4972) acc@1 0.4688 (0.4469) acc@5 0.8203 (0.7961)\n",
      "\u001b[32m[2020-07-02 19:55:07] __main__ INFO: \u001b[0mEpoch 51 Step 100/351 lr 0.100000 loss 1.3971 (1.5005) acc@1 0.4531 (0.4455) acc@5 0.8516 (0.7947)\n",
      "\u001b[32m[2020-07-02 19:55:12] __main__ INFO: \u001b[0mEpoch 51 Step 150/351 lr 0.100000 loss 1.5623 (1.5130) acc@1 0.4141 (0.4414) acc@5 0.7812 (0.7897)\n",
      "\u001b[32m[2020-07-02 19:55:17] __main__ INFO: \u001b[0mEpoch 51 Step 200/351 lr 0.100000 loss 1.5612 (1.5183) acc@1 0.3906 (0.4386) acc@5 0.8203 (0.7894)\n",
      "\u001b[32m[2020-07-02 19:55:22] __main__ INFO: \u001b[0mEpoch 51 Step 250/351 lr 0.100000 loss 1.5473 (1.5180) acc@1 0.4375 (0.4383) acc@5 0.7969 (0.7890)\n",
      "\u001b[32m[2020-07-02 19:55:26] __main__ INFO: \u001b[0mEpoch 51 Step 300/351 lr 0.100000 loss 1.5772 (1.5247) acc@1 0.4297 (0.4358) acc@5 0.7812 (0.7884)\n",
      "\u001b[32m[2020-07-02 19:55:31] __main__ INFO: \u001b[0mEpoch 51 Step 350/351 lr 0.100000 loss 1.5791 (1.5302) acc@1 0.4219 (0.4333) acc@5 0.7578 (0.7866)\n",
      "\u001b[32m[2020-07-02 19:55:31] __main__ INFO: \u001b[0mEpoch 51 Step 351/351 lr 0.100000 loss 1.4009 (1.5298) acc@1 0.4531 (0.4333) acc@5 0.7969 (0.7866)\n",
      "\u001b[32m[2020-07-02 19:55:31] __main__ INFO: \u001b[0mElapsed 33.49\n",
      "\u001b[32m[2020-07-02 19:55:31] __main__ INFO: \u001b[0mVal 51\n",
      "\u001b[32m[2020-07-02 19:55:32] __main__ INFO: \u001b[0mEpoch 51 loss 1.6715 acc@1 0.3896 acc@5 0.7730\n",
      "\u001b[32m[2020-07-02 19:55:32] __main__ INFO: \u001b[0mElapsed 1.08\n",
      "\u001b[32m[2020-07-02 19:55:32] __main__ INFO: \u001b[0mTrain 52 17901\n",
      "\u001b[32m[2020-07-02 19:55:37] __main__ INFO: \u001b[0mEpoch 52 Step 50/351 lr 0.100000 loss 1.5392 (1.5255) acc@1 0.3984 (0.4370) acc@5 0.8281 (0.7820)\n",
      "\u001b[32m[2020-07-02 19:55:42] __main__ INFO: \u001b[0mEpoch 52 Step 100/351 lr 0.100000 loss 1.5088 (1.5220) acc@1 0.4453 (0.4369) acc@5 0.7812 (0.7871)\n",
      "\u001b[32m[2020-07-02 19:55:47] __main__ INFO: \u001b[0mEpoch 52 Step 150/351 lr 0.100000 loss 1.4465 (1.5285) acc@1 0.4844 (0.4318) acc@5 0.7734 (0.7860)\n",
      "\u001b[32m[2020-07-02 19:55:51] __main__ INFO: \u001b[0mEpoch 52 Step 200/351 lr 0.100000 loss 1.4467 (1.5242) acc@1 0.4453 (0.4329) acc@5 0.8203 (0.7882)\n",
      "\u001b[32m[2020-07-02 19:55:56] __main__ INFO: \u001b[0mEpoch 52 Step 250/351 lr 0.100000 loss 1.4768 (1.5296) acc@1 0.4219 (0.4316) acc@5 0.8047 (0.7854)\n",
      "\u001b[32m[2020-07-02 19:56:01] __main__ INFO: \u001b[0mEpoch 52 Step 300/351 lr 0.100000 loss 1.4782 (1.5276) acc@1 0.4453 (0.4320) acc@5 0.7812 (0.7867)\n",
      "\u001b[32m[2020-07-02 19:56:06] __main__ INFO: \u001b[0mEpoch 52 Step 350/351 lr 0.100000 loss 1.5364 (1.5311) acc@1 0.4297 (0.4309) acc@5 0.7812 (0.7867)\n",
      "\u001b[32m[2020-07-02 19:56:06] __main__ INFO: \u001b[0mEpoch 52 Step 351/351 lr 0.100000 loss 1.5094 (1.5311) acc@1 0.4375 (0.4309) acc@5 0.8281 (0.7868)\n",
      "\u001b[32m[2020-07-02 19:56:06] __main__ INFO: \u001b[0mElapsed 33.52\n",
      "\u001b[32m[2020-07-02 19:56:06] __main__ INFO: \u001b[0mVal 52\n",
      "\u001b[32m[2020-07-02 19:56:07] __main__ INFO: \u001b[0mEpoch 52 loss 1.6888 acc@1 0.3824 acc@5 0.7696\n",
      "\u001b[32m[2020-07-02 19:56:07] __main__ INFO: \u001b[0mElapsed 1.06\n",
      "\u001b[32m[2020-07-02 19:56:07] __main__ INFO: \u001b[0mTrain 53 18252\n",
      "\u001b[32m[2020-07-02 19:56:12] __main__ INFO: \u001b[0mEpoch 53 Step 50/351 lr 0.100000 loss 1.6001 (1.5186) acc@1 0.4219 (0.4330) acc@5 0.7734 (0.7881)\n",
      "\u001b[32m[2020-07-02 19:56:17] __main__ INFO: \u001b[0mEpoch 53 Step 100/351 lr 0.100000 loss 1.5012 (1.5200) acc@1 0.4141 (0.4348) acc@5 0.8125 (0.7907)\n",
      "\u001b[32m[2020-07-02 19:56:21] __main__ INFO: \u001b[0mEpoch 53 Step 150/351 lr 0.100000 loss 1.5681 (1.5212) acc@1 0.4219 (0.4356) acc@5 0.7812 (0.7904)\n",
      "\u001b[32m[2020-07-02 19:56:26] __main__ INFO: \u001b[0mEpoch 53 Step 200/351 lr 0.100000 loss 1.3899 (1.5235) acc@1 0.5000 (0.4346) acc@5 0.7891 (0.7907)\n",
      "\u001b[32m[2020-07-02 19:56:31] __main__ INFO: \u001b[0mEpoch 53 Step 250/351 lr 0.100000 loss 1.4962 (1.5258) acc@1 0.4219 (0.4350) acc@5 0.7578 (0.7903)\n",
      "\u001b[32m[2020-07-02 19:56:36] __main__ INFO: \u001b[0mEpoch 53 Step 300/351 lr 0.100000 loss 1.5272 (1.5224) acc@1 0.4297 (0.4361) acc@5 0.8125 (0.7909)\n",
      "\u001b[32m[2020-07-02 19:56:40] __main__ INFO: \u001b[0mEpoch 53 Step 350/351 lr 0.100000 loss 1.5477 (1.5237) acc@1 0.4219 (0.4352) acc@5 0.7812 (0.7902)\n",
      "\u001b[32m[2020-07-02 19:56:40] __main__ INFO: \u001b[0mEpoch 53 Step 351/351 lr 0.100000 loss 1.6360 (1.5240) acc@1 0.3984 (0.4351) acc@5 0.7109 (0.7900)\n",
      "\u001b[32m[2020-07-02 19:56:41] __main__ INFO: \u001b[0mElapsed 33.58\n",
      "\u001b[32m[2020-07-02 19:56:41] __main__ INFO: \u001b[0mVal 53\n",
      "\u001b[32m[2020-07-02 19:56:42] __main__ INFO: \u001b[0mEpoch 53 loss 1.6530 acc@1 0.4108 acc@5 0.7750\n",
      "\u001b[32m[2020-07-02 19:56:42] __main__ INFO: \u001b[0mElapsed 1.07\n",
      "\u001b[32m[2020-07-02 19:56:42] __main__ INFO: \u001b[0mTrain 54 18603\n",
      "\u001b[32m[2020-07-02 19:56:46] __main__ INFO: \u001b[0mEpoch 54 Step 50/351 lr 0.100000 loss 1.4257 (1.5014) acc@1 0.4688 (0.4339) acc@5 0.7891 (0.7883)\n",
      "\u001b[32m[2020-07-02 19:56:51] __main__ INFO: \u001b[0mEpoch 54 Step 100/351 lr 0.100000 loss 1.5078 (1.4987) acc@1 0.4688 (0.4366) acc@5 0.8125 (0.7880)\n",
      "\u001b[32m[2020-07-02 19:56:56] __main__ INFO: \u001b[0mEpoch 54 Step 150/351 lr 0.100000 loss 1.4983 (1.5043) acc@1 0.4688 (0.4367) acc@5 0.7891 (0.7885)\n",
      "\u001b[32m[2020-07-02 19:57:01] __main__ INFO: \u001b[0mEpoch 54 Step 200/351 lr 0.100000 loss 1.4594 (1.5125) acc@1 0.4688 (0.4341) acc@5 0.7812 (0.7886)\n",
      "\u001b[32m[2020-07-02 19:57:05] __main__ INFO: \u001b[0mEpoch 54 Step 250/351 lr 0.100000 loss 1.4797 (1.5123) acc@1 0.4453 (0.4352) acc@5 0.8125 (0.7891)\n",
      "\u001b[32m[2020-07-02 19:57:10] __main__ INFO: \u001b[0mEpoch 54 Step 300/351 lr 0.100000 loss 1.6054 (1.5152) acc@1 0.3984 (0.4334) acc@5 0.8047 (0.7893)\n",
      "\u001b[32m[2020-07-02 19:57:15] __main__ INFO: \u001b[0mEpoch 54 Step 350/351 lr 0.100000 loss 1.6375 (1.5172) acc@1 0.4219 (0.4333) acc@5 0.7422 (0.7898)\n",
      "\u001b[32m[2020-07-02 19:57:15] __main__ INFO: \u001b[0mEpoch 54 Step 351/351 lr 0.100000 loss 1.4713 (1.5171) acc@1 0.4922 (0.4335) acc@5 0.7422 (0.7896)\n",
      "\u001b[32m[2020-07-02 19:57:15] __main__ INFO: \u001b[0mElapsed 33.52\n",
      "\u001b[32m[2020-07-02 19:57:15] __main__ INFO: \u001b[0mVal 54\n",
      "\u001b[32m[2020-07-02 19:57:16] __main__ INFO: \u001b[0mEpoch 54 loss 1.7072 acc@1 0.3830 acc@5 0.7692\n",
      "\u001b[32m[2020-07-02 19:57:16] __main__ INFO: \u001b[0mElapsed 1.05\n",
      "\u001b[32m[2020-07-02 19:57:16] __main__ INFO: \u001b[0mTrain 55 18954\n",
      "\u001b[32m[2020-07-02 19:57:21] __main__ INFO: \u001b[0mEpoch 55 Step 50/351 lr 0.100000 loss 1.3409 (1.5092) acc@1 0.5234 (0.4442) acc@5 0.8594 (0.7922)\n",
      "\u001b[32m[2020-07-02 19:57:26] __main__ INFO: \u001b[0mEpoch 55 Step 100/351 lr 0.100000 loss 1.3472 (1.5003) acc@1 0.4844 (0.4443) acc@5 0.7812 (0.7907)\n",
      "\u001b[32m[2020-07-02 19:57:31] __main__ INFO: \u001b[0mEpoch 55 Step 150/351 lr 0.100000 loss 1.4188 (1.5078) acc@1 0.5312 (0.4419) acc@5 0.8203 (0.7897)\n",
      "\u001b[32m[2020-07-02 19:57:35] __main__ INFO: \u001b[0mEpoch 55 Step 200/351 lr 0.100000 loss 1.5066 (1.5060) acc@1 0.4844 (0.4434) acc@5 0.7812 (0.7911)\n",
      "\u001b[32m[2020-07-02 19:57:40] __main__ INFO: \u001b[0mEpoch 55 Step 250/351 lr 0.100000 loss 1.5639 (1.5073) acc@1 0.4297 (0.4425) acc@5 0.8281 (0.7899)\n",
      "\u001b[32m[2020-07-02 19:57:45] __main__ INFO: \u001b[0mEpoch 55 Step 300/351 lr 0.100000 loss 1.3738 (1.5105) acc@1 0.4609 (0.4422) acc@5 0.8047 (0.7890)\n",
      "\u001b[32m[2020-07-02 19:57:50] __main__ INFO: \u001b[0mEpoch 55 Step 350/351 lr 0.100000 loss 1.8308 (1.5127) acc@1 0.3438 (0.4408) acc@5 0.7188 (0.7889)\n",
      "\u001b[32m[2020-07-02 19:57:50] __main__ INFO: \u001b[0mEpoch 55 Step 351/351 lr 0.100000 loss 1.5214 (1.5128) acc@1 0.4531 (0.4408) acc@5 0.7656 (0.7888)\n",
      "\u001b[32m[2020-07-02 19:57:50] __main__ INFO: \u001b[0mElapsed 33.59\n",
      "\u001b[32m[2020-07-02 19:57:50] __main__ INFO: \u001b[0mVal 55\n",
      "\u001b[32m[2020-07-02 19:57:51] __main__ INFO: \u001b[0mEpoch 55 loss 1.6775 acc@1 0.3932 acc@5 0.7632\n",
      "\u001b[32m[2020-07-02 19:57:51] __main__ INFO: \u001b[0mElapsed 1.06\n",
      "\u001b[32m[2020-07-02 19:57:51] __main__ INFO: \u001b[0mTrain 56 19305\n",
      "\u001b[32m[2020-07-02 19:57:56] __main__ INFO: \u001b[0mEpoch 56 Step 50/351 lr 0.100000 loss 1.4460 (1.5014) acc@1 0.4375 (0.4342) acc@5 0.7734 (0.7872)\n",
      "\u001b[32m[2020-07-02 19:58:00] __main__ INFO: \u001b[0mEpoch 56 Step 100/351 lr 0.100000 loss 1.3663 (1.5009) acc@1 0.5078 (0.4359) acc@5 0.8125 (0.7860)\n",
      "\u001b[32m[2020-07-02 19:58:05] __main__ INFO: \u001b[0mEpoch 56 Step 150/351 lr 0.100000 loss 1.5330 (1.5016) acc@1 0.4453 (0.4374) acc@5 0.7734 (0.7894)\n",
      "\u001b[32m[2020-07-02 19:58:10] __main__ INFO: \u001b[0mEpoch 56 Step 200/351 lr 0.100000 loss 1.6413 (1.5016) acc@1 0.3594 (0.4398) acc@5 0.8203 (0.7897)\n",
      "\u001b[32m[2020-07-02 19:58:15] __main__ INFO: \u001b[0mEpoch 56 Step 250/351 lr 0.100000 loss 1.5471 (1.5090) acc@1 0.4531 (0.4369) acc@5 0.7734 (0.7886)\n",
      "\u001b[32m[2020-07-02 19:58:19] __main__ INFO: \u001b[0mEpoch 56 Step 300/351 lr 0.100000 loss 1.4488 (1.5099) acc@1 0.4297 (0.4363) acc@5 0.8047 (0.7891)\n",
      "\u001b[32m[2020-07-02 19:58:24] __main__ INFO: \u001b[0mEpoch 56 Step 350/351 lr 0.100000 loss 1.5340 (1.5098) acc@1 0.4609 (0.4363) acc@5 0.7891 (0.7900)\n",
      "\u001b[32m[2020-07-02 19:58:24] __main__ INFO: \u001b[0mEpoch 56 Step 351/351 lr 0.100000 loss 1.3680 (1.5094) acc@1 0.4844 (0.4365) acc@5 0.7812 (0.7900)\n",
      "\u001b[32m[2020-07-02 19:58:24] __main__ INFO: \u001b[0mElapsed 33.51\n",
      "\u001b[32m[2020-07-02 19:58:24] __main__ INFO: \u001b[0mVal 56\n",
      "\u001b[32m[2020-07-02 19:58:25] __main__ INFO: \u001b[0mEpoch 56 loss 1.6702 acc@1 0.3962 acc@5 0.7832\n",
      "\u001b[32m[2020-07-02 19:58:25] __main__ INFO: \u001b[0mElapsed 1.06\n",
      "\u001b[32m[2020-07-02 19:58:25] __main__ INFO: \u001b[0mTrain 57 19656\n",
      "\u001b[32m[2020-07-02 19:58:30] __main__ INFO: \u001b[0mEpoch 57 Step 50/351 lr 0.100000 loss 1.4491 (1.5146) acc@1 0.4453 (0.4370) acc@5 0.8203 (0.7841)\n",
      "\u001b[32m[2020-07-02 19:58:35] __main__ INFO: \u001b[0mEpoch 57 Step 100/351 lr 0.100000 loss 1.5563 (1.5029) acc@1 0.3828 (0.4434) acc@5 0.7734 (0.7902)\n",
      "\u001b[32m[2020-07-02 19:58:40] __main__ INFO: \u001b[0mEpoch 57 Step 150/351 lr 0.100000 loss 1.5097 (1.4983) acc@1 0.4609 (0.4442) acc@5 0.7656 (0.7910)\n",
      "\u001b[32m[2020-07-02 19:58:45] __main__ INFO: \u001b[0mEpoch 57 Step 200/351 lr 0.100000 loss 1.4197 (1.4993) acc@1 0.4766 (0.4423) acc@5 0.8359 (0.7905)\n",
      "\u001b[32m[2020-07-02 19:58:49] __main__ INFO: \u001b[0mEpoch 57 Step 250/351 lr 0.100000 loss 1.5097 (1.5012) acc@1 0.4453 (0.4410) acc@5 0.8047 (0.7897)\n",
      "\u001b[32m[2020-07-02 19:58:54] __main__ INFO: \u001b[0mEpoch 57 Step 300/351 lr 0.100000 loss 1.3208 (1.5028) acc@1 0.5000 (0.4399) acc@5 0.8750 (0.7893)\n",
      "\u001b[32m[2020-07-02 19:58:59] __main__ INFO: \u001b[0mEpoch 57 Step 350/351 lr 0.100000 loss 1.4987 (1.5035) acc@1 0.4062 (0.4400) acc@5 0.7656 (0.7896)\n",
      "\u001b[32m[2020-07-02 19:58:59] __main__ INFO: \u001b[0mEpoch 57 Step 351/351 lr 0.100000 loss 1.5838 (1.5037) acc@1 0.3984 (0.4399) acc@5 0.8047 (0.7897)\n",
      "\u001b[32m[2020-07-02 19:58:59] __main__ INFO: \u001b[0mElapsed 33.56\n",
      "\u001b[32m[2020-07-02 19:58:59] __main__ INFO: \u001b[0mVal 57\n",
      "\u001b[32m[2020-07-02 19:59:00] __main__ INFO: \u001b[0mEpoch 57 loss 1.6420 acc@1 0.4124 acc@5 0.7736\n",
      "\u001b[32m[2020-07-02 19:59:00] __main__ INFO: \u001b[0mElapsed 1.06\n",
      "\u001b[32m[2020-07-02 19:59:00] __main__ INFO: \u001b[0mTrain 58 20007\n",
      "\u001b[32m[2020-07-02 19:59:05] __main__ INFO: \u001b[0mEpoch 58 Step 50/351 lr 0.100000 loss 1.6693 (1.4570) acc@1 0.3359 (0.4552) acc@5 0.7188 (0.7950)\n",
      "\u001b[32m[2020-07-02 19:59:10] __main__ INFO: \u001b[0mEpoch 58 Step 100/351 lr 0.100000 loss 1.4372 (1.4764) acc@1 0.4219 (0.4460) acc@5 0.8047 (0.7913)\n",
      "\u001b[32m[2020-07-02 19:59:14] __main__ INFO: \u001b[0mEpoch 58 Step 150/351 lr 0.100000 loss 1.5413 (1.4808) acc@1 0.4688 (0.4472) acc@5 0.7969 (0.7892)\n",
      "\u001b[32m[2020-07-02 19:59:19] __main__ INFO: \u001b[0mEpoch 58 Step 200/351 lr 0.100000 loss 1.5480 (1.4944) acc@1 0.3906 (0.4416) acc@5 0.7969 (0.7874)\n",
      "\u001b[32m[2020-07-02 19:59:24] __main__ INFO: \u001b[0mEpoch 58 Step 250/351 lr 0.100000 loss 1.6292 (1.4968) acc@1 0.3828 (0.4412) acc@5 0.7734 (0.7860)\n",
      "\u001b[32m[2020-07-02 19:59:29] __main__ INFO: \u001b[0mEpoch 58 Step 300/351 lr 0.100000 loss 1.3136 (1.5041) acc@1 0.5000 (0.4382) acc@5 0.8438 (0.7857)\n",
      "\u001b[32m[2020-07-02 19:59:33] __main__ INFO: \u001b[0mEpoch 58 Step 350/351 lr 0.100000 loss 1.4773 (1.4992) acc@1 0.4453 (0.4396) acc@5 0.8203 (0.7877)\n",
      "\u001b[32m[2020-07-02 19:59:34] __main__ INFO: \u001b[0mEpoch 58 Step 351/351 lr 0.100000 loss 1.4702 (1.4992) acc@1 0.4219 (0.4395) acc@5 0.7734 (0.7876)\n",
      "\u001b[32m[2020-07-02 19:59:34] __main__ INFO: \u001b[0mElapsed 33.56\n",
      "\u001b[32m[2020-07-02 19:59:34] __main__ INFO: \u001b[0mVal 58\n",
      "\u001b[32m[2020-07-02 19:59:35] __main__ INFO: \u001b[0mEpoch 58 loss 1.6756 acc@1 0.4034 acc@5 0.7724\n",
      "\u001b[32m[2020-07-02 19:59:35] __main__ INFO: \u001b[0mElapsed 1.08\n",
      "\u001b[32m[2020-07-02 19:59:35] __main__ INFO: \u001b[0mTrain 59 20358\n",
      "\u001b[32m[2020-07-02 19:59:40] __main__ INFO: \u001b[0mEpoch 59 Step 50/351 lr 0.100000 loss 1.4785 (1.4680) acc@1 0.3984 (0.4483) acc@5 0.7422 (0.7925)\n",
      "\u001b[32m[2020-07-02 19:59:44] __main__ INFO: \u001b[0mEpoch 59 Step 100/351 lr 0.100000 loss 1.3534 (1.4785) acc@1 0.5156 (0.4471) acc@5 0.8281 (0.7945)\n",
      "\u001b[32m[2020-07-02 19:59:49] __main__ INFO: \u001b[0mEpoch 59 Step 150/351 lr 0.100000 loss 1.4200 (1.4732) acc@1 0.4844 (0.4508) acc@5 0.8281 (0.7974)\n",
      "\u001b[32m[2020-07-02 19:59:54] __main__ INFO: \u001b[0mEpoch 59 Step 200/351 lr 0.100000 loss 1.3064 (1.4801) acc@1 0.5234 (0.4481) acc@5 0.8281 (0.7963)\n",
      "\u001b[32m[2020-07-02 19:59:59] __main__ INFO: \u001b[0mEpoch 59 Step 250/351 lr 0.100000 loss 1.6147 (1.4934) acc@1 0.4219 (0.4428) acc@5 0.7188 (0.7924)\n",
      "\u001b[32m[2020-07-02 20:00:03] __main__ INFO: \u001b[0mEpoch 59 Step 300/351 lr 0.100000 loss 1.5242 (1.4948) acc@1 0.4766 (0.4416) acc@5 0.7734 (0.7917)\n",
      "\u001b[32m[2020-07-02 20:00:08] __main__ INFO: \u001b[0mEpoch 59 Step 350/351 lr 0.100000 loss 1.3718 (1.4959) acc@1 0.5078 (0.4419) acc@5 0.8359 (0.7921)\n",
      "\u001b[32m[2020-07-02 20:00:08] __main__ INFO: \u001b[0mEpoch 59 Step 351/351 lr 0.100000 loss 1.5755 (1.4962) acc@1 0.4453 (0.4419) acc@5 0.7969 (0.7921)\n",
      "\u001b[32m[2020-07-02 20:00:08] __main__ INFO: \u001b[0mElapsed 33.57\n",
      "\u001b[32m[2020-07-02 20:00:08] __main__ INFO: \u001b[0mVal 59\n",
      "\u001b[32m[2020-07-02 20:00:09] __main__ INFO: \u001b[0mEpoch 59 loss 1.6510 acc@1 0.4040 acc@5 0.7694\n",
      "\u001b[32m[2020-07-02 20:00:09] __main__ INFO: \u001b[0mElapsed 1.07\n",
      "\u001b[32m[2020-07-02 20:00:09] __main__ INFO: \u001b[0mTrain 60 20709\n",
      "\u001b[32m[2020-07-02 20:00:14] __main__ INFO: \u001b[0mEpoch 60 Step 50/351 lr 0.100000 loss 1.3308 (1.4753) acc@1 0.4766 (0.4519) acc@5 0.8672 (0.7994)\n",
      "\u001b[32m[2020-07-02 20:00:19] __main__ INFO: \u001b[0mEpoch 60 Step 100/351 lr 0.100000 loss 1.5001 (1.4793) acc@1 0.5000 (0.4525) acc@5 0.8125 (0.7963)\n",
      "\u001b[32m[2020-07-02 20:00:24] __main__ INFO: \u001b[0mEpoch 60 Step 150/351 lr 0.100000 loss 1.4037 (1.4829) acc@1 0.5156 (0.4509) acc@5 0.7656 (0.7964)\n",
      "\u001b[32m[2020-07-02 20:00:28] __main__ INFO: \u001b[0mEpoch 60 Step 200/351 lr 0.100000 loss 1.5887 (1.4816) acc@1 0.4297 (0.4496) acc@5 0.7812 (0.7945)\n",
      "\u001b[32m[2020-07-02 20:00:33] __main__ INFO: \u001b[0mEpoch 60 Step 250/351 lr 0.100000 loss 1.4146 (1.4848) acc@1 0.4609 (0.4487) acc@5 0.7969 (0.7941)\n",
      "\u001b[32m[2020-07-02 20:00:38] __main__ INFO: \u001b[0mEpoch 60 Step 300/351 lr 0.100000 loss 1.4332 (1.4885) acc@1 0.5234 (0.4477) acc@5 0.8516 (0.7931)\n",
      "\u001b[32m[2020-07-02 20:00:43] __main__ INFO: \u001b[0mEpoch 60 Step 350/351 lr 0.100000 loss 1.5494 (1.4896) acc@1 0.3984 (0.4472) acc@5 0.7578 (0.7926)\n",
      "\u001b[32m[2020-07-02 20:00:43] __main__ INFO: \u001b[0mEpoch 60 Step 351/351 lr 0.100000 loss 1.5172 (1.4897) acc@1 0.4297 (0.4472) acc@5 0.8125 (0.7926)\n",
      "\u001b[32m[2020-07-02 20:00:43] __main__ INFO: \u001b[0mElapsed 33.54\n",
      "\u001b[32m[2020-07-02 20:00:43] __main__ INFO: \u001b[0mVal 60\n",
      "\u001b[32m[2020-07-02 20:00:44] __main__ INFO: \u001b[0mEpoch 60 loss 1.6553 acc@1 0.4066 acc@5 0.7662\n",
      "\u001b[32m[2020-07-02 20:00:44] __main__ INFO: \u001b[0mElapsed 1.07\n",
      "\u001b[32m[2020-07-02 20:00:44] __main__ INFO: \u001b[0mTrain 61 21060\n",
      "\u001b[32m[2020-07-02 20:00:49] __main__ INFO: \u001b[0mEpoch 61 Step 50/351 lr 0.100000 loss 1.5425 (1.4620) acc@1 0.3906 (0.4575) acc@5 0.7969 (0.7952)\n",
      "\u001b[32m[2020-07-02 20:00:54] __main__ INFO: \u001b[0mEpoch 61 Step 100/351 lr 0.100000 loss 1.3026 (1.4611) acc@1 0.5156 (0.4581) acc@5 0.8281 (0.7967)\n",
      "\u001b[32m[2020-07-02 20:00:58] __main__ INFO: \u001b[0mEpoch 61 Step 150/351 lr 0.100000 loss 1.5101 (1.4785) acc@1 0.4219 (0.4499) acc@5 0.7891 (0.7950)\n",
      "\u001b[32m[2020-07-02 20:01:03] __main__ INFO: \u001b[0mEpoch 61 Step 200/351 lr 0.100000 loss 1.3144 (1.4879) acc@1 0.5234 (0.4457) acc@5 0.8203 (0.7932)\n",
      "\u001b[32m[2020-07-02 20:01:08] __main__ INFO: \u001b[0mEpoch 61 Step 250/351 lr 0.100000 loss 1.4356 (1.4862) acc@1 0.5234 (0.4459) acc@5 0.7969 (0.7934)\n",
      "\u001b[32m[2020-07-02 20:01:13] __main__ INFO: \u001b[0mEpoch 61 Step 300/351 lr 0.100000 loss 1.3062 (1.4860) acc@1 0.5000 (0.4469) acc@5 0.8594 (0.7929)\n",
      "\u001b[32m[2020-07-02 20:01:17] __main__ INFO: \u001b[0mEpoch 61 Step 350/351 lr 0.100000 loss 1.5058 (1.4873) acc@1 0.4844 (0.4456) acc@5 0.8047 (0.7922)\n",
      "\u001b[32m[2020-07-02 20:01:17] __main__ INFO: \u001b[0mEpoch 61 Step 351/351 lr 0.100000 loss 1.7147 (1.4879) acc@1 0.3906 (0.4455) acc@5 0.7812 (0.7922)\n",
      "\u001b[32m[2020-07-02 20:01:17] __main__ INFO: \u001b[0mElapsed 33.55\n",
      "\u001b[32m[2020-07-02 20:01:17] __main__ INFO: \u001b[0mVal 61\n",
      "\u001b[32m[2020-07-02 20:01:19] __main__ INFO: \u001b[0mEpoch 61 loss 1.7035 acc@1 0.3914 acc@5 0.7680\n",
      "\u001b[32m[2020-07-02 20:01:19] __main__ INFO: \u001b[0mElapsed 1.08\n",
      "\u001b[32m[2020-07-02 20:01:19] __main__ INFO: \u001b[0mTrain 62 21411\n",
      "\u001b[32m[2020-07-02 20:01:23] __main__ INFO: \u001b[0mEpoch 62 Step 50/351 lr 0.100000 loss 1.4509 (1.4747) acc@1 0.4297 (0.4578) acc@5 0.8359 (0.7964)\n",
      "\u001b[32m[2020-07-02 20:01:28] __main__ INFO: \u001b[0mEpoch 62 Step 100/351 lr 0.100000 loss 1.3779 (1.4649) acc@1 0.5078 (0.4571) acc@5 0.7578 (0.7958)\n",
      "\u001b[32m[2020-07-02 20:01:33] __main__ INFO: \u001b[0mEpoch 62 Step 150/351 lr 0.100000 loss 1.4412 (1.4683) acc@1 0.4531 (0.4540) acc@5 0.7969 (0.7948)\n",
      "\u001b[32m[2020-07-02 20:01:38] __main__ INFO: \u001b[0mEpoch 62 Step 200/351 lr 0.100000 loss 1.4616 (1.4706) acc@1 0.4922 (0.4537) acc@5 0.7734 (0.7938)\n",
      "\u001b[32m[2020-07-02 20:01:42] __main__ INFO: \u001b[0mEpoch 62 Step 250/351 lr 0.100000 loss 1.4532 (1.4724) acc@1 0.4766 (0.4526) acc@5 0.7812 (0.7939)\n",
      "\u001b[32m[2020-07-02 20:01:47] __main__ INFO: \u001b[0mEpoch 62 Step 300/351 lr 0.100000 loss 1.6367 (1.4820) acc@1 0.4297 (0.4490) acc@5 0.8203 (0.7918)\n",
      "\u001b[32m[2020-07-02 20:01:52] __main__ INFO: \u001b[0mEpoch 62 Step 350/351 lr 0.100000 loss 1.6161 (1.4802) acc@1 0.3984 (0.4495) acc@5 0.6953 (0.7919)\n",
      "\u001b[32m[2020-07-02 20:01:52] __main__ INFO: \u001b[0mEpoch 62 Step 351/351 lr 0.100000 loss 1.3982 (1.4799) acc@1 0.4844 (0.4496) acc@5 0.8125 (0.7920)\n",
      "\u001b[32m[2020-07-02 20:01:52] __main__ INFO: \u001b[0mElapsed 33.52\n",
      "\u001b[32m[2020-07-02 20:01:52] __main__ INFO: \u001b[0mVal 62\n",
      "\u001b[32m[2020-07-02 20:01:53] __main__ INFO: \u001b[0mEpoch 62 loss 1.6332 acc@1 0.4070 acc@5 0.7744\n",
      "\u001b[32m[2020-07-02 20:01:53] __main__ INFO: \u001b[0mElapsed 1.06\n",
      "\u001b[32m[2020-07-02 20:01:53] __main__ INFO: \u001b[0mTrain 63 21762\n",
      "\u001b[32m[2020-07-02 20:01:58] __main__ INFO: \u001b[0mEpoch 63 Step 50/351 lr 0.100000 loss 1.3220 (1.4662) acc@1 0.5234 (0.4569) acc@5 0.8359 (0.7961)\n",
      "\u001b[32m[2020-07-02 20:02:03] __main__ INFO: \u001b[0mEpoch 63 Step 100/351 lr 0.100000 loss 1.4816 (1.4660) acc@1 0.4609 (0.4566) acc@5 0.7891 (0.7952)\n",
      "\u001b[32m[2020-07-02 20:02:08] __main__ INFO: \u001b[0mEpoch 63 Step 150/351 lr 0.100000 loss 1.5796 (1.4646) acc@1 0.3594 (0.4547) acc@5 0.7812 (0.7976)\n",
      "\u001b[32m[2020-07-02 20:02:12] __main__ INFO: \u001b[0mEpoch 63 Step 200/351 lr 0.100000 loss 1.4910 (1.4663) acc@1 0.4453 (0.4540) acc@5 0.7969 (0.7955)\n",
      "\u001b[32m[2020-07-02 20:02:17] __main__ INFO: \u001b[0mEpoch 63 Step 250/351 lr 0.100000 loss 1.4465 (1.4714) acc@1 0.4688 (0.4519) acc@5 0.7891 (0.7959)\n",
      "\u001b[32m[2020-07-02 20:02:22] __main__ INFO: \u001b[0mEpoch 63 Step 300/351 lr 0.100000 loss 1.4766 (1.4798) acc@1 0.4766 (0.4480) acc@5 0.7812 (0.7943)\n",
      "\u001b[32m[2020-07-02 20:02:27] __main__ INFO: \u001b[0mEpoch 63 Step 350/351 lr 0.100000 loss 1.5007 (1.4829) acc@1 0.4297 (0.4471) acc@5 0.7812 (0.7928)\n",
      "\u001b[32m[2020-07-02 20:02:27] __main__ INFO: \u001b[0mEpoch 63 Step 351/351 lr 0.100000 loss 1.5391 (1.4831) acc@1 0.4219 (0.4470) acc@5 0.8125 (0.7928)\n",
      "\u001b[32m[2020-07-02 20:02:27] __main__ INFO: \u001b[0mElapsed 33.56\n",
      "\u001b[32m[2020-07-02 20:02:27] __main__ INFO: \u001b[0mVal 63\n",
      "\u001b[32m[2020-07-02 20:02:28] __main__ INFO: \u001b[0mEpoch 63 loss 1.6932 acc@1 0.3940 acc@5 0.7724\n",
      "\u001b[32m[2020-07-02 20:02:28] __main__ INFO: \u001b[0mElapsed 1.06\n",
      "\u001b[32m[2020-07-02 20:02:28] __main__ INFO: \u001b[0mTrain 64 22113\n",
      "\u001b[32m[2020-07-02 20:02:33] __main__ INFO: \u001b[0mEpoch 64 Step 50/351 lr 0.100000 loss 1.5181 (1.4668) acc@1 0.4609 (0.4469) acc@5 0.8125 (0.7895)\n",
      "\u001b[32m[2020-07-02 20:02:37] __main__ INFO: \u001b[0mEpoch 64 Step 100/351 lr 0.100000 loss 1.4277 (1.4643) acc@1 0.4766 (0.4545) acc@5 0.8203 (0.7896)\n",
      "\u001b[32m[2020-07-02 20:02:42] __main__ INFO: \u001b[0mEpoch 64 Step 150/351 lr 0.100000 loss 1.7084 (1.4666) acc@1 0.3750 (0.4541) acc@5 0.7734 (0.7902)\n",
      "\u001b[32m[2020-07-02 20:02:47] __main__ INFO: \u001b[0mEpoch 64 Step 200/351 lr 0.100000 loss 1.4537 (1.4687) acc@1 0.4766 (0.4543) acc@5 0.7734 (0.7918)\n",
      "\u001b[32m[2020-07-02 20:02:52] __main__ INFO: \u001b[0mEpoch 64 Step 250/351 lr 0.100000 loss 1.5820 (1.4732) acc@1 0.4062 (0.4527) acc@5 0.7969 (0.7922)\n",
      "\u001b[32m[2020-07-02 20:02:56] __main__ INFO: \u001b[0mEpoch 64 Step 300/351 lr 0.100000 loss 1.4956 (1.4749) acc@1 0.4688 (0.4517) acc@5 0.7734 (0.7911)\n",
      "\u001b[32m[2020-07-02 20:03:01] __main__ INFO: \u001b[0mEpoch 64 Step 350/351 lr 0.100000 loss 1.5305 (1.4767) acc@1 0.4141 (0.4499) acc@5 0.7734 (0.7902)\n",
      "\u001b[32m[2020-07-02 20:03:01] __main__ INFO: \u001b[0mEpoch 64 Step 351/351 lr 0.100000 loss 1.5771 (1.4770) acc@1 0.3984 (0.4497) acc@5 0.7109 (0.7900)\n",
      "\u001b[32m[2020-07-02 20:03:01] __main__ INFO: \u001b[0mElapsed 33.53\n",
      "\u001b[32m[2020-07-02 20:03:01] __main__ INFO: \u001b[0mVal 64\n",
      "\u001b[32m[2020-07-02 20:03:02] __main__ INFO: \u001b[0mEpoch 64 loss 1.6636 acc@1 0.4018 acc@5 0.7644\n",
      "\u001b[32m[2020-07-02 20:03:02] __main__ INFO: \u001b[0mElapsed 1.05\n",
      "\u001b[32m[2020-07-02 20:03:02] __main__ INFO: \u001b[0mTrain 65 22464\n",
      "\u001b[32m[2020-07-02 20:03:07] __main__ INFO: \u001b[0mEpoch 65 Step 50/351 lr 0.100000 loss 1.4171 (1.4448) acc@1 0.4688 (0.4630) acc@5 0.7969 (0.7997)\n",
      "\u001b[32m[2020-07-02 20:03:12] __main__ INFO: \u001b[0mEpoch 65 Step 100/351 lr 0.100000 loss 1.4578 (1.4616) acc@1 0.4453 (0.4557) acc@5 0.8594 (0.7959)\n",
      "\u001b[32m[2020-07-02 20:03:17] __main__ INFO: \u001b[0mEpoch 65 Step 150/351 lr 0.100000 loss 1.3963 (1.4683) acc@1 0.4844 (0.4534) acc@5 0.7891 (0.7951)\n",
      "\u001b[32m[2020-07-02 20:03:22] __main__ INFO: \u001b[0mEpoch 65 Step 200/351 lr 0.100000 loss 1.3484 (1.4684) acc@1 0.4922 (0.4525) acc@5 0.8125 (0.7953)\n",
      "\u001b[32m[2020-07-02 20:03:26] __main__ INFO: \u001b[0mEpoch 65 Step 250/351 lr 0.100000 loss 1.6539 (1.4714) acc@1 0.3672 (0.4526) acc@5 0.7578 (0.7953)\n",
      "\u001b[32m[2020-07-02 20:03:31] __main__ INFO: \u001b[0mEpoch 65 Step 300/351 lr 0.100000 loss 1.6224 (1.4714) acc@1 0.3906 (0.4543) acc@5 0.7891 (0.7953)\n",
      "\u001b[32m[2020-07-02 20:03:36] __main__ INFO: \u001b[0mEpoch 65 Step 350/351 lr 0.100000 loss 1.4893 (1.4733) acc@1 0.4531 (0.4525) acc@5 0.7734 (0.7943)\n",
      "\u001b[32m[2020-07-02 20:03:36] __main__ INFO: \u001b[0mEpoch 65 Step 351/351 lr 0.100000 loss 1.4157 (1.4732) acc@1 0.4375 (0.4525) acc@5 0.8438 (0.7944)\n",
      "\u001b[32m[2020-07-02 20:03:36] __main__ INFO: \u001b[0mElapsed 33.53\n",
      "\u001b[32m[2020-07-02 20:03:36] __main__ INFO: \u001b[0mVal 65\n",
      "\u001b[32m[2020-07-02 20:03:37] __main__ INFO: \u001b[0mEpoch 65 loss 1.5990 acc@1 0.4148 acc@5 0.7792\n",
      "\u001b[32m[2020-07-02 20:03:37] __main__ INFO: \u001b[0mElapsed 1.06\n",
      "\u001b[32m[2020-07-02 20:03:37] __main__ INFO: \u001b[0mTrain 66 22815\n",
      "\u001b[32m[2020-07-02 20:03:42] __main__ INFO: \u001b[0mEpoch 66 Step 50/351 lr 0.100000 loss 1.3805 (1.4723) acc@1 0.4688 (0.4517) acc@5 0.7500 (0.7863)\n",
      "\u001b[32m[2020-07-02 20:03:47] __main__ INFO: \u001b[0mEpoch 66 Step 100/351 lr 0.100000 loss 1.4143 (1.4584) acc@1 0.4688 (0.4559) acc@5 0.7969 (0.7925)\n",
      "\u001b[32m[2020-07-02 20:03:51] __main__ INFO: \u001b[0mEpoch 66 Step 150/351 lr 0.100000 loss 1.3931 (1.4596) acc@1 0.4844 (0.4565) acc@5 0.8047 (0.7934)\n",
      "\u001b[32m[2020-07-02 20:03:56] __main__ INFO: \u001b[0mEpoch 66 Step 200/351 lr 0.100000 loss 1.3654 (1.4640) acc@1 0.4062 (0.4542) acc@5 0.7656 (0.7926)\n",
      "\u001b[32m[2020-07-02 20:04:01] __main__ INFO: \u001b[0mEpoch 66 Step 250/351 lr 0.100000 loss 1.4451 (1.4643) acc@1 0.4609 (0.4539) acc@5 0.8281 (0.7943)\n",
      "\u001b[32m[2020-07-02 20:04:06] __main__ INFO: \u001b[0mEpoch 66 Step 300/351 lr 0.100000 loss 1.5671 (1.4638) acc@1 0.3906 (0.4547) acc@5 0.8125 (0.7942)\n",
      "\u001b[32m[2020-07-02 20:04:10] __main__ INFO: \u001b[0mEpoch 66 Step 350/351 lr 0.100000 loss 1.5706 (1.4676) acc@1 0.3984 (0.4547) acc@5 0.7734 (0.7923)\n",
      "\u001b[32m[2020-07-02 20:04:11] __main__ INFO: \u001b[0mEpoch 66 Step 351/351 lr 0.100000 loss 1.5036 (1.4677) acc@1 0.4609 (0.4547) acc@5 0.7734 (0.7923)\n",
      "\u001b[32m[2020-07-02 20:04:11] __main__ INFO: \u001b[0mElapsed 33.57\n",
      "\u001b[32m[2020-07-02 20:04:11] __main__ INFO: \u001b[0mVal 66\n",
      "\u001b[32m[2020-07-02 20:04:12] __main__ INFO: \u001b[0mEpoch 66 loss 1.6368 acc@1 0.4168 acc@5 0.7782\n",
      "\u001b[32m[2020-07-02 20:04:12] __main__ INFO: \u001b[0mElapsed 1.06\n",
      "\u001b[32m[2020-07-02 20:04:12] __main__ INFO: \u001b[0mTrain 67 23166\n",
      "\u001b[32m[2020-07-02 20:04:16] __main__ INFO: \u001b[0mEpoch 67 Step 50/351 lr 0.100000 loss 1.6927 (1.4764) acc@1 0.3672 (0.4505) acc@5 0.7422 (0.7948)\n",
      "\u001b[32m[2020-07-02 20:04:21] __main__ INFO: \u001b[0mEpoch 67 Step 100/351 lr 0.100000 loss 1.4657 (1.4738) acc@1 0.4297 (0.4538) acc@5 0.8203 (0.7932)\n",
      "\u001b[32m[2020-07-02 20:04:26] __main__ INFO: \u001b[0mEpoch 67 Step 150/351 lr 0.100000 loss 1.4294 (1.4807) acc@1 0.4844 (0.4514) acc@5 0.7969 (0.7942)\n",
      "\u001b[32m[2020-07-02 20:04:31] __main__ INFO: \u001b[0mEpoch 67 Step 200/351 lr 0.100000 loss 1.4694 (1.4747) acc@1 0.4453 (0.4527) acc@5 0.8047 (0.7929)\n",
      "\u001b[32m[2020-07-02 20:04:35] __main__ INFO: \u001b[0mEpoch 67 Step 250/351 lr 0.100000 loss 1.5751 (1.4744) acc@1 0.4141 (0.4540) acc@5 0.7656 (0.7927)\n",
      "\u001b[32m[2020-07-02 20:04:40] __main__ INFO: \u001b[0mEpoch 67 Step 300/351 lr 0.100000 loss 1.5240 (1.4717) acc@1 0.4766 (0.4552) acc@5 0.7578 (0.7932)\n",
      "\u001b[32m[2020-07-02 20:04:45] __main__ INFO: \u001b[0mEpoch 67 Step 350/351 lr 0.100000 loss 1.3577 (1.4714) acc@1 0.5000 (0.4542) acc@5 0.7891 (0.7926)\n",
      "\u001b[32m[2020-07-02 20:04:45] __main__ INFO: \u001b[0mEpoch 67 Step 351/351 lr 0.100000 loss 1.4712 (1.4714) acc@1 0.5156 (0.4544) acc@5 0.8438 (0.7927)\n",
      "\u001b[32m[2020-07-02 20:04:45] __main__ INFO: \u001b[0mElapsed 33.49\n",
      "\u001b[32m[2020-07-02 20:04:45] __main__ INFO: \u001b[0mVal 67\n",
      "\u001b[32m[2020-07-02 20:04:46] __main__ INFO: \u001b[0mEpoch 67 loss 1.6218 acc@1 0.4182 acc@5 0.7758\n",
      "\u001b[32m[2020-07-02 20:04:46] __main__ INFO: \u001b[0mElapsed 1.07\n",
      "\u001b[32m[2020-07-02 20:04:46] __main__ INFO: \u001b[0mTrain 68 23517\n",
      "\u001b[32m[2020-07-02 20:04:51] __main__ INFO: \u001b[0mEpoch 68 Step 50/351 lr 0.100000 loss 1.2726 (1.4335) acc@1 0.5312 (0.4695) acc@5 0.8516 (0.7991)\n",
      "\u001b[32m[2020-07-02 20:04:56] __main__ INFO: \u001b[0mEpoch 68 Step 100/351 lr 0.100000 loss 1.6315 (1.4533) acc@1 0.4531 (0.4582) acc@5 0.7734 (0.7958)\n",
      "\u001b[32m[2020-07-02 20:05:00] __main__ INFO: \u001b[0mEpoch 68 Step 150/351 lr 0.100000 loss 1.4144 (1.4568) acc@1 0.4609 (0.4559) acc@5 0.8359 (0.7943)\n",
      "\u001b[32m[2020-07-02 20:05:05] __main__ INFO: \u001b[0mEpoch 68 Step 200/351 lr 0.100000 loss 1.4203 (1.4549) acc@1 0.5234 (0.4573) acc@5 0.8047 (0.7950)\n",
      "\u001b[32m[2020-07-02 20:05:10] __main__ INFO: \u001b[0mEpoch 68 Step 250/351 lr 0.100000 loss 1.5935 (1.4547) acc@1 0.4219 (0.4577) acc@5 0.7656 (0.7957)\n",
      "\u001b[32m[2020-07-02 20:05:15] __main__ INFO: \u001b[0mEpoch 68 Step 300/351 lr 0.100000 loss 1.3913 (1.4573) acc@1 0.4453 (0.4576) acc@5 0.7734 (0.7953)\n",
      "\u001b[32m[2020-07-02 20:05:20] __main__ INFO: \u001b[0mEpoch 68 Step 350/351 lr 0.100000 loss 1.4885 (1.4624) acc@1 0.4531 (0.4556) acc@5 0.7969 (0.7952)\n",
      "\u001b[32m[2020-07-02 20:05:20] __main__ INFO: \u001b[0mEpoch 68 Step 351/351 lr 0.100000 loss 1.4642 (1.4624) acc@1 0.4453 (0.4556) acc@5 0.7734 (0.7951)\n",
      "\u001b[32m[2020-07-02 20:05:20] __main__ INFO: \u001b[0mElapsed 33.49\n",
      "\u001b[32m[2020-07-02 20:05:20] __main__ INFO: \u001b[0mVal 68\n",
      "\u001b[32m[2020-07-02 20:05:21] __main__ INFO: \u001b[0mEpoch 68 loss 1.6845 acc@1 0.4044 acc@5 0.7758\n",
      "\u001b[32m[2020-07-02 20:05:21] __main__ INFO: \u001b[0mElapsed 1.08\n",
      "\u001b[32m[2020-07-02 20:05:21] __main__ INFO: \u001b[0mTrain 69 23868\n",
      "\u001b[32m[2020-07-02 20:05:26] __main__ INFO: \u001b[0mEpoch 69 Step 50/351 lr 0.100000 loss 1.3340 (1.4485) acc@1 0.4766 (0.4600) acc@5 0.8203 (0.7994)\n",
      "\u001b[32m[2020-07-02 20:05:30] __main__ INFO: \u001b[0mEpoch 69 Step 100/351 lr 0.100000 loss 1.5222 (1.4613) acc@1 0.4062 (0.4551) acc@5 0.7500 (0.7969)\n",
      "\u001b[32m[2020-07-02 20:05:35] __main__ INFO: \u001b[0mEpoch 69 Step 150/351 lr 0.100000 loss 1.4568 (1.4663) acc@1 0.4453 (0.4532) acc@5 0.8125 (0.7964)\n",
      "\u001b[32m[2020-07-02 20:05:40] __main__ INFO: \u001b[0mEpoch 69 Step 200/351 lr 0.100000 loss 1.4234 (1.4585) acc@1 0.4766 (0.4562) acc@5 0.7500 (0.7963)\n",
      "\u001b[32m[2020-07-02 20:05:45] __main__ INFO: \u001b[0mEpoch 69 Step 250/351 lr 0.100000 loss 1.4815 (1.4575) acc@1 0.4609 (0.4553) acc@5 0.8125 (0.7958)\n",
      "\u001b[32m[2020-07-02 20:05:49] __main__ INFO: \u001b[0mEpoch 69 Step 300/351 lr 0.100000 loss 1.3729 (1.4594) acc@1 0.4844 (0.4542) acc@5 0.8438 (0.7959)\n",
      "\u001b[32m[2020-07-02 20:05:54] __main__ INFO: \u001b[0mEpoch 69 Step 350/351 lr 0.100000 loss 1.3261 (1.4591) acc@1 0.5000 (0.4542) acc@5 0.8125 (0.7958)\n",
      "\u001b[32m[2020-07-02 20:05:54] __main__ INFO: \u001b[0mEpoch 69 Step 351/351 lr 0.100000 loss 1.4789 (1.4591) acc@1 0.4375 (0.4541) acc@5 0.7891 (0.7958)\n",
      "\u001b[32m[2020-07-02 20:05:54] __main__ INFO: \u001b[0mElapsed 33.57\n",
      "\u001b[32m[2020-07-02 20:05:54] __main__ INFO: \u001b[0mVal 69\n",
      "\u001b[32m[2020-07-02 20:05:55] __main__ INFO: \u001b[0mEpoch 69 loss 1.7004 acc@1 0.3928 acc@5 0.7766\n",
      "\u001b[32m[2020-07-02 20:05:55] __main__ INFO: \u001b[0mElapsed 1.07\n",
      "\u001b[32m[2020-07-02 20:05:55] __main__ INFO: \u001b[0mTrain 70 24219\n",
      "\u001b[32m[2020-07-02 20:06:00] __main__ INFO: \u001b[0mEpoch 70 Step 50/351 lr 0.100000 loss 1.3543 (1.4336) acc@1 0.5078 (0.4627) acc@5 0.7969 (0.7881)\n",
      "\u001b[32m[2020-07-02 20:06:05] __main__ INFO: \u001b[0mEpoch 70 Step 100/351 lr 0.100000 loss 1.5917 (1.4530) acc@1 0.4297 (0.4590) acc@5 0.8047 (0.7894)\n",
      "\u001b[32m[2020-07-02 20:06:10] __main__ INFO: \u001b[0mEpoch 70 Step 150/351 lr 0.100000 loss 1.3682 (1.4479) acc@1 0.4844 (0.4605) acc@5 0.8359 (0.7935)\n",
      "\u001b[32m[2020-07-02 20:06:15] __main__ INFO: \u001b[0mEpoch 70 Step 200/351 lr 0.100000 loss 1.3995 (1.4524) acc@1 0.4531 (0.4584) acc@5 0.8203 (0.7937)\n",
      "\u001b[32m[2020-07-02 20:06:19] __main__ INFO: \u001b[0mEpoch 70 Step 250/351 lr 0.100000 loss 1.6286 (1.4547) acc@1 0.3984 (0.4586) acc@5 0.7031 (0.7913)\n",
      "\u001b[32m[2020-07-02 20:06:24] __main__ INFO: \u001b[0mEpoch 70 Step 300/351 lr 0.100000 loss 1.5495 (1.4551) acc@1 0.4297 (0.4586) acc@5 0.8281 (0.7921)\n",
      "\u001b[32m[2020-07-02 20:06:29] __main__ INFO: \u001b[0mEpoch 70 Step 350/351 lr 0.100000 loss 1.3486 (1.4575) acc@1 0.5000 (0.4579) acc@5 0.8125 (0.7925)\n",
      "\u001b[32m[2020-07-02 20:06:29] __main__ INFO: \u001b[0mEpoch 70 Step 351/351 lr 0.100000 loss 1.4618 (1.4576) acc@1 0.4609 (0.4580) acc@5 0.7891 (0.7924)\n",
      "\u001b[32m[2020-07-02 20:06:29] __main__ INFO: \u001b[0mElapsed 33.57\n",
      "\u001b[32m[2020-07-02 20:06:29] __main__ INFO: \u001b[0mVal 70\n",
      "\u001b[32m[2020-07-02 20:06:30] __main__ INFO: \u001b[0mEpoch 70 loss 1.7384 acc@1 0.3966 acc@5 0.7702\n",
      "\u001b[32m[2020-07-02 20:06:30] __main__ INFO: \u001b[0mElapsed 1.06\n",
      "\u001b[32m[2020-07-02 20:06:30] __main__ INFO: \u001b[0mTrain 71 24570\n",
      "\u001b[32m[2020-07-02 20:06:35] __main__ INFO: \u001b[0mEpoch 71 Step 50/351 lr 0.100000 loss 1.4508 (1.4083) acc@1 0.4531 (0.4769) acc@5 0.7969 (0.8056)\n",
      "\u001b[32m[2020-07-02 20:06:40] __main__ INFO: \u001b[0mEpoch 71 Step 100/351 lr 0.100000 loss 1.8262 (1.4439) acc@1 0.3906 (0.4634) acc@5 0.7656 (0.8018)\n",
      "\u001b[32m[2020-07-02 20:06:44] __main__ INFO: \u001b[0mEpoch 71 Step 150/351 lr 0.100000 loss 1.3987 (1.4518) acc@1 0.5547 (0.4608) acc@5 0.7578 (0.7960)\n",
      "\u001b[32m[2020-07-02 20:06:49] __main__ INFO: \u001b[0mEpoch 71 Step 200/351 lr 0.100000 loss 1.4453 (1.4554) acc@1 0.4766 (0.4580) acc@5 0.8359 (0.7957)\n",
      "\u001b[32m[2020-07-02 20:06:54] __main__ INFO: \u001b[0mEpoch 71 Step 250/351 lr 0.100000 loss 1.4087 (1.4607) acc@1 0.4766 (0.4552) acc@5 0.7969 (0.7954)\n",
      "\u001b[32m[2020-07-02 20:06:59] __main__ INFO: \u001b[0mEpoch 71 Step 300/351 lr 0.100000 loss 1.5181 (1.4592) acc@1 0.4219 (0.4560) acc@5 0.8125 (0.7952)\n",
      "\u001b[32m[2020-07-02 20:07:03] __main__ INFO: \u001b[0mEpoch 71 Step 350/351 lr 0.100000 loss 1.4112 (1.4573) acc@1 0.4453 (0.4567) acc@5 0.8203 (0.7958)\n",
      "\u001b[32m[2020-07-02 20:07:04] __main__ INFO: \u001b[0mEpoch 71 Step 351/351 lr 0.100000 loss 1.3509 (1.4570) acc@1 0.5156 (0.4568) acc@5 0.8047 (0.7959)\n",
      "\u001b[32m[2020-07-02 20:07:04] __main__ INFO: \u001b[0mElapsed 33.55\n",
      "\u001b[32m[2020-07-02 20:07:04] __main__ INFO: \u001b[0mVal 71\n",
      "\u001b[32m[2020-07-02 20:07:05] __main__ INFO: \u001b[0mEpoch 71 loss 1.7097 acc@1 0.3914 acc@5 0.7824\n",
      "\u001b[32m[2020-07-02 20:07:05] __main__ INFO: \u001b[0mElapsed 1.09\n",
      "\u001b[32m[2020-07-02 20:07:05] __main__ INFO: \u001b[0mTrain 72 24921\n",
      "\u001b[32m[2020-07-02 20:07:09] __main__ INFO: \u001b[0mEpoch 72 Step 50/351 lr 0.100000 loss 1.5696 (1.4299) acc@1 0.5000 (0.4689) acc@5 0.7656 (0.8033)\n",
      "\u001b[32m[2020-07-02 20:07:14] __main__ INFO: \u001b[0mEpoch 72 Step 100/351 lr 0.100000 loss 1.4968 (1.4336) acc@1 0.4375 (0.4665) acc@5 0.7812 (0.8020)\n",
      "\u001b[32m[2020-07-02 20:07:19] __main__ INFO: \u001b[0mEpoch 72 Step 150/351 lr 0.100000 loss 1.6217 (1.4436) acc@1 0.3984 (0.4640) acc@5 0.8281 (0.7990)\n",
      "\u001b[32m[2020-07-02 20:07:24] __main__ INFO: \u001b[0mEpoch 72 Step 200/351 lr 0.100000 loss 1.6272 (1.4471) acc@1 0.4453 (0.4611) acc@5 0.7969 (0.7950)\n",
      "\u001b[32m[2020-07-02 20:07:29] __main__ INFO: \u001b[0mEpoch 72 Step 250/351 lr 0.100000 loss 1.3777 (1.4493) acc@1 0.4688 (0.4601) acc@5 0.8281 (0.7953)\n",
      "\u001b[32m[2020-07-02 20:07:33] __main__ INFO: \u001b[0mEpoch 72 Step 300/351 lr 0.100000 loss 1.5822 (1.4512) acc@1 0.4141 (0.4597) acc@5 0.7500 (0.7953)\n",
      "\u001b[32m[2020-07-02 20:07:38] __main__ INFO: \u001b[0mEpoch 72 Step 350/351 lr 0.100000 loss 1.4627 (1.4547) acc@1 0.4688 (0.4585) acc@5 0.8047 (0.7951)\n",
      "\u001b[32m[2020-07-02 20:07:38] __main__ INFO: \u001b[0mEpoch 72 Step 351/351 lr 0.100000 loss 1.4891 (1.4548) acc@1 0.4531 (0.4585) acc@5 0.8438 (0.7953)\n",
      "\u001b[32m[2020-07-02 20:07:38] __main__ INFO: \u001b[0mElapsed 33.54\n",
      "\u001b[32m[2020-07-02 20:07:38] __main__ INFO: \u001b[0mVal 72\n",
      "\u001b[32m[2020-07-02 20:07:39] __main__ INFO: \u001b[0mEpoch 72 loss 1.6253 acc@1 0.4110 acc@5 0.7698\n",
      "\u001b[32m[2020-07-02 20:07:39] __main__ INFO: \u001b[0mElapsed 1.07\n",
      "\u001b[32m[2020-07-02 20:07:39] __main__ INFO: \u001b[0mTrain 73 25272\n",
      "\u001b[32m[2020-07-02 20:07:44] __main__ INFO: \u001b[0mEpoch 73 Step 50/351 lr 0.100000 loss 1.5208 (1.4058) acc@1 0.4609 (0.4744) acc@5 0.7891 (0.7952)\n",
      "\u001b[32m[2020-07-02 20:07:49] __main__ INFO: \u001b[0mEpoch 73 Step 100/351 lr 0.100000 loss 1.3970 (1.4236) acc@1 0.4766 (0.4667) acc@5 0.7891 (0.7976)\n",
      "\u001b[32m[2020-07-02 20:07:54] __main__ INFO: \u001b[0mEpoch 73 Step 150/351 lr 0.100000 loss 1.3507 (1.4302) acc@1 0.4766 (0.4663) acc@5 0.8516 (0.7988)\n",
      "\u001b[32m[2020-07-02 20:07:58] __main__ INFO: \u001b[0mEpoch 73 Step 200/351 lr 0.100000 loss 1.4759 (1.4354) acc@1 0.4297 (0.4645) acc@5 0.7969 (0.7986)\n",
      "\u001b[32m[2020-07-02 20:08:03] __main__ INFO: \u001b[0mEpoch 73 Step 250/351 lr 0.100000 loss 1.3784 (1.4431) acc@1 0.4844 (0.4607) acc@5 0.8281 (0.7979)\n",
      "\u001b[32m[2020-07-02 20:08:08] __main__ INFO: \u001b[0mEpoch 73 Step 300/351 lr 0.100000 loss 1.4118 (1.4442) acc@1 0.4453 (0.4603) acc@5 0.8125 (0.7979)\n",
      "\u001b[32m[2020-07-02 20:08:13] __main__ INFO: \u001b[0mEpoch 73 Step 350/351 lr 0.100000 loss 1.3595 (1.4462) acc@1 0.5000 (0.4609) acc@5 0.8047 (0.7986)\n",
      "\u001b[32m[2020-07-02 20:08:13] __main__ INFO: \u001b[0mEpoch 73 Step 351/351 lr 0.100000 loss 1.4795 (1.4463) acc@1 0.4609 (0.4609) acc@5 0.8047 (0.7986)\n",
      "\u001b[32m[2020-07-02 20:08:13] __main__ INFO: \u001b[0mElapsed 33.54\n",
      "\u001b[32m[2020-07-02 20:08:13] __main__ INFO: \u001b[0mVal 73\n",
      "\u001b[32m[2020-07-02 20:08:14] __main__ INFO: \u001b[0mEpoch 73 loss 1.6497 acc@1 0.4022 acc@5 0.7730\n",
      "\u001b[32m[2020-07-02 20:08:14] __main__ INFO: \u001b[0mElapsed 1.08\n",
      "\u001b[32m[2020-07-02 20:08:14] __main__ INFO: \u001b[0mTrain 74 25623\n",
      "\u001b[32m[2020-07-02 20:08:19] __main__ INFO: \u001b[0mEpoch 74 Step 50/351 lr 0.100000 loss 1.5108 (1.4197) acc@1 0.3906 (0.4658) acc@5 0.7188 (0.8048)\n",
      "\u001b[32m[2020-07-02 20:08:24] __main__ INFO: \u001b[0mEpoch 74 Step 100/351 lr 0.100000 loss 1.3233 (1.4241) acc@1 0.5078 (0.4684) acc@5 0.8359 (0.8013)\n",
      "\u001b[32m[2020-07-02 20:08:28] __main__ INFO: \u001b[0mEpoch 74 Step 150/351 lr 0.100000 loss 1.4583 (1.4370) acc@1 0.4531 (0.4638) acc@5 0.8047 (0.7989)\n",
      "\u001b[32m[2020-07-02 20:08:33] __main__ INFO: \u001b[0mEpoch 74 Step 200/351 lr 0.100000 loss 1.4043 (1.4423) acc@1 0.4531 (0.4622) acc@5 0.7422 (0.7996)\n",
      "\u001b[32m[2020-07-02 20:08:38] __main__ INFO: \u001b[0mEpoch 74 Step 250/351 lr 0.100000 loss 1.4694 (1.4416) acc@1 0.4609 (0.4630) acc@5 0.7656 (0.7993)\n",
      "\u001b[32m[2020-07-02 20:08:43] __main__ INFO: \u001b[0mEpoch 74 Step 300/351 lr 0.100000 loss 1.3229 (1.4394) acc@1 0.5312 (0.4640) acc@5 0.8516 (0.7989)\n",
      "\u001b[32m[2020-07-02 20:08:47] __main__ INFO: \u001b[0mEpoch 74 Step 350/351 lr 0.100000 loss 1.5783 (1.4433) acc@1 0.3750 (0.4631) acc@5 0.7500 (0.7987)\n",
      "\u001b[32m[2020-07-02 20:08:47] __main__ INFO: \u001b[0mEpoch 74 Step 351/351 lr 0.100000 loss 1.5828 (1.4437) acc@1 0.3984 (0.4629) acc@5 0.8047 (0.7987)\n",
      "\u001b[32m[2020-07-02 20:08:47] __main__ INFO: \u001b[0mElapsed 33.59\n",
      "\u001b[32m[2020-07-02 20:08:47] __main__ INFO: \u001b[0mVal 74\n",
      "\u001b[32m[2020-07-02 20:08:49] __main__ INFO: \u001b[0mEpoch 74 loss 1.7044 acc@1 0.3920 acc@5 0.7648\n",
      "\u001b[32m[2020-07-02 20:08:49] __main__ INFO: \u001b[0mElapsed 1.07\n",
      "\u001b[32m[2020-07-02 20:08:49] __main__ INFO: \u001b[0mTrain 75 25974\n",
      "\u001b[32m[2020-07-02 20:08:53] __main__ INFO: \u001b[0mEpoch 75 Step 50/351 lr 0.100000 loss 1.4757 (1.4293) acc@1 0.4453 (0.4641) acc@5 0.7500 (0.7969)\n",
      "\u001b[32m[2020-07-02 20:08:58] __main__ INFO: \u001b[0mEpoch 75 Step 100/351 lr 0.100000 loss 1.3687 (1.4363) acc@1 0.4844 (0.4629) acc@5 0.7578 (0.7968)\n",
      "\u001b[32m[2020-07-02 20:09:03] __main__ INFO: \u001b[0mEpoch 75 Step 150/351 lr 0.100000 loss 1.4860 (1.4387) acc@1 0.4375 (0.4628) acc@5 0.7500 (0.7963)\n",
      "\u001b[32m[2020-07-02 20:09:08] __main__ INFO: \u001b[0mEpoch 75 Step 200/351 lr 0.100000 loss 1.5612 (1.4456) acc@1 0.3750 (0.4600) acc@5 0.7969 (0.7943)\n",
      "\u001b[32m[2020-07-02 20:09:12] __main__ INFO: \u001b[0mEpoch 75 Step 250/351 lr 0.100000 loss 1.4028 (1.4455) acc@1 0.4531 (0.4603) acc@5 0.7812 (0.7962)\n",
      "\u001b[32m[2020-07-02 20:09:17] __main__ INFO: \u001b[0mEpoch 75 Step 300/351 lr 0.100000 loss 1.6370 (1.4435) acc@1 0.3359 (0.4614) acc@5 0.7344 (0.7974)\n",
      "\u001b[32m[2020-07-02 20:09:22] __main__ INFO: \u001b[0mEpoch 75 Step 350/351 lr 0.100000 loss 1.5687 (1.4442) acc@1 0.3516 (0.4616) acc@5 0.7656 (0.7965)\n",
      "\u001b[32m[2020-07-02 20:09:22] __main__ INFO: \u001b[0mEpoch 75 Step 351/351 lr 0.100000 loss 1.4211 (1.4441) acc@1 0.4531 (0.4615) acc@5 0.7734 (0.7964)\n",
      "\u001b[32m[2020-07-02 20:09:22] __main__ INFO: \u001b[0mElapsed 33.56\n",
      "\u001b[32m[2020-07-02 20:09:22] __main__ INFO: \u001b[0mVal 75\n",
      "\u001b[32m[2020-07-02 20:09:23] __main__ INFO: \u001b[0mEpoch 75 loss 1.6754 acc@1 0.4048 acc@5 0.7748\n",
      "\u001b[32m[2020-07-02 20:09:23] __main__ INFO: \u001b[0mElapsed 1.06\n",
      "\u001b[32m[2020-07-02 20:09:23] __main__ INFO: \u001b[0mTrain 76 26325\n",
      "\u001b[32m[2020-07-02 20:09:28] __main__ INFO: \u001b[0mEpoch 76 Step 50/351 lr 0.100000 loss 1.4923 (1.4351) acc@1 0.4141 (0.4639) acc@5 0.7656 (0.8006)\n",
      "\u001b[32m[2020-07-02 20:09:33] __main__ INFO: \u001b[0mEpoch 76 Step 100/351 lr 0.100000 loss 1.4156 (1.4234) acc@1 0.4453 (0.4696) acc@5 0.7734 (0.7993)\n",
      "\u001b[32m[2020-07-02 20:09:38] __main__ INFO: \u001b[0mEpoch 76 Step 150/351 lr 0.100000 loss 1.4936 (1.4302) acc@1 0.4219 (0.4676) acc@5 0.7578 (0.7965)\n",
      "\u001b[32m[2020-07-02 20:09:42] __main__ INFO: \u001b[0mEpoch 76 Step 200/351 lr 0.100000 loss 1.5424 (1.4376) acc@1 0.4531 (0.4655) acc@5 0.8203 (0.7949)\n",
      "\u001b[32m[2020-07-02 20:09:47] __main__ INFO: \u001b[0mEpoch 76 Step 250/351 lr 0.100000 loss 1.3531 (1.4403) acc@1 0.5000 (0.4646) acc@5 0.8281 (0.7950)\n",
      "\u001b[32m[2020-07-02 20:09:52] __main__ INFO: \u001b[0mEpoch 76 Step 300/351 lr 0.100000 loss 1.3548 (1.4369) acc@1 0.5078 (0.4651) acc@5 0.8906 (0.7955)\n",
      "\u001b[32m[2020-07-02 20:09:57] __main__ INFO: \u001b[0mEpoch 76 Step 350/351 lr 0.100000 loss 1.5854 (1.4408) acc@1 0.4141 (0.4644) acc@5 0.7578 (0.7954)\n",
      "\u001b[32m[2020-07-02 20:09:57] __main__ INFO: \u001b[0mEpoch 76 Step 351/351 lr 0.100000 loss 1.4236 (1.4408) acc@1 0.4531 (0.4644) acc@5 0.7812 (0.7954)\n",
      "\u001b[32m[2020-07-02 20:09:57] __main__ INFO: \u001b[0mElapsed 33.60\n",
      "\u001b[32m[2020-07-02 20:09:57] __main__ INFO: \u001b[0mVal 76\n",
      "\u001b[32m[2020-07-02 20:09:58] __main__ INFO: \u001b[0mEpoch 76 loss 1.6905 acc@1 0.3942 acc@5 0.7736\n",
      "\u001b[32m[2020-07-02 20:09:58] __main__ INFO: \u001b[0mElapsed 1.07\n",
      "\u001b[32m[2020-07-02 20:09:58] __main__ INFO: \u001b[0mTrain 77 26676\n",
      "\u001b[32m[2020-07-02 20:10:03] __main__ INFO: \u001b[0mEpoch 77 Step 50/351 lr 0.100000 loss 1.4673 (1.4253) acc@1 0.4219 (0.4656) acc@5 0.7656 (0.7869)\n",
      "\u001b[32m[2020-07-02 20:10:07] __main__ INFO: \u001b[0mEpoch 77 Step 100/351 lr 0.100000 loss 1.4087 (1.4127) acc@1 0.4844 (0.4713) acc@5 0.7734 (0.7914)\n",
      "\u001b[32m[2020-07-02 20:10:12] __main__ INFO: \u001b[0mEpoch 77 Step 150/351 lr 0.100000 loss 1.3810 (1.4271) acc@1 0.4844 (0.4656) acc@5 0.8047 (0.7914)\n",
      "\u001b[32m[2020-07-02 20:10:17] __main__ INFO: \u001b[0mEpoch 77 Step 200/351 lr 0.100000 loss 1.5259 (1.4351) acc@1 0.4219 (0.4639) acc@5 0.7891 (0.7929)\n",
      "\u001b[32m[2020-07-02 20:10:22] __main__ INFO: \u001b[0mEpoch 77 Step 250/351 lr 0.100000 loss 1.4689 (1.4335) acc@1 0.4219 (0.4644) acc@5 0.7812 (0.7940)\n",
      "\u001b[32m[2020-07-02 20:10:27] __main__ INFO: \u001b[0mEpoch 77 Step 300/351 lr 0.100000 loss 1.5067 (1.4337) acc@1 0.4453 (0.4643) acc@5 0.7734 (0.7939)\n",
      "\u001b[32m[2020-07-02 20:10:31] __main__ INFO: \u001b[0mEpoch 77 Step 350/351 lr 0.100000 loss 1.3400 (1.4382) acc@1 0.4609 (0.4621) acc@5 0.7969 (0.7928)\n",
      "\u001b[32m[2020-07-02 20:10:31] __main__ INFO: \u001b[0mEpoch 77 Step 351/351 lr 0.100000 loss 1.3786 (1.4381) acc@1 0.4844 (0.4621) acc@5 0.8047 (0.7929)\n",
      "\u001b[32m[2020-07-02 20:10:31] __main__ INFO: \u001b[0mElapsed 33.54\n",
      "\u001b[32m[2020-07-02 20:10:31] __main__ INFO: \u001b[0mVal 77\n",
      "\u001b[32m[2020-07-02 20:10:32] __main__ INFO: \u001b[0mEpoch 77 loss 1.6526 acc@1 0.3956 acc@5 0.7742\n",
      "\u001b[32m[2020-07-02 20:10:32] __main__ INFO: \u001b[0mElapsed 1.07\n",
      "\u001b[32m[2020-07-02 20:10:32] __main__ INFO: \u001b[0mTrain 78 27027\n",
      "\u001b[32m[2020-07-02 20:10:37] __main__ INFO: \u001b[0mEpoch 78 Step 50/351 lr 0.100000 loss 1.4997 (1.4114) acc@1 0.4062 (0.4675) acc@5 0.8203 (0.7981)\n",
      "\u001b[32m[2020-07-02 20:10:42] __main__ INFO: \u001b[0mEpoch 78 Step 100/351 lr 0.100000 loss 1.4056 (1.4178) acc@1 0.4766 (0.4710) acc@5 0.7891 (0.7934)\n",
      "\u001b[32m[2020-07-02 20:10:47] __main__ INFO: \u001b[0mEpoch 78 Step 150/351 lr 0.100000 loss 1.4914 (1.4207) acc@1 0.3750 (0.4686) acc@5 0.7266 (0.7926)\n"
     ]
    }
   ],
   "source": [
    "# Train the model per the settings specified in the original paper\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "!python train.py --config configs/cifar/resnet.yaml \\\n",
    "    model.resnet.depth 32 \\\n",
    "    train.batch_size 128 \\\n",
    "    dataset.name CIFAR10_RA_2_20 \\\n",
    "    train.base_lr 0.1 \\\n",
    "    train.output_dir /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/july2/exp00 \\\n",
    "    scheduler.epochs 350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Errors\n",
    "# Resume training with the un-augmented data\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "#!python train.py --config configs/cifar/resnet.yaml \\\n",
    "!python train.py --config /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/july2/exp00/config.yaml \\\n",
    "    train.checkpoint /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/july2/exp00/checkpoint_00350.pth \\\n",
    "    dataset.name CIFAR10 \\\n",
    "    train.base_lr .001 \\\n",
    "    train.output_dir /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/july2/exp00_resume350_100 \\\n",
    "    scheduler.epochs 100\n",
    "\n",
    "#### Set LEARNING RATE based on ending LR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-03 22:14:37] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/july2/exp00/checkpoint_00350.pth\n",
      "Files already downloaded and verified\n",
      "100%|███████████████████████████████████████████| 79/79 [00:03<00:00, 25.57it/s]\n",
      "\u001b[32m[2020-07-03 22:14:41] __main__ INFO: \u001b[0mElapsed 3.09\n",
      "\u001b[32m[2020-07-03 22:14:41] __main__ INFO: \u001b[0mLoss 1.8401 Accuracy 0.7096\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10 test dataset - without training on unaugmented data\n",
    "# Right the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/resnet.yaml \\\n",
    "   model.resnet.depth 32 \\\n",
    "   test.batch_size 128 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/july2/exp00/checkpoint_00350.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/july2/exp00/test_results_350_cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-03 22:18:16] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/july2/exp00_resume350_100/checkpoint_00100.pth\n",
      "Files already downloaded and verified\n",
      "100%|███████████████████████████████████████████| 79/79 [00:03<00:00, 25.51it/s]\n",
      "\u001b[32m[2020-07-03 22:18:20] __main__ INFO: \u001b[0mElapsed 3.10\n",
      "\u001b[32m[2020-07-03 22:18:20] __main__ INFO: \u001b[0mLoss 0.4560 Accuracy 0.8688\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/resnet.yaml \\\n",
    "   model.resnet.depth 32 \\\n",
    "   test.batch_size 128 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/july2/exp00_resume350_100/checkpoint_00100.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/july2/exp00_resume350_100/test_results_0100_cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-03 22:14:48] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/july2/exp00/checkpoint_00350.pth\n",
      "CIFAR 10.1\n",
      "100%|███████████████████████████████████████████| 16/16 [00:00<00:00, 16.76it/s]\n",
      "\u001b[32m[2020-07-03 22:14:49] __main__ INFO: \u001b[0mElapsed 0.96\n",
      "\u001b[32m[2020-07-03 22:14:49] __main__ INFO: \u001b[0mLoss 3.2345 Accuracy 0.5380\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10.1 test dataset - without training on unaugmented data\n",
    "# Right the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/resnet.yaml \\\n",
    "   model.resnet.depth 32 \\\n",
    "   test.batch_size 128 \\\n",
    "   dataset.name CIFAR101 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/july2/exp00/checkpoint_00350.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/july2/exp00/test_results_0350_cifar101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-03 22:14:52] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/july2/exp00_resume350_100/checkpoint_00100.pth\n",
      "CIFAR 10.1\n",
      "100%|███████████████████████████████████████████| 16/16 [00:00<00:00, 18.55it/s]\n",
      "\u001b[32m[2020-07-03 22:14:54] __main__ INFO: \u001b[0mElapsed 0.86\n",
      "\u001b[32m[2020-07-03 22:14:54] __main__ INFO: \u001b[0mLoss 0.8289 Accuracy 0.7760\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10.1 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/resnet.yaml \\\n",
    "   model.resnet.depth 32 \\\n",
    "   test.batch_size 128 \\\n",
    "   dataset.name CIFAR101 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/july2/exp00_resume350_100/checkpoint_00100.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/july2/exp00_resume350_100/test_results_0100_cifar101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "second pipeline: augmented 250, unaugmented 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model per the settings specified in the original paper\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "!python train.py --config configs/cifar/resnet.yaml \\\n",
    "    model.resnet.depth 32 \\\n",
    "    train.batch_size 128 \\\n",
    "    dataset.name CIFAR10_RA_2_20 \\\n",
    "    train.base_lr 0.1 \\\n",
    "    train.output_dir /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/july2/exp00 \\\n",
    "    scheduler.epochs 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Errors\n",
    "# Resume training with the un-augmented data\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "#!python train.py --config configs/cifar/resnet.yaml \\\n",
    "!python train.py --config /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/july2/exp00/config.yaml \\\n",
    "    train.checkpoint /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/july2/exp00/checkpoint_00250.pth \\\n",
    "    dataset.name CIFAR10 \\\n",
    "    train.base_lr .001 \\\n",
    "    train.output_dir /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/july2/exp00_resume250_200 \\\n",
    "    scheduler.epochs 200\n",
    "\n",
    "#### Set LEARNING RATE based on ending LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-03 22:19:44] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/july2/exp00/checkpoint_00250.pth\n",
      "Files already downloaded and verified\n",
      "100%|███████████████████████████████████████████| 79/79 [00:03<00:00, 25.43it/s]\n",
      "\u001b[32m[2020-07-03 22:19:48] __main__ INFO: \u001b[0mElapsed 3.11\n",
      "\u001b[32m[2020-07-03 22:19:48] __main__ INFO: \u001b[0mLoss 1.6258 Accuracy 0.7112\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10 test dataset - without training on unaugmented data\n",
    "# Right the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/resnet.yaml \\\n",
    "   model.resnet.depth 32 \\\n",
    "   test.batch_size 128 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/july2/exp00/checkpoint_00250.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/july2/exp00/test_results_0250_cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-03 22:19:52] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/july2/exp00_resume250_200/checkpoint_00200.pth\n",
      "Files already downloaded and verified\n",
      "100%|███████████████████████████████████████████| 79/79 [00:03<00:00, 25.39it/s]\n",
      "\u001b[32m[2020-07-03 22:19:56] __main__ INFO: \u001b[0mElapsed 3.11\n",
      "\u001b[32m[2020-07-03 22:19:56] __main__ INFO: \u001b[0mLoss 0.4352 Accuracy 0.8695\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/resnet.yaml \\\n",
    "   model.resnet.depth 32 \\\n",
    "   test.batch_size 128 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/july2/exp00_resume250_200/checkpoint_00200.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/july2/exp00_resume250_200/test_results_0200_cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-03 22:19:59] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/july2/exp00/checkpoint_00250.pth\n",
      "CIFAR 10.1\n",
      "100%|███████████████████████████████████████████| 16/16 [00:00<00:00, 19.62it/s]\n",
      "\u001b[32m[2020-07-03 22:20:00] __main__ INFO: \u001b[0mElapsed 0.82\n",
      "\u001b[32m[2020-07-03 22:20:00] __main__ INFO: \u001b[0mLoss 2.8602 Accuracy 0.5350\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10.1 test dataset - without training on unaugmented data\n",
    "# Right the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/resnet.yaml \\\n",
    "   model.resnet.depth 32 \\\n",
    "   test.batch_size 128 \\\n",
    "   dataset.name CIFAR101 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/july2/exp00/checkpoint_00250.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/july2/exp00/test_results_0250_cifar101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-03 22:20:03] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/july2/exp00_resume250_200/checkpoint_00200.pth\n",
      "CIFAR 10.1\n",
      "100%|███████████████████████████████████████████| 16/16 [00:00<00:00, 19.21it/s]\n",
      "\u001b[32m[2020-07-03 22:20:04] __main__ INFO: \u001b[0mElapsed 0.84\n",
      "\u001b[32m[2020-07-03 22:20:04] __main__ INFO: \u001b[0mLoss 0.7807 Accuracy 0.7715\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10.1 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/resnet.yaml \\\n",
    "   model.resnet.depth 32 \\\n",
    "   test.batch_size 128 \\\n",
    "   dataset.name CIFAR101 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/july2/exp00_resume250_200/checkpoint_00200.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/july2/exp00_resume250_200/test_results_0200_cifar101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "third pipeline: randaugment 150, refine 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model per the settings specified in the original paper\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "!python train.py --config configs/cifar/resnet.yaml \\\n",
    "    model.resnet.depth 32 \\\n",
    "    train.batch_size 128 \\\n",
    "    dataset.name CIFAR10_RA_2_20 \\\n",
    "    train.base_lr 0.1 \\\n",
    "    train.output_dir /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/july2/exp00 \\\n",
    "    scheduler.epochs 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Errors\n",
    "# Resume training with the un-augmented data\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "#!python train.py --config configs/cifar/resnet.yaml \\\n",
    "!python train.py --config /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/july2/exp00/config.yaml \\\n",
    "    train.checkpoint /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/july2/exp00/checkpoint_00150.pth \\\n",
    "    dataset.name CIFAR10 \\\n",
    "    train.base_lr .001 \\\n",
    "    train.output_dir /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/july2/exp00_resume150_300 \\\n",
    "    scheduler.epochs 300\n",
    "\n",
    "#### Set LEARNING RATE based on ending LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-03 22:22:21] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/july2/exp00/checkpoint_00150.pth\n",
      "Files already downloaded and verified\n",
      "100%|███████████████████████████████████████████| 79/79 [00:03<00:00, 25.29it/s]\n",
      "\u001b[32m[2020-07-03 22:22:25] __main__ INFO: \u001b[0mElapsed 3.13\n",
      "\u001b[32m[2020-07-03 22:22:25] __main__ INFO: \u001b[0mLoss 1.3270 Accuracy 0.7220\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10 test dataset - without training on unaugmented data\n",
    "# Right the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/resnet.yaml \\\n",
    "   model.resnet.depth 32 \\\n",
    "   test.batch_size 128 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/july2/exp00/checkpoint_00150.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/july2/exp00/test_results_0150_cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-03 22:22:28] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/july2/exp00_resume150_300/checkpoint_00300.pth\n",
      "Files already downloaded and verified\n",
      "100%|███████████████████████████████████████████| 79/79 [00:03<00:00, 25.71it/s]\n",
      "\u001b[32m[2020-07-03 22:22:32] __main__ INFO: \u001b[0mElapsed 3.07\n",
      "\u001b[32m[2020-07-03 22:22:32] __main__ INFO: \u001b[0mLoss 0.4104 Accuracy 0.8733\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/resnet.yaml \\\n",
    "   model.resnet.depth 32 \\\n",
    "   test.batch_size 128 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/july2/exp00_resume150_300/checkpoint_00300.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/july2/exp00_resume150_300/test_results_0300_cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-03 22:22:36] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/july2/exp00/checkpoint_00150.pth\n",
      "CIFAR 10.1\n",
      "100%|███████████████████████████████████████████| 16/16 [00:00<00:00, 19.40it/s]\n",
      "\u001b[32m[2020-07-03 22:22:37] __main__ INFO: \u001b[0mElapsed 0.83\n",
      "\u001b[32m[2020-07-03 22:22:37] __main__ INFO: \u001b[0mLoss 2.4433 Accuracy 0.5390\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10.1 test dataset - without training on unaugmented data\n",
    "# Right the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/resnet.yaml \\\n",
    "   model.resnet.depth 32 \\\n",
    "   test.batch_size 128 \\\n",
    "   dataset.name CIFAR101 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/july2/exp00/checkpoint_00150.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/july2/exp00/test_results_0150_cifar101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-03 22:22:40] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/july2/exp00_resume150_300/checkpoint_00300.pth\n",
      "CIFAR 10.1\n",
      "100%|███████████████████████████████████████████| 16/16 [00:00<00:00, 19.43it/s]\n",
      "\u001b[32m[2020-07-03 22:22:41] __main__ INFO: \u001b[0mElapsed 0.83\n",
      "\u001b[32m[2020-07-03 22:22:41] __main__ INFO: \u001b[0mLoss 0.7315 Accuracy 0.7755\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10.1 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/resnet.yaml \\\n",
    "   model.resnet.depth 32 \\\n",
    "   test.batch_size 128 \\\n",
    "   dataset.name CIFAR101 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/july2/exp00_resume150_300/checkpoint_00300.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/july2/exp00_resume150_300/test_results_0300_cifar101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "writeout results to s3\n",
    "*********** MANUAL INTERVENTION NEEDED BELOIW *******************\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Testset</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Original_Accuracy</th>\n",
       "      <th>Original_CI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>resnet_basic_32_ra_2_20</td>\n",
       "      <td>350</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>1.8401</td>\n",
       "      <td>0.7096</td>\n",
       "      <td>92.5</td>\n",
       "      <td>(92.0, 93.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>resnet_basic_32_ra_2_20</td>\n",
       "      <td>350</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>3.2345</td>\n",
       "      <td>0.538</td>\n",
       "      <td>84.9</td>\n",
       "      <td>(83.2, 86.4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>resnet_basic_32_ra_2_20_refined350</td>\n",
       "      <td>100</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.3978</td>\n",
       "      <td>0.87</td>\n",
       "      <td>92.5</td>\n",
       "      <td>(92.0, 93.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>resnet_basic_32_ra_2_20_refined350</td>\n",
       "      <td>100</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>0.8289</td>\n",
       "      <td>0.776</td>\n",
       "      <td>84.9</td>\n",
       "      <td>(83.2, 86.4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>resnet_basic_32_ra_2_20</td>\n",
       "      <td>250</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>1.6258</td>\n",
       "      <td>0.7112</td>\n",
       "      <td>92.5</td>\n",
       "      <td>(92.0, 93.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>resnet_basic_32_ra_2_20_refined250</td>\n",
       "      <td>200</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.4352</td>\n",
       "      <td>0.8695</td>\n",
       "      <td>92.5</td>\n",
       "      <td>(92.0, 93.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>resnet_basic_32_ra_2_20</td>\n",
       "      <td>250</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>2.8602</td>\n",
       "      <td>0.535</td>\n",
       "      <td>84.9</td>\n",
       "      <td>(83.2, 86.4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>resnet_basic_32_ra_2_20_refined250</td>\n",
       "      <td>200</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>0.7807</td>\n",
       "      <td>0.7715</td>\n",
       "      <td>84.9</td>\n",
       "      <td>(83.2, 86.4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>resnet_basic_32_ra_2_20</td>\n",
       "      <td>150</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>1.327</td>\n",
       "      <td>0.722</td>\n",
       "      <td>92.5</td>\n",
       "      <td>(92.0, 93.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>resnet_basic_32_ra_2_20</td>\n",
       "      <td>150</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>2.4433</td>\n",
       "      <td>0.539</td>\n",
       "      <td>84.9</td>\n",
       "      <td>(83.2, 86.4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>resnet_basic_32_ra_2_20_refined150</td>\n",
       "      <td>300</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.4104</td>\n",
       "      <td>0.8733</td>\n",
       "      <td>92.5</td>\n",
       "      <td>(92.0, 93.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>resnet_basic_32_ra_2_20_refined150</td>\n",
       "      <td>300</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>0.7755</td>\n",
       "      <td>0.7315</td>\n",
       "      <td>84.9</td>\n",
       "      <td>(83.2, 86.4)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model Epoch    Testset    Loss Accuracy  \\\n",
       "0              resnet_basic_32_ra_2_20   350    cifar10  1.8401   0.7096   \n",
       "1              resnet_basic_32_ra_2_20   350  cifar10.1  3.2345    0.538   \n",
       "2   resnet_basic_32_ra_2_20_refined350   100    cifar10  0.3978     0.87   \n",
       "3   resnet_basic_32_ra_2_20_refined350   100  cifar10.1  0.8289    0.776   \n",
       "4              resnet_basic_32_ra_2_20   250    cifar10  1.6258   0.7112   \n",
       "5   resnet_basic_32_ra_2_20_refined250   200    cifar10  0.4352   0.8695   \n",
       "6              resnet_basic_32_ra_2_20   250  cifar10.1  2.8602    0.535   \n",
       "7   resnet_basic_32_ra_2_20_refined250   200  cifar10.1  0.7807   0.7715   \n",
       "8              resnet_basic_32_ra_2_20   150    cifar10   1.327    0.722   \n",
       "9              resnet_basic_32_ra_2_20   150  cifar10.1  2.4433    0.539   \n",
       "10  resnet_basic_32_ra_2_20_refined150   300    cifar10  0.4104   0.8733   \n",
       "11  resnet_basic_32_ra_2_20_refined150   300  cifar10.1  0.7755   0.7315   \n",
       "\n",
       "    Original_Accuracy   Original_CI  \n",
       "0                92.5  (92.0, 93.0)  \n",
       "1                84.9  (83.2, 86.4)  \n",
       "2                92.5  (92.0, 93.0)  \n",
       "3                84.9  (83.2, 86.4)  \n",
       "4                92.5  (92.0, 93.0)  \n",
       "5                92.5  (92.0, 93.0)  \n",
       "6                84.9  (83.2, 86.4)  \n",
       "7                84.9  (83.2, 86.4)  \n",
       "8                92.5  (92.0, 93.0)  \n",
       "9                84.9  (83.2, 86.4)  \n",
       "10               92.5  (92.0, 93.0)  \n",
       "11               84.9  (83.2, 86.4)  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "a = pd.Series(['resnet_basic_32_ra_2_20', 350, 'cifar10', 1.8401, 0.7096]) #Loss 0.4279 Accuracy 0.8670\n",
    "b = pd.Series(['resnet_basic_32_ra_2_20', 350, 'cifar10.1', 3.2345, 0.5380]) #Loss 0.8061 Accuracy 0.7645\n",
    "c = pd.Series(['resnet_basic_32_ra_2_20_refined350', 100, 'cifar10', 0.3978, 0.8700]) #Loss 0.4342 Accuracy 0.8740\n",
    "d = pd.Series(['resnet_basic_32_ra_2_20_refined350', 100, 'cifar10.1', 0.8289, 0.7760]) #Loss 0.7919 Accuracy 0.7635\n",
    "\n",
    "\n",
    "e = pd.Series(['resnet_basic_32_ra_2_20', 250, 'cifar10', 1.6258, 0.7112]) #Loss 0.4279 Accuracy 0.8670\n",
    "g = pd.Series(['resnet_basic_32_ra_2_20', 250, 'cifar10.1', 2.8602, 0.5350]) #Loss 0.8061 Accuracy 0.7645\n",
    "f = pd.Series(['resnet_basic_32_ra_2_20_refined250', 200, 'cifar10', 0.4352, 0.8695]) #Loss 0.4342 Accuracy 0.8740\n",
    "h = pd.Series(['resnet_basic_32_ra_2_20_refined250', 200, 'cifar10.1', 0.7807, 0.7715]) #Loss 0.7919 Accuracy 0.7635\n",
    "\n",
    "i = pd.Series(['resnet_basic_32_ra_2_20', 150, 'cifar10', 1.3270, 0.7220]) #Loss 0.4279 Accuracy 0.8670\n",
    "j = pd.Series(['resnet_basic_32_ra_2_20', 150, 'cifar10.1', 2.4433, 0.5390]) #Loss 0.4342 Accuracy 0.8740\n",
    "k = pd.Series(['resnet_basic_32_ra_2_20_refined150', 300, 'cifar10', 0.4104, 0.8733]) #Loss 0.8061 Accuracy 0.7645\n",
    "l = pd.Series(['resnet_basic_32_ra_2_20_refined150', 300, 'cifar10.1', 0.7755, 0.7315]) #Loss 0.7919 Accuracy 0.7635\n",
    "               \n",
    "df_results = pd.concat([a,b,c,d,e,f, g, h, i, j, k, l], axis=1).T\n",
    "df_results.columns = ['Model', 'Epoch', 'Testset', 'Loss', 'Accuracy']\n",
    "\n",
    "df_results['Original_Accuracy'] = df_results.apply((lambda row: 92.5 if row[2] == 'cifar10' else 84.9), axis=1)\n",
    "df_results['Original_CI'] = df_results.apply((lambda row: (92.0, 93.0) if row[2] == 'cifar10' else (83.2, 86.4)), axis=1)\n",
    "\n",
    "df_results.to_csv('/home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/july2/results.csv')\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/ec2-user/SageMaker/experiments/resnet_basic_32/exp00/test_results_0200_cifar101/predictions.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-1a16f692c7b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/home/ec2-user/SageMaker/experiments/resnet_basic_32/exp00/test_results_0200_cifar101/predictions.npz'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mnpzfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpzfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mnpzfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'preds'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/ec2-user/SageMaker/experiments/resnet_basic_32/exp00/test_results_0200_cifar101/predictions.npz'"
     ]
    }
   ],
   "source": [
    "#error\n",
    "# Peak inside the output file for predictions\n",
    "import numpy as np\n",
    "output = '/home/ec2-user/SageMaker/experiments/resnet_basic_32/exp00/test_results_0200_cifar101/predictions.npz'\n",
    "npzfile = np.load(output)\n",
    "print(npzfile.files)\n",
    "npzfile['preds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local File: /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/results.csv\n",
      "      Dest: sagemaker/results/original-models/resnet_basic_32_ra_2_20/results.csv\n",
      "Local File: /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/exp00/log_plain.txt\n",
      "      Dest: sagemaker/results/original-models/resnet_basic_32_ra_2_20/exp00/log_plain.txt\n",
      "Local File: /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/exp00/checkpoint_00100.pth\n",
      "      Dest: sagemaker/results/original-models/resnet_basic_32_ra_2_20/exp00/checkpoint_00100.pth\n",
      "Local File: /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/exp00/checkpoint_00200.pth\n",
      "      Dest: sagemaker/results/original-models/resnet_basic_32_ra_2_20/exp00/checkpoint_00200.pth\n",
      "Local File: /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/exp00/env.yaml\n",
      "      Dest: sagemaker/results/original-models/resnet_basic_32_ra_2_20/exp00/env.yaml\n",
      "Local File: /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/exp00/log.txt\n",
      "      Dest: sagemaker/results/original-models/resnet_basic_32_ra_2_20/exp00/log.txt\n",
      "Local File: /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/exp00/events.out.tfevents.1593302392.ip-172-16-86-238.25491.0\n",
      "      Dest: sagemaker/results/original-models/resnet_basic_32_ra_2_20/exp00/events.out.tfevents.1593302392.ip-172-16-86-238.25491.0\n",
      "Local File: /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/exp00/config_min.yaml\n",
      "      Dest: sagemaker/results/original-models/resnet_basic_32_ra_2_20/exp00/config_min.yaml\n",
      "Local File: /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/exp00/last_checkpoint\n",
      "      Dest: sagemaker/results/original-models/resnet_basic_32_ra_2_20/exp00/last_checkpoint\n",
      "Local File: /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/exp00/config.yaml\n",
      "      Dest: sagemaker/results/original-models/resnet_basic_32_ra_2_20/exp00/config.yaml\n",
      "Local File: /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/exp00/test_results_0200_cifar101/predictions.npz\n",
      "      Dest: sagemaker/results/original-models/resnet_basic_32_ra_2_20/exp00/test_results_0200_cifar101/predictions.npz\n",
      "Local File: /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/exp00/test_results_0200_cifar10/predictions.npz\n",
      "      Dest: sagemaker/results/original-models/resnet_basic_32_ra_2_20/exp00/test_results_0200_cifar10/predictions.npz\n",
      "Local File: /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/exp00/test_results_0100_cifar101/predictions.npz\n",
      "      Dest: sagemaker/results/original-models/resnet_basic_32_ra_2_20/exp00/test_results_0100_cifar101/predictions.npz\n",
      "Local File: /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/exp00/running/events.out.tfevents.1593302392.ip-172-16-86-238.25491.1\n",
      "      Dest: sagemaker/results/original-models/resnet_basic_32_ra_2_20/exp00/running/events.out.tfevents.1593302392.ip-172-16-86-238.25491.1\n",
      "Local File: /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/exp00/test_results_0100_cifar10/predictions.npz\n",
      "      Dest: sagemaker/results/original-models/resnet_basic_32_ra_2_20/exp00/test_results_0100_cifar10/predictions.npz\n",
      "Local File: /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/exp00_resume200_250/log_plain.txt\n",
      "      Dest: sagemaker/results/original-models/resnet_basic_32_ra_2_20/exp00_resume200_250/log_plain.txt\n",
      "Local File: /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/exp00_resume200_250/checkpoint_00100.pth\n",
      "      Dest: sagemaker/results/original-models/resnet_basic_32_ra_2_20/exp00_resume200_250/checkpoint_00100.pth\n",
      "Local File: /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/exp00_resume200_250/checkpoint_00200.pth\n",
      "      Dest: sagemaker/results/original-models/resnet_basic_32_ra_2_20/exp00_resume200_250/checkpoint_00200.pth\n",
      "Local File: /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/exp00_resume200_250/env.yaml\n",
      "      Dest: sagemaker/results/original-models/resnet_basic_32_ra_2_20/exp00_resume200_250/env.yaml\n",
      "Local File: /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/exp00_resume200_250/log.txt\n",
      "      Dest: sagemaker/results/original-models/resnet_basic_32_ra_2_20/exp00_resume200_250/log.txt\n",
      "Local File: /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/exp00_resume200_250/events.out.tfevents.1593309053.ip-172-16-86-238.28597.0\n",
      "      Dest: sagemaker/results/original-models/resnet_basic_32_ra_2_20/exp00_resume200_250/events.out.tfevents.1593309053.ip-172-16-86-238.28597.0\n",
      "Local File: /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/exp00_resume200_250/checkpoint_00250.pth\n",
      "      Dest: sagemaker/results/original-models/resnet_basic_32_ra_2_20/exp00_resume200_250/checkpoint_00250.pth\n",
      "Local File: /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/exp00_resume200_250/config_min.yaml\n",
      "      Dest: sagemaker/results/original-models/resnet_basic_32_ra_2_20/exp00_resume200_250/config_min.yaml\n",
      "Local File: /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/exp00_resume200_250/last_checkpoint\n",
      "      Dest: sagemaker/results/original-models/resnet_basic_32_ra_2_20/exp00_resume200_250/last_checkpoint\n",
      "Local File: /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/exp00_resume200_250/config.yaml\n",
      "      Dest: sagemaker/results/original-models/resnet_basic_32_ra_2_20/exp00_resume200_250/config.yaml\n",
      "Local File: /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/exp00_resume200_250/test_results_0250_cifar10/predictions.npz\n",
      "      Dest: sagemaker/results/original-models/resnet_basic_32_ra_2_20/exp00_resume200_250/test_results_0250_cifar10/predictions.npz\n",
      "Local File: /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/exp00_resume200_250/test_results_0250_cifar101/predictions.npz\n",
      "      Dest: sagemaker/results/original-models/resnet_basic_32_ra_2_20/exp00_resume200_250/test_results_0250_cifar101/predictions.npz\n",
      "Local File: /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/exp00_resume200_250/running/events.out.tfevents.1593309053.ip-172-16-86-238.28597.1\n",
      "      Dest: sagemaker/results/original-models/resnet_basic_32_ra_2_20/exp00_resume200_250/running/events.out.tfevents.1593309053.ip-172-16-86-238.28597.1\n",
      "Local File: /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/exp00_resume100_350/checkpoint_00300.pth\n",
      "      Dest: sagemaker/results/original-models/resnet_basic_32_ra_2_20/exp00_resume100_350/checkpoint_00300.pth\n",
      "Local File: /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/exp00_resume100_350/log_plain.txt\n",
      "      Dest: sagemaker/results/original-models/resnet_basic_32_ra_2_20/exp00_resume100_350/log_plain.txt\n",
      "Local File: /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/exp00_resume100_350/checkpoint_00350.pth\n",
      "      Dest: sagemaker/results/original-models/resnet_basic_32_ra_2_20/exp00_resume100_350/checkpoint_00350.pth\n",
      "Local File: /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/exp00_resume100_350/checkpoint_00100.pth\n",
      "      Dest: sagemaker/results/original-models/resnet_basic_32_ra_2_20/exp00_resume100_350/checkpoint_00100.pth\n",
      "Local File: /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/exp00_resume100_350/checkpoint_00200.pth\n",
      "      Dest: sagemaker/results/original-models/resnet_basic_32_ra_2_20/exp00_resume100_350/checkpoint_00200.pth\n",
      "Local File: /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/exp00_resume100_350/env.yaml\n",
      "      Dest: sagemaker/results/original-models/resnet_basic_32_ra_2_20/exp00_resume100_350/env.yaml\n",
      "Local File: /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/exp00_resume100_350/log.txt\n",
      "      Dest: sagemaker/results/original-models/resnet_basic_32_ra_2_20/exp00_resume100_350/log.txt\n",
      "Local File: /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/exp00_resume100_350/events.out.tfevents.1593317410.ip-172-16-86-238.31161.0\n",
      "      Dest: sagemaker/results/original-models/resnet_basic_32_ra_2_20/exp00_resume100_350/events.out.tfevents.1593317410.ip-172-16-86-238.31161.0\n",
      "Local File: /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/exp00_resume100_350/config_min.yaml\n",
      "      Dest: sagemaker/results/original-models/resnet_basic_32_ra_2_20/exp00_resume100_350/config_min.yaml\n",
      "Local File: /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/exp00_resume100_350/last_checkpoint\n",
      "      Dest: sagemaker/results/original-models/resnet_basic_32_ra_2_20/exp00_resume100_350/last_checkpoint\n",
      "Local File: /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/exp00_resume100_350/config.yaml\n",
      "      Dest: sagemaker/results/original-models/resnet_basic_32_ra_2_20/exp00_resume100_350/config.yaml\n",
      "Local File: /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/exp00_resume100_350/test_results_0350_cifar10/predictions.npz\n",
      "      Dest: sagemaker/results/original-models/resnet_basic_32_ra_2_20/exp00_resume100_350/test_results_0350_cifar10/predictions.npz\n",
      "Local File: /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/exp00_resume100_350/running/events.out.tfevents.1593317410.ip-172-16-86-238.31161.1\n",
      "      Dest: sagemaker/results/original-models/resnet_basic_32_ra_2_20/exp00_resume100_350/running/events.out.tfevents.1593317410.ip-172-16-86-238.31161.1\n",
      "Local File: /home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20/exp00_resume100_350/test_results_0350_cifar101/predictions.npz\n",
      "      Dest: sagemaker/results/original-models/resnet_basic_32_ra_2_20/exp00_resume100_350/test_results_0350_cifar101/predictions.npz\n"
     ]
    }
   ],
   "source": [
    "# Upload the model checkpoints, configs, and results to S3 \n",
    "bucket='sagemaker-may29'\n",
    "prefix = 'sagemaker/results/original-models/resnet_basic_32_ra_2_20'\n",
    "path = '/home/ec2-user/SageMaker/experiments/resnet_basic_32_ra_2_20'\n",
    "\n",
    "s3_resource = boto3.resource(\"s3\", region_name=\"us-east-2\")\n",
    "\n",
    "def uploadDirectory(local_path,bucket_name,s3_prefix):\n",
    "\n",
    "    my_bucket = s3_resource.Bucket(bucket_name)\n",
    "    \n",
    "    for path, subdirs, files in os.walk(local_path):\n",
    "        path = path.replace(\"\\\\\",\"/\")\n",
    "        directory_name = path.replace(local_path,\"\")\n",
    "        for file in files:\n",
    "            print(\"Local File:\", os.path.join(path, file))\n",
    "            print(\"      Dest:\", s3_prefix+directory_name+'/'+file)\n",
    "            my_bucket.upload_file(os.path.join(path, file), s3_prefix+directory_name+'/'+file)\n",
    "    \n",
    "uploadDirectory(path,bucket,prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Model</th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Testset</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Original_Accuracy</th>\n",
       "      <th>Original_CI</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>resnet_basic_32_ra_2_20</td>\n",
       "      <td>200</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>1.4212</td>\n",
       "      <td>0.7254</td>\n",
       "      <td>92.5</td>\n",
       "      <td>(92.0, 93.0)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>resnet_basic_32_ra_2_20</td>\n",
       "      <td>100</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>1.6066</td>\n",
       "      <td>0.7281</td>\n",
       "      <td>92.5</td>\n",
       "      <td>(92.0, 93.0)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>resnet_basic_32_ra_2_20</td>\n",
       "      <td>200</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>2.5458</td>\n",
       "      <td>0.5505</td>\n",
       "      <td>84.9</td>\n",
       "      <td>(83.2, 86.4)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>resnet_basic_32_ra_2_20</td>\n",
       "      <td>100</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>1.9095</td>\n",
       "      <td>0.5490</td>\n",
       "      <td>84.9</td>\n",
       "      <td>(83.2, 86.4)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>resnet_basic_32_ra_2_20_refined200</td>\n",
       "      <td>250</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.4312</td>\n",
       "      <td>0.8691</td>\n",
       "      <td>92.5</td>\n",
       "      <td>(92.0, 93.0)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>resnet_basic_32_ra_2_20_refined100</td>\n",
       "      <td>350</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.3978</td>\n",
       "      <td>0.8700</td>\n",
       "      <td>92.5</td>\n",
       "      <td>(92.0, 93.0)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>resnet_basic_32_ra_2_20_refined200</td>\n",
       "      <td>250</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>0.7625</td>\n",
       "      <td>0.7630</td>\n",
       "      <td>84.9</td>\n",
       "      <td>(83.2, 86.4)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>resnet_basic_32_ra_2_20_refined100</td>\n",
       "      <td>350</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>0.7138</td>\n",
       "      <td>0.7700</td>\n",
       "      <td>84.9</td>\n",
       "      <td>(83.2, 86.4)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>resnet_basic_32_ra_2_20</td>\n",
       "      <td>400</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>1.7503</td>\n",
       "      <td>0.7241</td>\n",
       "      <td>92.5</td>\n",
       "      <td>(92.0, 93.0)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>resnet_basic_32_ra_2_20</td>\n",
       "      <td>300</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>1.6118</td>\n",
       "      <td>0.7259</td>\n",
       "      <td>92.5</td>\n",
       "      <td>(92.0, 93.0)</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>resnet_basic_32_ra_2_20</td>\n",
       "      <td>400</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>3.1568</td>\n",
       "      <td>0.5375</td>\n",
       "      <td>84.9</td>\n",
       "      <td>(83.2, 86.4)</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>resnet_basic_32_ra_2_20</td>\n",
       "      <td>300</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>3.0470</td>\n",
       "      <td>0.5265</td>\n",
       "      <td>84.9</td>\n",
       "      <td>(83.2, 86.4)</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4</td>\n",
       "      <td>resnet_basic_32_ra_2_20_refined400</td>\n",
       "      <td>50</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.4279</td>\n",
       "      <td>0.8670</td>\n",
       "      <td>92.5</td>\n",
       "      <td>(92.0, 93.0)</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5</td>\n",
       "      <td>resnet_basic_32_ra_2_20_refined300</td>\n",
       "      <td>150</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.4342</td>\n",
       "      <td>0.8740</td>\n",
       "      <td>92.5</td>\n",
       "      <td>(92.0, 93.0)</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>6</td>\n",
       "      <td>resnet_basic_32_ra_2_20_refined400</td>\n",
       "      <td>50</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>0.8061</td>\n",
       "      <td>0.7645</td>\n",
       "      <td>84.9</td>\n",
       "      <td>(83.2, 86.4)</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7</td>\n",
       "      <td>resnet_basic_32_ra_2_20_refined300</td>\n",
       "      <td>150</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>0.7919</td>\n",
       "      <td>0.7635</td>\n",
       "      <td>84.9</td>\n",
       "      <td>(83.2, 86.4)</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                               Model  Epoch    Testset    Loss  \\\n",
       "0            0             resnet_basic_32_ra_2_20    200    cifar10  1.4212   \n",
       "1            1             resnet_basic_32_ra_2_20    100    cifar10  1.6066   \n",
       "2            2             resnet_basic_32_ra_2_20    200  cifar10.1  2.5458   \n",
       "3            3             resnet_basic_32_ra_2_20    100  cifar10.1  1.9095   \n",
       "4            4  resnet_basic_32_ra_2_20_refined200    250    cifar10  0.4312   \n",
       "5            5  resnet_basic_32_ra_2_20_refined100    350    cifar10  0.3978   \n",
       "6            6  resnet_basic_32_ra_2_20_refined200    250  cifar10.1  0.7625   \n",
       "7            7  resnet_basic_32_ra_2_20_refined100    350  cifar10.1  0.7138   \n",
       "8            0             resnet_basic_32_ra_2_20    400    cifar10  1.7503   \n",
       "9            1             resnet_basic_32_ra_2_20    300    cifar10  1.6118   \n",
       "10           2             resnet_basic_32_ra_2_20    400  cifar10.1  3.1568   \n",
       "11           3             resnet_basic_32_ra_2_20    300  cifar10.1  3.0470   \n",
       "12           4  resnet_basic_32_ra_2_20_refined400     50    cifar10  0.4279   \n",
       "13           5  resnet_basic_32_ra_2_20_refined300    150    cifar10  0.4342   \n",
       "14           6  resnet_basic_32_ra_2_20_refined400     50  cifar10.1  0.8061   \n",
       "15           7  resnet_basic_32_ra_2_20_refined300    150  cifar10.1  0.7919   \n",
       "\n",
       "    Accuracy  Original_Accuracy   Original_CI  Unnamed: 0.1  \n",
       "0     0.7254               92.5  (92.0, 93.0)           NaN  \n",
       "1     0.7281               92.5  (92.0, 93.0)           NaN  \n",
       "2     0.5505               84.9  (83.2, 86.4)           NaN  \n",
       "3     0.5490               84.9  (83.2, 86.4)           NaN  \n",
       "4     0.8691               92.5  (92.0, 93.0)           NaN  \n",
       "5     0.8700               92.5  (92.0, 93.0)           NaN  \n",
       "6     0.7630               84.9  (83.2, 86.4)           NaN  \n",
       "7     0.7700               84.9  (83.2, 86.4)           NaN  \n",
       "8     0.7241               92.5  (92.0, 93.0)           0.0  \n",
       "9     0.7259               92.5  (92.0, 93.0)           1.0  \n",
       "10    0.5375               84.9  (83.2, 86.4)           2.0  \n",
       "11    0.5265               84.9  (83.2, 86.4)           3.0  \n",
       "12    0.8670               92.5  (92.0, 93.0)           4.0  \n",
       "13    0.8740               92.5  (92.0, 93.0)           5.0  \n",
       "14    0.7645               84.9  (83.2, 86.4)           6.0  \n",
       "15    0.7635               84.9  (83.2, 86.4)           7.0  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_res = pd.read_csv(\"../../original_model_results/resnet_basic_32_ra_2_20/results.csv\")\n",
    "old_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Testset</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Original_Accuracy</th>\n",
       "      <th>Original_CI</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>resnet_basic_32_ra_2_20</td>\n",
       "      <td>200</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>1.4212</td>\n",
       "      <td>0.7254</td>\n",
       "      <td>92.5</td>\n",
       "      <td>(92.0, 93.0)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>resnet_basic_32_ra_2_20</td>\n",
       "      <td>100</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>1.6066</td>\n",
       "      <td>0.7281</td>\n",
       "      <td>92.5</td>\n",
       "      <td>(92.0, 93.0)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>resnet_basic_32_ra_2_20</td>\n",
       "      <td>200</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>2.5458</td>\n",
       "      <td>0.5505</td>\n",
       "      <td>84.9</td>\n",
       "      <td>(83.2, 86.4)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>resnet_basic_32_ra_2_20</td>\n",
       "      <td>100</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>1.9095</td>\n",
       "      <td>0.549</td>\n",
       "      <td>84.9</td>\n",
       "      <td>(83.2, 86.4)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>resnet_basic_32_ra_2_20_refined200</td>\n",
       "      <td>250</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.4312</td>\n",
       "      <td>0.8691</td>\n",
       "      <td>92.5</td>\n",
       "      <td>(92.0, 93.0)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>resnet_basic_32_ra_2_20_refined100</td>\n",
       "      <td>350</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.3978</td>\n",
       "      <td>0.87</td>\n",
       "      <td>92.5</td>\n",
       "      <td>(92.0, 93.0)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>resnet_basic_32_ra_2_20_refined200</td>\n",
       "      <td>250</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>0.7625</td>\n",
       "      <td>0.763</td>\n",
       "      <td>84.9</td>\n",
       "      <td>(83.2, 86.4)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>resnet_basic_32_ra_2_20_refined100</td>\n",
       "      <td>350</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>0.7138</td>\n",
       "      <td>0.77</td>\n",
       "      <td>84.9</td>\n",
       "      <td>(83.2, 86.4)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>resnet_basic_32_ra_2_20</td>\n",
       "      <td>400</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>1.7503</td>\n",
       "      <td>0.7241</td>\n",
       "      <td>92.5</td>\n",
       "      <td>(92.0, 93.0)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>resnet_basic_32_ra_2_20</td>\n",
       "      <td>300</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>1.6118</td>\n",
       "      <td>0.7259</td>\n",
       "      <td>92.5</td>\n",
       "      <td>(92.0, 93.0)</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>resnet_basic_32_ra_2_20</td>\n",
       "      <td>400</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>3.1568</td>\n",
       "      <td>0.5375</td>\n",
       "      <td>84.9</td>\n",
       "      <td>(83.2, 86.4)</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>resnet_basic_32_ra_2_20</td>\n",
       "      <td>300</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>3.047</td>\n",
       "      <td>0.5265</td>\n",
       "      <td>84.9</td>\n",
       "      <td>(83.2, 86.4)</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>resnet_basic_32_ra_2_20_refined400</td>\n",
       "      <td>50</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.4279</td>\n",
       "      <td>0.867</td>\n",
       "      <td>92.5</td>\n",
       "      <td>(92.0, 93.0)</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>resnet_basic_32_ra_2_20_refined300</td>\n",
       "      <td>150</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.4342</td>\n",
       "      <td>0.874</td>\n",
       "      <td>92.5</td>\n",
       "      <td>(92.0, 93.0)</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>resnet_basic_32_ra_2_20_refined400</td>\n",
       "      <td>50</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>0.8061</td>\n",
       "      <td>0.7645</td>\n",
       "      <td>84.9</td>\n",
       "      <td>(83.2, 86.4)</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>resnet_basic_32_ra_2_20_refined300</td>\n",
       "      <td>150</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>0.7919</td>\n",
       "      <td>0.7635</td>\n",
       "      <td>84.9</td>\n",
       "      <td>(83.2, 86.4)</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Model Epoch    Testset    Loss Accuracy  \\\n",
       "0             resnet_basic_32_ra_2_20   200    cifar10  1.4212   0.7254   \n",
       "1             resnet_basic_32_ra_2_20   100    cifar10  1.6066   0.7281   \n",
       "2             resnet_basic_32_ra_2_20   200  cifar10.1  2.5458   0.5505   \n",
       "3             resnet_basic_32_ra_2_20   100  cifar10.1  1.9095    0.549   \n",
       "4  resnet_basic_32_ra_2_20_refined200   250    cifar10  0.4312   0.8691   \n",
       "5  resnet_basic_32_ra_2_20_refined100   350    cifar10  0.3978     0.87   \n",
       "6  resnet_basic_32_ra_2_20_refined200   250  cifar10.1  0.7625    0.763   \n",
       "7  resnet_basic_32_ra_2_20_refined100   350  cifar10.1  0.7138     0.77   \n",
       "0             resnet_basic_32_ra_2_20   400    cifar10  1.7503   0.7241   \n",
       "1             resnet_basic_32_ra_2_20   300    cifar10  1.6118   0.7259   \n",
       "2             resnet_basic_32_ra_2_20   400  cifar10.1  3.1568   0.5375   \n",
       "3             resnet_basic_32_ra_2_20   300  cifar10.1   3.047   0.5265   \n",
       "4  resnet_basic_32_ra_2_20_refined400    50    cifar10  0.4279    0.867   \n",
       "5  resnet_basic_32_ra_2_20_refined300   150    cifar10  0.4342    0.874   \n",
       "6  resnet_basic_32_ra_2_20_refined400    50  cifar10.1  0.8061   0.7645   \n",
       "7  resnet_basic_32_ra_2_20_refined300   150  cifar10.1  0.7919   0.7635   \n",
       "\n",
       "   Original_Accuracy   Original_CI  Unnamed: 0  \n",
       "0               92.5  (92.0, 93.0)         NaN  \n",
       "1               92.5  (92.0, 93.0)         NaN  \n",
       "2               84.9  (83.2, 86.4)         NaN  \n",
       "3               84.9  (83.2, 86.4)         NaN  \n",
       "4               92.5  (92.0, 93.0)         NaN  \n",
       "5               92.5  (92.0, 93.0)         NaN  \n",
       "6               84.9  (83.2, 86.4)         NaN  \n",
       "7               84.9  (83.2, 86.4)         NaN  \n",
       "0               92.5  (92.0, 93.0)         0.0  \n",
       "1               92.5  (92.0, 93.0)         1.0  \n",
       "2               84.9  (83.2, 86.4)         2.0  \n",
       "3               84.9  (83.2, 86.4)         3.0  \n",
       "4               92.5  (92.0, 93.0)         4.0  \n",
       "5               92.5  (92.0, 93.0)         5.0  \n",
       "6               84.9  (83.2, 86.4)         6.0  \n",
       "7               84.9  (83.2, 86.4)         7.0  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df = pd.concat([df_results, old_res], axis = 0)\n",
    "temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df.to_csv(\"../../original_model_results/resnet_basic_32_ra_2_20/results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Model</th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Testset</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Original_Accuracy</th>\n",
       "      <th>Original_CI</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>resnet_basic_32_ra_2_20</td>\n",
       "      <td>200</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>1.4212</td>\n",
       "      <td>0.7254</td>\n",
       "      <td>92.5</td>\n",
       "      <td>(92.0, 93.0)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>resnet_basic_32_ra_2_20</td>\n",
       "      <td>100</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>1.6066</td>\n",
       "      <td>0.7281</td>\n",
       "      <td>92.5</td>\n",
       "      <td>(92.0, 93.0)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>resnet_basic_32_ra_2_20</td>\n",
       "      <td>200</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>2.5458</td>\n",
       "      <td>0.5505</td>\n",
       "      <td>84.9</td>\n",
       "      <td>(83.2, 86.4)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>resnet_basic_32_ra_2_20</td>\n",
       "      <td>100</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>1.9095</td>\n",
       "      <td>0.5490</td>\n",
       "      <td>84.9</td>\n",
       "      <td>(83.2, 86.4)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>resnet_basic_32_ra_2_20_refined200</td>\n",
       "      <td>250</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.4312</td>\n",
       "      <td>0.8691</td>\n",
       "      <td>92.5</td>\n",
       "      <td>(92.0, 93.0)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>resnet_basic_32_ra_2_20_refined100</td>\n",
       "      <td>350</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.3978</td>\n",
       "      <td>0.8700</td>\n",
       "      <td>92.5</td>\n",
       "      <td>(92.0, 93.0)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>resnet_basic_32_ra_2_20_refined200</td>\n",
       "      <td>250</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>0.7625</td>\n",
       "      <td>0.7630</td>\n",
       "      <td>84.9</td>\n",
       "      <td>(83.2, 86.4)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>resnet_basic_32_ra_2_20_refined100</td>\n",
       "      <td>350</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>0.7138</td>\n",
       "      <td>0.7700</td>\n",
       "      <td>84.9</td>\n",
       "      <td>(83.2, 86.4)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>resnet_basic_32_ra_2_20</td>\n",
       "      <td>400</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>1.7503</td>\n",
       "      <td>0.7241</td>\n",
       "      <td>92.5</td>\n",
       "      <td>(92.0, 93.0)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>resnet_basic_32_ra_2_20</td>\n",
       "      <td>300</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>1.6118</td>\n",
       "      <td>0.7259</td>\n",
       "      <td>92.5</td>\n",
       "      <td>(92.0, 93.0)</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>resnet_basic_32_ra_2_20</td>\n",
       "      <td>400</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>3.1568</td>\n",
       "      <td>0.5375</td>\n",
       "      <td>84.9</td>\n",
       "      <td>(83.2, 86.4)</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>resnet_basic_32_ra_2_20</td>\n",
       "      <td>300</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>3.0470</td>\n",
       "      <td>0.5265</td>\n",
       "      <td>84.9</td>\n",
       "      <td>(83.2, 86.4)</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4</td>\n",
       "      <td>resnet_basic_32_ra_2_20_refined400</td>\n",
       "      <td>50</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.4279</td>\n",
       "      <td>0.8670</td>\n",
       "      <td>92.5</td>\n",
       "      <td>(92.0, 93.0)</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5</td>\n",
       "      <td>resnet_basic_32_ra_2_20_refined300</td>\n",
       "      <td>150</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.4342</td>\n",
       "      <td>0.8740</td>\n",
       "      <td>92.5</td>\n",
       "      <td>(92.0, 93.0)</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>6</td>\n",
       "      <td>resnet_basic_32_ra_2_20_refined400</td>\n",
       "      <td>50</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>0.8061</td>\n",
       "      <td>0.7645</td>\n",
       "      <td>84.9</td>\n",
       "      <td>(83.2, 86.4)</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7</td>\n",
       "      <td>resnet_basic_32_ra_2_20_refined300</td>\n",
       "      <td>150</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>0.7919</td>\n",
       "      <td>0.7635</td>\n",
       "      <td>84.9</td>\n",
       "      <td>(83.2, 86.4)</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                               Model  Epoch    Testset    Loss  \\\n",
       "0            0             resnet_basic_32_ra_2_20    200    cifar10  1.4212   \n",
       "1            1             resnet_basic_32_ra_2_20    100    cifar10  1.6066   \n",
       "2            2             resnet_basic_32_ra_2_20    200  cifar10.1  2.5458   \n",
       "3            3             resnet_basic_32_ra_2_20    100  cifar10.1  1.9095   \n",
       "4            4  resnet_basic_32_ra_2_20_refined200    250    cifar10  0.4312   \n",
       "5            5  resnet_basic_32_ra_2_20_refined100    350    cifar10  0.3978   \n",
       "6            6  resnet_basic_32_ra_2_20_refined200    250  cifar10.1  0.7625   \n",
       "7            7  resnet_basic_32_ra_2_20_refined100    350  cifar10.1  0.7138   \n",
       "8            0             resnet_basic_32_ra_2_20    400    cifar10  1.7503   \n",
       "9            1             resnet_basic_32_ra_2_20    300    cifar10  1.6118   \n",
       "10           2             resnet_basic_32_ra_2_20    400  cifar10.1  3.1568   \n",
       "11           3             resnet_basic_32_ra_2_20    300  cifar10.1  3.0470   \n",
       "12           4  resnet_basic_32_ra_2_20_refined400     50    cifar10  0.4279   \n",
       "13           5  resnet_basic_32_ra_2_20_refined300    150    cifar10  0.4342   \n",
       "14           6  resnet_basic_32_ra_2_20_refined400     50  cifar10.1  0.8061   \n",
       "15           7  resnet_basic_32_ra_2_20_refined300    150  cifar10.1  0.7919   \n",
       "\n",
       "    Accuracy  Original_Accuracy   Original_CI  Unnamed: 0.1  \n",
       "0     0.7254               92.5  (92.0, 93.0)           NaN  \n",
       "1     0.7281               92.5  (92.0, 93.0)           NaN  \n",
       "2     0.5505               84.9  (83.2, 86.4)           NaN  \n",
       "3     0.5490               84.9  (83.2, 86.4)           NaN  \n",
       "4     0.8691               92.5  (92.0, 93.0)           NaN  \n",
       "5     0.8700               92.5  (92.0, 93.0)           NaN  \n",
       "6     0.7630               84.9  (83.2, 86.4)           NaN  \n",
       "7     0.7700               84.9  (83.2, 86.4)           NaN  \n",
       "8     0.7241               92.5  (92.0, 93.0)           0.0  \n",
       "9     0.7259               92.5  (92.0, 93.0)           1.0  \n",
       "10    0.5375               84.9  (83.2, 86.4)           2.0  \n",
       "11    0.5265               84.9  (83.2, 86.4)           3.0  \n",
       "12    0.8670               92.5  (92.0, 93.0)           4.0  \n",
       "13    0.8740               92.5  (92.0, 93.0)           5.0  \n",
       "14    0.7645               84.9  (83.2, 86.4)           6.0  \n",
       "15    0.7635               84.9  (83.2, 86.4)           7.0  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"../../original_model_results/resnet_basic_32_ra_2_20/results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
