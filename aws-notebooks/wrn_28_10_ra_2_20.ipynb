{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wide Residual Net 29 4x64\n",
    "- Training Dataset: RandAugment, N=2, M=20\n",
    "- Sagemaker Notebook must be of type, conda_pytorch_p36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import numpy \n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 1)) (1.18.1)\n",
      "Requirement already satisfied: torch>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 2)) (1.4.0)\n",
      "Requirement already satisfied: torchvision in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (0.5.0)\n",
      "Requirement already satisfied: fvcore in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (0.1.1.post20200620)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 5)) (4.44.1)\n",
      "Requirement already satisfied: yacs in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 6)) (0.1.7)\n",
      "Requirement already satisfied: apex from git+https://github.com/NVIDIA/apex.git#egg=apex in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 7)) (0.1)\n",
      "Requirement already satisfied: termcolor in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 8)) (1.1.0)\n",
      "Requirement already satisfied: thop<0.0.31.post2004070130 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 9)) (0.0.31.post2001170342)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (1.14.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (7.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (5.3.1)\n",
      "Requirement already satisfied: tabulate in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (0.8.7)\n",
      "Requirement already satisfied: portalocker in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (1.7.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Sagemaker Notebook must be of type, conda_pytorch_p36\n",
    "\n",
    "!pip install -r '/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorboard in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (2.2.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.0.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.18.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.29.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.14.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (0.9.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (3.2.2)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (0.34.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (46.1.3.post20200330)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.6.0.post3)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (0.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (2.23.0)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (3.12.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.18.1)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (4.1.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (3.4.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (0.2.8)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard) (1.5.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (1.25.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (2020.4.5.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (3.0.4)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard) (0.4.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard) (2.2.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Need to add this to requirements.txt\n",
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-21 22:06:55] __main__ INFO: \u001b[0mdevice: cuda\n",
      "cudnn:\n",
      "  benchmark: True\n",
      "  deterministic: False\n",
      "dataset:\n",
      "  name: CIFAR10_RA_2_20\n",
      "  dataset_dir: ''\n",
      "  image_size: 32\n",
      "  n_channels: 3\n",
      "  n_classes: 10\n",
      "model:\n",
      "  type: cifar\n",
      "  name: wrn\n",
      "  init_mode: kaiming_fan_in\n",
      "  vgg:\n",
      "    n_channels: [64, 128, 256, 512, 512]\n",
      "    n_layers: [2, 2, 3, 3, 3]\n",
      "    use_bn: True\n",
      "  resnet:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "  resnet_preact:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "  wrn:\n",
      "    depth: 28\n",
      "    initial_channels: 16\n",
      "    widening_factor: 10\n",
      "    drop_rate: 0.0\n",
      "  densenet:\n",
      "    depth: 100\n",
      "    n_blocks: [6, 12, 24, 16]\n",
      "    block_type: bottleneck\n",
      "    growth_rate: 12\n",
      "    drop_rate: 0.0\n",
      "    compression_rate: 0.5\n",
      "  pyramidnet:\n",
      "    depth: 272\n",
      "    n_blocks: [3, 24, 36, 3]\n",
      "    initial_channels: 16\n",
      "    block_type: bottleneck\n",
      "    alpha: 200\n",
      "  resnext:\n",
      "    depth: 29\n",
      "    n_blocks: [3, 4, 6, 3]\n",
      "    initial_channels: 64\n",
      "    cardinality: 8\n",
      "    base_channels: 4\n",
      "  shake_shake:\n",
      "    depth: 26\n",
      "    initial_channels: 96\n",
      "    shake_forward: True\n",
      "    shake_backward: True\n",
      "    shake_image: True\n",
      "  se_resnet_preact:\n",
      "    depth: 110\n",
      "    initial_channels: 16\n",
      "    se_reduction: 16\n",
      "    block_type: basic\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "train:\n",
      "  checkpoint: ''\n",
      "  resume: False\n",
      "  precision: O0\n",
      "  batch_size: 128\n",
      "  subdivision: 1\n",
      "  optimizer: sgd\n",
      "  base_lr: 0.1\n",
      "  momentum: 0.9\n",
      "  nesterov: True\n",
      "  weight_decay: 0.0005\n",
      "  no_weight_decay_on_bn: False\n",
      "  gradient_clip: 0\n",
      "  start_epoch: 0\n",
      "  seed: 0\n",
      "  val_first: True\n",
      "  val_period: 1\n",
      "  val_ratio: 0.1\n",
      "  use_test_as_val: False\n",
      "  output_dir: /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_20/exp00\n",
      "  log_period: 100\n",
      "  checkpoint_period: 100\n",
      "  use_tensorboard: True\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: True\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "  distributed: False\n",
      "  dist:\n",
      "    backend: nccl\n",
      "    init_method: env://\n",
      "    world_size: -1\n",
      "    node_rank: -1\n",
      "    local_rank: 0\n",
      "    use_sync_bn: False\n",
      "tensorboard:\n",
      "  train_images: False\n",
      "  val_images: False\n",
      "  model_params: False\n",
      "optim:\n",
      "  adam:\n",
      "    betas: (0.9, 0.999)\n",
      "  lars:\n",
      "    eps: 1e-09\n",
      "    threshold: 0.01\n",
      "  adabound:\n",
      "    betas: (0.9, 0.999)\n",
      "    final_lr: 0.1\n",
      "    gamma: 0.001\n",
      "scheduler:\n",
      "  epochs: 400\n",
      "  warmup:\n",
      "    type: none\n",
      "    epochs: 0\n",
      "    start_factor: 0.001\n",
      "    exponent: 4\n",
      "  type: multistep\n",
      "  milestones: [60, 120, 160]\n",
      "  lr_decay: 0.2\n",
      "  lr_min_factor: 0.001\n",
      "  T0: 10\n",
      "  T_mul: 1.0\n",
      "validation:\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: False\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "augmentation:\n",
      "  use_random_crop: True\n",
      "  use_random_horizontal_flip: True\n",
      "  use_cutout: False\n",
      "  use_random_erasing: False\n",
      "  use_dual_cutout: False\n",
      "  use_mixup: False\n",
      "  use_ricap: False\n",
      "  use_cutmix: False\n",
      "  use_label_smoothing: False\n",
      "  random_crop:\n",
      "    padding: 4\n",
      "    fill: 0\n",
      "    padding_mode: constant\n",
      "  random_horizontal_flip:\n",
      "    prob: 0.5\n",
      "  cutout:\n",
      "    prob: 1.0\n",
      "    mask_size: 16\n",
      "    cut_inside: False\n",
      "    mask_color: 0\n",
      "    dual_cutout_alpha: 0.1\n",
      "  random_erasing:\n",
      "    prob: 0.5\n",
      "    area_ratio_range: [0.02, 0.4]\n",
      "    min_aspect_ratio: 0.3\n",
      "    max_attempt: 20\n",
      "  mixup:\n",
      "    alpha: 1.0\n",
      "  ricap:\n",
      "    beta: 0.3\n",
      "  cutmix:\n",
      "    alpha: 1.0\n",
      "  label_smoothing:\n",
      "    epsilon: 0.1\n",
      "tta:\n",
      "  use_resize: False\n",
      "  use_center_crop: False\n",
      "  resize: 256\n",
      "test:\n",
      "  checkpoint: ''\n",
      "  output_dir: ''\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    pin_memory: False\n",
      "\u001b[32m[2020-06-21 22:06:55] __main__ INFO: \u001b[0menv_info:\n",
      "  pytorch_version: 1.4.0\n",
      "  cuda_version: 10.1\n",
      "  cudnn_version: 7603\n",
      "  num_gpus: 1\n",
      "  gpu_name: Tesla K80\n",
      "  gpu_capability: 3.7\n",
      "(50000, 32, 32, 3)\n",
      "\u001b[32m[2020-06-21 22:07:02] __main__ INFO: \u001b[0mMACs  : 5.25G\n",
      "\u001b[32m[2020-06-21 22:07:02] __main__ INFO: \u001b[0m#params: 36.48M\n",
      "Selected optimization level O0:  Pure FP32 training.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\",)\n",
      "\u001b[32m[2020-06-21 22:07:02] __main__ INFO: \u001b[0mVal 0\n",
      "\u001b[32m[2020-06-21 22:07:22] __main__ INFO: \u001b[0mEpoch 0 loss 294.3030 acc@1 0.1016 acc@5 0.4964\n",
      "\u001b[32m[2020-06-21 22:07:22] __main__ INFO: \u001b[0mElapsed 20.16\n",
      "\u001b[32m[2020-06-21 22:07:22] __main__ INFO: \u001b[0mTrain 1 0\n",
      "\u001b[32m[2020-06-21 22:09:22] __main__ INFO: \u001b[0mEpoch 1 Step 100/351 lr 0.100000 loss 2.3166 (2.4402) acc@1 0.1016 (0.1096) acc@5 0.5156 (0.5185)\n",
      "\u001b[32m[2020-06-21 22:11:17] __main__ INFO: \u001b[0mEpoch 1 Step 200/351 lr 0.100000 loss 2.2196 (2.3602) acc@1 0.1562 (0.1180) acc@5 0.6797 (0.5445)\n",
      "\u001b[32m[2020-06-21 22:13:12] __main__ INFO: \u001b[0mEpoch 1 Step 300/351 lr 0.100000 loss 2.2142 (2.3215) acc@1 0.1406 (0.1262) acc@5 0.6328 (0.5650)\n",
      "\u001b[32m[2020-06-21 22:14:11] __main__ INFO: \u001b[0mEpoch 1 Step 351/351 lr 0.100000 loss 2.2391 (2.3100) acc@1 0.1406 (0.1289) acc@5 0.5391 (0.5700)\n",
      "\u001b[32m[2020-06-21 22:14:11] __main__ INFO: \u001b[0mElapsed 409.26\n",
      "\u001b[32m[2020-06-21 22:14:11] __main__ INFO: \u001b[0mVal 1\n",
      "\u001b[32m[2020-06-21 22:14:24] __main__ INFO: \u001b[0mEpoch 1 loss 2.2334 acc@1 0.1482 acc@5 0.6080\n",
      "\u001b[32m[2020-06-21 22:14:24] __main__ INFO: \u001b[0mElapsed 13.51\n",
      "\u001b[32m[2020-06-21 22:14:24] __main__ INFO: \u001b[0mTrain 2 351\n",
      "\u001b[32m[2020-06-21 22:16:19] __main__ INFO: \u001b[0mEpoch 2 Step 100/351 lr 0.100000 loss 2.2956 (2.2065) acc@1 0.1562 (0.1620) acc@5 0.6094 (0.6292)\n",
      "\u001b[32m[2020-06-21 22:18:14] __main__ INFO: \u001b[0mEpoch 2 Step 200/351 lr 0.100000 loss 2.1119 (2.1960) acc@1 0.1875 (0.1647) acc@5 0.7188 (0.6367)\n",
      "\u001b[32m[2020-06-21 22:20:09] __main__ INFO: \u001b[0mEpoch 2 Step 300/351 lr 0.100000 loss 2.2609 (2.1852) acc@1 0.1172 (0.1721) acc@5 0.5547 (0.6396)\n",
      "\u001b[32m[2020-06-21 22:21:07] __main__ INFO: \u001b[0mEpoch 2 Step 351/351 lr 0.100000 loss 2.1716 (2.1908) acc@1 0.1562 (0.1703) acc@5 0.6719 (0.6349)\n",
      "\u001b[32m[2020-06-21 22:21:07] __main__ INFO: \u001b[0mElapsed 402.95\n",
      "\u001b[32m[2020-06-21 22:21:07] __main__ INFO: \u001b[0mVal 2\n",
      "\u001b[32m[2020-06-21 22:21:21] __main__ INFO: \u001b[0mEpoch 2 loss 2.2406 acc@1 0.1592 acc@5 0.6188\n",
      "\u001b[32m[2020-06-21 22:21:21] __main__ INFO: \u001b[0mElapsed 13.47\n",
      "\u001b[32m[2020-06-21 22:21:21] __main__ INFO: \u001b[0mTrain 3 702\n",
      "\u001b[32m[2020-06-21 22:23:16] __main__ INFO: \u001b[0mEpoch 3 Step 100/351 lr 0.100000 loss 2.2244 (2.1807) acc@1 0.1719 (0.1770) acc@5 0.6016 (0.6438)\n",
      "\u001b[32m[2020-06-21 22:25:11] __main__ INFO: \u001b[0mEpoch 3 Step 200/351 lr 0.100000 loss 2.1675 (2.1680) acc@1 0.1719 (0.1831) acc@5 0.6719 (0.6500)\n",
      "\u001b[32m[2020-06-21 22:27:06] __main__ INFO: \u001b[0mEpoch 3 Step 300/351 lr 0.100000 loss 2.1776 (2.1546) acc@1 0.1562 (0.1868) acc@5 0.6094 (0.6562)\n",
      "\u001b[32m[2020-06-21 22:28:05] __main__ INFO: \u001b[0mEpoch 3 Step 351/351 lr 0.100000 loss 2.1194 (2.1493) acc@1 0.2031 (0.1889) acc@5 0.6953 (0.6591)\n",
      "\u001b[32m[2020-06-21 22:28:05] __main__ INFO: \u001b[0mElapsed 404.20\n",
      "\u001b[32m[2020-06-21 22:28:05] __main__ INFO: \u001b[0mVal 3\n",
      "\u001b[32m[2020-06-21 22:28:19] __main__ INFO: \u001b[0mEpoch 3 loss 2.1728 acc@1 0.1840 acc@5 0.6510\n",
      "\u001b[32m[2020-06-21 22:28:19] __main__ INFO: \u001b[0mElapsed 13.55\n",
      "\u001b[32m[2020-06-21 22:28:19] __main__ INFO: \u001b[0mTrain 4 1053\n",
      "\u001b[32m[2020-06-21 22:30:13] __main__ INFO: \u001b[0mEpoch 4 Step 100/351 lr 0.100000 loss 2.1070 (2.0890) acc@1 0.1875 (0.2152) acc@5 0.6797 (0.6805)\n",
      "\u001b[32m[2020-06-21 22:32:08] __main__ INFO: \u001b[0mEpoch 4 Step 200/351 lr 0.100000 loss 1.9922 (2.0874) acc@1 0.2734 (0.2175) acc@5 0.7109 (0.6833)\n",
      "\u001b[32m[2020-06-21 22:34:03] __main__ INFO: \u001b[0mEpoch 4 Step 300/351 lr 0.100000 loss 2.1272 (2.0789) acc@1 0.2422 (0.2205) acc@5 0.6641 (0.6819)\n",
      "\u001b[32m[2020-06-21 22:35:01] __main__ INFO: \u001b[0mEpoch 4 Step 351/351 lr 0.100000 loss 2.1283 (2.0753) acc@1 0.2266 (0.2221) acc@5 0.6641 (0.6836)\n",
      "\u001b[32m[2020-06-21 22:35:01] __main__ INFO: \u001b[0mElapsed 402.49\n",
      "\u001b[32m[2020-06-21 22:35:01] __main__ INFO: \u001b[0mVal 4\n",
      "\u001b[32m[2020-06-21 22:35:15] __main__ INFO: \u001b[0mEpoch 4 loss 2.0299 acc@1 0.2438 acc@5 0.6942\n",
      "\u001b[32m[2020-06-21 22:35:15] __main__ INFO: \u001b[0mElapsed 13.50\n",
      "\u001b[32m[2020-06-21 22:35:15] __main__ INFO: \u001b[0mTrain 5 1404\n",
      "\u001b[32m[2020-06-21 22:37:10] __main__ INFO: \u001b[0mEpoch 5 Step 100/351 lr 0.100000 loss 2.0565 (2.0298) acc@1 0.2344 (0.2391) acc@5 0.6953 (0.7005)\n",
      "\u001b[32m[2020-06-21 22:39:05] __main__ INFO: \u001b[0mEpoch 5 Step 200/351 lr 0.100000 loss 1.9507 (2.0150) acc@1 0.2422 (0.2468) acc@5 0.7109 (0.7071)\n",
      "\u001b[32m[2020-06-21 22:41:00] __main__ INFO: \u001b[0mEpoch 5 Step 300/351 lr 0.100000 loss 2.0246 (2.0094) acc@1 0.2500 (0.2497) acc@5 0.7266 (0.7093)\n",
      "\u001b[32m[2020-06-21 22:41:59] __main__ INFO: \u001b[0mEpoch 5 Step 351/351 lr 0.100000 loss 2.0425 (2.0046) acc@1 0.2344 (0.2515) acc@5 0.6719 (0.7092)\n",
      "\u001b[32m[2020-06-21 22:41:59] __main__ INFO: \u001b[0mElapsed 404.01\n",
      "\u001b[32m[2020-06-21 22:41:59] __main__ INFO: \u001b[0mVal 5\n",
      "\u001b[32m[2020-06-21 22:42:12] __main__ INFO: \u001b[0mEpoch 5 loss 2.0306 acc@1 0.2488 acc@5 0.7080\n",
      "\u001b[32m[2020-06-21 22:42:12] __main__ INFO: \u001b[0mElapsed 13.52\n",
      "\u001b[32m[2020-06-21 22:42:12] __main__ INFO: \u001b[0mTrain 6 1755\n",
      "\u001b[32m[2020-06-21 22:44:07] __main__ INFO: \u001b[0mEpoch 6 Step 100/351 lr 0.100000 loss 1.9462 (1.9537) acc@1 0.2812 (0.2693) acc@5 0.7734 (0.7180)\n",
      "\u001b[32m[2020-06-21 22:46:02] __main__ INFO: \u001b[0mEpoch 6 Step 200/351 lr 0.100000 loss 2.0164 (1.9499) acc@1 0.2344 (0.2714) acc@5 0.7188 (0.7200)\n",
      "\u001b[32m[2020-06-21 22:47:56] __main__ INFO: \u001b[0mEpoch 6 Step 300/351 lr 0.100000 loss 2.0054 (1.9468) acc@1 0.2188 (0.2730) acc@5 0.7031 (0.7225)\n",
      "\u001b[32m[2020-06-21 22:48:55] __main__ INFO: \u001b[0mEpoch 6 Step 351/351 lr 0.100000 loss 2.0641 (1.9459) acc@1 0.2656 (0.2730) acc@5 0.7109 (0.7227)\n",
      "\u001b[32m[2020-06-21 22:48:55] __main__ INFO: \u001b[0mElapsed 402.51\n",
      "\u001b[32m[2020-06-21 22:48:55] __main__ INFO: \u001b[0mVal 6\n",
      "\u001b[32m[2020-06-21 22:49:08] __main__ INFO: \u001b[0mEpoch 6 loss 2.0265 acc@1 0.2642 acc@5 0.6718\n",
      "\u001b[32m[2020-06-21 22:49:08] __main__ INFO: \u001b[0mElapsed 13.53\n",
      "\u001b[32m[2020-06-21 22:49:08] __main__ INFO: \u001b[0mTrain 7 2106\n",
      "\u001b[32m[2020-06-21 22:51:03] __main__ INFO: \u001b[0mEpoch 7 Step 100/351 lr 0.100000 loss 1.8570 (1.9027) acc@1 0.2656 (0.2931) acc@5 0.7344 (0.7266)\n",
      "\u001b[32m[2020-06-21 22:52:58] __main__ INFO: \u001b[0mEpoch 7 Step 200/351 lr 0.100000 loss 1.9337 (1.8985) acc@1 0.2422 (0.2966) acc@5 0.7266 (0.7275)\n",
      "\u001b[32m[2020-06-21 22:54:53] __main__ INFO: \u001b[0mEpoch 7 Step 300/351 lr 0.100000 loss 1.8409 (1.8892) acc@1 0.3672 (0.2989) acc@5 0.7344 (0.7322)\n",
      "\u001b[32m[2020-06-21 22:55:52] __main__ INFO: \u001b[0mEpoch 7 Step 351/351 lr 0.100000 loss 1.7655 (1.8852) acc@1 0.3594 (0.3004) acc@5 0.7891 (0.7337)\n",
      "\u001b[32m[2020-06-21 22:55:52] __main__ INFO: \u001b[0mElapsed 403.71\n",
      "\u001b[32m[2020-06-21 22:55:52] __main__ INFO: \u001b[0mVal 7\n",
      "\u001b[32m[2020-06-21 22:56:05] __main__ INFO: \u001b[0mEpoch 7 loss 2.0021 acc@1 0.2678 acc@5 0.7092\n",
      "\u001b[32m[2020-06-21 22:56:05] __main__ INFO: \u001b[0mElapsed 13.50\n",
      "\u001b[32m[2020-06-21 22:56:05] __main__ INFO: \u001b[0mTrain 8 2457\n",
      "\u001b[32m[2020-06-21 22:58:00] __main__ INFO: \u001b[0mEpoch 8 Step 100/351 lr 0.100000 loss 1.7732 (1.8450) acc@1 0.3516 (0.3148) acc@5 0.7891 (0.7409)\n",
      "\u001b[32m[2020-06-21 22:59:55] __main__ INFO: \u001b[0mEpoch 8 Step 200/351 lr 0.100000 loss 1.8387 (1.8407) acc@1 0.2578 (0.3145) acc@5 0.7422 (0.7423)\n",
      "\u001b[32m[2020-06-21 23:01:49] __main__ INFO: \u001b[0mEpoch 8 Step 300/351 lr 0.100000 loss 1.7707 (1.8387) acc@1 0.3516 (0.3145) acc@5 0.7969 (0.7419)\n",
      "\u001b[32m[2020-06-21 23:02:48] __main__ INFO: \u001b[0mEpoch 8 Step 351/351 lr 0.100000 loss 1.9420 (1.8395) acc@1 0.2734 (0.3144) acc@5 0.7109 (0.7427)\n",
      "\u001b[32m[2020-06-21 23:02:48] __main__ INFO: \u001b[0mElapsed 402.11\n",
      "\u001b[32m[2020-06-21 23:02:48] __main__ INFO: \u001b[0mVal 8\n",
      "\u001b[32m[2020-06-21 23:03:01] __main__ INFO: \u001b[0mEpoch 8 loss 2.1184 acc@1 0.2586 acc@5 0.6684\n",
      "\u001b[32m[2020-06-21 23:03:01] __main__ INFO: \u001b[0mElapsed 13.49\n",
      "\u001b[32m[2020-06-21 23:03:01] __main__ INFO: \u001b[0mTrain 9 2808\n",
      "\u001b[32m[2020-06-21 23:04:56] __main__ INFO: \u001b[0mEpoch 9 Step 100/351 lr 0.100000 loss 1.7932 (1.8068) acc@1 0.3516 (0.3273) acc@5 0.7500 (0.7517)\n",
      "\u001b[32m[2020-06-21 23:06:51] __main__ INFO: \u001b[0mEpoch 9 Step 200/351 lr 0.100000 loss 1.7379 (1.8029) acc@1 0.3438 (0.3289) acc@5 0.7109 (0.7507)\n",
      "\u001b[32m[2020-06-21 23:08:46] __main__ INFO: \u001b[0mEpoch 9 Step 300/351 lr 0.100000 loss 1.7890 (1.8003) acc@1 0.3828 (0.3318) acc@5 0.7891 (0.7510)\n",
      "\u001b[32m[2020-06-21 23:09:44] __main__ INFO: \u001b[0mEpoch 9 Step 351/351 lr 0.100000 loss 1.9059 (1.8012) acc@1 0.2578 (0.3320) acc@5 0.7031 (0.7518)\n",
      "\u001b[32m[2020-06-21 23:09:44] __main__ INFO: \u001b[0mElapsed 403.46\n",
      "\u001b[32m[2020-06-21 23:09:44] __main__ INFO: \u001b[0mVal 9\n",
      "\u001b[32m[2020-06-21 23:09:58] __main__ INFO: \u001b[0mEpoch 9 loss 1.7996 acc@1 0.3328 acc@5 0.7498\n",
      "\u001b[32m[2020-06-21 23:09:58] __main__ INFO: \u001b[0mElapsed 13.50\n",
      "\u001b[32m[2020-06-21 23:09:58] __main__ INFO: \u001b[0mTrain 10 3159\n",
      "\u001b[32m[2020-06-21 23:11:53] __main__ INFO: \u001b[0mEpoch 10 Step 100/351 lr 0.100000 loss 1.8842 (1.7722) acc@1 0.2812 (0.3389) acc@5 0.7422 (0.7538)\n",
      "\u001b[32m[2020-06-21 23:13:47] __main__ INFO: \u001b[0mEpoch 10 Step 200/351 lr 0.100000 loss 1.8368 (1.7709) acc@1 0.3281 (0.3402) acc@5 0.7031 (0.7565)\n",
      "\u001b[32m[2020-06-21 23:15:42] __main__ INFO: \u001b[0mEpoch 10 Step 300/351 lr 0.100000 loss 1.6257 (1.7689) acc@1 0.4062 (0.3409) acc@5 0.7734 (0.7547)\n",
      "\u001b[32m[2020-06-21 23:16:40] __main__ INFO: \u001b[0mEpoch 10 Step 351/351 lr 0.100000 loss 1.7068 (1.7670) acc@1 0.3516 (0.3414) acc@5 0.7109 (0.7551)\n",
      "\u001b[32m[2020-06-21 23:16:40] __main__ INFO: \u001b[0mElapsed 402.18\n",
      "\u001b[32m[2020-06-21 23:16:40] __main__ INFO: \u001b[0mVal 10\n",
      "\u001b[32m[2020-06-21 23:16:54] __main__ INFO: \u001b[0mEpoch 10 loss 1.8468 acc@1 0.3106 acc@5 0.7454\n",
      "\u001b[32m[2020-06-21 23:16:54] __main__ INFO: \u001b[0mElapsed 13.51\n",
      "\u001b[32m[2020-06-21 23:16:54] __main__ INFO: \u001b[0mTrain 11 3510\n",
      "\u001b[32m[2020-06-21 23:18:49] __main__ INFO: \u001b[0mEpoch 11 Step 100/351 lr 0.100000 loss 1.7533 (1.7360) acc@1 0.3359 (0.3537) acc@5 0.7188 (0.7571)\n",
      "\u001b[32m[2020-06-21 23:20:43] __main__ INFO: \u001b[0mEpoch 11 Step 200/351 lr 0.100000 loss 1.7224 (1.7530) acc@1 0.3438 (0.3479) acc@5 0.7344 (0.7541)\n",
      "\u001b[32m[2020-06-21 23:22:38] __main__ INFO: \u001b[0mEpoch 11 Step 300/351 lr 0.100000 loss 1.7947 (1.7480) acc@1 0.3359 (0.3512) acc@5 0.7422 (0.7577)\n",
      "\u001b[32m[2020-06-21 23:23:37] __main__ INFO: \u001b[0mEpoch 11 Step 351/351 lr 0.100000 loss 1.6342 (1.7484) acc@1 0.3750 (0.3501) acc@5 0.7734 (0.7577)\n",
      "\u001b[32m[2020-06-21 23:23:37] __main__ INFO: \u001b[0mElapsed 403.25\n",
      "\u001b[32m[2020-06-21 23:23:37] __main__ INFO: \u001b[0mVal 11\n",
      "\u001b[32m[2020-06-21 23:23:50] __main__ INFO: \u001b[0mEpoch 11 loss 1.7446 acc@1 0.3528 acc@5 0.7726\n",
      "\u001b[32m[2020-06-21 23:23:50] __main__ INFO: \u001b[0mElapsed 13.47\n",
      "\u001b[32m[2020-06-21 23:23:50] __main__ INFO: \u001b[0mTrain 12 3861\n",
      "\u001b[32m[2020-06-21 23:25:45] __main__ INFO: \u001b[0mEpoch 12 Step 100/351 lr 0.100000 loss 1.8297 (1.7076) acc@1 0.3359 (0.3680) acc@5 0.7812 (0.7662)\n",
      "\u001b[32m[2020-06-21 23:27:39] __main__ INFO: \u001b[0mEpoch 12 Step 200/351 lr 0.100000 loss 1.7345 (1.7211) acc@1 0.3516 (0.3629) acc@5 0.8125 (0.7632)\n",
      "\u001b[32m[2020-06-21 23:29:34] __main__ INFO: \u001b[0mEpoch 12 Step 300/351 lr 0.100000 loss 1.6244 (1.7208) acc@1 0.4141 (0.3628) acc@5 0.8281 (0.7611)\n",
      "\u001b[32m[2020-06-21 23:30:32] __main__ INFO: \u001b[0mEpoch 12 Step 351/351 lr 0.100000 loss 1.5801 (1.7193) acc@1 0.3906 (0.3633) acc@5 0.7969 (0.7620)\n",
      "\u001b[32m[2020-06-21 23:30:32] __main__ INFO: \u001b[0mElapsed 401.90\n",
      "\u001b[32m[2020-06-21 23:30:32] __main__ INFO: \u001b[0mVal 12\n",
      "\u001b[32m[2020-06-21 23:30:46] __main__ INFO: \u001b[0mEpoch 12 loss 1.8649 acc@1 0.3168 acc@5 0.7508\n",
      "\u001b[32m[2020-06-21 23:30:46] __main__ INFO: \u001b[0mElapsed 13.52\n",
      "\u001b[32m[2020-06-21 23:30:46] __main__ INFO: \u001b[0mTrain 13 4212\n",
      "\u001b[32m[2020-06-21 23:32:41] __main__ INFO: \u001b[0mEpoch 13 Step 100/351 lr 0.100000 loss 1.9440 (1.7120) acc@1 0.3281 (0.3679) acc@5 0.6875 (0.7609)\n",
      "\u001b[32m[2020-06-21 23:34:36] __main__ INFO: \u001b[0mEpoch 13 Step 200/351 lr 0.100000 loss 1.7562 (1.7125) acc@1 0.3359 (0.3656) acc@5 0.7500 (0.7622)\n",
      "\u001b[32m[2020-06-21 23:36:30] __main__ INFO: \u001b[0mEpoch 13 Step 300/351 lr 0.100000 loss 1.5966 (1.7070) acc@1 0.4375 (0.3672) acc@5 0.7578 (0.7648)\n",
      "\u001b[32m[2020-06-21 23:37:29] __main__ INFO: \u001b[0mEpoch 13 Step 351/351 lr 0.100000 loss 1.7137 (1.7048) acc@1 0.3359 (0.3686) acc@5 0.8203 (0.7651)\n",
      "\u001b[32m[2020-06-21 23:37:29] __main__ INFO: \u001b[0mElapsed 403.03\n",
      "\u001b[32m[2020-06-21 23:37:29] __main__ INFO: \u001b[0mVal 13\n",
      "\u001b[32m[2020-06-21 23:37:42] __main__ INFO: \u001b[0mEpoch 13 loss 1.7342 acc@1 0.3646 acc@5 0.7680\n",
      "\u001b[32m[2020-06-21 23:37:42] __main__ INFO: \u001b[0mElapsed 13.48\n",
      "\u001b[32m[2020-06-21 23:37:42] __main__ INFO: \u001b[0mTrain 14 4563\n",
      "\u001b[32m[2020-06-21 23:39:37] __main__ INFO: \u001b[0mEpoch 14 Step 100/351 lr 0.100000 loss 1.5788 (1.6863) acc@1 0.3906 (0.3762) acc@5 0.7188 (0.7678)\n",
      "\u001b[32m[2020-06-21 23:41:31] __main__ INFO: \u001b[0mEpoch 14 Step 200/351 lr 0.100000 loss 1.6613 (1.6814) acc@1 0.4062 (0.3773) acc@5 0.7656 (0.7667)\n",
      "\u001b[32m[2020-06-21 23:43:26] __main__ INFO: \u001b[0mEpoch 14 Step 300/351 lr 0.100000 loss 1.6829 (1.6786) acc@1 0.3984 (0.3780) acc@5 0.7578 (0.7688)\n",
      "\u001b[32m[2020-06-21 23:44:24] __main__ INFO: \u001b[0mEpoch 14 Step 351/351 lr 0.100000 loss 1.9734 (1.6856) acc@1 0.2578 (0.3756) acc@5 0.6875 (0.7674)\n",
      "\u001b[32m[2020-06-21 23:44:24] __main__ INFO: \u001b[0mElapsed 401.53\n",
      "\u001b[32m[2020-06-21 23:44:24] __main__ INFO: \u001b[0mVal 14\n",
      "\u001b[32m[2020-06-21 23:44:37] __main__ INFO: \u001b[0mEpoch 14 loss 1.9796 acc@1 0.3106 acc@5 0.7444\n",
      "\u001b[32m[2020-06-21 23:44:37] __main__ INFO: \u001b[0mElapsed 13.46\n",
      "\u001b[32m[2020-06-21 23:44:37] __main__ INFO: \u001b[0mTrain 15 4914\n",
      "\u001b[32m[2020-06-21 23:46:32] __main__ INFO: \u001b[0mEpoch 15 Step 100/351 lr 0.100000 loss 1.6479 (1.6611) acc@1 0.3984 (0.3875) acc@5 0.7344 (0.7719)\n",
      "\u001b[32m[2020-06-21 23:48:27] __main__ INFO: \u001b[0mEpoch 15 Step 200/351 lr 0.100000 loss 1.6327 (1.6706) acc@1 0.4062 (0.3826) acc@5 0.7734 (0.7684)\n",
      "\u001b[32m[2020-06-21 23:50:22] __main__ INFO: \u001b[0mEpoch 15 Step 300/351 lr 0.100000 loss 1.7257 (1.6692) acc@1 0.3828 (0.3832) acc@5 0.7109 (0.7696)\n",
      "\u001b[32m[2020-06-21 23:51:20] __main__ INFO: \u001b[0mEpoch 15 Step 351/351 lr 0.100000 loss 1.6405 (1.6719) acc@1 0.3984 (0.3819) acc@5 0.7969 (0.7696)\n",
      "\u001b[32m[2020-06-21 23:51:20] __main__ INFO: \u001b[0mElapsed 402.79\n",
      "\u001b[32m[2020-06-21 23:51:20] __main__ INFO: \u001b[0mVal 15\n",
      "\u001b[32m[2020-06-21 23:51:34] __main__ INFO: \u001b[0mEpoch 15 loss 1.7304 acc@1 0.3814 acc@5 0.7592\n",
      "\u001b[32m[2020-06-21 23:51:34] __main__ INFO: \u001b[0mElapsed 13.45\n",
      "\u001b[32m[2020-06-21 23:51:34] __main__ INFO: \u001b[0mTrain 16 5265\n",
      "\u001b[32m[2020-06-21 23:53:28] __main__ INFO: \u001b[0mEpoch 16 Step 100/351 lr 0.100000 loss 1.7402 (1.6530) acc@1 0.3750 (0.3909) acc@5 0.7031 (0.7698)\n",
      "\u001b[32m[2020-06-21 23:55:22] __main__ INFO: \u001b[0mEpoch 16 Step 200/351 lr 0.100000 loss 1.5514 (1.6544) acc@1 0.4609 (0.3887) acc@5 0.7578 (0.7711)\n",
      "\u001b[32m[2020-06-21 23:57:17] __main__ INFO: \u001b[0mEpoch 16 Step 300/351 lr 0.100000 loss 1.6121 (1.6572) acc@1 0.3984 (0.3886) acc@5 0.7422 (0.7697)\n",
      "\u001b[32m[2020-06-21 23:58:15] __main__ INFO: \u001b[0mEpoch 16 Step 351/351 lr 0.100000 loss 1.7598 (1.6572) acc@1 0.3984 (0.3885) acc@5 0.7656 (0.7700)\n",
      "\u001b[32m[2020-06-21 23:58:15] __main__ INFO: \u001b[0mElapsed 401.39\n",
      "\u001b[32m[2020-06-21 23:58:15] __main__ INFO: \u001b[0mVal 16\n",
      "\u001b[32m[2020-06-21 23:58:28] __main__ INFO: \u001b[0mEpoch 16 loss 1.8177 acc@1 0.3530 acc@5 0.7594\n",
      "\u001b[32m[2020-06-21 23:58:28] __main__ INFO: \u001b[0mElapsed 13.46\n",
      "\u001b[32m[2020-06-21 23:58:28] __main__ INFO: \u001b[0mTrain 17 5616\n",
      "\u001b[32m[2020-06-22 00:00:23] __main__ INFO: \u001b[0mEpoch 17 Step 100/351 lr 0.100000 loss 1.7396 (1.6338) acc@1 0.3359 (0.3997) acc@5 0.7812 (0.7710)\n",
      "\u001b[32m[2020-06-22 00:02:18] __main__ INFO: \u001b[0mEpoch 17 Step 200/351 lr 0.100000 loss 1.7141 (1.6338) acc@1 0.3516 (0.3970) acc@5 0.7734 (0.7745)\n",
      "\u001b[32m[2020-06-22 00:04:12] __main__ INFO: \u001b[0mEpoch 17 Step 300/351 lr 0.100000 loss 1.6571 (1.6402) acc@1 0.4062 (0.3950) acc@5 0.7969 (0.7738)\n",
      "\u001b[32m[2020-06-22 00:05:11] __main__ INFO: \u001b[0mEpoch 17 Step 351/351 lr 0.100000 loss 1.6538 (1.6410) acc@1 0.4141 (0.3947) acc@5 0.7500 (0.7740)\n",
      "\u001b[32m[2020-06-22 00:05:11] __main__ INFO: \u001b[0mElapsed 402.53\n",
      "\u001b[32m[2020-06-22 00:05:11] __main__ INFO: \u001b[0mVal 17\n",
      "\u001b[32m[2020-06-22 00:05:24] __main__ INFO: \u001b[0mEpoch 17 loss 1.9433 acc@1 0.3314 acc@5 0.7402\n",
      "\u001b[32m[2020-06-22 00:05:24] __main__ INFO: \u001b[0mElapsed 13.44\n",
      "\u001b[32m[2020-06-22 00:05:24] __main__ INFO: \u001b[0mTrain 18 5967\n",
      "\u001b[32m[2020-06-22 00:07:19] __main__ INFO: \u001b[0mEpoch 18 Step 100/351 lr 0.100000 loss 1.7636 (1.6341) acc@1 0.3359 (0.4003) acc@5 0.7422 (0.7766)\n",
      "\u001b[32m[2020-06-22 00:09:13] __main__ INFO: \u001b[0mEpoch 18 Step 200/351 lr 0.100000 loss 1.5826 (1.6338) acc@1 0.4219 (0.3968) acc@5 0.7344 (0.7754)\n",
      "\u001b[32m[2020-06-22 00:11:07] __main__ INFO: \u001b[0mEpoch 18 Step 300/351 lr 0.100000 loss 1.6773 (1.6324) acc@1 0.3438 (0.3969) acc@5 0.7656 (0.7760)\n",
      "\u001b[32m[2020-06-22 00:12:05] __main__ INFO: \u001b[0mEpoch 18 Step 351/351 lr 0.100000 loss 1.6251 (1.6326) acc@1 0.4297 (0.3964) acc@5 0.7500 (0.7761)\n",
      "\u001b[32m[2020-06-22 00:12:05] __main__ INFO: \u001b[0mElapsed 401.08\n",
      "\u001b[32m[2020-06-22 00:12:05] __main__ INFO: \u001b[0mVal 18\n",
      "\u001b[32m[2020-06-22 00:12:19] __main__ INFO: \u001b[0mEpoch 18 loss 1.7446 acc@1 0.3784 acc@5 0.7622\n",
      "\u001b[32m[2020-06-22 00:12:19] __main__ INFO: \u001b[0mElapsed 13.45\n",
      "\u001b[32m[2020-06-22 00:12:19] __main__ INFO: \u001b[0mTrain 19 6318\n",
      "\u001b[32m[2020-06-22 00:14:14] __main__ INFO: \u001b[0mEpoch 19 Step 100/351 lr 0.100000 loss 1.5487 (1.6111) acc@1 0.4375 (0.4030) acc@5 0.7500 (0.7805)\n",
      "\u001b[32m[2020-06-22 00:16:08] __main__ INFO: \u001b[0mEpoch 19 Step 200/351 lr 0.100000 loss 1.6476 (1.6166) acc@1 0.3594 (0.3995) acc@5 0.7734 (0.7782)\n",
      "\u001b[32m[2020-06-22 00:18:03] __main__ INFO: \u001b[0mEpoch 19 Step 300/351 lr 0.100000 loss 1.6692 (1.6190) acc@1 0.4141 (0.3977) acc@5 0.7812 (0.7771)\n",
      "\u001b[32m[2020-06-22 00:19:01] __main__ INFO: \u001b[0mEpoch 19 Step 351/351 lr 0.100000 loss 1.6853 (1.6210) acc@1 0.4844 (0.3990) acc@5 0.8047 (0.7770)\n",
      "\u001b[32m[2020-06-22 00:19:01] __main__ INFO: \u001b[0mElapsed 402.18\n",
      "\u001b[32m[2020-06-22 00:19:01] __main__ INFO: \u001b[0mVal 19\n",
      "\u001b[32m[2020-06-22 00:19:15] __main__ INFO: \u001b[0mEpoch 19 loss 1.7448 acc@1 0.3618 acc@5 0.7682\n",
      "\u001b[32m[2020-06-22 00:19:15] __main__ INFO: \u001b[0mElapsed 13.44\n",
      "\u001b[32m[2020-06-22 00:19:15] __main__ INFO: \u001b[0mTrain 20 6669\n",
      "\u001b[32m[2020-06-22 00:21:09] __main__ INFO: \u001b[0mEpoch 20 Step 100/351 lr 0.100000 loss 1.4931 (1.6041) acc@1 0.4844 (0.4128) acc@5 0.7734 (0.7782)\n",
      "\u001b[32m[2020-06-22 00:23:03] __main__ INFO: \u001b[0mEpoch 20 Step 200/351 lr 0.100000 loss 1.4771 (1.6094) acc@1 0.4609 (0.4062) acc@5 0.7969 (0.7784)\n",
      "\u001b[32m[2020-06-22 00:24:57] __main__ INFO: \u001b[0mEpoch 20 Step 300/351 lr 0.100000 loss 1.5131 (1.6114) acc@1 0.5000 (0.4044) acc@5 0.8594 (0.7790)\n",
      "\u001b[32m[2020-06-22 00:25:55] __main__ INFO: \u001b[0mEpoch 20 Step 351/351 lr 0.100000 loss 1.6772 (1.6113) acc@1 0.3906 (0.4042) acc@5 0.7344 (0.7791)\n",
      "\u001b[32m[2020-06-22 00:25:55] __main__ INFO: \u001b[0mElapsed 400.81\n",
      "\u001b[32m[2020-06-22 00:25:55] __main__ INFO: \u001b[0mVal 20\n",
      "\u001b[32m[2020-06-22 00:26:09] __main__ INFO: \u001b[0mEpoch 20 loss 1.7851 acc@1 0.3650 acc@5 0.7500\n",
      "\u001b[32m[2020-06-22 00:26:09] __main__ INFO: \u001b[0mElapsed 13.44\n",
      "\u001b[32m[2020-06-22 00:26:09] __main__ INFO: \u001b[0mTrain 21 7020\n",
      "\u001b[32m[2020-06-22 00:28:03] __main__ INFO: \u001b[0mEpoch 21 Step 100/351 lr 0.100000 loss 1.4909 (1.5743) acc@1 0.5000 (0.4183) acc@5 0.7891 (0.7848)\n",
      "\u001b[32m[2020-06-22 00:29:58] __main__ INFO: \u001b[0mEpoch 21 Step 200/351 lr 0.100000 loss 1.7828 (1.5977) acc@1 0.2969 (0.4080) acc@5 0.6953 (0.7791)\n",
      "\u001b[32m[2020-06-22 00:31:53] __main__ INFO: \u001b[0mEpoch 21 Step 300/351 lr 0.100000 loss 1.4086 (1.6024) acc@1 0.4531 (0.4066) acc@5 0.8125 (0.7788)\n",
      "\u001b[32m[2020-06-22 00:32:51] __main__ INFO: \u001b[0mEpoch 21 Step 351/351 lr 0.100000 loss 1.6699 (1.6028) acc@1 0.3359 (0.4052) acc@5 0.8203 (0.7787)\n",
      "\u001b[32m[2020-06-22 00:32:51] __main__ INFO: \u001b[0mElapsed 402.23\n",
      "\u001b[32m[2020-06-22 00:32:51] __main__ INFO: \u001b[0mVal 21\n",
      "\u001b[32m[2020-06-22 00:33:04] __main__ INFO: \u001b[0mEpoch 21 loss 1.7068 acc@1 0.3862 acc@5 0.7720\n",
      "\u001b[32m[2020-06-22 00:33:04] __main__ INFO: \u001b[0mElapsed 13.44\n",
      "\u001b[32m[2020-06-22 00:33:04] __main__ INFO: \u001b[0mTrain 22 7371\n",
      "\u001b[32m[2020-06-22 00:34:59] __main__ INFO: \u001b[0mEpoch 22 Step 100/351 lr 0.100000 loss 1.6827 (1.5953) acc@1 0.3984 (0.4110) acc@5 0.7734 (0.7811)\n",
      "\u001b[32m[2020-06-22 00:36:53] __main__ INFO: \u001b[0mEpoch 22 Step 200/351 lr 0.100000 loss 1.5279 (1.5923) acc@1 0.4062 (0.4114) acc@5 0.8203 (0.7796)\n",
      "\u001b[32m[2020-06-22 00:38:47] __main__ INFO: \u001b[0mEpoch 22 Step 300/351 lr 0.100000 loss 1.7740 (1.5918) acc@1 0.3594 (0.4125) acc@5 0.7812 (0.7805)\n",
      "\u001b[32m[2020-06-22 00:39:45] __main__ INFO: \u001b[0mEpoch 22 Step 351/351 lr 0.100000 loss 1.4738 (1.5920) acc@1 0.4609 (0.4130) acc@5 0.8984 (0.7802)\n",
      "\u001b[32m[2020-06-22 00:39:45] __main__ INFO: \u001b[0mElapsed 400.97\n",
      "\u001b[32m[2020-06-22 00:39:45] __main__ INFO: \u001b[0mVal 22\n",
      "\u001b[32m[2020-06-22 00:39:59] __main__ INFO: \u001b[0mEpoch 22 loss 1.7360 acc@1 0.3724 acc@5 0.7666\n",
      "\u001b[32m[2020-06-22 00:39:59] __main__ INFO: \u001b[0mElapsed 13.45\n",
      "\u001b[32m[2020-06-22 00:39:59] __main__ INFO: \u001b[0mTrain 23 7722\n",
      "\u001b[32m[2020-06-22 00:41:54] __main__ INFO: \u001b[0mEpoch 23 Step 100/351 lr 0.100000 loss 1.5956 (1.5827) acc@1 0.4297 (0.4167) acc@5 0.7578 (0.7799)\n",
      "\u001b[32m[2020-06-22 00:43:48] __main__ INFO: \u001b[0mEpoch 23 Step 200/351 lr 0.100000 loss 1.6023 (1.5844) acc@1 0.4375 (0.4135) acc@5 0.7031 (0.7793)\n",
      "\u001b[32m[2020-06-22 00:45:43] __main__ INFO: \u001b[0mEpoch 23 Step 300/351 lr 0.100000 loss 1.5966 (1.5880) acc@1 0.4141 (0.4129) acc@5 0.7812 (0.7814)\n",
      "\u001b[32m[2020-06-22 00:46:41] __main__ INFO: \u001b[0mEpoch 23 Step 351/351 lr 0.100000 loss 1.6736 (1.5864) acc@1 0.3672 (0.4146) acc@5 0.7109 (0.7821)\n",
      "\u001b[32m[2020-06-22 00:46:41] __main__ INFO: \u001b[0mElapsed 402.20\n",
      "\u001b[32m[2020-06-22 00:46:41] __main__ INFO: \u001b[0mVal 23\n",
      "\u001b[32m[2020-06-22 00:46:55] __main__ INFO: \u001b[0mEpoch 23 loss 1.7072 acc@1 0.3836 acc@5 0.7706\n",
      "\u001b[32m[2020-06-22 00:46:55] __main__ INFO: \u001b[0mElapsed 13.46\n",
      "\u001b[32m[2020-06-22 00:46:55] __main__ INFO: \u001b[0mTrain 24 8073\n",
      "\u001b[32m[2020-06-22 00:48:49] __main__ INFO: \u001b[0mEpoch 24 Step 100/351 lr 0.100000 loss 1.5954 (1.5772) acc@1 0.3828 (0.4166) acc@5 0.7656 (0.7827)\n",
      "\u001b[32m[2020-06-22 00:50:43] __main__ INFO: \u001b[0mEpoch 24 Step 200/351 lr 0.100000 loss 1.4943 (1.5791) acc@1 0.4375 (0.4150) acc@5 0.7578 (0.7845)\n",
      "\u001b[32m[2020-06-22 00:52:37] __main__ INFO: \u001b[0mEpoch 24 Step 300/351 lr 0.100000 loss 1.4748 (1.5829) acc@1 0.4766 (0.4134) acc@5 0.7891 (0.7819)\n",
      "\u001b[32m[2020-06-22 00:53:35] __main__ INFO: \u001b[0mEpoch 24 Step 351/351 lr 0.100000 loss 1.6894 (1.5838) acc@1 0.3438 (0.4136) acc@5 0.7266 (0.7823)\n",
      "\u001b[32m[2020-06-22 00:53:35] __main__ INFO: \u001b[0mElapsed 400.78\n",
      "\u001b[32m[2020-06-22 00:53:35] __main__ INFO: \u001b[0mVal 24\n",
      "\u001b[32m[2020-06-22 00:53:49] __main__ INFO: \u001b[0mEpoch 24 loss 1.7993 acc@1 0.3604 acc@5 0.7488\n",
      "\u001b[32m[2020-06-22 00:53:49] __main__ INFO: \u001b[0mElapsed 13.46\n",
      "\u001b[32m[2020-06-22 00:53:49] __main__ INFO: \u001b[0mTrain 25 8424\n",
      "\u001b[32m[2020-06-22 00:55:43] __main__ INFO: \u001b[0mEpoch 25 Step 100/351 lr 0.100000 loss 1.5136 (1.5579) acc@1 0.5078 (0.4235) acc@5 0.8203 (0.7814)\n",
      "\u001b[32m[2020-06-22 00:57:38] __main__ INFO: \u001b[0mEpoch 25 Step 200/351 lr 0.100000 loss 1.4834 (1.5618) acc@1 0.4453 (0.4235) acc@5 0.7812 (0.7821)\n",
      "\u001b[32m[2020-06-22 00:59:33] __main__ INFO: \u001b[0mEpoch 25 Step 300/351 lr 0.100000 loss 1.6412 (1.5703) acc@1 0.4062 (0.4203) acc@5 0.7812 (0.7806)\n",
      "\u001b[32m[2020-06-22 01:00:31] __main__ INFO: \u001b[0mEpoch 25 Step 351/351 lr 0.100000 loss 1.5879 (1.5728) acc@1 0.4297 (0.4184) acc@5 0.7812 (0.7804)\n",
      "\u001b[32m[2020-06-22 01:00:31] __main__ INFO: \u001b[0mElapsed 402.22\n",
      "\u001b[32m[2020-06-22 01:00:31] __main__ INFO: \u001b[0mVal 25\n",
      "\u001b[32m[2020-06-22 01:00:44] __main__ INFO: \u001b[0mEpoch 25 loss 1.7926 acc@1 0.3758 acc@5 0.7618\n",
      "\u001b[32m[2020-06-22 01:00:44] __main__ INFO: \u001b[0mElapsed 13.45\n",
      "\u001b[32m[2020-06-22 01:00:44] __main__ INFO: \u001b[0mTrain 26 8775\n",
      "\u001b[32m[2020-06-22 01:02:39] __main__ INFO: \u001b[0mEpoch 26 Step 100/351 lr 0.100000 loss 1.3972 (1.5633) acc@1 0.4766 (0.4196) acc@5 0.8047 (0.7846)\n",
      "\u001b[32m[2020-06-22 01:04:33] __main__ INFO: \u001b[0mEpoch 26 Step 200/351 lr 0.100000 loss 1.5866 (1.5576) acc@1 0.4141 (0.4207) acc@5 0.7500 (0.7862)\n",
      "\u001b[32m[2020-06-22 01:06:27] __main__ INFO: \u001b[0mEpoch 26 Step 300/351 lr 0.100000 loss 1.6881 (1.5632) acc@1 0.4375 (0.4211) acc@5 0.8203 (0.7854)\n",
      "\u001b[32m[2020-06-22 01:07:26] __main__ INFO: \u001b[0mEpoch 26 Step 351/351 lr 0.100000 loss 1.5583 (1.5671) acc@1 0.4141 (0.4204) acc@5 0.8281 (0.7836)\n",
      "\u001b[32m[2020-06-22 01:07:26] __main__ INFO: \u001b[0mElapsed 401.25\n",
      "\u001b[32m[2020-06-22 01:07:26] __main__ INFO: \u001b[0mVal 26\n",
      "\u001b[32m[2020-06-22 01:07:39] __main__ INFO: \u001b[0mEpoch 26 loss 1.6264 acc@1 0.3966 acc@5 0.7716\n",
      "\u001b[32m[2020-06-22 01:07:39] __main__ INFO: \u001b[0mElapsed 13.46\n",
      "\u001b[32m[2020-06-22 01:07:39] __main__ INFO: \u001b[0mTrain 27 9126\n",
      "\u001b[32m[2020-06-22 01:09:34] __main__ INFO: \u001b[0mEpoch 27 Step 100/351 lr 0.100000 loss 1.5409 (1.5472) acc@1 0.4141 (0.4266) acc@5 0.7656 (0.7856)\n",
      "\u001b[32m[2020-06-22 01:11:28] __main__ INFO: \u001b[0mEpoch 27 Step 200/351 lr 0.100000 loss 1.6144 (1.5578) acc@1 0.3828 (0.4224) acc@5 0.7656 (0.7817)\n",
      "\u001b[32m[2020-06-22 01:13:23] __main__ INFO: \u001b[0mEpoch 27 Step 300/351 lr 0.100000 loss 1.5934 (1.5629) acc@1 0.3594 (0.4214) acc@5 0.7500 (0.7808)\n",
      "\u001b[32m[2020-06-22 01:14:21] __main__ INFO: \u001b[0mEpoch 27 Step 351/351 lr 0.100000 loss 1.4557 (1.5613) acc@1 0.5000 (0.4220) acc@5 0.7422 (0.7819)\n",
      "\u001b[32m[2020-06-22 01:14:21] __main__ INFO: \u001b[0mElapsed 402.27\n",
      "\u001b[32m[2020-06-22 01:14:21] __main__ INFO: \u001b[0mVal 27\n",
      "\u001b[32m[2020-06-22 01:14:35] __main__ INFO: \u001b[0mEpoch 27 loss 1.6253 acc@1 0.3936 acc@5 0.7712\n",
      "\u001b[32m[2020-06-22 01:14:35] __main__ INFO: \u001b[0mElapsed 13.44\n",
      "\u001b[32m[2020-06-22 01:14:35] __main__ INFO: \u001b[0mTrain 28 9477\n",
      "\u001b[32m[2020-06-22 01:16:29] __main__ INFO: \u001b[0mEpoch 28 Step 100/351 lr 0.100000 loss 1.6394 (1.5402) acc@1 0.3438 (0.4284) acc@5 0.7500 (0.7855)\n",
      "\u001b[32m[2020-06-22 01:18:23] __main__ INFO: \u001b[0mEpoch 28 Step 200/351 lr 0.100000 loss 1.5432 (1.5514) acc@1 0.4219 (0.4244) acc@5 0.7812 (0.7832)\n",
      "\u001b[32m[2020-06-22 01:20:18] __main__ INFO: \u001b[0mEpoch 28 Step 300/351 lr 0.100000 loss 1.6565 (1.5515) acc@1 0.3516 (0.4238) acc@5 0.7031 (0.7807)\n",
      "\u001b[32m[2020-06-22 01:21:16] __main__ INFO: \u001b[0mEpoch 28 Step 351/351 lr 0.100000 loss 1.7543 (1.5539) acc@1 0.3516 (0.4217) acc@5 0.7891 (0.7810)\n",
      "\u001b[32m[2020-06-22 01:21:16] __main__ INFO: \u001b[0mElapsed 401.01\n",
      "\u001b[32m[2020-06-22 01:21:16] __main__ INFO: \u001b[0mVal 28\n",
      "\u001b[32m[2020-06-22 01:21:29] __main__ INFO: \u001b[0mEpoch 28 loss 1.7127 acc@1 0.3780 acc@5 0.7466\n",
      "\u001b[32m[2020-06-22 01:21:29] __main__ INFO: \u001b[0mElapsed 13.47\n",
      "\u001b[32m[2020-06-22 01:21:29] __main__ INFO: \u001b[0mTrain 29 9828\n",
      "\u001b[32m[2020-06-22 01:23:24] __main__ INFO: \u001b[0mEpoch 29 Step 100/351 lr 0.100000 loss 1.5728 (1.5333) acc@1 0.4062 (0.4327) acc@5 0.8359 (0.7887)\n",
      "\u001b[32m[2020-06-22 01:25:19] __main__ INFO: \u001b[0mEpoch 29 Step 200/351 lr 0.100000 loss 1.5433 (1.5365) acc@1 0.3984 (0.4329) acc@5 0.8047 (0.7884)\n",
      "\u001b[32m[2020-06-22 01:27:14] __main__ INFO: \u001b[0mEpoch 29 Step 300/351 lr 0.100000 loss 1.5356 (1.5498) acc@1 0.4062 (0.4272) acc@5 0.8125 (0.7851)\n",
      "\u001b[32m[2020-06-22 01:28:12] __main__ INFO: \u001b[0mEpoch 29 Step 351/351 lr 0.100000 loss 1.5914 (1.5511) acc@1 0.3438 (0.4267) acc@5 0.7734 (0.7839)\n",
      "\u001b[32m[2020-06-22 01:28:12] __main__ INFO: \u001b[0mElapsed 402.84\n",
      "\u001b[32m[2020-06-22 01:28:12] __main__ INFO: \u001b[0mVal 29\n",
      "\u001b[32m[2020-06-22 01:28:26] __main__ INFO: \u001b[0mEpoch 29 loss 1.6630 acc@1 0.3874 acc@5 0.7710\n",
      "\u001b[32m[2020-06-22 01:28:26] __main__ INFO: \u001b[0mElapsed 13.46\n",
      "\u001b[32m[2020-06-22 01:28:26] __main__ INFO: \u001b[0mTrain 30 10179\n",
      "\u001b[32m[2020-06-22 01:30:20] __main__ INFO: \u001b[0mEpoch 30 Step 100/351 lr 0.100000 loss 1.5504 (1.5374) acc@1 0.3984 (0.4313) acc@5 0.7812 (0.7879)\n",
      "\u001b[32m[2020-06-22 01:32:15] __main__ INFO: \u001b[0mEpoch 30 Step 200/351 lr 0.100000 loss 1.6358 (1.5378) acc@1 0.3750 (0.4320) acc@5 0.7969 (0.7870)\n",
      "\u001b[32m[2020-06-22 01:34:09] __main__ INFO: \u001b[0mEpoch 30 Step 300/351 lr 0.100000 loss 1.5248 (1.5431) acc@1 0.4844 (0.4313) acc@5 0.8359 (0.7872)\n",
      "\u001b[32m[2020-06-22 01:35:08] __main__ INFO: \u001b[0mEpoch 30 Step 351/351 lr 0.100000 loss 1.4507 (1.5474) acc@1 0.4609 (0.4294) acc@5 0.8281 (0.7863)\n",
      "\u001b[32m[2020-06-22 01:35:08] __main__ INFO: \u001b[0mElapsed 402.01\n",
      "\u001b[32m[2020-06-22 01:35:08] __main__ INFO: \u001b[0mVal 30\n",
      "\u001b[32m[2020-06-22 01:35:21] __main__ INFO: \u001b[0mEpoch 30 loss 1.9826 acc@1 0.3518 acc@5 0.7350\n",
      "\u001b[32m[2020-06-22 01:35:21] __main__ INFO: \u001b[0mElapsed 13.48\n",
      "\u001b[32m[2020-06-22 01:35:21] __main__ INFO: \u001b[0mTrain 31 10530\n",
      "\u001b[32m[2020-06-22 01:37:16] __main__ INFO: \u001b[0mEpoch 31 Step 100/351 lr 0.100000 loss 1.6527 (1.5215) acc@1 0.3906 (0.4385) acc@5 0.7578 (0.7890)\n",
      "\u001b[32m[2020-06-22 01:39:11] __main__ INFO: \u001b[0mEpoch 31 Step 200/351 lr 0.100000 loss 1.4024 (1.5394) acc@1 0.4531 (0.4307) acc@5 0.7734 (0.7860)\n",
      "\u001b[32m[2020-06-22 01:41:06] __main__ INFO: \u001b[0mEpoch 31 Step 300/351 lr 0.100000 loss 1.5948 (1.5426) acc@1 0.3906 (0.4303) acc@5 0.8125 (0.7859)\n",
      "\u001b[32m[2020-06-22 01:42:05] __main__ INFO: \u001b[0mEpoch 31 Step 351/351 lr 0.100000 loss 1.5216 (1.5430) acc@1 0.4375 (0.4297) acc@5 0.7422 (0.7861)\n",
      "\u001b[32m[2020-06-22 01:42:05] __main__ INFO: \u001b[0mElapsed 403.52\n",
      "\u001b[32m[2020-06-22 01:42:05] __main__ INFO: \u001b[0mVal 31\n",
      "\u001b[32m[2020-06-22 01:42:18] __main__ INFO: \u001b[0mEpoch 31 loss 1.7048 acc@1 0.3742 acc@5 0.7708\n",
      "\u001b[32m[2020-06-22 01:42:18] __main__ INFO: \u001b[0mElapsed 13.49\n",
      "\u001b[32m[2020-06-22 01:42:18] __main__ INFO: \u001b[0mTrain 32 10881\n",
      "\u001b[32m[2020-06-22 01:44:13] __main__ INFO: \u001b[0mEpoch 32 Step 100/351 lr 0.100000 loss 1.6104 (1.5151) acc@1 0.3906 (0.4398) acc@5 0.7656 (0.7880)\n",
      "\u001b[32m[2020-06-22 01:46:07] __main__ INFO: \u001b[0mEpoch 32 Step 200/351 lr 0.100000 loss 1.4875 (1.5294) acc@1 0.4375 (0.4335) acc@5 0.7500 (0.7852)\n",
      "\u001b[32m[2020-06-22 01:48:02] __main__ INFO: \u001b[0mEpoch 32 Step 300/351 lr 0.100000 loss 1.5637 (1.5333) acc@1 0.4219 (0.4339) acc@5 0.7734 (0.7856)\n",
      "\u001b[32m[2020-06-22 01:49:00] __main__ INFO: \u001b[0mEpoch 32 Step 351/351 lr 0.100000 loss 1.6255 (1.5330) acc@1 0.4844 (0.4337) acc@5 0.7734 (0.7864)\n",
      "\u001b[32m[2020-06-22 01:49:00] __main__ INFO: \u001b[0mElapsed 402.08\n",
      "\u001b[32m[2020-06-22 01:49:00] __main__ INFO: \u001b[0mVal 32\n",
      "\u001b[32m[2020-06-22 01:49:14] __main__ INFO: \u001b[0mEpoch 32 loss 1.6563 acc@1 0.3920 acc@5 0.7718\n",
      "\u001b[32m[2020-06-22 01:49:14] __main__ INFO: \u001b[0mElapsed 13.49\n",
      "\u001b[32m[2020-06-22 01:49:14] __main__ INFO: \u001b[0mTrain 33 11232\n",
      "\u001b[32m[2020-06-22 01:51:09] __main__ INFO: \u001b[0mEpoch 33 Step 100/351 lr 0.100000 loss 1.5242 (1.5229) acc@1 0.4297 (0.4313) acc@5 0.8281 (0.7852)\n",
      "\u001b[32m[2020-06-22 01:53:04] __main__ INFO: \u001b[0mEpoch 33 Step 200/351 lr 0.100000 loss 1.6578 (1.5220) acc@1 0.3516 (0.4348) acc@5 0.7188 (0.7863)\n",
      "\u001b[32m[2020-06-22 01:54:59] __main__ INFO: \u001b[0mEpoch 33 Step 300/351 lr 0.100000 loss 1.6532 (1.5253) acc@1 0.3984 (0.4339) acc@5 0.7656 (0.7860)\n",
      "\u001b[32m[2020-06-22 01:55:57] __main__ INFO: \u001b[0mEpoch 33 Step 351/351 lr 0.100000 loss 1.5769 (1.5279) acc@1 0.4062 (0.4333) acc@5 0.7656 (0.7850)\n",
      "\u001b[32m[2020-06-22 01:55:57] __main__ INFO: \u001b[0mElapsed 403.40\n",
      "\u001b[32m[2020-06-22 01:55:57] __main__ INFO: \u001b[0mVal 33\n",
      "\u001b[32m[2020-06-22 01:56:11] __main__ INFO: \u001b[0mEpoch 33 loss 1.7408 acc@1 0.3664 acc@5 0.7638\n",
      "\u001b[32m[2020-06-22 01:56:11] __main__ INFO: \u001b[0mElapsed 13.48\n",
      "\u001b[32m[2020-06-22 01:56:11] __main__ INFO: \u001b[0mTrain 34 11583\n",
      "\u001b[32m[2020-06-22 01:58:05] __main__ INFO: \u001b[0mEpoch 34 Step 100/351 lr 0.100000 loss 1.6159 (1.5200) acc@1 0.3672 (0.4372) acc@5 0.7188 (0.7841)\n",
      "\u001b[32m[2020-06-22 02:00:00] __main__ INFO: \u001b[0mEpoch 34 Step 200/351 lr 0.100000 loss 1.4798 (1.5208) acc@1 0.4609 (0.4374) acc@5 0.7812 (0.7882)\n",
      "\u001b[32m[2020-06-22 02:01:54] __main__ INFO: \u001b[0mEpoch 34 Step 300/351 lr 0.100000 loss 1.6105 (1.5253) acc@1 0.3828 (0.4345) acc@5 0.7500 (0.7873)\n",
      "\u001b[32m[2020-06-22 02:02:53] __main__ INFO: \u001b[0mEpoch 34 Step 351/351 lr 0.100000 loss 1.4153 (1.5266) acc@1 0.4688 (0.4338) acc@5 0.8438 (0.7872)\n",
      "\u001b[32m[2020-06-22 02:02:53] __main__ INFO: \u001b[0mElapsed 401.90\n",
      "\u001b[32m[2020-06-22 02:02:53] __main__ INFO: \u001b[0mVal 34\n",
      "\u001b[32m[2020-06-22 02:03:06] __main__ INFO: \u001b[0mEpoch 34 loss 1.6109 acc@1 0.4070 acc@5 0.7784\n",
      "\u001b[32m[2020-06-22 02:03:06] __main__ INFO: \u001b[0mElapsed 13.45\n",
      "\u001b[32m[2020-06-22 02:03:06] __main__ INFO: \u001b[0mTrain 35 11934\n",
      "\u001b[32m[2020-06-22 02:05:01] __main__ INFO: \u001b[0mEpoch 35 Step 100/351 lr 0.100000 loss 1.5783 (1.5207) acc@1 0.4062 (0.4374) acc@5 0.7031 (0.7885)\n",
      "\u001b[32m[2020-06-22 02:06:56] __main__ INFO: \u001b[0mEpoch 35 Step 200/351 lr 0.100000 loss 1.5283 (1.5243) acc@1 0.4297 (0.4364) acc@5 0.7656 (0.7883)\n",
      "\u001b[32m[2020-06-22 02:08:51] __main__ INFO: \u001b[0mEpoch 35 Step 300/351 lr 0.100000 loss 1.4796 (1.5227) acc@1 0.4766 (0.4363) acc@5 0.8359 (0.7875)\n",
      "\u001b[32m[2020-06-22 02:09:49] __main__ INFO: \u001b[0mEpoch 35 Step 351/351 lr 0.100000 loss 1.5995 (1.5238) acc@1 0.4141 (0.4366) acc@5 0.7891 (0.7877)\n",
      "\u001b[32m[2020-06-22 02:09:49] __main__ INFO: \u001b[0mElapsed 403.28\n",
      "\u001b[32m[2020-06-22 02:09:49] __main__ INFO: \u001b[0mVal 35\n",
      "\u001b[32m[2020-06-22 02:10:03] __main__ INFO: \u001b[0mEpoch 35 loss 1.7204 acc@1 0.3852 acc@5 0.7686\n",
      "\u001b[32m[2020-06-22 02:10:03] __main__ INFO: \u001b[0mElapsed 13.47\n",
      "\u001b[32m[2020-06-22 02:10:03] __main__ INFO: \u001b[0mTrain 36 12285\n",
      "\u001b[32m[2020-06-22 02:11:57] __main__ INFO: \u001b[0mEpoch 36 Step 100/351 lr 0.100000 loss 1.4798 (1.4991) acc@1 0.4219 (0.4458) acc@5 0.7812 (0.7911)\n",
      "\u001b[32m[2020-06-22 02:13:52] __main__ INFO: \u001b[0mEpoch 36 Step 200/351 lr 0.100000 loss 1.5615 (1.5147) acc@1 0.4219 (0.4387) acc@5 0.7656 (0.7884)\n",
      "\u001b[32m[2020-06-22 02:15:46] __main__ INFO: \u001b[0mEpoch 36 Step 300/351 lr 0.100000 loss 1.6383 (1.5174) acc@1 0.4297 (0.4363) acc@5 0.7891 (0.7897)\n",
      "\u001b[32m[2020-06-22 02:16:45] __main__ INFO: \u001b[0mEpoch 36 Step 351/351 lr 0.100000 loss 1.6140 (1.5224) acc@1 0.3828 (0.4350) acc@5 0.7891 (0.7891)\n",
      "\u001b[32m[2020-06-22 02:16:45] __main__ INFO: \u001b[0mElapsed 401.91\n",
      "\u001b[32m[2020-06-22 02:16:45] __main__ INFO: \u001b[0mVal 36\n",
      "\u001b[32m[2020-06-22 02:16:58] __main__ INFO: \u001b[0mEpoch 36 loss 1.6492 acc@1 0.4122 acc@5 0.7678\n",
      "\u001b[32m[2020-06-22 02:16:58] __main__ INFO: \u001b[0mElapsed 13.45\n",
      "\u001b[32m[2020-06-22 02:16:58] __main__ INFO: \u001b[0mTrain 37 12636\n",
      "\u001b[32m[2020-06-22 02:18:53] __main__ INFO: \u001b[0mEpoch 37 Step 100/351 lr 0.100000 loss 1.5323 (1.4978) acc@1 0.4453 (0.4476) acc@5 0.7500 (0.7890)\n",
      "\u001b[32m[2020-06-22 02:20:48] __main__ INFO: \u001b[0mEpoch 37 Step 200/351 lr 0.100000 loss 1.5808 (1.5030) acc@1 0.4219 (0.4429) acc@5 0.7500 (0.7848)\n",
      "\u001b[32m[2020-06-22 02:22:43] __main__ INFO: \u001b[0mEpoch 37 Step 300/351 lr 0.100000 loss 1.4266 (1.5108) acc@1 0.4688 (0.4394) acc@5 0.8203 (0.7856)\n",
      "\u001b[32m[2020-06-22 02:23:41] __main__ INFO: \u001b[0mEpoch 37 Step 351/351 lr 0.100000 loss 1.4045 (1.5109) acc@1 0.5000 (0.4387) acc@5 0.7969 (0.7857)\n",
      "\u001b[32m[2020-06-22 02:23:41] __main__ INFO: \u001b[0mElapsed 403.18\n",
      "\u001b[32m[2020-06-22 02:23:41] __main__ INFO: \u001b[0mVal 37\n",
      "\u001b[32m[2020-06-22 02:23:55] __main__ INFO: \u001b[0mEpoch 37 loss 1.7326 acc@1 0.3884 acc@5 0.7636\n",
      "\u001b[32m[2020-06-22 02:23:55] __main__ INFO: \u001b[0mElapsed 13.47\n",
      "\u001b[32m[2020-06-22 02:23:55] __main__ INFO: \u001b[0mTrain 38 12987\n",
      "\u001b[32m[2020-06-22 02:25:49] __main__ INFO: \u001b[0mEpoch 38 Step 100/351 lr 0.100000 loss 1.5507 (1.4901) acc@1 0.4531 (0.4527) acc@5 0.7812 (0.7973)\n",
      "\u001b[32m[2020-06-22 02:27:43] __main__ INFO: \u001b[0mEpoch 38 Step 200/351 lr 0.100000 loss 1.5035 (1.5032) acc@1 0.4844 (0.4459) acc@5 0.7891 (0.7889)\n",
      "\u001b[32m[2020-06-22 02:29:38] __main__ INFO: \u001b[0mEpoch 38 Step 300/351 lr 0.100000 loss 1.4223 (1.5113) acc@1 0.4531 (0.4418) acc@5 0.7422 (0.7880)\n",
      "\u001b[32m[2020-06-22 02:30:36] __main__ INFO: \u001b[0mEpoch 38 Step 351/351 lr 0.100000 loss 1.5548 (1.5128) acc@1 0.4297 (0.4419) acc@5 0.7578 (0.7874)\n",
      "\u001b[32m[2020-06-22 02:30:36] __main__ INFO: \u001b[0mElapsed 401.35\n",
      "\u001b[32m[2020-06-22 02:30:36] __main__ INFO: \u001b[0mVal 38\n",
      "\u001b[32m[2020-06-22 02:30:50] __main__ INFO: \u001b[0mEpoch 38 loss 1.8760 acc@1 0.3516 acc@5 0.7676\n",
      "\u001b[32m[2020-06-22 02:30:50] __main__ INFO: \u001b[0mElapsed 13.44\n",
      "\u001b[32m[2020-06-22 02:30:50] __main__ INFO: \u001b[0mTrain 39 13338\n",
      "\u001b[32m[2020-06-22 02:32:44] __main__ INFO: \u001b[0mEpoch 39 Step 100/351 lr 0.100000 loss 1.4951 (1.4943) acc@1 0.5000 (0.4476) acc@5 0.7734 (0.7837)\n",
      "\u001b[32m[2020-06-22 02:34:39] __main__ INFO: \u001b[0mEpoch 39 Step 200/351 lr 0.100000 loss 1.4474 (1.4944) acc@1 0.4453 (0.4467) acc@5 0.7578 (0.7880)\n",
      "\u001b[32m[2020-06-22 02:36:34] __main__ INFO: \u001b[0mEpoch 39 Step 300/351 lr 0.100000 loss 1.6001 (1.5042) acc@1 0.4062 (0.4421) acc@5 0.7734 (0.7868)\n",
      "\u001b[32m[2020-06-22 02:37:32] __main__ INFO: \u001b[0mEpoch 39 Step 351/351 lr 0.100000 loss 1.5537 (1.5071) acc@1 0.4062 (0.4418) acc@5 0.7656 (0.7857)\n",
      "\u001b[32m[2020-06-22 02:37:32] __main__ INFO: \u001b[0mElapsed 402.80\n",
      "\u001b[32m[2020-06-22 02:37:32] __main__ INFO: \u001b[0mVal 39\n",
      "\u001b[32m[2020-06-22 02:37:46] __main__ INFO: \u001b[0mEpoch 39 loss 1.6730 acc@1 0.3906 acc@5 0.7780\n",
      "\u001b[32m[2020-06-22 02:37:46] __main__ INFO: \u001b[0mElapsed 13.46\n",
      "\u001b[32m[2020-06-22 02:37:46] __main__ INFO: \u001b[0mTrain 40 13689\n",
      "\u001b[32m[2020-06-22 02:39:40] __main__ INFO: \u001b[0mEpoch 40 Step 100/351 lr 0.100000 loss 1.4634 (1.4842) acc@1 0.4531 (0.4523) acc@5 0.7500 (0.7916)\n",
      "\u001b[32m[2020-06-22 02:41:35] __main__ INFO: \u001b[0mEpoch 40 Step 200/351 lr 0.100000 loss 1.3735 (1.4939) acc@1 0.4922 (0.4472) acc@5 0.8359 (0.7910)\n",
      "\u001b[32m[2020-06-22 02:43:29] __main__ INFO: \u001b[0mEpoch 40 Step 300/351 lr 0.100000 loss 1.5109 (1.5073) acc@1 0.4531 (0.4424) acc@5 0.7422 (0.7898)\n",
      "\u001b[32m[2020-06-22 02:44:28] __main__ INFO: \u001b[0mEpoch 40 Step 351/351 lr 0.100000 loss 1.5159 (1.5100) acc@1 0.4453 (0.4423) acc@5 0.7656 (0.7898)\n",
      "\u001b[32m[2020-06-22 02:44:28] __main__ INFO: \u001b[0mElapsed 401.81\n",
      "\u001b[32m[2020-06-22 02:44:28] __main__ INFO: \u001b[0mVal 40\n",
      "\u001b[32m[2020-06-22 02:44:41] __main__ INFO: \u001b[0mEpoch 40 loss 1.6854 acc@1 0.3936 acc@5 0.7734\n",
      "\u001b[32m[2020-06-22 02:44:41] __main__ INFO: \u001b[0mElapsed 13.47\n",
      "\u001b[32m[2020-06-22 02:44:41] __main__ INFO: \u001b[0mTrain 41 14040\n",
      "\u001b[32m[2020-06-22 02:46:36] __main__ INFO: \u001b[0mEpoch 41 Step 100/351 lr 0.100000 loss 1.6152 (1.4903) acc@1 0.3828 (0.4513) acc@5 0.7578 (0.7880)\n",
      "\u001b[32m[2020-06-22 02:48:31] __main__ INFO: \u001b[0mEpoch 41 Step 200/351 lr 0.100000 loss 1.6119 (1.4994) acc@1 0.3828 (0.4471) acc@5 0.7344 (0.7858)\n",
      "\u001b[32m[2020-06-22 02:50:26] __main__ INFO: \u001b[0mEpoch 41 Step 300/351 lr 0.100000 loss 1.6008 (1.5014) acc@1 0.3516 (0.4462) acc@5 0.7422 (0.7855)\n",
      "\u001b[32m[2020-06-22 02:51:24] __main__ INFO: \u001b[0mEpoch 41 Step 351/351 lr 0.100000 loss 1.5089 (1.5043) acc@1 0.4688 (0.4446) acc@5 0.7734 (0.7855)\n",
      "\u001b[32m[2020-06-22 02:51:24] __main__ INFO: \u001b[0mElapsed 403.21\n",
      "\u001b[32m[2020-06-22 02:51:24] __main__ INFO: \u001b[0mVal 41\n",
      "\u001b[32m[2020-06-22 02:51:38] __main__ INFO: \u001b[0mEpoch 41 loss 1.6464 acc@1 0.4076 acc@5 0.7680\n",
      "\u001b[32m[2020-06-22 02:51:38] __main__ INFO: \u001b[0mElapsed 13.46\n",
      "\u001b[32m[2020-06-22 02:51:38] __main__ INFO: \u001b[0mTrain 42 14391\n",
      "\u001b[32m[2020-06-22 02:53:32] __main__ INFO: \u001b[0mEpoch 42 Step 100/351 lr 0.100000 loss 1.5178 (1.4826) acc@1 0.4219 (0.4459) acc@5 0.8125 (0.7907)\n",
      "\u001b[32m[2020-06-22 02:55:27] __main__ INFO: \u001b[0mEpoch 42 Step 200/351 lr 0.100000 loss 1.5082 (1.4853) acc@1 0.4297 (0.4468) acc@5 0.7578 (0.7902)\n",
      "\u001b[32m[2020-06-22 02:57:21] __main__ INFO: \u001b[0mEpoch 42 Step 300/351 lr 0.100000 loss 1.6339 (1.4975) acc@1 0.3906 (0.4447) acc@5 0.8047 (0.7908)\n",
      "\u001b[32m[2020-06-22 02:58:20] __main__ INFO: \u001b[0mEpoch 42 Step 351/351 lr 0.100000 loss 1.4404 (1.5024) acc@1 0.4609 (0.4422) acc@5 0.7344 (0.7894)\n",
      "\u001b[32m[2020-06-22 02:58:20] __main__ INFO: \u001b[0mElapsed 401.83\n",
      "\u001b[32m[2020-06-22 02:58:20] __main__ INFO: \u001b[0mVal 42\n",
      "\u001b[32m[2020-06-22 02:58:33] __main__ INFO: \u001b[0mEpoch 42 loss 1.6792 acc@1 0.3950 acc@5 0.7636\n",
      "\u001b[32m[2020-06-22 02:58:33] __main__ INFO: \u001b[0mElapsed 13.46\n",
      "\u001b[32m[2020-06-22 02:58:33] __main__ INFO: \u001b[0mTrain 43 14742\n",
      "\u001b[32m[2020-06-22 03:00:28] __main__ INFO: \u001b[0mEpoch 43 Step 100/351 lr 0.100000 loss 1.5069 (1.4691) acc@1 0.4375 (0.4550) acc@5 0.7969 (0.7949)\n",
      "\u001b[32m[2020-06-22 03:02:23] __main__ INFO: \u001b[0mEpoch 43 Step 200/351 lr 0.100000 loss 1.5291 (1.4834) acc@1 0.4453 (0.4511) acc@5 0.7891 (0.7929)\n",
      "\u001b[32m[2020-06-22 03:04:18] __main__ INFO: \u001b[0mEpoch 43 Step 300/351 lr 0.100000 loss 1.4824 (1.4876) acc@1 0.4453 (0.4495) acc@5 0.8203 (0.7929)\n",
      "\u001b[32m[2020-06-22 03:05:16] __main__ INFO: \u001b[0mEpoch 43 Step 351/351 lr 0.100000 loss 1.7102 (1.4919) acc@1 0.3594 (0.4480) acc@5 0.7891 (0.7926)\n",
      "\u001b[32m[2020-06-22 03:05:16] __main__ INFO: \u001b[0mElapsed 403.02\n",
      "\u001b[32m[2020-06-22 03:05:16] __main__ INFO: \u001b[0mVal 43\n",
      "\u001b[32m[2020-06-22 03:05:30] __main__ INFO: \u001b[0mEpoch 43 loss 1.6457 acc@1 0.4052 acc@5 0.7764\n",
      "\u001b[32m[2020-06-22 03:05:30] __main__ INFO: \u001b[0mElapsed 13.42\n",
      "\u001b[32m[2020-06-22 03:05:30] __main__ INFO: \u001b[0mTrain 44 15093\n",
      "\u001b[32m[2020-06-22 03:07:24] __main__ INFO: \u001b[0mEpoch 44 Step 100/351 lr 0.100000 loss 1.4632 (1.4684) acc@1 0.4531 (0.4595) acc@5 0.7969 (0.7906)\n",
      "\u001b[32m[2020-06-22 03:09:18] __main__ INFO: \u001b[0mEpoch 44 Step 200/351 lr 0.100000 loss 1.4096 (1.4800) acc@1 0.5078 (0.4537) acc@5 0.8438 (0.7918)\n",
      "\u001b[32m[2020-06-22 03:11:13] __main__ INFO: \u001b[0mEpoch 44 Step 300/351 lr 0.100000 loss 1.6257 (1.4907) acc@1 0.4219 (0.4492) acc@5 0.7734 (0.7903)\n",
      "\u001b[32m[2020-06-22 03:12:11] __main__ INFO: \u001b[0mEpoch 44 Step 351/351 lr 0.100000 loss 1.7668 (1.4963) acc@1 0.3906 (0.4470) acc@5 0.7656 (0.7891)\n",
      "\u001b[32m[2020-06-22 03:12:11] __main__ INFO: \u001b[0mElapsed 401.50\n",
      "\u001b[32m[2020-06-22 03:12:11] __main__ INFO: \u001b[0mVal 44\n",
      "\u001b[32m[2020-06-22 03:12:25] __main__ INFO: \u001b[0mEpoch 44 loss 1.5901 acc@1 0.4160 acc@5 0.7790\n",
      "\u001b[32m[2020-06-22 03:12:25] __main__ INFO: \u001b[0mElapsed 13.47\n",
      "\u001b[32m[2020-06-22 03:12:25] __main__ INFO: \u001b[0mTrain 45 15444\n",
      "\u001b[32m[2020-06-22 03:14:19] __main__ INFO: \u001b[0mEpoch 45 Step 100/351 lr 0.100000 loss 1.7346 (1.4724) acc@1 0.3281 (0.4542) acc@5 0.7656 (0.7876)\n",
      "\u001b[32m[2020-06-22 03:16:14] __main__ INFO: \u001b[0mEpoch 45 Step 200/351 lr 0.100000 loss 1.6235 (1.4815) acc@1 0.4297 (0.4519) acc@5 0.7734 (0.7905)\n",
      "\u001b[32m[2020-06-22 03:18:09] __main__ INFO: \u001b[0mEpoch 45 Step 300/351 lr 0.100000 loss 1.3772 (1.4865) acc@1 0.4531 (0.4509) acc@5 0.8281 (0.7903)\n",
      "\u001b[32m[2020-06-22 03:19:07] __main__ INFO: \u001b[0mEpoch 45 Step 351/351 lr 0.100000 loss 1.5299 (1.4903) acc@1 0.4219 (0.4495) acc@5 0.7969 (0.7909)\n",
      "\u001b[32m[2020-06-22 03:19:07] __main__ INFO: \u001b[0mElapsed 402.74\n",
      "\u001b[32m[2020-06-22 03:19:07] __main__ INFO: \u001b[0mVal 45\n",
      "\u001b[32m[2020-06-22 03:19:21] __main__ INFO: \u001b[0mEpoch 45 loss 1.6231 acc@1 0.4192 acc@5 0.7846\n",
      "\u001b[32m[2020-06-22 03:19:21] __main__ INFO: \u001b[0mElapsed 13.45\n",
      "\u001b[32m[2020-06-22 03:19:21] __main__ INFO: \u001b[0mTrain 46 15795\n",
      "\u001b[32m[2020-06-22 03:21:15] __main__ INFO: \u001b[0mEpoch 46 Step 100/351 lr 0.100000 loss 1.4124 (1.4878) acc@1 0.4375 (0.4512) acc@5 0.8125 (0.7877)\n",
      "\u001b[32m[2020-06-22 03:23:10] __main__ INFO: \u001b[0mEpoch 46 Step 200/351 lr 0.100000 loss 1.3365 (1.4876) acc@1 0.5312 (0.4520) acc@5 0.8359 (0.7922)\n",
      "\u001b[32m[2020-06-22 03:25:04] __main__ INFO: \u001b[0mEpoch 46 Step 300/351 lr 0.100000 loss 1.4059 (1.4882) acc@1 0.5078 (0.4523) acc@5 0.8125 (0.7934)\n",
      "\u001b[32m[2020-06-22 03:26:02] __main__ INFO: \u001b[0mEpoch 46 Step 351/351 lr 0.100000 loss 1.5099 (1.4922) acc@1 0.4453 (0.4497) acc@5 0.7656 (0.7920)\n",
      "\u001b[32m[2020-06-22 03:26:02] __main__ INFO: \u001b[0mElapsed 401.45\n",
      "\u001b[32m[2020-06-22 03:26:02] __main__ INFO: \u001b[0mVal 46\n",
      "\u001b[32m[2020-06-22 03:26:16] __main__ INFO: \u001b[0mEpoch 46 loss 2.1172 acc@1 0.3212 acc@5 0.7488\n",
      "\u001b[32m[2020-06-22 03:26:16] __main__ INFO: \u001b[0mElapsed 13.46\n",
      "\u001b[32m[2020-06-22 03:26:16] __main__ INFO: \u001b[0mTrain 47 16146\n",
      "\u001b[32m[2020-06-22 03:28:10] __main__ INFO: \u001b[0mEpoch 47 Step 100/351 lr 0.100000 loss 1.6533 (1.4670) acc@1 0.3828 (0.4551) acc@5 0.7891 (0.7927)\n",
      "\u001b[32m[2020-06-22 03:30:05] __main__ INFO: \u001b[0mEpoch 47 Step 200/351 lr 0.100000 loss 1.7523 (1.4786) acc@1 0.3359 (0.4513) acc@5 0.7422 (0.7948)\n",
      "\u001b[32m[2020-06-22 03:32:00] __main__ INFO: \u001b[0mEpoch 47 Step 300/351 lr 0.100000 loss 1.4664 (1.4830) acc@1 0.4688 (0.4488) acc@5 0.7578 (0.7909)\n",
      "\u001b[32m[2020-06-22 03:32:58] __main__ INFO: \u001b[0mEpoch 47 Step 351/351 lr 0.100000 loss 1.5141 (1.4854) acc@1 0.3828 (0.4476) acc@5 0.7891 (0.7904)\n",
      "\u001b[32m[2020-06-22 03:32:58] __main__ INFO: \u001b[0mElapsed 402.57\n",
      "\u001b[32m[2020-06-22 03:32:58] __main__ INFO: \u001b[0mVal 47\n",
      "\u001b[32m[2020-06-22 03:33:12] __main__ INFO: \u001b[0mEpoch 47 loss 1.8275 acc@1 0.3606 acc@5 0.7574\n",
      "\u001b[32m[2020-06-22 03:33:12] __main__ INFO: \u001b[0mElapsed 13.45\n",
      "\u001b[32m[2020-06-22 03:33:12] __main__ INFO: \u001b[0mTrain 48 16497\n",
      "\u001b[32m[2020-06-22 03:35:06] __main__ INFO: \u001b[0mEpoch 48 Step 100/351 lr 0.100000 loss 1.2953 (1.4618) acc@1 0.5312 (0.4608) acc@5 0.7969 (0.7987)\n",
      "\u001b[32m[2020-06-22 03:37:00] __main__ INFO: \u001b[0mEpoch 48 Step 200/351 lr 0.100000 loss 1.5766 (1.4777) acc@1 0.4297 (0.4540) acc@5 0.8516 (0.7968)\n",
      "\u001b[32m[2020-06-22 03:38:55] __main__ INFO: \u001b[0mEpoch 48 Step 300/351 lr 0.100000 loss 1.3693 (1.4849) acc@1 0.4844 (0.4510) acc@5 0.8047 (0.7960)\n",
      "\u001b[32m[2020-06-22 03:39:53] __main__ INFO: \u001b[0mEpoch 48 Step 351/351 lr 0.100000 loss 1.3496 (1.4826) acc@1 0.4766 (0.4515) acc@5 0.7656 (0.7943)\n",
      "\u001b[32m[2020-06-22 03:39:53] __main__ INFO: \u001b[0mElapsed 401.30\n",
      "\u001b[32m[2020-06-22 03:39:53] __main__ INFO: \u001b[0mVal 48\n",
      "\u001b[32m[2020-06-22 03:40:06] __main__ INFO: \u001b[0mEpoch 48 loss 1.7193 acc@1 0.3780 acc@5 0.7660\n",
      "\u001b[32m[2020-06-22 03:40:06] __main__ INFO: \u001b[0mElapsed 13.44\n",
      "\u001b[32m[2020-06-22 03:40:06] __main__ INFO: \u001b[0mTrain 49 16848\n",
      "\u001b[32m[2020-06-22 03:42:01] __main__ INFO: \u001b[0mEpoch 49 Step 100/351 lr 0.100000 loss 1.4343 (1.4764) acc@1 0.4609 (0.4513) acc@5 0.7578 (0.7929)\n",
      "\u001b[32m[2020-06-22 03:43:56] __main__ INFO: \u001b[0mEpoch 49 Step 200/351 lr 0.100000 loss 1.3596 (1.4809) acc@1 0.4766 (0.4512) acc@5 0.8359 (0.7924)\n",
      "\u001b[32m[2020-06-22 03:45:51] __main__ INFO: \u001b[0mEpoch 49 Step 300/351 lr 0.100000 loss 1.5235 (1.4811) acc@1 0.4453 (0.4523) acc@5 0.8047 (0.7935)\n",
      "\u001b[32m[2020-06-22 03:46:49] __main__ INFO: \u001b[0mEpoch 49 Step 351/351 lr 0.100000 loss 1.5944 (1.4805) acc@1 0.4297 (0.4526) acc@5 0.8359 (0.7930)\n",
      "\u001b[32m[2020-06-22 03:46:49] __main__ INFO: \u001b[0mElapsed 402.96\n",
      "\u001b[32m[2020-06-22 03:46:49] __main__ INFO: \u001b[0mVal 49\n",
      "\u001b[32m[2020-06-22 03:47:03] __main__ INFO: \u001b[0mEpoch 49 loss 1.5384 acc@1 0.4350 acc@5 0.7788\n",
      "\u001b[32m[2020-06-22 03:47:03] __main__ INFO: \u001b[0mElapsed 13.46\n",
      "\u001b[32m[2020-06-22 03:47:03] __main__ INFO: \u001b[0mTrain 50 17199\n",
      "\u001b[32m[2020-06-22 03:48:57] __main__ INFO: \u001b[0mEpoch 50 Step 100/351 lr 0.100000 loss 1.2948 (1.4574) acc@1 0.4844 (0.4564) acc@5 0.8359 (0.7941)\n",
      "\u001b[32m[2020-06-22 03:50:52] __main__ INFO: \u001b[0mEpoch 50 Step 200/351 lr 0.100000 loss 1.3770 (1.4643) acc@1 0.4766 (0.4580) acc@5 0.8281 (0.7941)\n",
      "\u001b[32m[2020-06-22 03:52:46] __main__ INFO: \u001b[0mEpoch 50 Step 300/351 lr 0.100000 loss 1.4315 (1.4734) acc@1 0.4844 (0.4539) acc@5 0.7812 (0.7912)\n",
      "\u001b[32m[2020-06-22 03:53:44] __main__ INFO: \u001b[0mEpoch 50 Step 351/351 lr 0.100000 loss 1.6423 (1.4796) acc@1 0.4141 (0.4510) acc@5 0.7734 (0.7907)\n",
      "\u001b[32m[2020-06-22 03:53:44] __main__ INFO: \u001b[0mElapsed 401.49\n",
      "\u001b[32m[2020-06-22 03:53:44] __main__ INFO: \u001b[0mVal 50\n",
      "\u001b[32m[2020-06-22 03:53:58] __main__ INFO: \u001b[0mEpoch 50 loss 1.7585 acc@1 0.3938 acc@5 0.7676\n",
      "\u001b[32m[2020-06-22 03:53:58] __main__ INFO: \u001b[0mElapsed 13.46\n",
      "\u001b[32m[2020-06-22 03:53:58] __main__ INFO: \u001b[0mTrain 51 17550\n",
      "\u001b[32m[2020-06-22 03:55:53] __main__ INFO: \u001b[0mEpoch 51 Step 100/351 lr 0.100000 loss 1.5333 (1.4669) acc@1 0.5000 (0.4553) acc@5 0.7969 (0.7900)\n",
      "\u001b[32m[2020-06-22 03:57:47] __main__ INFO: \u001b[0mEpoch 51 Step 200/351 lr 0.100000 loss 1.6414 (1.4704) acc@1 0.3984 (0.4537) acc@5 0.7500 (0.7891)\n",
      "\u001b[32m[2020-06-22 03:59:42] __main__ INFO: \u001b[0mEpoch 51 Step 300/351 lr 0.100000 loss 1.5356 (1.4778) acc@1 0.4297 (0.4514) acc@5 0.7266 (0.7898)\n",
      "\u001b[32m[2020-06-22 04:00:41] __main__ INFO: \u001b[0mEpoch 51 Step 351/351 lr 0.100000 loss 1.5137 (1.4792) acc@1 0.4375 (0.4516) acc@5 0.7734 (0.7887)\n",
      "\u001b[32m[2020-06-22 04:00:41] __main__ INFO: \u001b[0mElapsed 402.98\n",
      "\u001b[32m[2020-06-22 04:00:41] __main__ INFO: \u001b[0mVal 51\n",
      "\u001b[32m[2020-06-22 04:00:54] __main__ INFO: \u001b[0mEpoch 51 loss 1.6569 acc@1 0.4070 acc@5 0.7830\n",
      "\u001b[32m[2020-06-22 04:00:54] __main__ INFO: \u001b[0mElapsed 13.45\n",
      "\u001b[32m[2020-06-22 04:00:54] __main__ INFO: \u001b[0mTrain 52 17901\n",
      "\u001b[32m[2020-06-22 04:02:49] __main__ INFO: \u001b[0mEpoch 52 Step 100/351 lr 0.100000 loss 1.4712 (1.4579) acc@1 0.4766 (0.4582) acc@5 0.7734 (0.7938)\n",
      "\u001b[32m[2020-06-22 04:04:43] __main__ INFO: \u001b[0mEpoch 52 Step 200/351 lr 0.100000 loss 1.4696 (1.4661) acc@1 0.4688 (0.4555) acc@5 0.7734 (0.7919)\n",
      "\u001b[32m[2020-06-22 04:06:37] __main__ INFO: \u001b[0mEpoch 52 Step 300/351 lr 0.100000 loss 1.5555 (1.4761) acc@1 0.4141 (0.4533) acc@5 0.8125 (0.7891)\n",
      "\u001b[32m[2020-06-22 04:07:36] __main__ INFO: \u001b[0mEpoch 52 Step 351/351 lr 0.100000 loss 1.6228 (1.4756) acc@1 0.4141 (0.4530) acc@5 0.7891 (0.7904)\n",
      "\u001b[32m[2020-06-22 04:07:36] __main__ INFO: \u001b[0mElapsed 401.62\n",
      "\u001b[32m[2020-06-22 04:07:36] __main__ INFO: \u001b[0mVal 52\n",
      "\u001b[32m[2020-06-22 04:07:49] __main__ INFO: \u001b[0mEpoch 52 loss 1.6281 acc@1 0.4100 acc@5 0.7844\n",
      "\u001b[32m[2020-06-22 04:07:49] __main__ INFO: \u001b[0mElapsed 13.47\n",
      "\u001b[32m[2020-06-22 04:07:49] __main__ INFO: \u001b[0mTrain 53 18252\n",
      "\u001b[32m[2020-06-22 04:09:44] __main__ INFO: \u001b[0mEpoch 53 Step 100/351 lr 0.100000 loss 1.2479 (1.4671) acc@1 0.5781 (0.4616) acc@5 0.8828 (0.7959)\n",
      "\u001b[32m[2020-06-22 04:11:39] __main__ INFO: \u001b[0mEpoch 53 Step 200/351 lr 0.100000 loss 1.4171 (1.4705) acc@1 0.4922 (0.4563) acc@5 0.8281 (0.7971)\n"
     ]
    }
   ],
   "source": [
    "# Train the model per the settings specified in the original paper\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "!python train.py --config configs/cifar/wrn.yaml \\\n",
    "    model.wrn.depth 28 \\\n",
    "    model.wrn.widening_factor 10 \\\n",
    "    train.batch_size 128 \\\n",
    "    train.base_lr 0.1 \\\n",
    "    dataset.name CIFAR10_RA_2_20 \\\n",
    "    train.output_dir /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_20/exp00 \\\n",
    "    scheduler.epochs 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-23 21:22:10] __main__ INFO: \u001b[0mdevice: cuda\n",
      "cudnn:\n",
      "  benchmark: True\n",
      "  deterministic: False\n",
      "dataset:\n",
      "  name: CIFAR10\n",
      "  dataset_dir: ~/.torch/datasets/CIFAR10\n",
      "  image_size: 32\n",
      "  n_channels: 3\n",
      "  n_classes: 10\n",
      "model:\n",
      "  type: cifar\n",
      "  name: wrn\n",
      "  init_mode: kaiming_fan_in\n",
      "  vgg:\n",
      "    n_channels: [64, 128, 256, 512, 512]\n",
      "    n_layers: [2, 2, 3, 3, 3]\n",
      "    use_bn: True\n",
      "  resnet:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "  resnet_preact:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "  wrn:\n",
      "    depth: 28\n",
      "    initial_channels: 16\n",
      "    widening_factor: 10\n",
      "    drop_rate: 0.0\n",
      "  densenet:\n",
      "    depth: 100\n",
      "    n_blocks: [6, 12, 24, 16]\n",
      "    block_type: bottleneck\n",
      "    growth_rate: 12\n",
      "    drop_rate: 0.0\n",
      "    compression_rate: 0.5\n",
      "  pyramidnet:\n",
      "    depth: 272\n",
      "    n_blocks: [3, 24, 36, 3]\n",
      "    initial_channels: 16\n",
      "    block_type: bottleneck\n",
      "    alpha: 200\n",
      "  resnext:\n",
      "    depth: 29\n",
      "    n_blocks: [3, 4, 6, 3]\n",
      "    initial_channels: 64\n",
      "    cardinality: 8\n",
      "    base_channels: 4\n",
      "  shake_shake:\n",
      "    depth: 26\n",
      "    initial_channels: 96\n",
      "    shake_forward: True\n",
      "    shake_backward: True\n",
      "    shake_image: True\n",
      "  se_resnet_preact:\n",
      "    depth: 110\n",
      "    initial_channels: 16\n",
      "    se_reduction: 16\n",
      "    block_type: basic\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "train:\n",
      "  checkpoint: /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_20/exp00/checkpoint_00400.pth\n",
      "  resume: False\n",
      "  precision: O0\n",
      "  batch_size: 128\n",
      "  subdivision: 1\n",
      "  optimizer: sgd\n",
      "  base_lr: 0.001\n",
      "  momentum: 0.9\n",
      "  nesterov: True\n",
      "  weight_decay: 0.0005\n",
      "  no_weight_decay_on_bn: False\n",
      "  gradient_clip: 0\n",
      "  start_epoch: 0\n",
      "  seed: 0\n",
      "  val_first: True\n",
      "  val_period: 1\n",
      "  val_ratio: 0.1\n",
      "  use_test_as_val: False\n",
      "  output_dir: /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_20/exp00_resume400_50\n",
      "  log_period: 100\n",
      "  checkpoint_period: 100\n",
      "  use_tensorboard: True\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: True\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "  distributed: False\n",
      "  dist:\n",
      "    backend: nccl\n",
      "    init_method: env://\n",
      "    world_size: -1\n",
      "    node_rank: -1\n",
      "    local_rank: 0\n",
      "    use_sync_bn: False\n",
      "tensorboard:\n",
      "  train_images: False\n",
      "  val_images: False\n",
      "  model_params: False\n",
      "optim:\n",
      "  adam:\n",
      "    betas: (0.9, 0.999)\n",
      "  lars:\n",
      "    eps: 1e-09\n",
      "    threshold: 0.01\n",
      "  adabound:\n",
      "    betas: (0.9, 0.999)\n",
      "    final_lr: 0.1\n",
      "    gamma: 0.001\n",
      "scheduler:\n",
      "  epochs: 50\n",
      "  warmup:\n",
      "    type: none\n",
      "    epochs: 0\n",
      "    start_factor: 0.001\n",
      "    exponent: 4\n",
      "  type: multistep\n",
      "  milestones: [60, 120, 160]\n",
      "  lr_decay: 0.2\n",
      "  lr_min_factor: 0.001\n",
      "  T0: 10\n",
      "  T_mul: 1.0\n",
      "validation:\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: False\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "augmentation:\n",
      "  use_random_crop: True\n",
      "  use_random_horizontal_flip: True\n",
      "  use_cutout: False\n",
      "  use_random_erasing: False\n",
      "  use_dual_cutout: False\n",
      "  use_mixup: False\n",
      "  use_ricap: False\n",
      "  use_cutmix: False\n",
      "  use_label_smoothing: False\n",
      "  random_crop:\n",
      "    padding: 4\n",
      "    fill: 0\n",
      "    padding_mode: constant\n",
      "  random_horizontal_flip:\n",
      "    prob: 0.5\n",
      "  cutout:\n",
      "    prob: 1.0\n",
      "    mask_size: 16\n",
      "    cut_inside: False\n",
      "    mask_color: 0\n",
      "    dual_cutout_alpha: 0.1\n",
      "  random_erasing:\n",
      "    prob: 0.5\n",
      "    area_ratio_range: [0.02, 0.4]\n",
      "    min_aspect_ratio: 0.3\n",
      "    max_attempt: 20\n",
      "  mixup:\n",
      "    alpha: 1.0\n",
      "  ricap:\n",
      "    beta: 0.3\n",
      "  cutmix:\n",
      "    alpha: 1.0\n",
      "  label_smoothing:\n",
      "    epsilon: 0.1\n",
      "tta:\n",
      "  use_resize: False\n",
      "  use_center_crop: False\n",
      "  resize: 256\n",
      "test:\n",
      "  checkpoint: ''\n",
      "  output_dir: ''\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    pin_memory: False\n",
      "\u001b[32m[2020-06-23 21:22:10] __main__ INFO: \u001b[0menv_info:\n",
      "  pytorch_version: 1.4.0\n",
      "  cuda_version: 10.1\n",
      "  cudnn_version: 7603\n",
      "  num_gpus: 1\n",
      "  gpu_name: Tesla K80\n",
      "  gpu_capability: 3.7\n",
      "Files already downloaded and verified\n",
      "\u001b[32m[2020-06-23 21:22:15] __main__ INFO: \u001b[0mMACs  : 5.25G\n",
      "\u001b[32m[2020-06-23 21:22:15] __main__ INFO: \u001b[0m#params: 36.48M\n",
      "Selected optimization level O0:  Pure FP32 training.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\",)\n",
      "\u001b[32m[2020-06-23 21:22:15] __main__ INFO: \u001b[0mVal 0\n",
      "\u001b[32m[2020-06-23 21:22:35] __main__ INFO: \u001b[0mEpoch 0 loss 0.6997 acc@1 0.8318 acc@5 0.9882\n",
      "\u001b[32m[2020-06-23 21:22:35] __main__ INFO: \u001b[0mElapsed 20.01\n",
      "\u001b[32m[2020-06-23 21:22:35] __main__ INFO: \u001b[0mTrain 1 0\n",
      "\u001b[32m[2020-06-23 21:24:34] __main__ INFO: \u001b[0mEpoch 1 Step 100/351 lr 0.001000 loss 0.1580 (0.3052) acc@1 0.9375 (0.9086) acc@5 1.0000 (0.9956)\n",
      "\u001b[32m[2020-06-23 21:26:29] __main__ INFO: \u001b[0mEpoch 1 Step 200/351 lr 0.001000 loss 0.3502 (0.2849) acc@1 0.9062 (0.9127) acc@5 1.0000 (0.9965)\n",
      "\u001b[32m[2020-06-23 21:28:24] __main__ INFO: \u001b[0mEpoch 1 Step 300/351 lr 0.001000 loss 0.2202 (0.2677) acc@1 0.9297 (0.9169) acc@5 0.9922 (0.9966)\n",
      "\u001b[32m[2020-06-23 21:29:22] __main__ INFO: \u001b[0mEpoch 1 Step 351/351 lr 0.001000 loss 0.1890 (0.2612) acc@1 0.9297 (0.9185) acc@5 1.0000 (0.9968)\n",
      "\u001b[32m[2020-06-23 21:29:22] __main__ INFO: \u001b[0mElapsed 407.48\n",
      "\u001b[32m[2020-06-23 21:29:22] __main__ INFO: \u001b[0mVal 1\n",
      "\u001b[32m[2020-06-23 21:29:36] __main__ INFO: \u001b[0mEpoch 1 loss 0.2794 acc@1 0.9088 acc@5 0.9972\n",
      "\u001b[32m[2020-06-23 21:29:36] __main__ INFO: \u001b[0mElapsed 13.51\n",
      "\u001b[32m[2020-06-23 21:29:36] __main__ INFO: \u001b[0mTrain 2 351\n",
      "\u001b[32m[2020-06-23 21:31:31] __main__ INFO: \u001b[0mEpoch 2 Step 100/351 lr 0.001000 loss 0.1729 (0.1743) acc@1 0.9531 (0.9460) acc@5 0.9922 (0.9981)\n",
      "\u001b[32m[2020-06-23 21:33:25] __main__ INFO: \u001b[0mEpoch 2 Step 200/351 lr 0.001000 loss 0.1747 (0.1756) acc@1 0.9375 (0.9459) acc@5 1.0000 (0.9985)\n",
      "\u001b[32m[2020-06-23 21:35:20] __main__ INFO: \u001b[0mEpoch 2 Step 300/351 lr 0.001000 loss 0.1577 (0.1733) acc@1 0.9688 (0.9464) acc@5 1.0000 (0.9987)\n",
      "\u001b[32m[2020-06-23 21:36:18] __main__ INFO: \u001b[0mEpoch 2 Step 351/351 lr 0.001000 loss 0.2117 (0.1728) acc@1 0.9297 (0.9464) acc@5 0.9922 (0.9986)\n",
      "\u001b[32m[2020-06-23 21:36:18] __main__ INFO: \u001b[0mElapsed 402.56\n",
      "\u001b[32m[2020-06-23 21:36:18] __main__ INFO: \u001b[0mVal 2\n",
      "\u001b[32m[2020-06-23 21:36:32] __main__ INFO: \u001b[0mEpoch 2 loss 0.2692 acc@1 0.9132 acc@5 0.9968\n",
      "\u001b[32m[2020-06-23 21:36:32] __main__ INFO: \u001b[0mElapsed 13.50\n",
      "\u001b[32m[2020-06-23 21:36:32] __main__ INFO: \u001b[0mTrain 3 702\n",
      "\u001b[32m[2020-06-23 21:38:27] __main__ INFO: \u001b[0mEpoch 3 Step 100/351 lr 0.001000 loss 0.1097 (0.1266) acc@1 0.9688 (0.9599) acc@5 1.0000 (0.9995)\n",
      "\u001b[32m[2020-06-23 21:40:21] __main__ INFO: \u001b[0mEpoch 3 Step 200/351 lr 0.001000 loss 0.1284 (0.1323) acc@1 0.9609 (0.9593) acc@5 1.0000 (0.9993)\n",
      "\u001b[32m[2020-06-23 21:42:16] __main__ INFO: \u001b[0mEpoch 3 Step 300/351 lr 0.001000 loss 0.2124 (0.1337) acc@1 0.9297 (0.9591) acc@5 0.9844 (0.9993)\n",
      "\u001b[32m[2020-06-23 21:43:14] __main__ INFO: \u001b[0mEpoch 3 Step 351/351 lr 0.001000 loss 0.1848 (0.1347) acc@1 0.9375 (0.9592) acc@5 1.0000 (0.9992)\n",
      "\u001b[32m[2020-06-23 21:43:14] __main__ INFO: \u001b[0mElapsed 402.38\n",
      "\u001b[32m[2020-06-23 21:43:14] __main__ INFO: \u001b[0mVal 3\n",
      "\u001b[32m[2020-06-23 21:43:28] __main__ INFO: \u001b[0mEpoch 3 loss 0.2608 acc@1 0.9170 acc@5 0.9966\n",
      "\u001b[32m[2020-06-23 21:43:28] __main__ INFO: \u001b[0mElapsed 13.53\n",
      "\u001b[32m[2020-06-23 21:43:28] __main__ INFO: \u001b[0mTrain 4 1053\n",
      "\u001b[32m[2020-06-23 21:45:22] __main__ INFO: \u001b[0mEpoch 4 Step 100/351 lr 0.001000 loss 0.1520 (0.0977) acc@1 0.9453 (0.9723) acc@5 1.0000 (0.9995)\n",
      "\u001b[32m[2020-06-23 21:47:17] __main__ INFO: \u001b[0mEpoch 4 Step 200/351 lr 0.001000 loss 0.1540 (0.0993) acc@1 0.9609 (0.9710) acc@5 1.0000 (0.9995)\n",
      "\u001b[32m[2020-06-23 21:49:12] __main__ INFO: \u001b[0mEpoch 4 Step 300/351 lr 0.001000 loss 0.1157 (0.1013) acc@1 0.9609 (0.9708) acc@5 1.0000 (0.9993)\n",
      "\u001b[32m[2020-06-23 21:50:10] __main__ INFO: \u001b[0mEpoch 4 Step 351/351 lr 0.001000 loss 0.1954 (0.1038) acc@1 0.9375 (0.9701) acc@5 1.0000 (0.9993)\n",
      "\u001b[32m[2020-06-23 21:50:10] __main__ INFO: \u001b[0mElapsed 402.47\n",
      "\u001b[32m[2020-06-23 21:50:10] __main__ INFO: \u001b[0mVal 4\n",
      "\u001b[32m[2020-06-23 21:50:24] __main__ INFO: \u001b[0mEpoch 4 loss 0.2663 acc@1 0.9192 acc@5 0.9970\n",
      "\u001b[32m[2020-06-23 21:50:24] __main__ INFO: \u001b[0mElapsed 13.51\n",
      "\u001b[32m[2020-06-23 21:50:24] __main__ INFO: \u001b[0mTrain 5 1404\n",
      "\u001b[32m[2020-06-23 21:52:19] __main__ INFO: \u001b[0mEpoch 5 Step 100/351 lr 0.001000 loss 0.0481 (0.0853) acc@1 0.9844 (0.9766) acc@5 1.0000 (0.9995)\n",
      "\u001b[32m[2020-06-23 21:54:13] __main__ INFO: \u001b[0mEpoch 5 Step 200/351 lr 0.001000 loss 0.1013 (0.0835) acc@1 0.9609 (0.9762) acc@5 1.0000 (0.9994)\n",
      "\u001b[32m[2020-06-23 21:56:08] __main__ INFO: \u001b[0mEpoch 5 Step 300/351 lr 0.001000 loss 0.1236 (0.0860) acc@1 0.9453 (0.9752) acc@5 1.0000 (0.9995)\n",
      "\u001b[32m[2020-06-23 21:57:07] __main__ INFO: \u001b[0mEpoch 5 Step 351/351 lr 0.001000 loss 0.1098 (0.0853) acc@1 0.9766 (0.9756) acc@5 1.0000 (0.9995)\n",
      "\u001b[32m[2020-06-23 21:57:07] __main__ INFO: \u001b[0mElapsed 402.90\n",
      "\u001b[32m[2020-06-23 21:57:07] __main__ INFO: \u001b[0mVal 5\n",
      "\u001b[32m[2020-06-23 21:57:20] __main__ INFO: \u001b[0mEpoch 5 loss 0.2605 acc@1 0.9200 acc@5 0.9978\n",
      "\u001b[32m[2020-06-23 21:57:20] __main__ INFO: \u001b[0mElapsed 13.53\n",
      "\u001b[32m[2020-06-23 21:57:20] __main__ INFO: \u001b[0mTrain 6 1755\n",
      "\u001b[32m[2020-06-23 21:59:15] __main__ INFO: \u001b[0mEpoch 6 Step 100/351 lr 0.001000 loss 0.0486 (0.0671) acc@1 0.9922 (0.9818) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-23 22:01:10] __main__ INFO: \u001b[0mEpoch 6 Step 200/351 lr 0.001000 loss 0.0401 (0.0666) acc@1 0.9922 (0.9814) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-06-23 22:03:05] __main__ INFO: \u001b[0mEpoch 6 Step 300/351 lr 0.001000 loss 0.0433 (0.0673) acc@1 0.9922 (0.9813) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-06-23 22:04:03] __main__ INFO: \u001b[0mEpoch 6 Step 351/351 lr 0.001000 loss 0.0469 (0.0670) acc@1 0.9922 (0.9816) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-06-23 22:04:03] __main__ INFO: \u001b[0mElapsed 403.03\n",
      "\u001b[32m[2020-06-23 22:04:03] __main__ INFO: \u001b[0mVal 6\n",
      "\u001b[32m[2020-06-23 22:04:17] __main__ INFO: \u001b[0mEpoch 6 loss 0.2593 acc@1 0.9242 acc@5 0.9978\n",
      "\u001b[32m[2020-06-23 22:04:17] __main__ INFO: \u001b[0mElapsed 13.51\n",
      "\u001b[32m[2020-06-23 22:04:17] __main__ INFO: \u001b[0mTrain 7 2106\n",
      "\u001b[32m[2020-06-23 22:06:12] __main__ INFO: \u001b[0mEpoch 7 Step 100/351 lr 0.001000 loss 0.0641 (0.0608) acc@1 0.9844 (0.9844) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-06-23 22:08:06] __main__ INFO: \u001b[0mEpoch 7 Step 200/351 lr 0.001000 loss 0.0709 (0.0585) acc@1 0.9844 (0.9845) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-23 22:10:01] __main__ INFO: \u001b[0mEpoch 7 Step 300/351 lr 0.001000 loss 0.0349 (0.0572) acc@1 1.0000 (0.9850) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-23 22:11:00] __main__ INFO: \u001b[0mEpoch 7 Step 351/351 lr 0.001000 loss 0.0544 (0.0579) acc@1 0.9766 (0.9847) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-23 22:11:00] __main__ INFO: \u001b[0mElapsed 403.05\n",
      "\u001b[32m[2020-06-23 22:11:00] __main__ INFO: \u001b[0mVal 7\n",
      "\u001b[32m[2020-06-23 22:11:13] __main__ INFO: \u001b[0mEpoch 7 loss 0.2702 acc@1 0.9176 acc@5 0.9980\n",
      "\u001b[32m[2020-06-23 22:11:13] __main__ INFO: \u001b[0mElapsed 13.52\n",
      "\u001b[32m[2020-06-23 22:11:13] __main__ INFO: \u001b[0mTrain 8 2457\n",
      "\u001b[32m[2020-06-23 22:13:08] __main__ INFO: \u001b[0mEpoch 8 Step 100/351 lr 0.001000 loss 0.0183 (0.0474) acc@1 1.0000 (0.9882) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-23 22:15:03] __main__ INFO: \u001b[0mEpoch 8 Step 200/351 lr 0.001000 loss 0.0336 (0.0463) acc@1 1.0000 (0.9881) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-23 22:16:58] __main__ INFO: \u001b[0mEpoch 8 Step 300/351 lr 0.001000 loss 0.0458 (0.0472) acc@1 0.9844 (0.9877) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-23 22:17:56] __main__ INFO: \u001b[0mEpoch 8 Step 351/351 lr 0.001000 loss 0.0531 (0.0473) acc@1 0.9688 (0.9877) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-23 22:17:56] __main__ INFO: \u001b[0mElapsed 402.83\n",
      "\u001b[32m[2020-06-23 22:17:56] __main__ INFO: \u001b[0mVal 8\n",
      "\u001b[32m[2020-06-23 22:18:10] __main__ INFO: \u001b[0mEpoch 8 loss 0.2655 acc@1 0.9216 acc@5 0.9972\n",
      "\u001b[32m[2020-06-23 22:18:10] __main__ INFO: \u001b[0mElapsed 13.51\n",
      "\u001b[32m[2020-06-23 22:18:10] __main__ INFO: \u001b[0mTrain 9 2808\n",
      "\u001b[32m[2020-06-23 22:20:04] __main__ INFO: \u001b[0mEpoch 9 Step 100/351 lr 0.001000 loss 0.0404 (0.0382) acc@1 0.9844 (0.9909) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-06-23 22:21:59] __main__ INFO: \u001b[0mEpoch 9 Step 200/351 lr 0.001000 loss 0.0232 (0.0384) acc@1 1.0000 (0.9905) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-23 22:23:54] __main__ INFO: \u001b[0mEpoch 9 Step 300/351 lr 0.001000 loss 0.0392 (0.0386) acc@1 1.0000 (0.9905) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-23 22:24:52] __main__ INFO: \u001b[0mEpoch 9 Step 351/351 lr 0.001000 loss 0.0540 (0.0386) acc@1 0.9844 (0.9904) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-23 22:24:52] __main__ INFO: \u001b[0mElapsed 402.85\n",
      "\u001b[32m[2020-06-23 22:24:52] __main__ INFO: \u001b[0mVal 9\n",
      "\u001b[32m[2020-06-23 22:25:06] __main__ INFO: \u001b[0mEpoch 9 loss 0.2657 acc@1 0.9228 acc@5 0.9970\n",
      "\u001b[32m[2020-06-23 22:25:06] __main__ INFO: \u001b[0mElapsed 13.52\n",
      "\u001b[32m[2020-06-23 22:25:06] __main__ INFO: \u001b[0mTrain 10 3159\n",
      "\u001b[32m[2020-06-23 22:27:01] __main__ INFO: \u001b[0mEpoch 10 Step 100/351 lr 0.001000 loss 0.0584 (0.0307) acc@1 0.9922 (0.9937) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 22:28:56] __main__ INFO: \u001b[0mEpoch 10 Step 200/351 lr 0.001000 loss 0.0258 (0.0306) acc@1 1.0000 (0.9935) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 22:30:50] __main__ INFO: \u001b[0mEpoch 10 Step 300/351 lr 0.001000 loss 0.0158 (0.0318) acc@1 1.0000 (0.9926) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 22:31:49] __main__ INFO: \u001b[0mEpoch 10 Step 351/351 lr 0.001000 loss 0.0312 (0.0325) acc@1 1.0000 (0.9925) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 22:31:49] __main__ INFO: \u001b[0mElapsed 402.94\n",
      "\u001b[32m[2020-06-23 22:31:49] __main__ INFO: \u001b[0mVal 10\n",
      "\u001b[32m[2020-06-23 22:32:02] __main__ INFO: \u001b[0mEpoch 10 loss 0.2793 acc@1 0.9228 acc@5 0.9970\n",
      "\u001b[32m[2020-06-23 22:32:02] __main__ INFO: \u001b[0mElapsed 13.53\n",
      "\u001b[32m[2020-06-23 22:32:02] __main__ INFO: \u001b[0mTrain 11 3510\n",
      "\u001b[32m[2020-06-23 22:33:57] __main__ INFO: \u001b[0mEpoch 11 Step 100/351 lr 0.001000 loss 0.0151 (0.0226) acc@1 1.0000 (0.9955) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 22:35:52] __main__ INFO: \u001b[0mEpoch 11 Step 200/351 lr 0.001000 loss 0.0100 (0.0246) acc@1 1.0000 (0.9946) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 22:37:47] __main__ INFO: \u001b[0mEpoch 11 Step 300/351 lr 0.001000 loss 0.0333 (0.0258) acc@1 0.9922 (0.9942) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 22:38:45] __main__ INFO: \u001b[0mEpoch 11 Step 351/351 lr 0.001000 loss 0.0241 (0.0265) acc@1 0.9922 (0.9939) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 22:38:45] __main__ INFO: \u001b[0mElapsed 402.94\n",
      "\u001b[32m[2020-06-23 22:38:45] __main__ INFO: \u001b[0mVal 11\n",
      "\u001b[32m[2020-06-23 22:38:59] __main__ INFO: \u001b[0mEpoch 11 loss 0.2827 acc@1 0.9230 acc@5 0.9970\n",
      "\u001b[32m[2020-06-23 22:38:59] __main__ INFO: \u001b[0mElapsed 13.51\n",
      "\u001b[32m[2020-06-23 22:38:59] __main__ INFO: \u001b[0mTrain 12 3861\n",
      "\u001b[32m[2020-06-23 22:40:54] __main__ INFO: \u001b[0mEpoch 12 Step 100/351 lr 0.001000 loss 0.0311 (0.0244) acc@1 0.9922 (0.9943) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 22:42:49] __main__ INFO: \u001b[0mEpoch 12 Step 200/351 lr 0.001000 loss 0.0292 (0.0252) acc@1 0.9844 (0.9941) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 22:44:43] __main__ INFO: \u001b[0mEpoch 12 Step 300/351 lr 0.001000 loss 0.0077 (0.0251) acc@1 1.0000 (0.9943) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 22:45:42] __main__ INFO: \u001b[0mEpoch 12 Step 351/351 lr 0.001000 loss 0.0140 (0.0256) acc@1 1.0000 (0.9941) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 22:45:42] __main__ INFO: \u001b[0mElapsed 402.94\n",
      "\u001b[32m[2020-06-23 22:45:42] __main__ INFO: \u001b[0mVal 12\n",
      "\u001b[32m[2020-06-23 22:45:55] __main__ INFO: \u001b[0mEpoch 12 loss 0.2865 acc@1 0.9234 acc@5 0.9972\n",
      "\u001b[32m[2020-06-23 22:45:55] __main__ INFO: \u001b[0mElapsed 13.54\n",
      "\u001b[32m[2020-06-23 22:45:55] __main__ INFO: \u001b[0mTrain 13 4212\n",
      "\u001b[32m[2020-06-23 22:47:50] __main__ INFO: \u001b[0mEpoch 13 Step 100/351 lr 0.001000 loss 0.0412 (0.0211) acc@1 0.9922 (0.9955) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 22:49:45] __main__ INFO: \u001b[0mEpoch 13 Step 200/351 lr 0.001000 loss 0.0375 (0.0215) acc@1 0.9844 (0.9952) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 22:51:40] __main__ INFO: \u001b[0mEpoch 13 Step 300/351 lr 0.001000 loss 0.0151 (0.0210) acc@1 0.9922 (0.9955) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 22:52:38] __main__ INFO: \u001b[0mEpoch 13 Step 351/351 lr 0.001000 loss 0.0254 (0.0210) acc@1 0.9922 (0.9954) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 22:52:38] __main__ INFO: \u001b[0mElapsed 402.67\n",
      "\u001b[32m[2020-06-23 22:52:38] __main__ INFO: \u001b[0mVal 13\n",
      "\u001b[32m[2020-06-23 22:52:52] __main__ INFO: \u001b[0mEpoch 13 loss 0.2831 acc@1 0.9234 acc@5 0.9964\n",
      "\u001b[32m[2020-06-23 22:52:52] __main__ INFO: \u001b[0mElapsed 13.52\n",
      "\u001b[32m[2020-06-23 22:52:52] __main__ INFO: \u001b[0mTrain 14 4563\n",
      "\u001b[32m[2020-06-23 22:54:46] __main__ INFO: \u001b[0mEpoch 14 Step 100/351 lr 0.001000 loss 0.0422 (0.0203) acc@1 0.9844 (0.9950) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 22:56:41] __main__ INFO: \u001b[0mEpoch 14 Step 200/351 lr 0.001000 loss 0.0269 (0.0210) acc@1 0.9922 (0.9950) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 22:58:36] __main__ INFO: \u001b[0mEpoch 14 Step 300/351 lr 0.001000 loss 0.0246 (0.0213) acc@1 0.9844 (0.9946) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 22:59:34] __main__ INFO: \u001b[0mEpoch 14 Step 351/351 lr 0.001000 loss 0.0065 (0.0210) acc@1 1.0000 (0.9949) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 22:59:34] __main__ INFO: \u001b[0mElapsed 402.86\n",
      "\u001b[32m[2020-06-23 22:59:34] __main__ INFO: \u001b[0mVal 14\n",
      "\u001b[32m[2020-06-23 22:59:48] __main__ INFO: \u001b[0mEpoch 14 loss 0.2774 acc@1 0.9262 acc@5 0.9974\n",
      "\u001b[32m[2020-06-23 22:59:48] __main__ INFO: \u001b[0mElapsed 13.52\n",
      "\u001b[32m[2020-06-23 22:59:48] __main__ INFO: \u001b[0mTrain 15 4914\n",
      "\u001b[32m[2020-06-23 23:01:43] __main__ INFO: \u001b[0mEpoch 15 Step 100/351 lr 0.001000 loss 0.0071 (0.0156) acc@1 1.0000 (0.9970) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:03:37] __main__ INFO: \u001b[0mEpoch 15 Step 200/351 lr 0.001000 loss 0.0095 (0.0159) acc@1 1.0000 (0.9970) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:05:32] __main__ INFO: \u001b[0mEpoch 15 Step 300/351 lr 0.001000 loss 0.0049 (0.0161) acc@1 1.0000 (0.9971) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:06:31] __main__ INFO: \u001b[0mEpoch 15 Step 351/351 lr 0.001000 loss 0.0119 (0.0165) acc@1 1.0000 (0.9969) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:06:31] __main__ INFO: \u001b[0mElapsed 402.66\n",
      "\u001b[32m[2020-06-23 23:06:31] __main__ INFO: \u001b[0mVal 15\n",
      "\u001b[32m[2020-06-23 23:06:44] __main__ INFO: \u001b[0mEpoch 15 loss 0.2800 acc@1 0.9272 acc@5 0.9966\n",
      "\u001b[32m[2020-06-23 23:06:44] __main__ INFO: \u001b[0mElapsed 13.51\n",
      "\u001b[32m[2020-06-23 23:06:44] __main__ INFO: \u001b[0mTrain 16 5265\n",
      "\u001b[32m[2020-06-23 23:08:39] __main__ INFO: \u001b[0mEpoch 16 Step 100/351 lr 0.001000 loss 0.0086 (0.0140) acc@1 1.0000 (0.9975) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:10:33] __main__ INFO: \u001b[0mEpoch 16 Step 200/351 lr 0.001000 loss 0.0066 (0.0147) acc@1 1.0000 (0.9971) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:12:28] __main__ INFO: \u001b[0mEpoch 16 Step 300/351 lr 0.001000 loss 0.0080 (0.0156) acc@1 1.0000 (0.9967) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:13:26] __main__ INFO: \u001b[0mEpoch 16 Step 351/351 lr 0.001000 loss 0.0077 (0.0157) acc@1 1.0000 (0.9967) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:13:26] __main__ INFO: \u001b[0mElapsed 401.98\n",
      "\u001b[32m[2020-06-23 23:13:26] __main__ INFO: \u001b[0mVal 16\n",
      "\u001b[32m[2020-06-23 23:13:40] __main__ INFO: \u001b[0mEpoch 16 loss 0.2737 acc@1 0.9246 acc@5 0.9968\n",
      "\u001b[32m[2020-06-23 23:13:40] __main__ INFO: \u001b[0mElapsed 13.49\n",
      "\u001b[32m[2020-06-23 23:13:40] __main__ INFO: \u001b[0mTrain 17 5616\n",
      "\u001b[32m[2020-06-23 23:15:34] __main__ INFO: \u001b[0mEpoch 17 Step 100/351 lr 0.001000 loss 0.0118 (0.0124) acc@1 1.0000 (0.9977) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:17:29] __main__ INFO: \u001b[0mEpoch 17 Step 200/351 lr 0.001000 loss 0.0153 (0.0132) acc@1 1.0000 (0.9975) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:19:23] __main__ INFO: \u001b[0mEpoch 17 Step 300/351 lr 0.001000 loss 0.0224 (0.0129) acc@1 0.9922 (0.9977) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:20:22] __main__ INFO: \u001b[0mEpoch 17 Step 351/351 lr 0.001000 loss 0.0196 (0.0132) acc@1 0.9922 (0.9975) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:20:22] __main__ INFO: \u001b[0mElapsed 402.05\n",
      "\u001b[32m[2020-06-23 23:20:22] __main__ INFO: \u001b[0mVal 17\n",
      "\u001b[32m[2020-06-23 23:20:35] __main__ INFO: \u001b[0mEpoch 17 loss 0.2845 acc@1 0.9252 acc@5 0.9964\n",
      "\u001b[32m[2020-06-23 23:20:35] __main__ INFO: \u001b[0mElapsed 13.50\n",
      "\u001b[32m[2020-06-23 23:20:35] __main__ INFO: \u001b[0mTrain 18 5967\n",
      "\u001b[32m[2020-06-23 23:22:30] __main__ INFO: \u001b[0mEpoch 18 Step 100/351 lr 0.001000 loss 0.0077 (0.0151) acc@1 1.0000 (0.9971) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-06-23 23:24:24] __main__ INFO: \u001b[0mEpoch 18 Step 200/351 lr 0.001000 loss 0.0127 (0.0141) acc@1 0.9922 (0.9973) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:26:19] __main__ INFO: \u001b[0mEpoch 18 Step 300/351 lr 0.001000 loss 0.0130 (0.0142) acc@1 1.0000 (0.9974) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:27:17] __main__ INFO: \u001b[0mEpoch 18 Step 351/351 lr 0.001000 loss 0.0273 (0.0139) acc@1 0.9922 (0.9975) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:27:17] __main__ INFO: \u001b[0mElapsed 401.99\n",
      "\u001b[32m[2020-06-23 23:27:17] __main__ INFO: \u001b[0mVal 18\n",
      "\u001b[32m[2020-06-23 23:27:31] __main__ INFO: \u001b[0mEpoch 18 loss 0.2727 acc@1 0.9284 acc@5 0.9968\n",
      "\u001b[32m[2020-06-23 23:27:31] __main__ INFO: \u001b[0mElapsed 13.52\n",
      "\u001b[32m[2020-06-23 23:27:31] __main__ INFO: \u001b[0mTrain 19 6318\n",
      "\u001b[32m[2020-06-23 23:29:25] __main__ INFO: \u001b[0mEpoch 19 Step 100/351 lr 0.001000 loss 0.0232 (0.0135) acc@1 1.0000 (0.9970) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:31:20] __main__ INFO: \u001b[0mEpoch 19 Step 200/351 lr 0.001000 loss 0.0271 (0.0136) acc@1 0.9844 (0.9970) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:33:14] __main__ INFO: \u001b[0mEpoch 19 Step 300/351 lr 0.001000 loss 0.0118 (0.0140) acc@1 1.0000 (0.9969) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:34:13] __main__ INFO: \u001b[0mEpoch 19 Step 351/351 lr 0.001000 loss 0.0061 (0.0139) acc@1 1.0000 (0.9970) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:34:13] __main__ INFO: \u001b[0mElapsed 402.12\n",
      "\u001b[32m[2020-06-23 23:34:13] __main__ INFO: \u001b[0mVal 19\n",
      "\u001b[32m[2020-06-23 23:34:26] __main__ INFO: \u001b[0mEpoch 19 loss 0.2898 acc@1 0.9210 acc@5 0.9972\n",
      "\u001b[32m[2020-06-23 23:34:26] __main__ INFO: \u001b[0mElapsed 13.51\n",
      "\u001b[32m[2020-06-23 23:34:26] __main__ INFO: \u001b[0mTrain 20 6669\n",
      "\u001b[32m[2020-06-23 23:36:21] __main__ INFO: \u001b[0mEpoch 20 Step 100/351 lr 0.001000 loss 0.0105 (0.0130) acc@1 1.0000 (0.9977) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:38:15] __main__ INFO: \u001b[0mEpoch 20 Step 200/351 lr 0.001000 loss 0.0065 (0.0126) acc@1 1.0000 (0.9977) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:40:10] __main__ INFO: \u001b[0mEpoch 20 Step 300/351 lr 0.001000 loss 0.0054 (0.0123) acc@1 1.0000 (0.9978) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:41:08] __main__ INFO: \u001b[0mEpoch 20 Step 351/351 lr 0.001000 loss 0.0052 (0.0121) acc@1 1.0000 (0.9978) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:41:08] __main__ INFO: \u001b[0mElapsed 401.85\n",
      "\u001b[32m[2020-06-23 23:41:08] __main__ INFO: \u001b[0mVal 20\n",
      "\u001b[32m[2020-06-23 23:41:22] __main__ INFO: \u001b[0mEpoch 20 loss 0.2844 acc@1 0.9264 acc@5 0.9968\n",
      "\u001b[32m[2020-06-23 23:41:22] __main__ INFO: \u001b[0mElapsed 13.50\n",
      "\u001b[32m[2020-06-23 23:41:22] __main__ INFO: \u001b[0mTrain 21 7020\n",
      "\u001b[32m[2020-06-23 23:43:16] __main__ INFO: \u001b[0mEpoch 21 Step 100/351 lr 0.001000 loss 0.0089 (0.0104) acc@1 1.0000 (0.9984) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:45:11] __main__ INFO: \u001b[0mEpoch 21 Step 200/351 lr 0.001000 loss 0.0194 (0.0101) acc@1 0.9922 (0.9984) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:47:05] __main__ INFO: \u001b[0mEpoch 21 Step 300/351 lr 0.001000 loss 0.0038 (0.0098) acc@1 1.0000 (0.9985) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:48:03] __main__ INFO: \u001b[0mEpoch 21 Step 351/351 lr 0.001000 loss 0.0252 (0.0103) acc@1 0.9844 (0.9983) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:48:03] __main__ INFO: \u001b[0mElapsed 401.83\n",
      "\u001b[32m[2020-06-23 23:48:03] __main__ INFO: \u001b[0mVal 21\n",
      "\u001b[32m[2020-06-23 23:48:17] __main__ INFO: \u001b[0mEpoch 21 loss 0.2783 acc@1 0.9272 acc@5 0.9972\n",
      "\u001b[32m[2020-06-23 23:48:17] __main__ INFO: \u001b[0mElapsed 13.51\n",
      "\u001b[32m[2020-06-23 23:48:17] __main__ INFO: \u001b[0mTrain 22 7371\n",
      "\u001b[32m[2020-06-23 23:50:12] __main__ INFO: \u001b[0mEpoch 22 Step 100/351 lr 0.001000 loss 0.0053 (0.0097) acc@1 1.0000 (0.9985) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:52:06] __main__ INFO: \u001b[0mEpoch 22 Step 200/351 lr 0.001000 loss 0.0404 (0.0094) acc@1 0.9922 (0.9985) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:54:00] __main__ INFO: \u001b[0mEpoch 22 Step 300/351 lr 0.001000 loss 0.0086 (0.0097) acc@1 1.0000 (0.9983) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:54:59] __main__ INFO: \u001b[0mEpoch 22 Step 351/351 lr 0.001000 loss 0.0047 (0.0095) acc@1 1.0000 (0.9984) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:54:59] __main__ INFO: \u001b[0mElapsed 401.82\n",
      "\u001b[32m[2020-06-23 23:54:59] __main__ INFO: \u001b[0mVal 22\n",
      "\u001b[32m[2020-06-23 23:55:12] __main__ INFO: \u001b[0mEpoch 22 loss 0.2860 acc@1 0.9256 acc@5 0.9972\n",
      "\u001b[32m[2020-06-23 23:55:12] __main__ INFO: \u001b[0mElapsed 13.50\n",
      "\u001b[32m[2020-06-23 23:55:12] __main__ INFO: \u001b[0mTrain 23 7722\n",
      "\u001b[32m[2020-06-23 23:57:07] __main__ INFO: \u001b[0mEpoch 23 Step 100/351 lr 0.001000 loss 0.0067 (0.0091) acc@1 1.0000 (0.9981) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-23 23:59:01] __main__ INFO: \u001b[0mEpoch 23 Step 200/351 lr 0.001000 loss 0.0058 (0.0093) acc@1 1.0000 (0.9982) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:00:56] __main__ INFO: \u001b[0mEpoch 23 Step 300/351 lr 0.001000 loss 0.0050 (0.0092) acc@1 1.0000 (0.9982) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:01:54] __main__ INFO: \u001b[0mEpoch 23 Step 351/351 lr 0.001000 loss 0.0046 (0.0092) acc@1 1.0000 (0.9982) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:01:54] __main__ INFO: \u001b[0mElapsed 401.77\n",
      "\u001b[32m[2020-06-24 00:01:54] __main__ INFO: \u001b[0mVal 23\n",
      "\u001b[32m[2020-06-24 00:02:08] __main__ INFO: \u001b[0mEpoch 23 loss 0.2863 acc@1 0.9262 acc@5 0.9974\n",
      "\u001b[32m[2020-06-24 00:02:08] __main__ INFO: \u001b[0mElapsed 13.49\n",
      "\u001b[32m[2020-06-24 00:02:08] __main__ INFO: \u001b[0mTrain 24 8073\n",
      "\u001b[32m[2020-06-24 00:04:02] __main__ INFO: \u001b[0mEpoch 24 Step 100/351 lr 0.001000 loss 0.0101 (0.0100) acc@1 1.0000 (0.9980) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:05:56] __main__ INFO: \u001b[0mEpoch 24 Step 200/351 lr 0.001000 loss 0.0102 (0.0105) acc@1 1.0000 (0.9981) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:07:51] __main__ INFO: \u001b[0mEpoch 24 Step 300/351 lr 0.001000 loss 0.0068 (0.0103) acc@1 1.0000 (0.9979) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:08:49] __main__ INFO: \u001b[0mEpoch 24 Step 351/351 lr 0.001000 loss 0.0038 (0.0104) acc@1 1.0000 (0.9979) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:08:49] __main__ INFO: \u001b[0mElapsed 401.62\n",
      "\u001b[32m[2020-06-24 00:08:49] __main__ INFO: \u001b[0mVal 24\n",
      "\u001b[32m[2020-06-24 00:09:03] __main__ INFO: \u001b[0mEpoch 24 loss 0.3071 acc@1 0.9232 acc@5 0.9968\n",
      "\u001b[32m[2020-06-24 00:09:03] __main__ INFO: \u001b[0mElapsed 13.49\n",
      "\u001b[32m[2020-06-24 00:09:03] __main__ INFO: \u001b[0mTrain 25 8424\n",
      "\u001b[32m[2020-06-24 00:10:57] __main__ INFO: \u001b[0mEpoch 25 Step 100/351 lr 0.001000 loss 0.0060 (0.0101) acc@1 1.0000 (0.9980) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:12:51] __main__ INFO: \u001b[0mEpoch 25 Step 200/351 lr 0.001000 loss 0.0129 (0.0098) acc@1 1.0000 (0.9980) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:14:46] __main__ INFO: \u001b[0mEpoch 25 Step 300/351 lr 0.001000 loss 0.0461 (0.0101) acc@1 0.9922 (0.9980) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:15:44] __main__ INFO: \u001b[0mEpoch 25 Step 351/351 lr 0.001000 loss 0.0051 (0.0100) acc@1 1.0000 (0.9980) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:15:44] __main__ INFO: \u001b[0mElapsed 401.46\n",
      "\u001b[32m[2020-06-24 00:15:44] __main__ INFO: \u001b[0mVal 25\n",
      "\u001b[32m[2020-06-24 00:15:58] __main__ INFO: \u001b[0mEpoch 25 loss 0.2934 acc@1 0.9286 acc@5 0.9964\n",
      "\u001b[32m[2020-06-24 00:15:58] __main__ INFO: \u001b[0mElapsed 13.50\n",
      "\u001b[32m[2020-06-24 00:15:58] __main__ INFO: \u001b[0mTrain 26 8775\n",
      "\u001b[32m[2020-06-24 00:17:52] __main__ INFO: \u001b[0mEpoch 26 Step 100/351 lr 0.001000 loss 0.0069 (0.0084) acc@1 1.0000 (0.9990) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:19:46] __main__ INFO: \u001b[0mEpoch 26 Step 200/351 lr 0.001000 loss 0.0053 (0.0079) acc@1 1.0000 (0.9988) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:21:41] __main__ INFO: \u001b[0mEpoch 26 Step 300/351 lr 0.001000 loss 0.0044 (0.0081) acc@1 1.0000 (0.9988) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:22:39] __main__ INFO: \u001b[0mEpoch 26 Step 351/351 lr 0.001000 loss 0.0043 (0.0078) acc@1 1.0000 (0.9988) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:22:39] __main__ INFO: \u001b[0mElapsed 401.16\n",
      "\u001b[32m[2020-06-24 00:22:39] __main__ INFO: \u001b[0mVal 26\n",
      "\u001b[32m[2020-06-24 00:22:52] __main__ INFO: \u001b[0mEpoch 26 loss 0.2789 acc@1 0.9272 acc@5 0.9972\n",
      "\u001b[32m[2020-06-24 00:22:52] __main__ INFO: \u001b[0mElapsed 13.48\n",
      "\u001b[32m[2020-06-24 00:22:52] __main__ INFO: \u001b[0mTrain 27 9126\n",
      "\u001b[32m[2020-06-24 00:24:47] __main__ INFO: \u001b[0mEpoch 27 Step 100/351 lr 0.001000 loss 0.0102 (0.0075) acc@1 0.9922 (0.9988) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:26:41] __main__ INFO: \u001b[0mEpoch 27 Step 200/351 lr 0.001000 loss 0.0060 (0.0072) acc@1 1.0000 (0.9989) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:28:35] __main__ INFO: \u001b[0mEpoch 27 Step 300/351 lr 0.001000 loss 0.0283 (0.0074) acc@1 0.9922 (0.9988) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:29:33] __main__ INFO: \u001b[0mEpoch 27 Step 351/351 lr 0.001000 loss 0.0208 (0.0074) acc@1 0.9844 (0.9988) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:29:33] __main__ INFO: \u001b[0mElapsed 401.09\n",
      "\u001b[32m[2020-06-24 00:29:33] __main__ INFO: \u001b[0mVal 27\n",
      "\u001b[32m[2020-06-24 00:29:47] __main__ INFO: \u001b[0mEpoch 27 loss 0.2946 acc@1 0.9270 acc@5 0.9968\n",
      "\u001b[32m[2020-06-24 00:29:47] __main__ INFO: \u001b[0mElapsed 13.47\n",
      "\u001b[32m[2020-06-24 00:29:47] __main__ INFO: \u001b[0mTrain 28 9477\n",
      "\u001b[32m[2020-06-24 00:31:41] __main__ INFO: \u001b[0mEpoch 28 Step 100/351 lr 0.001000 loss 0.0064 (0.0070) acc@1 1.0000 (0.9990) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:33:36] __main__ INFO: \u001b[0mEpoch 28 Step 200/351 lr 0.001000 loss 0.0050 (0.0077) acc@1 1.0000 (0.9988) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:35:30] __main__ INFO: \u001b[0mEpoch 28 Step 300/351 lr 0.001000 loss 0.0029 (0.0071) acc@1 1.0000 (0.9991) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:36:28] __main__ INFO: \u001b[0mEpoch 28 Step 351/351 lr 0.001000 loss 0.0052 (0.0072) acc@1 1.0000 (0.9990) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:36:28] __main__ INFO: \u001b[0mElapsed 401.25\n",
      "\u001b[32m[2020-06-24 00:36:28] __main__ INFO: \u001b[0mVal 28\n",
      "\u001b[32m[2020-06-24 00:36:42] __main__ INFO: \u001b[0mEpoch 28 loss 0.2939 acc@1 0.9270 acc@5 0.9964\n",
      "\u001b[32m[2020-06-24 00:36:42] __main__ INFO: \u001b[0mElapsed 13.49\n",
      "\u001b[32m[2020-06-24 00:36:42] __main__ INFO: \u001b[0mTrain 29 9828\n",
      "\u001b[32m[2020-06-24 00:38:36] __main__ INFO: \u001b[0mEpoch 29 Step 100/351 lr 0.001000 loss 0.0036 (0.0067) acc@1 1.0000 (0.9990) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:40:30] __main__ INFO: \u001b[0mEpoch 29 Step 200/351 lr 0.001000 loss 0.0110 (0.0073) acc@1 0.9922 (0.9987) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:42:25] __main__ INFO: \u001b[0mEpoch 29 Step 300/351 lr 0.001000 loss 0.0071 (0.0070) acc@1 1.0000 (0.9989) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:43:23] __main__ INFO: \u001b[0mEpoch 29 Step 351/351 lr 0.001000 loss 0.0047 (0.0071) acc@1 1.0000 (0.9988) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:43:23] __main__ INFO: \u001b[0mElapsed 401.24\n",
      "\u001b[32m[2020-06-24 00:43:23] __main__ INFO: \u001b[0mVal 29\n",
      "\u001b[32m[2020-06-24 00:43:36] __main__ INFO: \u001b[0mEpoch 29 loss 0.2891 acc@1 0.9296 acc@5 0.9968\n",
      "\u001b[32m[2020-06-24 00:43:36] __main__ INFO: \u001b[0mElapsed 13.48\n",
      "\u001b[32m[2020-06-24 00:43:36] __main__ INFO: \u001b[0mTrain 30 10179\n",
      "\u001b[32m[2020-06-24 00:45:31] __main__ INFO: \u001b[0mEpoch 30 Step 100/351 lr 0.001000 loss 0.0112 (0.0066) acc@1 0.9922 (0.9989) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:47:25] __main__ INFO: \u001b[0mEpoch 30 Step 200/351 lr 0.001000 loss 0.0043 (0.0063) acc@1 1.0000 (0.9990) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:49:19] __main__ INFO: \u001b[0mEpoch 30 Step 300/351 lr 0.001000 loss 0.0041 (0.0060) acc@1 1.0000 (0.9991) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:50:17] __main__ INFO: \u001b[0mEpoch 30 Step 351/351 lr 0.001000 loss 0.0026 (0.0059) acc@1 1.0000 (0.9992) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:50:17] __main__ INFO: \u001b[0mElapsed 401.18\n",
      "\u001b[32m[2020-06-24 00:50:17] __main__ INFO: \u001b[0mVal 30\n",
      "\u001b[32m[2020-06-24 00:50:31] __main__ INFO: \u001b[0mEpoch 30 loss 0.2761 acc@1 0.9312 acc@5 0.9974\n",
      "\u001b[32m[2020-06-24 00:50:31] __main__ INFO: \u001b[0mElapsed 13.50\n",
      "\u001b[32m[2020-06-24 00:50:31] __main__ INFO: \u001b[0mTrain 31 10530\n",
      "\u001b[32m[2020-06-24 00:52:25] __main__ INFO: \u001b[0mEpoch 31 Step 100/351 lr 0.001000 loss 0.0038 (0.0053) acc@1 1.0000 (0.9994) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:54:20] __main__ INFO: \u001b[0mEpoch 31 Step 200/351 lr 0.001000 loss 0.0038 (0.0059) acc@1 1.0000 (0.9991) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:56:14] __main__ INFO: \u001b[0mEpoch 31 Step 300/351 lr 0.001000 loss 0.0038 (0.0058) acc@1 1.0000 (0.9992) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:57:12] __main__ INFO: \u001b[0mEpoch 31 Step 351/351 lr 0.001000 loss 0.0029 (0.0060) acc@1 1.0000 (0.9992) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 00:57:12] __main__ INFO: \u001b[0mElapsed 401.16\n",
      "\u001b[32m[2020-06-24 00:57:12] __main__ INFO: \u001b[0mVal 31\n",
      "\u001b[32m[2020-06-24 00:57:26] __main__ INFO: \u001b[0mEpoch 31 loss 0.2894 acc@1 0.9292 acc@5 0.9968\n",
      "\u001b[32m[2020-06-24 00:57:26] __main__ INFO: \u001b[0mElapsed 13.51\n",
      "\u001b[32m[2020-06-24 00:57:26] __main__ INFO: \u001b[0mTrain 32 10881\n",
      "\u001b[32m[2020-06-24 00:59:20] __main__ INFO: \u001b[0mEpoch 32 Step 100/351 lr 0.001000 loss 0.0067 (0.0069) acc@1 1.0000 (0.9988) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 01:01:14] __main__ INFO: \u001b[0mEpoch 32 Step 200/351 lr 0.001000 loss 0.0049 (0.0067) acc@1 1.0000 (0.9990) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 01:03:09] __main__ INFO: \u001b[0mEpoch 32 Step 300/351 lr 0.001000 loss 0.0052 (0.0068) acc@1 1.0000 (0.9988) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 01:04:07] __main__ INFO: \u001b[0mEpoch 32 Step 351/351 lr 0.001000 loss 0.0297 (0.0068) acc@1 0.9844 (0.9988) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 01:04:07] __main__ INFO: \u001b[0mElapsed 401.17\n",
      "\u001b[32m[2020-06-24 01:04:07] __main__ INFO: \u001b[0mVal 32\n",
      "\u001b[32m[2020-06-24 01:04:20] __main__ INFO: \u001b[0mEpoch 32 loss 0.2817 acc@1 0.9296 acc@5 0.9976\n",
      "\u001b[32m[2020-06-24 01:04:20] __main__ INFO: \u001b[0mElapsed 13.47\n",
      "\u001b[32m[2020-06-24 01:04:20] __main__ INFO: \u001b[0mTrain 33 11232\n",
      "\u001b[32m[2020-06-24 01:06:15] __main__ INFO: \u001b[0mEpoch 33 Step 100/351 lr 0.001000 loss 0.0046 (0.0061) acc@1 1.0000 (0.9992) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 01:08:09] __main__ INFO: \u001b[0mEpoch 33 Step 200/351 lr 0.001000 loss 0.0032 (0.0059) acc@1 1.0000 (0.9991) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 01:10:03] __main__ INFO: \u001b[0mEpoch 33 Step 300/351 lr 0.001000 loss 0.0059 (0.0057) acc@1 1.0000 (0.9993) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 01:11:01] __main__ INFO: \u001b[0mEpoch 33 Step 351/351 lr 0.001000 loss 0.0022 (0.0056) acc@1 1.0000 (0.9993) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-06-24 01:11:01] __main__ INFO: \u001b[0mElapsed 401.06\n",
      "\u001b[32m[2020-06-24 01:11:01] __main__ INFO: \u001b[0mVal 33\n",
      "\u001b[32m[2020-06-24 01:11:15] __main__ INFO: \u001b[0mEpoch 33 loss 0.2903 acc@1 0.9288 acc@5 0.9976\n",
      "\u001b[32m[2020-06-24 01:11:15] __main__ INFO: \u001b[0mElapsed 13.49\n",
      "\u001b[32m[2020-06-24 01:11:15] __main__ INFO: \u001b[0mTrain 34 11583\n",
      "\u001b[32m[2020-06-24 01:13:09] __main__ INFO: \u001b[0mEpoch 34 Step 100/351 lr 0.001000 loss 0.0054 (0.0052) acc@1 1.0000 (0.9993) acc@5 1.0000 (1.0000)\n"
     ]
    }
   ],
   "source": [
    "# Resume training with the un-augmented data\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "#!python train.py --config configs/cifar/resnet.yaml \\\n",
    "!python train.py --config /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_20/exp00/config.yaml \\\n",
    "    train.checkpoint /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_20/exp00/checkpoint_00400.pth \\\n",
    "    dataset.name CIFAR10 \\\n",
    "    train.base_lr .001 \\\n",
    "    train.output_dir /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_20/exp00_resume400_50 \\\n",
    "    scheduler.epochs 50\n",
    "\n",
    "#### Set LEARNING RATE based on ending LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-25 22:12:09] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_20/exp00_resume400_50/checkpoint_00050.pth\n",
      "Files already downloaded and verified\n",
      "100%|| 79/79 [00:27<00:00,  2.89it/s]\n",
      "\u001b[32m[2020-06-25 22:12:37] __main__ INFO: \u001b[0mElapsed 27.31\n",
      "\u001b[32m[2020-06-25 22:12:37] __main__ INFO: \u001b[0mLoss 0.2885 Accuracy 0.9315\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10 test dataset \n",
    "# write the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/wrn.yaml \\\n",
    "   model.resnet.depth 32 \\\n",
    "   test.batch_size 128 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_20/exp00_resume400_50/checkpoint_00050.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_20/exp00_resume400_50/test_results_0050_cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-25 22:14:31] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_20/exp00_resume400_50/checkpoint_00050.pth\n",
      "CIFAR 10.1\n",
      "100%|| 16/16 [00:05<00:00,  2.77it/s]\n",
      "\u001b[32m[2020-06-25 22:14:38] __main__ INFO: \u001b[0mElapsed 5.77\n",
      "\u001b[32m[2020-06-25 22:14:38] __main__ INFO: \u001b[0mLoss 0.6531 Accuracy 0.8470\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10.1 test dataset \n",
    "# write the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/wrn.yaml \\\n",
    "   model.resnet.depth 32 \\\n",
    "   test.batch_size 128 \\\n",
    "   dataset.name CIFAR101 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_20/exp00_resume400_50/checkpoint_00050.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_20/exp00_resume400_50/test_results_0050_cifar101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-25 22:16:42] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_20/exp00/checkpoint_00400.pth\n",
      "Files already downloaded and verified\n",
      "100%|| 79/79 [00:27<00:00,  2.90it/s]\n",
      "\u001b[32m[2020-06-25 22:17:11] __main__ INFO: \u001b[0mElapsed 27.29\n",
      "\u001b[32m[2020-06-25 22:17:11] __main__ INFO: \u001b[0mLoss 0.6982 Accuracy 0.8337\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10 test dataset \n",
    "# write the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/wrn.yaml \\\n",
    "   model.resnet.depth 32 \\\n",
    "   test.batch_size 128 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_20/exp00/checkpoint_00400.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_20/exp00/test_results_0400_cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-25 22:18:26] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_20/exp00/checkpoint_00400.pth\n",
      "CIFAR 10.1\n",
      "100%|| 16/16 [00:05<00:00,  2.78it/s]\n",
      "\u001b[32m[2020-06-25 22:18:33] __main__ INFO: \u001b[0mElapsed 5.75\n",
      "\u001b[32m[2020-06-25 22:18:33] __main__ INFO: \u001b[0mLoss 1.3437 Accuracy 0.6940\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10.1 test dataset \n",
    "# write the results to the test output directory specified.\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "!python evaluate.py --config configs/cifar/wrn.yaml \\\n",
    "   model.resnet.depth 32 \\\n",
    "   test.batch_size 128 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_20/exp00/checkpoint_00400.pth \\\n",
    "   dataset.name CIFAR101 \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_20/exp00/test_results_0400_cifar101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Testset</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Original_Accuracy</th>\n",
       "      <th>Original_CI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wrn_28_10_ra_2_20</td>\n",
       "      <td>400</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>1.6982</td>\n",
       "      <td>0.8337</td>\n",
       "      <td>95.9</td>\n",
       "      <td>(95.5, 96.3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wrn_28_10_ra_2_20</td>\n",
       "      <td>400</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>1.3437</td>\n",
       "      <td>0.694</td>\n",
       "      <td>89.7</td>\n",
       "      <td>(88.3, 91.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wrn_28_10_ra_2_20_refined400</td>\n",
       "      <td>50</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>0.6531</td>\n",
       "      <td>0.847</td>\n",
       "      <td>89.7</td>\n",
       "      <td>(88.3, 91.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wrn_28_10_ra_2_20_refined400</td>\n",
       "      <td>50</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.2885</td>\n",
       "      <td>0.9315</td>\n",
       "      <td>95.9</td>\n",
       "      <td>(95.5, 96.3)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model Epoch    Testset    Loss Accuracy  \\\n",
       "0             wrn_28_10_ra_2_20   400    cifar10  1.6982   0.8337   \n",
       "1             wrn_28_10_ra_2_20   400  cifar10.1  1.3437    0.694   \n",
       "2  wrn_28_10_ra_2_20_refined400    50  cifar10.1  0.6531    0.847   \n",
       "3  wrn_28_10_ra_2_20_refined400    50    cifar10  0.2885   0.9315   \n",
       "\n",
       "   Original_Accuracy   Original_CI  \n",
       "0               95.9  (95.5, 96.3)  \n",
       "1               89.7  (88.3, 91.0)  \n",
       "2               89.7  (88.3, 91.0)  \n",
       "3               95.9  (95.5, 96.3)  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "a = pd.Series(['wrn_28_10_ra_2_20', 400, 'cifar10', 1.6982, 0.8337]) #Loss 0.6982 Accuracy 0.8337\n",
    "#b = pd.Series(['wrn_28_10_ra_2_20', 300, 'cifar10', 1.0633, 0.8138])\n",
    "c = pd.Series(['wrn_28_10_ra_2_20', 400, 'cifar10.1', 1.3437, 0.6940]) #Loss 1.3437 Accuracy 0.6940\n",
    "#d = pd.Series(['wrn_28_10_ra_2_20', 300, 'cifar10.1',  1.9379, 0.6705])\n",
    "\n",
    "\n",
    "e = pd.Series(['wrn_28_10_ra_2_20_refined400', 50, 'cifar10.1', 0.6531, 0.8470]) #Loss 0.6531 Accuracy 0.8470\n",
    "f = pd.Series(['wrn_28_10_ra_2_20_refined400', 50, 'cifar10', 0.2885,0.9315]) #Loss 0.2885 Accuracy 0.9315\n",
    "#g = pd.Series(['wrn_28_10_ra_2_20_refined300', 150, 'cifar10', 0.4499, 0.8795])\n",
    "#h = pd.Series(['wrn_28_10_ra_2_20_refined300', 150, 'cifar10.1', 0.8206, 0.7710])\n",
    "               \n",
    "df_results = pd.concat([a,c,e,f], axis=1).T\n",
    "df_results.columns = ['Model', 'Epoch', 'Testset', 'Loss', 'Accuracy']\n",
    "\n",
    "df_results['Original_Accuracy'] = df_results.apply((lambda row: 95.9 if row[2] == 'cifar10' else 89.7), axis=1)\n",
    "df_results['Original_CI'] = df_results.apply((lambda row: (95.5, 96.3) if row[2] == 'cifar10' else (88.3, 91.0)), axis=1)\n",
    "\n",
    "df_results.to_csv('/home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_20/results.csv')\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['preds', 'probs', 'labels', 'loss', 'acc']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ -7.153804  ,  -0.1832159 ,  -0.69570637, ...,  -0.50926757,\n",
       "         -5.526208  , -12.987257  ],\n",
       "       [  2.862379  ,   7.963458  ,  -6.603018  , ...,  -4.740323  ,\n",
       "         25.90399   ,  -0.52988565],\n",
       "       [  4.25749   ,   8.408992  ,  -4.3299227 , ...,  -2.3715498 ,\n",
       "         13.468082  ,   4.5792727 ],\n",
       "       ...,\n",
       "       [ -4.7270765 ,  -1.2400844 ,   1.3852903 , ...,  -0.51062894,\n",
       "         -3.399443  ,  -2.4969094 ],\n",
       "       [ -2.7640457 ,  14.635863  ,   6.7449965 , ...,  -1.3011913 ,\n",
       "         -3.036379  ,  -7.061736  ],\n",
       "       [ -2.6933427 ,   1.8961854 ,  -3.6396854 , ...,  18.63456   ,\n",
       "         -2.7524152 ,  -3.2204888 ]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Peak inside the output file for predictions\n",
    "import numpy as np\n",
    "output = '/home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_20/exp00/test_results_0160/predictions.npz'\n",
    "npzfile = np.load(output)\n",
    "print(npzfile.files)\n",
    "npzfile['preds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the model checkpoints, configs, and results to S3 \n",
    "bucket='sagemaker-may29'\n",
    "prefix = 'sagemaker/results/original-models/wrn_28_10_ra_2_20'\n",
    "path = '/home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_20'\n",
    "\n",
    "s3_resource = boto3.resource(\"s3\", region_name=\"us-east-2\")\n",
    "\n",
    "def uploadDirectory(local_path,bucket_name,s3_prefix):\n",
    "\n",
    "    my_bucket = s3_resource.Bucket(bucket_name)\n",
    "    \n",
    "    for path, subdirs, files in os.walk(local_path):\n",
    "        path = path.replace(\"\\\\\",\"/\")\n",
    "        directory_name = path.replace(local_path,\"\")\n",
    "        for file in files:\n",
    "            #print(\"Local File:\", os.path.join(path, file))\n",
    "            #print(\"      Dest:\", s3_prefix+directory_name+'/'+file)\n",
    "            my_bucket.upload_file(os.path.join(path, file), s3_prefix+directory_name+'/'+file)\n",
    "    \n",
    "uploadDirectory(path,bucket,prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
