{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wide Residual Net\n",
    "\n",
    " - Training Dataset:  RandAugment, N=1, M=20\n",
    "   Validation with Unaugmented Data\n",
    " - Sagemaker Notebook must be of type, conda_pytorch_p36\n",
    " \n",
    "#### Install Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import numpy \n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 1)) (1.18.1)\n",
      "Requirement already satisfied: torch>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 2)) (1.4.0)\n",
      "Requirement already satisfied: torchvision in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (0.5.0)\n",
      "Requirement already satisfied: fvcore in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (0.1.1.post20200711)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 5)) (4.44.1)\n",
      "Requirement already satisfied: yacs in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 6)) (0.1.7)\n",
      "Requirement already satisfied: apex from git+https://github.com/NVIDIA/apex.git#egg=apex in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 7)) (0.1)\n",
      "Requirement already satisfied: termcolor in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 8)) (1.1.0)\n",
      "Requirement already satisfied: thop<0.0.31.post2004070130 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 9)) (0.0.31.post2001170342)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (1.14.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (7.0.0)\n",
      "Requirement already satisfied: portalocker in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (1.7.0)\n",
      "Requirement already satisfied: tabulate in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (0.8.7)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (5.3.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: tensorboard in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (2.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (3.2.2)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.18.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.30.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.18.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (0.9.0)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (3.12.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (46.1.3.post20200330)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (2.23.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.7.0)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (0.34.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (0.4.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.14.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard) (1.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (4.1.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (3.4.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (2020.4.5.2)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (1.25.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (2.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard) (2.2.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r '/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt'\n",
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-15 02:15:16] __main__ INFO: \u001b[0mdevice: cuda\n",
      "cudnn:\n",
      "  benchmark: True\n",
      "  deterministic: False\n",
      "dataset:\n",
      "  name: CIFAR10_RA_1_20\n",
      "  dataset_dir: ''\n",
      "  image_size: 32\n",
      "  n_channels: 3\n",
      "  n_classes: 10\n",
      "model:\n",
      "  type: cifar\n",
      "  name: wrn\n",
      "  init_mode: kaiming_fan_in\n",
      "  vgg:\n",
      "    n_channels: [64, 128, 256, 512, 512]\n",
      "    n_layers: [2, 2, 3, 3, 3]\n",
      "    use_bn: True\n",
      "  resnet:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "  resnet_preact:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "  wrn:\n",
      "    depth: 28\n",
      "    initial_channels: 16\n",
      "    widening_factor: 10\n",
      "    drop_rate: 0.0\n",
      "  densenet:\n",
      "    depth: 100\n",
      "    n_blocks: [6, 12, 24, 16]\n",
      "    block_type: bottleneck\n",
      "    growth_rate: 12\n",
      "    drop_rate: 0.0\n",
      "    compression_rate: 0.5\n",
      "  pyramidnet:\n",
      "    depth: 272\n",
      "    n_blocks: [3, 24, 36, 3]\n",
      "    initial_channels: 16\n",
      "    block_type: bottleneck\n",
      "    alpha: 200\n",
      "  resnext:\n",
      "    depth: 29\n",
      "    n_blocks: [3, 4, 6, 3]\n",
      "    initial_channels: 64\n",
      "    cardinality: 8\n",
      "    base_channels: 4\n",
      "  shake_shake:\n",
      "    depth: 26\n",
      "    initial_channels: 96\n",
      "    shake_forward: True\n",
      "    shake_backward: True\n",
      "    shake_image: True\n",
      "  se_resnet_preact:\n",
      "    depth: 110\n",
      "    initial_channels: 16\n",
      "    se_reduction: 16\n",
      "    block_type: basic\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "train:\n",
      "  checkpoint: ''\n",
      "  resume: False\n",
      "  precision: O0\n",
      "  batch_size: 128\n",
      "  subdivision: 1\n",
      "  optimizer: sgd\n",
      "  base_lr: 0.1\n",
      "  momentum: 0.9\n",
      "  nesterov: True\n",
      "  weight_decay: 0.0005\n",
      "  no_weight_decay_on_bn: False\n",
      "  gradient_clip: 0\n",
      "  start_epoch: 0\n",
      "  seed: 0\n",
      "  val_first: True\n",
      "  val_period: 1\n",
      "  val_ratio: 0.1\n",
      "  use_test_as_val: False\n",
      "  output_dir: /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_1_20_c10val/exp00\n",
      "  log_period: 100\n",
      "  checkpoint_period: 100\n",
      "  use_tensorboard: True\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: True\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "  distributed: False\n",
      "  dist:\n",
      "    backend: nccl\n",
      "    init_method: env://\n",
      "    world_size: -1\n",
      "    node_rank: -1\n",
      "    local_rank: 0\n",
      "    use_sync_bn: False\n",
      "tensorboard:\n",
      "  train_images: False\n",
      "  val_images: False\n",
      "  model_params: False\n",
      "optim:\n",
      "  adam:\n",
      "    betas: (0.9, 0.999)\n",
      "  lars:\n",
      "    eps: 1e-09\n",
      "    threshold: 0.01\n",
      "  adabound:\n",
      "    betas: (0.9, 0.999)\n",
      "    final_lr: 0.1\n",
      "    gamma: 0.001\n",
      "scheduler:\n",
      "  epochs: 400\n",
      "  warmup:\n",
      "    type: none\n",
      "    epochs: 0\n",
      "    start_factor: 0.001\n",
      "    exponent: 4\n",
      "  type: multistep\n",
      "  milestones: [60, 120, 160]\n",
      "  lr_decay: 0.2\n",
      "  lr_min_factor: 0.001\n",
      "  T0: 10\n",
      "  T_mul: 1.0\n",
      "validation:\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: False\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "augmentation:\n",
      "  use_random_crop: True\n",
      "  use_random_horizontal_flip: True\n",
      "  use_cutout: False\n",
      "  use_random_erasing: False\n",
      "  use_dual_cutout: False\n",
      "  use_mixup: False\n",
      "  use_ricap: False\n",
      "  use_cutmix: False\n",
      "  use_label_smoothing: False\n",
      "  random_crop:\n",
      "    padding: 4\n",
      "    fill: 0\n",
      "    padding_mode: constant\n",
      "  random_horizontal_flip:\n",
      "    prob: 0.5\n",
      "  cutout:\n",
      "    prob: 1.0\n",
      "    mask_size: 16\n",
      "    cut_inside: False\n",
      "    mask_color: 0\n",
      "    dual_cutout_alpha: 0.1\n",
      "  random_erasing:\n",
      "    prob: 0.5\n",
      "    area_ratio_range: [0.02, 0.4]\n",
      "    min_aspect_ratio: 0.3\n",
      "    max_attempt: 20\n",
      "  mixup:\n",
      "    alpha: 1.0\n",
      "  ricap:\n",
      "    beta: 0.3\n",
      "  cutmix:\n",
      "    alpha: 1.0\n",
      "  label_smoothing:\n",
      "    epsilon: 0.1\n",
      "tta:\n",
      "  use_resize: False\n",
      "  use_center_crop: False\n",
      "  resize: 256\n",
      "test:\n",
      "  checkpoint: ''\n",
      "  output_dir: ''\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    pin_memory: False\n",
      "\u001b[32m[2020-07-15 02:15:16] __main__ INFO: \u001b[0menv_info:\n",
      "  pytorch_version: 1.4.0\n",
      "  cuda_version: 10.1\n",
      "  cudnn_version: 7603\n",
      "  num_gpus: 1\n",
      "  gpu_name: Tesla K80\n",
      "  gpu_capability: 3.7\n",
      "Files already downloaded and verified\n",
      "\u001b[32m[2020-07-15 02:15:22] __main__ INFO: \u001b[0mMACs  : 5.25G\n",
      "\u001b[32m[2020-07-15 02:15:22] __main__ INFO: \u001b[0m#params: 36.48M\n",
      "Selected optimization level O0:  Pure FP32 training.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\",)\n",
      "\u001b[32m[2020-07-15 02:15:22] __main__ INFO: \u001b[0mVal 0\n",
      "\u001b[32m[2020-07-15 02:15:41] __main__ INFO: \u001b[0mEpoch 0 loss 148.1538 acc@1 0.0968 acc@5 0.5518\n",
      "\u001b[32m[2020-07-15 02:15:41] __main__ INFO: \u001b[0mElapsed 18.45\n",
      "\u001b[32m[2020-07-15 02:15:41] __main__ INFO: \u001b[0mTrain 1 0\n",
      "\u001b[32m[2020-07-15 02:17:31] __main__ INFO: \u001b[0mEpoch 1 Step 100/351 lr 0.100000 loss 2.2076 (2.4105) acc@1 0.1641 (0.1375) acc@5 0.6875 (0.5862)\n",
      "\u001b[32m[2020-07-15 02:19:16] __main__ INFO: \u001b[0mEpoch 1 Step 200/351 lr 0.100000 loss 2.1195 (2.2811) acc@1 0.1641 (0.1626) acc@5 0.7422 (0.6372)\n",
      "\u001b[32m[2020-07-15 02:21:01] __main__ INFO: \u001b[0mEpoch 1 Step 300/351 lr 0.100000 loss 1.9606 (2.2056) acc@1 0.2969 (0.1839) acc@5 0.7969 (0.6672)\n",
      "\u001b[32m[2020-07-15 02:21:54] __main__ INFO: \u001b[0mEpoch 1 Step 351/351 lr 0.100000 loss 2.0504 (2.1783) acc@1 0.2188 (0.1935) acc@5 0.7188 (0.6792)\n",
      "\u001b[32m[2020-07-15 02:21:54] __main__ INFO: \u001b[0mElapsed 373.28\n",
      "\u001b[32m[2020-07-15 02:21:54] __main__ INFO: \u001b[0mVal 1\n",
      "\u001b[32m[2020-07-15 02:22:06] __main__ INFO: \u001b[0mEpoch 1 loss 1.8038 acc@1 0.3110 acc@5 0.8550\n",
      "\u001b[32m[2020-07-15 02:22:06] __main__ INFO: \u001b[0mElapsed 12.29\n",
      "\u001b[32m[2020-07-15 02:22:06] __main__ INFO: \u001b[0mTrain 2 351\n",
      "\u001b[32m[2020-07-15 02:23:51] __main__ INFO: \u001b[0mEpoch 2 Step 100/351 lr 0.100000 loss 1.8445 (1.9501) acc@1 0.3438 (0.2773) acc@5 0.8047 (0.7645)\n",
      "\u001b[32m[2020-07-15 02:25:35] __main__ INFO: \u001b[0mEpoch 2 Step 200/351 lr 0.100000 loss 1.7686 (1.9289) acc@1 0.3359 (0.2818) acc@5 0.8359 (0.7719)\n",
      "\u001b[32m[2020-07-15 02:27:20] __main__ INFO: \u001b[0mEpoch 2 Step 300/351 lr 0.100000 loss 1.7545 (1.8911) acc@1 0.3281 (0.2979) acc@5 0.8125 (0.7785)\n",
      "\u001b[32m[2020-07-15 02:28:13] __main__ INFO: \u001b[0mEpoch 2 Step 351/351 lr 0.100000 loss 1.6665 (1.8745) acc@1 0.3438 (0.3051) acc@5 0.8125 (0.7831)\n",
      "\u001b[32m[2020-07-15 02:28:13] __main__ INFO: \u001b[0mElapsed 366.58\n",
      "\u001b[32m[2020-07-15 02:28:13] __main__ INFO: \u001b[0mVal 2\n",
      "\u001b[32m[2020-07-15 02:28:25] __main__ INFO: \u001b[0mEpoch 2 loss 1.3505 acc@1 0.5284 acc@5 0.9296\n",
      "\u001b[32m[2020-07-15 02:28:25] __main__ INFO: \u001b[0mElapsed 12.31\n",
      "\u001b[32m[2020-07-15 02:28:25] __main__ INFO: \u001b[0mTrain 3 702\n",
      "\u001b[32m[2020-07-15 02:30:10] __main__ INFO: \u001b[0mEpoch 3 Step 100/351 lr 0.100000 loss 1.7445 (1.7330) acc@1 0.3906 (0.3643) acc@5 0.7656 (0.8150)\n",
      "\u001b[32m[2020-07-15 02:31:55] __main__ INFO: \u001b[0mEpoch 3 Step 200/351 lr 0.100000 loss 1.5208 (1.7075) acc@1 0.4297 (0.3728) acc@5 0.8906 (0.8165)\n",
      "\u001b[32m[2020-07-15 02:33:40] __main__ INFO: \u001b[0mEpoch 3 Step 300/351 lr 0.100000 loss 1.7368 (1.6769) acc@1 0.3594 (0.3832) acc@5 0.8047 (0.8210)\n",
      "\u001b[32m[2020-07-15 02:34:34] __main__ INFO: \u001b[0mEpoch 3 Step 351/351 lr 0.100000 loss 1.6537 (1.6616) acc@1 0.4219 (0.3884) acc@5 0.8750 (0.8237)\n",
      "\u001b[32m[2020-07-15 02:34:34] __main__ INFO: \u001b[0mElapsed 368.45\n",
      "\u001b[32m[2020-07-15 02:34:34] __main__ INFO: \u001b[0mVal 3\n",
      "\u001b[32m[2020-07-15 02:34:46] __main__ INFO: \u001b[0mEpoch 3 loss 1.5304 acc@1 0.5026 acc@5 0.8974\n",
      "\u001b[32m[2020-07-15 02:34:46] __main__ INFO: \u001b[0mElapsed 12.31\n",
      "\u001b[32m[2020-07-15 02:34:46] __main__ INFO: \u001b[0mTrain 4 1053\n",
      "\u001b[32m[2020-07-15 02:36:31] __main__ INFO: \u001b[0mEpoch 4 Step 100/351 lr 0.100000 loss 1.4327 (1.5517) acc@1 0.4453 (0.4299) acc@5 0.8516 (0.8410)\n",
      "\u001b[32m[2020-07-15 02:38:15] __main__ INFO: \u001b[0mEpoch 4 Step 200/351 lr 0.100000 loss 1.4271 (1.5243) acc@1 0.5078 (0.4419) acc@5 0.8594 (0.8454)\n",
      "\u001b[32m[2020-07-15 02:39:59] __main__ INFO: \u001b[0mEpoch 4 Step 300/351 lr 0.100000 loss 1.4348 (1.5061) acc@1 0.4609 (0.4497) acc@5 0.9062 (0.8480)\n",
      "\u001b[32m[2020-07-15 02:40:53] __main__ INFO: \u001b[0mEpoch 4 Step 351/351 lr 0.100000 loss 1.2910 (1.4948) acc@1 0.5156 (0.4542) acc@5 0.8750 (0.8484)\n",
      "\u001b[32m[2020-07-15 02:40:53] __main__ INFO: \u001b[0mElapsed 366.54\n",
      "\u001b[32m[2020-07-15 02:40:53] __main__ INFO: \u001b[0mVal 4\n",
      "\u001b[32m[2020-07-15 02:41:05] __main__ INFO: \u001b[0mEpoch 4 loss 0.9242 acc@1 0.6660 acc@5 0.9718\n",
      "\u001b[32m[2020-07-15 02:41:05] __main__ INFO: \u001b[0mElapsed 12.31\n",
      "\u001b[32m[2020-07-15 02:41:05] __main__ INFO: \u001b[0mTrain 5 1404\n",
      "\u001b[32m[2020-07-15 02:42:50] __main__ INFO: \u001b[0mEpoch 5 Step 100/351 lr 0.100000 loss 1.2365 (1.4161) acc@1 0.5781 (0.4896) acc@5 0.8750 (0.8601)\n",
      "\u001b[32m[2020-07-15 02:44:35] __main__ INFO: \u001b[0mEpoch 5 Step 200/351 lr 0.100000 loss 1.4168 (1.3982) acc@1 0.5156 (0.4962) acc@5 0.8516 (0.8596)\n",
      "\u001b[32m[2020-07-15 02:46:20] __main__ INFO: \u001b[0mEpoch 5 Step 300/351 lr 0.100000 loss 1.2646 (1.3851) acc@1 0.5391 (0.4988) acc@5 0.8750 (0.8614)\n",
      "\u001b[32m[2020-07-15 02:47:13] __main__ INFO: \u001b[0mEpoch 5 Step 351/351 lr 0.100000 loss 1.3079 (1.3826) acc@1 0.5156 (0.4994) acc@5 0.8672 (0.8615)\n",
      "\u001b[32m[2020-07-15 02:47:13] __main__ INFO: \u001b[0mElapsed 368.49\n",
      "\u001b[32m[2020-07-15 02:47:13] __main__ INFO: \u001b[0mVal 5\n",
      "\u001b[32m[2020-07-15 02:47:26] __main__ INFO: \u001b[0mEpoch 5 loss 0.9538 acc@1 0.6742 acc@5 0.9686\n",
      "\u001b[32m[2020-07-15 02:47:26] __main__ INFO: \u001b[0mElapsed 12.31\n",
      "\u001b[32m[2020-07-15 02:47:26] __main__ INFO: \u001b[0mTrain 6 1755\n",
      "\u001b[32m[2020-07-15 02:49:10] __main__ INFO: \u001b[0mEpoch 6 Step 100/351 lr 0.100000 loss 1.3338 (1.3329) acc@1 0.5156 (0.5102) acc@5 0.8672 (0.8685)\n",
      "\u001b[32m[2020-07-15 02:50:55] __main__ INFO: \u001b[0mEpoch 6 Step 200/351 lr 0.100000 loss 1.1995 (1.3221) acc@1 0.5703 (0.5182) acc@5 0.8906 (0.8711)\n",
      "\u001b[32m[2020-07-15 02:52:39] __main__ INFO: \u001b[0mEpoch 6 Step 300/351 lr 0.100000 loss 1.1828 (1.3181) acc@1 0.5781 (0.5192) acc@5 0.9219 (0.8695)\n",
      "\u001b[32m[2020-07-15 02:53:32] __main__ INFO: \u001b[0mEpoch 6 Step 351/351 lr 0.100000 loss 1.4209 (1.3157) acc@1 0.5078 (0.5198) acc@5 0.8516 (0.8699)\n",
      "\u001b[32m[2020-07-15 02:53:32] __main__ INFO: \u001b[0mElapsed 366.44\n",
      "\u001b[32m[2020-07-15 02:53:32] __main__ INFO: \u001b[0mVal 6\n",
      "\u001b[32m[2020-07-15 02:53:45] __main__ INFO: \u001b[0mEpoch 6 loss 0.7482 acc@1 0.7402 acc@5 0.9822\n",
      "\u001b[32m[2020-07-15 02:53:45] __main__ INFO: \u001b[0mElapsed 12.34\n",
      "\u001b[32m[2020-07-15 02:53:45] __main__ INFO: \u001b[0mTrain 7 2106\n",
      "\u001b[32m[2020-07-15 02:55:29] __main__ INFO: \u001b[0mEpoch 7 Step 100/351 lr 0.100000 loss 1.2863 (1.2808) acc@1 0.5391 (0.5341) acc@5 0.8516 (0.8724)\n",
      "\u001b[32m[2020-07-15 02:57:14] __main__ INFO: \u001b[0mEpoch 7 Step 200/351 lr 0.100000 loss 1.3436 (1.2713) acc@1 0.5391 (0.5351) acc@5 0.8047 (0.8707)\n",
      "\u001b[32m[2020-07-15 02:58:59] __main__ INFO: \u001b[0mEpoch 7 Step 300/351 lr 0.100000 loss 1.3127 (1.2719) acc@1 0.5078 (0.5346) acc@5 0.8594 (0.8723)\n",
      "\u001b[32m[2020-07-15 02:59:53] __main__ INFO: \u001b[0mEpoch 7 Step 351/351 lr 0.100000 loss 1.0971 (1.2686) acc@1 0.6094 (0.5361) acc@5 0.8984 (0.8727)\n",
      "\u001b[32m[2020-07-15 02:59:53] __main__ INFO: \u001b[0mElapsed 368.13\n",
      "\u001b[32m[2020-07-15 02:59:53] __main__ INFO: \u001b[0mVal 7\n",
      "\u001b[32m[2020-07-15 03:00:05] __main__ INFO: \u001b[0mEpoch 7 loss 0.8331 acc@1 0.7192 acc@5 0.9756\n",
      "\u001b[32m[2020-07-15 03:00:05] __main__ INFO: \u001b[0mElapsed 12.31\n",
      "\u001b[32m[2020-07-15 03:00:05] __main__ INFO: \u001b[0mTrain 8 2457\n",
      "\u001b[32m[2020-07-15 03:01:49] __main__ INFO: \u001b[0mEpoch 8 Step 100/351 lr 0.100000 loss 1.1129 (1.2239) acc@1 0.5547 (0.5530) acc@5 0.8906 (0.8789)\n",
      "\u001b[32m[2020-07-15 03:03:34] __main__ INFO: \u001b[0mEpoch 8 Step 200/351 lr 0.100000 loss 1.1047 (1.2328) acc@1 0.5781 (0.5509) acc@5 0.9141 (0.8744)\n",
      "\u001b[32m[2020-07-15 03:05:18] __main__ INFO: \u001b[0mEpoch 8 Step 300/351 lr 0.100000 loss 1.1708 (1.2277) acc@1 0.5547 (0.5531) acc@5 0.8672 (0.8746)\n",
      "\u001b[32m[2020-07-15 03:06:11] __main__ INFO: \u001b[0mEpoch 8 Step 351/351 lr 0.100000 loss 1.1793 (1.2305) acc@1 0.5938 (0.5525) acc@5 0.8828 (0.8735)\n",
      "\u001b[32m[2020-07-15 03:06:11] __main__ INFO: \u001b[0mElapsed 366.30\n",
      "\u001b[32m[2020-07-15 03:06:11] __main__ INFO: \u001b[0mVal 8\n",
      "\u001b[32m[2020-07-15 03:06:24] __main__ INFO: \u001b[0mEpoch 8 loss 0.6840 acc@1 0.7656 acc@5 0.9874\n",
      "\u001b[32m[2020-07-15 03:06:24] __main__ INFO: \u001b[0mElapsed 12.33\n",
      "\u001b[32m[2020-07-15 03:06:24] __main__ INFO: \u001b[0mTrain 9 2808\n",
      "\u001b[32m[2020-07-15 03:08:09] __main__ INFO: \u001b[0mEpoch 9 Step 100/351 lr 0.100000 loss 1.1201 (1.1886) acc@1 0.6328 (0.5651) acc@5 0.9453 (0.8799)\n",
      "\u001b[32m[2020-07-15 03:09:53] __main__ INFO: \u001b[0mEpoch 9 Step 200/351 lr 0.100000 loss 1.1329 (1.1860) acc@1 0.5781 (0.5674) acc@5 0.8359 (0.8798)\n",
      "\u001b[32m[2020-07-15 03:11:38] __main__ INFO: \u001b[0mEpoch 9 Step 300/351 lr 0.100000 loss 1.2226 (1.1929) acc@1 0.5078 (0.5642) acc@5 0.8672 (0.8781)\n",
      "\u001b[32m[2020-07-15 03:12:32] __main__ INFO: \u001b[0mEpoch 9 Step 351/351 lr 0.100000 loss 1.2639 (1.1955) acc@1 0.5312 (0.5632) acc@5 0.8984 (0.8775)\n",
      "\u001b[32m[2020-07-15 03:12:32] __main__ INFO: \u001b[0mElapsed 368.10\n",
      "\u001b[32m[2020-07-15 03:12:32] __main__ INFO: \u001b[0mVal 9\n",
      "\u001b[32m[2020-07-15 03:12:44] __main__ INFO: \u001b[0mEpoch 9 loss 0.6493 acc@1 0.7698 acc@5 0.9868\n",
      "\u001b[32m[2020-07-15 03:12:44] __main__ INFO: \u001b[0mElapsed 12.30\n",
      "\u001b[32m[2020-07-15 03:12:44] __main__ INFO: \u001b[0mTrain 10 3159\n",
      "\u001b[32m[2020-07-15 03:14:28] __main__ INFO: \u001b[0mEpoch 10 Step 100/351 lr 0.100000 loss 1.1376 (1.1909) acc@1 0.6172 (0.5677) acc@5 0.9062 (0.8775)\n",
      "\u001b[32m[2020-07-15 03:16:13] __main__ INFO: \u001b[0mEpoch 10 Step 200/351 lr 0.100000 loss 1.1098 (1.1819) acc@1 0.6172 (0.5716) acc@5 0.8672 (0.8772)\n",
      "\u001b[32m[2020-07-15 03:17:57] __main__ INFO: \u001b[0mEpoch 10 Step 300/351 lr 0.100000 loss 1.2222 (1.1836) acc@1 0.5547 (0.5712) acc@5 0.8594 (0.8783)\n",
      "\u001b[32m[2020-07-15 03:18:50] __main__ INFO: \u001b[0mEpoch 10 Step 351/351 lr 0.100000 loss 1.0897 (1.1803) acc@1 0.6250 (0.5719) acc@5 0.8906 (0.8789)\n",
      "\u001b[32m[2020-07-15 03:18:50] __main__ INFO: \u001b[0mElapsed 366.47\n",
      "\u001b[32m[2020-07-15 03:18:50] __main__ INFO: \u001b[0mVal 10\n",
      "\u001b[32m[2020-07-15 03:19:03] __main__ INFO: \u001b[0mEpoch 10 loss 0.6279 acc@1 0.7938 acc@5 0.9830\n",
      "\u001b[32m[2020-07-15 03:19:03] __main__ INFO: \u001b[0mElapsed 12.30\n",
      "\u001b[32m[2020-07-15 03:19:03] __main__ INFO: \u001b[0mTrain 11 3510\n",
      "\u001b[32m[2020-07-15 03:20:48] __main__ INFO: \u001b[0mEpoch 11 Step 100/351 lr 0.100000 loss 1.1338 (1.1377) acc@1 0.6016 (0.5885) acc@5 0.9609 (0.8829)\n",
      "\u001b[32m[2020-07-15 03:22:33] __main__ INFO: \u001b[0mEpoch 11 Step 200/351 lr 0.100000 loss 1.0480 (1.1513) acc@1 0.6250 (0.5809) acc@5 0.9453 (0.8807)\n",
      "\u001b[32m[2020-07-15 03:24:18] __main__ INFO: \u001b[0mEpoch 11 Step 300/351 lr 0.100000 loss 1.1512 (1.1486) acc@1 0.5781 (0.5817) acc@5 0.8984 (0.8825)\n",
      "\u001b[32m[2020-07-15 03:25:11] __main__ INFO: \u001b[0mEpoch 11 Step 351/351 lr 0.100000 loss 1.1774 (1.1522) acc@1 0.5781 (0.5804) acc@5 0.8906 (0.8806)\n",
      "\u001b[32m[2020-07-15 03:25:11] __main__ INFO: \u001b[0mElapsed 368.30\n",
      "\u001b[32m[2020-07-15 03:25:11] __main__ INFO: \u001b[0mVal 11\n",
      "\u001b[32m[2020-07-15 03:25:23] __main__ INFO: \u001b[0mEpoch 11 loss 0.7310 acc@1 0.7436 acc@5 0.9824\n",
      "\u001b[32m[2020-07-15 03:25:23] __main__ INFO: \u001b[0mElapsed 12.30\n",
      "\u001b[32m[2020-07-15 03:25:23] __main__ INFO: \u001b[0mTrain 12 3861\n",
      "\u001b[32m[2020-07-15 03:27:08] __main__ INFO: \u001b[0mEpoch 12 Step 100/351 lr 0.100000 loss 1.2608 (1.1292) acc@1 0.5469 (0.5902) acc@5 0.8750 (0.8824)\n",
      "\u001b[32m[2020-07-15 03:28:52] __main__ INFO: \u001b[0mEpoch 12 Step 200/351 lr 0.100000 loss 1.1416 (1.1397) acc@1 0.6094 (0.5854) acc@5 0.8516 (0.8812)\n",
      "\u001b[32m[2020-07-15 03:30:36] __main__ INFO: \u001b[0mEpoch 12 Step 300/351 lr 0.100000 loss 1.2694 (1.1420) acc@1 0.5391 (0.5830) acc@5 0.8594 (0.8807)\n",
      "\u001b[32m[2020-07-15 03:31:30] __main__ INFO: \u001b[0mEpoch 12 Step 351/351 lr 0.100000 loss 1.0586 (1.1419) acc@1 0.6016 (0.5830) acc@5 0.9219 (0.8814)\n",
      "\u001b[32m[2020-07-15 03:31:30] __main__ INFO: \u001b[0mElapsed 366.30\n",
      "\u001b[32m[2020-07-15 03:31:30] __main__ INFO: \u001b[0mVal 12\n",
      "\u001b[32m[2020-07-15 03:31:42] __main__ INFO: \u001b[0mEpoch 12 loss 0.8484 acc@1 0.7302 acc@5 0.9646\n",
      "\u001b[32m[2020-07-15 03:31:42] __main__ INFO: \u001b[0mElapsed 12.32\n",
      "\u001b[32m[2020-07-15 03:31:42] __main__ INFO: \u001b[0mTrain 13 4212\n",
      "\u001b[32m[2020-07-15 03:33:27] __main__ INFO: \u001b[0mEpoch 13 Step 100/351 lr 0.100000 loss 1.1373 (1.1123) acc@1 0.5859 (0.5916) acc@5 0.8516 (0.8849)\n",
      "\u001b[32m[2020-07-15 03:35:12] __main__ INFO: \u001b[0mEpoch 13 Step 200/351 lr 0.100000 loss 1.0340 (1.1279) acc@1 0.6484 (0.5893) acc@5 0.8750 (0.8821)\n",
      "\u001b[32m[2020-07-15 03:36:57] __main__ INFO: \u001b[0mEpoch 13 Step 300/351 lr 0.100000 loss 1.0702 (1.1281) acc@1 0.6250 (0.5899) acc@5 0.8906 (0.8828)\n",
      "\u001b[32m[2020-07-15 03:37:50] __main__ INFO: \u001b[0mEpoch 13 Step 351/351 lr 0.100000 loss 1.0770 (1.1265) acc@1 0.6094 (0.5900) acc@5 0.8828 (0.8834)\n",
      "\u001b[32m[2020-07-15 03:37:50] __main__ INFO: \u001b[0mElapsed 368.13\n",
      "\u001b[32m[2020-07-15 03:37:50] __main__ INFO: \u001b[0mVal 13\n",
      "\u001b[32m[2020-07-15 03:38:02] __main__ INFO: \u001b[0mEpoch 13 loss 0.8179 acc@1 0.7306 acc@5 0.9684\n",
      "\u001b[32m[2020-07-15 03:38:02] __main__ INFO: \u001b[0mElapsed 12.31\n",
      "\u001b[32m[2020-07-15 03:38:02] __main__ INFO: \u001b[0mTrain 14 4563\n",
      "\u001b[32m[2020-07-15 03:39:47] __main__ INFO: \u001b[0mEpoch 14 Step 100/351 lr 0.100000 loss 1.0525 (1.1026) acc@1 0.5938 (0.5996) acc@5 0.8984 (0.8849)\n",
      "\u001b[32m[2020-07-15 03:41:31] __main__ INFO: \u001b[0mEpoch 14 Step 200/351 lr 0.100000 loss 1.1490 (1.1072) acc@1 0.5938 (0.5971) acc@5 0.8750 (0.8838)\n",
      "\u001b[32m[2020-07-15 03:43:16] __main__ INFO: \u001b[0mEpoch 14 Step 300/351 lr 0.100000 loss 1.1273 (1.1130) acc@1 0.6016 (0.5955) acc@5 0.8672 (0.8828)\n",
      "\u001b[32m[2020-07-15 03:44:09] __main__ INFO: \u001b[0mEpoch 14 Step 351/351 lr 0.100000 loss 1.2842 (1.1140) acc@1 0.5312 (0.5954) acc@5 0.8281 (0.8828)\n",
      "\u001b[32m[2020-07-15 03:44:09] __main__ INFO: \u001b[0mElapsed 366.31\n",
      "\u001b[32m[2020-07-15 03:44:09] __main__ INFO: \u001b[0mVal 14\n",
      "\u001b[32m[2020-07-15 03:44:21] __main__ INFO: \u001b[0mEpoch 14 loss 0.8252 acc@1 0.7438 acc@5 0.9784\n",
      "\u001b[32m[2020-07-15 03:44:21] __main__ INFO: \u001b[0mElapsed 12.31\n",
      "\u001b[32m[2020-07-15 03:44:21] __main__ INFO: \u001b[0mTrain 15 4914\n",
      "\u001b[32m[2020-07-15 03:46:06] __main__ INFO: \u001b[0mEpoch 15 Step 100/351 lr 0.100000 loss 1.2034 (1.0977) acc@1 0.5547 (0.5966) acc@5 0.8516 (0.8827)\n",
      "\u001b[32m[2020-07-15 03:47:51] __main__ INFO: \u001b[0mEpoch 15 Step 200/351 lr 0.100000 loss 1.1891 (1.0945) acc@1 0.5703 (0.6003) acc@5 0.8516 (0.8870)\n",
      "\u001b[32m[2020-07-15 03:49:36] __main__ INFO: \u001b[0mEpoch 15 Step 300/351 lr 0.100000 loss 0.9387 (1.0980) acc@1 0.6250 (0.5978) acc@5 0.8906 (0.8860)\n",
      "\u001b[32m[2020-07-15 03:50:29] __main__ INFO: \u001b[0mEpoch 15 Step 351/351 lr 0.100000 loss 1.2390 (1.1030) acc@1 0.5000 (0.5964) acc@5 0.8438 (0.8853)\n",
      "\u001b[32m[2020-07-15 03:50:29] __main__ INFO: \u001b[0mElapsed 367.93\n",
      "\u001b[32m[2020-07-15 03:50:29] __main__ INFO: \u001b[0mVal 15\n",
      "\u001b[32m[2020-07-15 03:50:41] __main__ INFO: \u001b[0mEpoch 15 loss 0.6730 acc@1 0.7706 acc@5 0.9848\n",
      "\u001b[32m[2020-07-15 03:50:41] __main__ INFO: \u001b[0mElapsed 12.31\n",
      "\u001b[32m[2020-07-15 03:50:41] __main__ INFO: \u001b[0mTrain 16 5265\n",
      "\u001b[32m[2020-07-15 03:52:26] __main__ INFO: \u001b[0mEpoch 16 Step 100/351 lr 0.100000 loss 1.0902 (1.0813) acc@1 0.5938 (0.6032) acc@5 0.9141 (0.8854)\n",
      "\u001b[32m[2020-07-15 03:54:10] __main__ INFO: \u001b[0mEpoch 16 Step 200/351 lr 0.100000 loss 1.0149 (1.0944) acc@1 0.6328 (0.6013) acc@5 0.9062 (0.8839)\n",
      "\u001b[32m[2020-07-15 03:55:54] __main__ INFO: \u001b[0mEpoch 16 Step 300/351 lr 0.100000 loss 1.2280 (1.0931) acc@1 0.5234 (0.6017) acc@5 0.8828 (0.8851)\n",
      "\u001b[32m[2020-07-15 03:56:47] __main__ INFO: \u001b[0mEpoch 16 Step 351/351 lr 0.100000 loss 1.1199 (1.0902) acc@1 0.5703 (0.6021) acc@5 0.8984 (0.8845)\n",
      "\u001b[32m[2020-07-15 03:56:47] __main__ INFO: \u001b[0mElapsed 366.11\n",
      "\u001b[32m[2020-07-15 03:56:47] __main__ INFO: \u001b[0mVal 16\n",
      "\u001b[32m[2020-07-15 03:57:00] __main__ INFO: \u001b[0mEpoch 16 loss 0.6549 acc@1 0.7780 acc@5 0.9876\n",
      "\u001b[32m[2020-07-15 03:57:00] __main__ INFO: \u001b[0mElapsed 12.30\n",
      "\u001b[32m[2020-07-15 03:57:00] __main__ INFO: \u001b[0mTrain 17 5616\n",
      "\u001b[32m[2020-07-15 03:58:45] __main__ INFO: \u001b[0mEpoch 17 Step 100/351 lr 0.100000 loss 1.0852 (1.0600) acc@1 0.6016 (0.6126) acc@5 0.8750 (0.8870)\n",
      "\u001b[32m[2020-07-15 04:00:29] __main__ INFO: \u001b[0mEpoch 17 Step 200/351 lr 0.100000 loss 1.1731 (1.0740) acc@1 0.5859 (0.6080) acc@5 0.8672 (0.8860)\n",
      "\u001b[32m[2020-07-15 04:02:14] __main__ INFO: \u001b[0mEpoch 17 Step 300/351 lr 0.100000 loss 1.1123 (1.0769) acc@1 0.6016 (0.6075) acc@5 0.8750 (0.8854)\n",
      "\u001b[32m[2020-07-15 04:03:08] __main__ INFO: \u001b[0mEpoch 17 Step 351/351 lr 0.100000 loss 1.1230 (1.0782) acc@1 0.5938 (0.6073) acc@5 0.8828 (0.8856)\n",
      "\u001b[32m[2020-07-15 04:03:08] __main__ INFO: \u001b[0mElapsed 367.94\n",
      "\u001b[32m[2020-07-15 04:03:08] __main__ INFO: \u001b[0mVal 17\n",
      "\u001b[32m[2020-07-15 04:03:20] __main__ INFO: \u001b[0mEpoch 17 loss 0.5600 acc@1 0.8026 acc@5 0.9910\n",
      "\u001b[32m[2020-07-15 04:03:20] __main__ INFO: \u001b[0mElapsed 12.32\n",
      "\u001b[32m[2020-07-15 04:03:20] __main__ INFO: \u001b[0mTrain 18 5967\n",
      "\u001b[32m[2020-07-15 04:05:04] __main__ INFO: \u001b[0mEpoch 18 Step 100/351 lr 0.100000 loss 1.1151 (1.0590) acc@1 0.5781 (0.6148) acc@5 0.8672 (0.8898)\n",
      "\u001b[32m[2020-07-15 04:06:49] __main__ INFO: \u001b[0mEpoch 18 Step 200/351 lr 0.100000 loss 0.8739 (1.0705) acc@1 0.7031 (0.6098) acc@5 0.9375 (0.8852)\n",
      "\u001b[32m[2020-07-15 04:08:33] __main__ INFO: \u001b[0mEpoch 18 Step 300/351 lr 0.100000 loss 1.0671 (1.0714) acc@1 0.5703 (0.6092) acc@5 0.8516 (0.8870)\n",
      "\u001b[32m[2020-07-15 04:09:26] __main__ INFO: \u001b[0mEpoch 18 Step 351/351 lr 0.100000 loss 1.1526 (1.0707) acc@1 0.5781 (0.6094) acc@5 0.8828 (0.8866)\n",
      "\u001b[32m[2020-07-15 04:09:26] __main__ INFO: \u001b[0mElapsed 366.11\n",
      "\u001b[32m[2020-07-15 04:09:26] __main__ INFO: \u001b[0mVal 18\n",
      "\u001b[32m[2020-07-15 04:09:38] __main__ INFO: \u001b[0mEpoch 18 loss 0.7089 acc@1 0.7642 acc@5 0.9874\n",
      "\u001b[32m[2020-07-15 04:09:38] __main__ INFO: \u001b[0mElapsed 12.30\n",
      "\u001b[32m[2020-07-15 04:09:38] __main__ INFO: \u001b[0mTrain 19 6318\n",
      "\u001b[32m[2020-07-15 04:11:23] __main__ INFO: \u001b[0mEpoch 19 Step 100/351 lr 0.100000 loss 0.9010 (1.0471) acc@1 0.6719 (0.6173) acc@5 0.9375 (0.8830)\n",
      "\u001b[32m[2020-07-15 04:13:08] __main__ INFO: \u001b[0mEpoch 19 Step 200/351 lr 0.100000 loss 0.9892 (1.0543) acc@1 0.6719 (0.6150) acc@5 0.9375 (0.8855)\n",
      "\u001b[32m[2020-07-15 04:14:53] __main__ INFO: \u001b[0mEpoch 19 Step 300/351 lr 0.100000 loss 0.9867 (1.0565) acc@1 0.6172 (0.6154) acc@5 0.8906 (0.8864)\n",
      "\u001b[32m[2020-07-15 04:15:46] __main__ INFO: \u001b[0mEpoch 19 Step 351/351 lr 0.100000 loss 1.1019 (1.0615) acc@1 0.5781 (0.6132) acc@5 0.9062 (0.8869)\n",
      "\u001b[32m[2020-07-15 04:15:46] __main__ INFO: \u001b[0mElapsed 367.84\n",
      "\u001b[32m[2020-07-15 04:15:46] __main__ INFO: \u001b[0mVal 19\n",
      "\u001b[32m[2020-07-15 04:15:59] __main__ INFO: \u001b[0mEpoch 19 loss 0.5727 acc@1 0.8014 acc@5 0.9892\n",
      "\u001b[32m[2020-07-15 04:15:59] __main__ INFO: \u001b[0mElapsed 12.33\n",
      "\u001b[32m[2020-07-15 04:15:59] __main__ INFO: \u001b[0mTrain 20 6669\n",
      "\u001b[32m[2020-07-15 04:17:43] __main__ INFO: \u001b[0mEpoch 20 Step 100/351 lr 0.100000 loss 0.8353 (1.0205) acc@1 0.6797 (0.6297) acc@5 0.9062 (0.8917)\n",
      "\u001b[32m[2020-07-15 04:19:27] __main__ INFO: \u001b[0mEpoch 20 Step 200/351 lr 0.100000 loss 1.2461 (1.0451) acc@1 0.5312 (0.6196) acc@5 0.8750 (0.8895)\n",
      "\u001b[32m[2020-07-15 04:21:11] __main__ INFO: \u001b[0mEpoch 20 Step 300/351 lr 0.100000 loss 1.1116 (1.0491) acc@1 0.5859 (0.6187) acc@5 0.9062 (0.8889)\n",
      "\u001b[32m[2020-07-15 04:22:04] __main__ INFO: \u001b[0mEpoch 20 Step 351/351 lr 0.100000 loss 1.2130 (1.0555) acc@1 0.5859 (0.6166) acc@5 0.8828 (0.8880)\n",
      "\u001b[32m[2020-07-15 04:22:04] __main__ INFO: \u001b[0mElapsed 365.89\n",
      "\u001b[32m[2020-07-15 04:22:04] __main__ INFO: \u001b[0mVal 20\n",
      "\u001b[32m[2020-07-15 04:22:17] __main__ INFO: \u001b[0mEpoch 20 loss 0.5618 acc@1 0.8036 acc@5 0.9860\n",
      "\u001b[32m[2020-07-15 04:22:17] __main__ INFO: \u001b[0mElapsed 12.30\n",
      "\u001b[32m[2020-07-15 04:22:17] __main__ INFO: \u001b[0mTrain 21 7020\n",
      "\u001b[32m[2020-07-15 04:24:02] __main__ INFO: \u001b[0mEpoch 21 Step 100/351 lr 0.100000 loss 1.0571 (1.0374) acc@1 0.6172 (0.6253) acc@5 0.9062 (0.8876)\n",
      "\u001b[32m[2020-07-15 04:25:46] __main__ INFO: \u001b[0mEpoch 21 Step 200/351 lr 0.100000 loss 1.2011 (1.0403) acc@1 0.5312 (0.6230) acc@5 0.8828 (0.8883)\n",
      "\u001b[32m[2020-07-15 04:27:31] __main__ INFO: \u001b[0mEpoch 21 Step 300/351 lr 0.100000 loss 1.1490 (1.0442) acc@1 0.5625 (0.6207) acc@5 0.8672 (0.8874)\n",
      "\u001b[32m[2020-07-15 04:28:24] __main__ INFO: \u001b[0mEpoch 21 Step 351/351 lr 0.100000 loss 1.1967 (1.0432) acc@1 0.5000 (0.6198) acc@5 0.8359 (0.8880)\n",
      "\u001b[32m[2020-07-15 04:28:24] __main__ INFO: \u001b[0mElapsed 367.63\n",
      "\u001b[32m[2020-07-15 04:28:24] __main__ INFO: \u001b[0mVal 21\n",
      "\u001b[32m[2020-07-15 04:28:37] __main__ INFO: \u001b[0mEpoch 21 loss 0.5972 acc@1 0.8060 acc@5 0.9868\n",
      "\u001b[32m[2020-07-15 04:28:37] __main__ INFO: \u001b[0mElapsed 12.28\n",
      "\u001b[32m[2020-07-15 04:28:37] __main__ INFO: \u001b[0mTrain 22 7371\n",
      "\u001b[32m[2020-07-15 04:30:21] __main__ INFO: \u001b[0mEpoch 22 Step 100/351 lr 0.100000 loss 1.0967 (1.0087) acc@1 0.5938 (0.6324) acc@5 0.9297 (0.8955)\n",
      "\u001b[32m[2020-07-15 04:32:05] __main__ INFO: \u001b[0mEpoch 22 Step 200/351 lr 0.100000 loss 1.0794 (1.0214) acc@1 0.6250 (0.6284) acc@5 0.8516 (0.8933)\n",
      "\u001b[32m[2020-07-15 04:33:49] __main__ INFO: \u001b[0mEpoch 22 Step 300/351 lr 0.100000 loss 1.0115 (1.0298) acc@1 0.5703 (0.6253) acc@5 0.9219 (0.8913)\n",
      "\u001b[32m[2020-07-15 04:34:42] __main__ INFO: \u001b[0mEpoch 22 Step 351/351 lr 0.100000 loss 1.0911 (1.0367) acc@1 0.5938 (0.6224) acc@5 0.8516 (0.8900)\n",
      "\u001b[32m[2020-07-15 04:34:43] __main__ INFO: \u001b[0mElapsed 365.81\n",
      "\u001b[32m[2020-07-15 04:34:43] __main__ INFO: \u001b[0mVal 22\n",
      "\u001b[32m[2020-07-15 04:34:55] __main__ INFO: \u001b[0mEpoch 22 loss 0.4927 acc@1 0.8294 acc@5 0.9916\n",
      "\u001b[32m[2020-07-15 04:34:55] __main__ INFO: \u001b[0mElapsed 12.28\n",
      "\u001b[32m[2020-07-15 04:34:55] __main__ INFO: \u001b[0mTrain 23 7722\n",
      "\u001b[32m[2020-07-15 04:36:40] __main__ INFO: \u001b[0mEpoch 23 Step 100/351 lr 0.100000 loss 0.8392 (1.0211) acc@1 0.6953 (0.6216) acc@5 0.9375 (0.8879)\n",
      "\u001b[32m[2020-07-15 04:38:24] __main__ INFO: \u001b[0mEpoch 23 Step 200/351 lr 0.100000 loss 1.2876 (1.0208) acc@1 0.5469 (0.6243) acc@5 0.8750 (0.8881)\n",
      "\u001b[32m[2020-07-15 04:40:09] __main__ INFO: \u001b[0mEpoch 23 Step 300/351 lr 0.100000 loss 0.9760 (1.0258) acc@1 0.6562 (0.6231) acc@5 0.8750 (0.8876)\n",
      "\u001b[32m[2020-07-15 04:41:02] __main__ INFO: \u001b[0mEpoch 23 Step 351/351 lr 0.100000 loss 0.9875 (1.0275) acc@1 0.6641 (0.6227) acc@5 0.8984 (0.8875)\n",
      "\u001b[32m[2020-07-15 04:41:02] __main__ INFO: \u001b[0mElapsed 367.55\n",
      "\u001b[32m[2020-07-15 04:41:02] __main__ INFO: \u001b[0mVal 23\n",
      "\u001b[32m[2020-07-15 04:41:15] __main__ INFO: \u001b[0mEpoch 23 loss 0.5709 acc@1 0.8084 acc@5 0.9902\n",
      "\u001b[32m[2020-07-15 04:41:15] __main__ INFO: \u001b[0mElapsed 12.29\n",
      "\u001b[32m[2020-07-15 04:41:15] __main__ INFO: \u001b[0mTrain 24 8073\n",
      "\u001b[32m[2020-07-15 04:42:59] __main__ INFO: \u001b[0mEpoch 24 Step 100/351 lr 0.100000 loss 0.9456 (0.9971) acc@1 0.6406 (0.6392) acc@5 0.9453 (0.8877)\n",
      "\u001b[32m[2020-07-15 04:44:43] __main__ INFO: \u001b[0mEpoch 24 Step 200/351 lr 0.100000 loss 0.9289 (1.0117) acc@1 0.6953 (0.6339) acc@5 0.8594 (0.8904)\n",
      "\u001b[32m[2020-07-15 04:46:27] __main__ INFO: \u001b[0mEpoch 24 Step 300/351 lr 0.100000 loss 1.2037 (1.0221) acc@1 0.5469 (0.6293) acc@5 0.9141 (0.8880)\n",
      "\u001b[32m[2020-07-15 04:47:20] __main__ INFO: \u001b[0mEpoch 24 Step 351/351 lr 0.100000 loss 1.0797 (1.0238) acc@1 0.5547 (0.6282) acc@5 0.8438 (0.8876)\n",
      "\u001b[32m[2020-07-15 04:47:21] __main__ INFO: \u001b[0mElapsed 365.88\n",
      "\u001b[32m[2020-07-15 04:47:21] __main__ INFO: \u001b[0mVal 24\n",
      "\u001b[32m[2020-07-15 04:47:33] __main__ INFO: \u001b[0mEpoch 24 loss 0.6890 acc@1 0.7686 acc@5 0.9894\n",
      "\u001b[32m[2020-07-15 04:47:33] __main__ INFO: \u001b[0mElapsed 12.30\n",
      "\u001b[32m[2020-07-15 04:47:33] __main__ INFO: \u001b[0mTrain 25 8424\n",
      "\u001b[32m[2020-07-15 04:49:18] __main__ INFO: \u001b[0mEpoch 25 Step 100/351 lr 0.100000 loss 1.0748 (0.9988) acc@1 0.6328 (0.6362) acc@5 0.8906 (0.8918)\n",
      "\u001b[32m[2020-07-15 04:51:02] __main__ INFO: \u001b[0mEpoch 25 Step 200/351 lr 0.100000 loss 0.8392 (1.0076) acc@1 0.7109 (0.6332) acc@5 0.9062 (0.8914)\n",
      "\u001b[32m[2020-07-15 04:52:47] __main__ INFO: \u001b[0mEpoch 25 Step 300/351 lr 0.100000 loss 0.8986 (1.0219) acc@1 0.6641 (0.6266) acc@5 0.9219 (0.8895)\n",
      "\u001b[32m[2020-07-15 04:53:40] __main__ INFO: \u001b[0mEpoch 25 Step 351/351 lr 0.100000 loss 1.1414 (1.0213) acc@1 0.6250 (0.6281) acc@5 0.8672 (0.8902)\n",
      "\u001b[32m[2020-07-15 04:53:40] __main__ INFO: \u001b[0mElapsed 367.48\n",
      "\u001b[32m[2020-07-15 04:53:40] __main__ INFO: \u001b[0mVal 25\n",
      "\u001b[32m[2020-07-15 04:53:53] __main__ INFO: \u001b[0mEpoch 25 loss 0.5503 acc@1 0.8094 acc@5 0.9924\n",
      "\u001b[32m[2020-07-15 04:53:53] __main__ INFO: \u001b[0mElapsed 12.30\n",
      "\u001b[32m[2020-07-15 04:53:53] __main__ INFO: \u001b[0mTrain 26 8775\n",
      "\u001b[32m[2020-07-15 04:55:37] __main__ INFO: \u001b[0mEpoch 26 Step 100/351 lr 0.100000 loss 0.9638 (1.0031) acc@1 0.6641 (0.6373) acc@5 0.8984 (0.8875)\n",
      "\u001b[32m[2020-07-15 04:57:21] __main__ INFO: \u001b[0mEpoch 26 Step 200/351 lr 0.100000 loss 1.0681 (1.0009) acc@1 0.6250 (0.6354) acc@5 0.9062 (0.8883)\n",
      "\u001b[32m[2020-07-15 04:59:05] __main__ INFO: \u001b[0mEpoch 26 Step 300/351 lr 0.100000 loss 0.9498 (1.0104) acc@1 0.6562 (0.6311) acc@5 0.8984 (0.8878)\n",
      "\u001b[32m[2020-07-15 04:59:58] __main__ INFO: \u001b[0mEpoch 26 Step 351/351 lr 0.100000 loss 0.9060 (1.0116) acc@1 0.6875 (0.6303) acc@5 0.9297 (0.8882)\n",
      "\u001b[32m[2020-07-15 04:59:58] __main__ INFO: \u001b[0mElapsed 365.80\n",
      "\u001b[32m[2020-07-15 04:59:58] __main__ INFO: \u001b[0mVal 26\n",
      "\u001b[32m[2020-07-15 05:00:11] __main__ INFO: \u001b[0mEpoch 26 loss 0.4674 acc@1 0.8368 acc@5 0.9924\n",
      "\u001b[32m[2020-07-15 05:00:11] __main__ INFO: \u001b[0mElapsed 12.28\n",
      "\u001b[32m[2020-07-15 05:00:11] __main__ INFO: \u001b[0mTrain 27 9126\n",
      "\u001b[32m[2020-07-15 05:01:55] __main__ INFO: \u001b[0mEpoch 27 Step 100/351 lr 0.100000 loss 1.0177 (0.9906) acc@1 0.6172 (0.6377) acc@5 0.9375 (0.8894)\n",
      "\u001b[32m[2020-07-15 05:03:40] __main__ INFO: \u001b[0mEpoch 27 Step 200/351 lr 0.100000 loss 1.1160 (0.9978) acc@1 0.5938 (0.6337) acc@5 0.8984 (0.8903)\n",
      "\u001b[32m[2020-07-15 05:05:25] __main__ INFO: \u001b[0mEpoch 27 Step 300/351 lr 0.100000 loss 1.1725 (1.0087) acc@1 0.5859 (0.6304) acc@5 0.8984 (0.8892)\n",
      "\u001b[32m[2020-07-15 05:06:18] __main__ INFO: \u001b[0mEpoch 27 Step 351/351 lr 0.100000 loss 0.9385 (1.0091) acc@1 0.6797 (0.6305) acc@5 0.9062 (0.8892)\n",
      "\u001b[32m[2020-07-15 05:06:18] __main__ INFO: \u001b[0mElapsed 367.45\n",
      "\u001b[32m[2020-07-15 05:06:18] __main__ INFO: \u001b[0mVal 27\n",
      "\u001b[32m[2020-07-15 05:06:30] __main__ INFO: \u001b[0mEpoch 27 loss 0.6344 acc@1 0.7870 acc@5 0.9888\n",
      "\u001b[32m[2020-07-15 05:06:30] __main__ INFO: \u001b[0mElapsed 12.28\n",
      "\u001b[32m[2020-07-15 05:06:30] __main__ INFO: \u001b[0mTrain 28 9477\n",
      "\u001b[32m[2020-07-15 05:08:15] __main__ INFO: \u001b[0mEpoch 28 Step 100/351 lr 0.100000 loss 1.1280 (0.9934) acc@1 0.5703 (0.6352) acc@5 0.8750 (0.8948)\n",
      "\u001b[32m[2020-07-15 05:09:59] __main__ INFO: \u001b[0mEpoch 28 Step 200/351 lr 0.100000 loss 1.1908 (1.0067) acc@1 0.5781 (0.6311) acc@5 0.8516 (0.8900)\n",
      "\u001b[32m[2020-07-15 05:11:43] __main__ INFO: \u001b[0mEpoch 28 Step 300/351 lr 0.100000 loss 1.0638 (1.0063) acc@1 0.6328 (0.6321) acc@5 0.8828 (0.8902)\n",
      "\u001b[32m[2020-07-15 05:12:36] __main__ INFO: \u001b[0mEpoch 28 Step 351/351 lr 0.100000 loss 0.8878 (1.0077) acc@1 0.6562 (0.6315) acc@5 0.8906 (0.8900)\n",
      "\u001b[32m[2020-07-15 05:12:36] __main__ INFO: \u001b[0mElapsed 365.76\n",
      "\u001b[32m[2020-07-15 05:12:36] __main__ INFO: \u001b[0mVal 28\n",
      "\u001b[32m[2020-07-15 05:12:48] __main__ INFO: \u001b[0mEpoch 28 loss 0.5492 acc@1 0.8176 acc@5 0.9926\n",
      "\u001b[32m[2020-07-15 05:12:48] __main__ INFO: \u001b[0mElapsed 12.30\n",
      "\u001b[32m[2020-07-15 05:12:48] __main__ INFO: \u001b[0mTrain 29 9828\n",
      "\u001b[32m[2020-07-15 05:14:33] __main__ INFO: \u001b[0mEpoch 29 Step 100/351 lr 0.100000 loss 1.0762 (0.9688) acc@1 0.6250 (0.6437) acc@5 0.8906 (0.8943)\n",
      "\u001b[32m[2020-07-15 05:16:18] __main__ INFO: \u001b[0mEpoch 29 Step 200/351 lr 0.100000 loss 0.7965 (0.9722) acc@1 0.7188 (0.6433) acc@5 0.9062 (0.8947)\n",
      "\u001b[32m[2020-07-15 05:18:02] __main__ INFO: \u001b[0mEpoch 29 Step 300/351 lr 0.100000 loss 1.0618 (0.9925) acc@1 0.6484 (0.6367) acc@5 0.8984 (0.8912)\n",
      "\u001b[32m[2020-07-15 05:18:56] __main__ INFO: \u001b[0mEpoch 29 Step 351/351 lr 0.100000 loss 1.0829 (0.9968) acc@1 0.6250 (0.6351) acc@5 0.9141 (0.8914)\n",
      "\u001b[32m[2020-07-15 05:18:56] __main__ INFO: \u001b[0mElapsed 367.36\n",
      "\u001b[32m[2020-07-15 05:18:56] __main__ INFO: \u001b[0mVal 29\n",
      "\u001b[32m[2020-07-15 05:19:08] __main__ INFO: \u001b[0mEpoch 29 loss 0.6259 acc@1 0.7876 acc@5 0.9890\n",
      "\u001b[32m[2020-07-15 05:19:08] __main__ INFO: \u001b[0mElapsed 12.28\n",
      "\u001b[32m[2020-07-15 05:19:08] __main__ INFO: \u001b[0mTrain 30 10179\n",
      "\u001b[32m[2020-07-15 05:20:52] __main__ INFO: \u001b[0mEpoch 30 Step 100/351 lr 0.100000 loss 0.9594 (0.9838) acc@1 0.6562 (0.6409) acc@5 0.9062 (0.8916)\n",
      "\u001b[32m[2020-07-15 05:22:36] __main__ INFO: \u001b[0mEpoch 30 Step 200/351 lr 0.100000 loss 0.9653 (0.9937) acc@1 0.6172 (0.6361) acc@5 0.8672 (0.8918)\n",
      "\u001b[32m[2020-07-15 05:24:20] __main__ INFO: \u001b[0mEpoch 30 Step 300/351 lr 0.100000 loss 1.0195 (0.9957) acc@1 0.5938 (0.6365) acc@5 0.8750 (0.8917)\n",
      "\u001b[32m[2020-07-15 05:25:14] __main__ INFO: \u001b[0mEpoch 30 Step 351/351 lr 0.100000 loss 1.0750 (0.9967) acc@1 0.5781 (0.6359) acc@5 0.9062 (0.8910)\n",
      "\u001b[32m[2020-07-15 05:25:14] __main__ INFO: \u001b[0mElapsed 365.52\n",
      "\u001b[32m[2020-07-15 05:25:14] __main__ INFO: \u001b[0mVal 30\n",
      "\u001b[32m[2020-07-15 05:25:26] __main__ INFO: \u001b[0mEpoch 30 loss 0.5066 acc@1 0.8294 acc@5 0.9926\n",
      "\u001b[32m[2020-07-15 05:25:26] __main__ INFO: \u001b[0mElapsed 12.28\n",
      "\u001b[32m[2020-07-15 05:25:26] __main__ INFO: \u001b[0mTrain 31 10530\n",
      "\u001b[32m[2020-07-15 05:27:11] __main__ INFO: \u001b[0mEpoch 31 Step 100/351 lr 0.100000 loss 0.8033 (0.9863) acc@1 0.7266 (0.6394) acc@5 0.8984 (0.8899)\n",
      "\u001b[32m[2020-07-15 05:28:55] __main__ INFO: \u001b[0mEpoch 31 Step 200/351 lr 0.100000 loss 1.0168 (0.9768) acc@1 0.6250 (0.6431) acc@5 0.8672 (0.8903)\n",
      "\u001b[32m[2020-07-15 05:30:40] __main__ INFO: \u001b[0mEpoch 31 Step 300/351 lr 0.100000 loss 1.0479 (0.9878) acc@1 0.5938 (0.6388) acc@5 0.8984 (0.8892)\n",
      "\u001b[32m[2020-07-15 05:31:33] __main__ INFO: \u001b[0mEpoch 31 Step 351/351 lr 0.100000 loss 0.9692 (0.9900) acc@1 0.6641 (0.6382) acc@5 0.8750 (0.8896)\n",
      "\u001b[32m[2020-07-15 05:31:33] __main__ INFO: \u001b[0mElapsed 367.14\n",
      "\u001b[32m[2020-07-15 05:31:33] __main__ INFO: \u001b[0mVal 31\n",
      "\u001b[32m[2020-07-15 05:31:45] __main__ INFO: \u001b[0mEpoch 31 loss 0.5468 acc@1 0.8184 acc@5 0.9876\n",
      "\u001b[32m[2020-07-15 05:31:45] __main__ INFO: \u001b[0mElapsed 12.28\n",
      "\u001b[32m[2020-07-15 05:31:45] __main__ INFO: \u001b[0mTrain 32 10881\n",
      "\u001b[32m[2020-07-15 05:33:30] __main__ INFO: \u001b[0mEpoch 32 Step 100/351 lr 0.100000 loss 1.0327 (0.9625) acc@1 0.6328 (0.6459) acc@5 0.8750 (0.8937)\n",
      "\u001b[32m[2020-07-15 05:35:14] __main__ INFO: \u001b[0mEpoch 32 Step 200/351 lr 0.100000 loss 0.7923 (0.9723) acc@1 0.7031 (0.6438) acc@5 0.9219 (0.8934)\n",
      "\u001b[32m[2020-07-15 05:36:58] __main__ INFO: \u001b[0mEpoch 32 Step 300/351 lr 0.100000 loss 0.8744 (0.9856) acc@1 0.6797 (0.6390) acc@5 0.8984 (0.8911)\n",
      "\u001b[32m[2020-07-15 05:37:51] __main__ INFO: \u001b[0mEpoch 32 Step 351/351 lr 0.100000 loss 0.9159 (0.9873) acc@1 0.6406 (0.6385) acc@5 0.8906 (0.8911)\n",
      "\u001b[32m[2020-07-15 05:37:51] __main__ INFO: \u001b[0mElapsed 365.40\n",
      "\u001b[32m[2020-07-15 05:37:51] __main__ INFO: \u001b[0mVal 32\n",
      "\u001b[32m[2020-07-15 05:38:03] __main__ INFO: \u001b[0mEpoch 32 loss 0.5113 acc@1 0.8256 acc@5 0.9882\n",
      "\u001b[32m[2020-07-15 05:38:03] __main__ INFO: \u001b[0mElapsed 12.27\n",
      "\u001b[32m[2020-07-15 05:38:03] __main__ INFO: \u001b[0mTrain 33 11232\n",
      "\u001b[32m[2020-07-15 05:39:48] __main__ INFO: \u001b[0mEpoch 33 Step 100/351 lr 0.100000 loss 0.8532 (0.9651) acc@1 0.6953 (0.6480) acc@5 0.9453 (0.8941)\n",
      "\u001b[32m[2020-07-15 05:41:32] __main__ INFO: \u001b[0mEpoch 33 Step 200/351 lr 0.100000 loss 1.0037 (0.9759) acc@1 0.5938 (0.6422) acc@5 0.8672 (0.8921)\n",
      "\u001b[32m[2020-07-15 05:43:17] __main__ INFO: \u001b[0mEpoch 33 Step 300/351 lr 0.100000 loss 1.0172 (0.9805) acc@1 0.6562 (0.6419) acc@5 0.8203 (0.8934)\n",
      "\u001b[32m[2020-07-15 05:44:10] __main__ INFO: \u001b[0mEpoch 33 Step 351/351 lr 0.100000 loss 1.0607 (0.9831) acc@1 0.5859 (0.6411) acc@5 0.8828 (0.8925)\n",
      "\u001b[32m[2020-07-15 05:44:10] __main__ INFO: \u001b[0mElapsed 367.34\n",
      "\u001b[32m[2020-07-15 05:44:10] __main__ INFO: \u001b[0mVal 33\n",
      "\u001b[32m[2020-07-15 05:44:23] __main__ INFO: \u001b[0mEpoch 33 loss 0.5788 acc@1 0.8014 acc@5 0.9904\n",
      "\u001b[32m[2020-07-15 05:44:23] __main__ INFO: \u001b[0mElapsed 12.27\n",
      "\u001b[32m[2020-07-15 05:44:23] __main__ INFO: \u001b[0mTrain 34 11583\n",
      "\u001b[32m[2020-07-15 05:46:07] __main__ INFO: \u001b[0mEpoch 34 Step 100/351 lr 0.100000 loss 0.9675 (0.9527) acc@1 0.6719 (0.6502) acc@5 0.8750 (0.8962)\n",
      "\u001b[32m[2020-07-15 05:47:51] __main__ INFO: \u001b[0mEpoch 34 Step 200/351 lr 0.100000 loss 1.0207 (0.9701) acc@1 0.6484 (0.6468) acc@5 0.8906 (0.8936)\n",
      "\u001b[32m[2020-07-15 05:49:35] __main__ INFO: \u001b[0mEpoch 34 Step 300/351 lr 0.100000 loss 0.9732 (0.9801) acc@1 0.6406 (0.6430) acc@5 0.9062 (0.8921)\n",
      "\u001b[32m[2020-07-15 05:50:28] __main__ INFO: \u001b[0mEpoch 34 Step 351/351 lr 0.100000 loss 1.2437 (0.9824) acc@1 0.5000 (0.6419) acc@5 0.9141 (0.8920)\n",
      "\u001b[32m[2020-07-15 05:50:28] __main__ INFO: \u001b[0mElapsed 365.50\n",
      "\u001b[32m[2020-07-15 05:50:28] __main__ INFO: \u001b[0mVal 34\n",
      "\u001b[32m[2020-07-15 05:50:40] __main__ INFO: \u001b[0mEpoch 34 loss 0.5983 acc@1 0.7850 acc@5 0.9936\n",
      "\u001b[32m[2020-07-15 05:50:40] __main__ INFO: \u001b[0mElapsed 12.26\n",
      "\u001b[32m[2020-07-15 05:50:40] __main__ INFO: \u001b[0mTrain 35 11934\n",
      "\u001b[32m[2020-07-15 05:52:25] __main__ INFO: \u001b[0mEpoch 35 Step 100/351 lr 0.100000 loss 0.9699 (0.9536) acc@1 0.6172 (0.6550) acc@5 0.9531 (0.8971)\n",
      "\u001b[32m[2020-07-15 05:54:10] __main__ INFO: \u001b[0mEpoch 35 Step 200/351 lr 0.100000 loss 1.0792 (0.9647) acc@1 0.6172 (0.6477) acc@5 0.9141 (0.8935)\n",
      "\u001b[32m[2020-07-15 05:55:54] __main__ INFO: \u001b[0mEpoch 35 Step 300/351 lr 0.100000 loss 1.0212 (0.9722) acc@1 0.6250 (0.6449) acc@5 0.9062 (0.8924)\n",
      "\u001b[32m[2020-07-15 05:56:48] __main__ INFO: \u001b[0mEpoch 35 Step 351/351 lr 0.100000 loss 0.9457 (0.9724) acc@1 0.6562 (0.6450) acc@5 0.8750 (0.8917)\n",
      "\u001b[32m[2020-07-15 05:56:48] __main__ INFO: \u001b[0mElapsed 367.22\n",
      "\u001b[32m[2020-07-15 05:56:48] __main__ INFO: \u001b[0mVal 35\n",
      "\u001b[32m[2020-07-15 05:57:00] __main__ INFO: \u001b[0mEpoch 35 loss 0.4672 acc@1 0.8502 acc@5 0.9920\n",
      "\u001b[32m[2020-07-15 05:57:00] __main__ INFO: \u001b[0mElapsed 12.27\n",
      "\u001b[32m[2020-07-15 05:57:00] __main__ INFO: \u001b[0mTrain 36 12285\n",
      "\u001b[32m[2020-07-15 05:58:44] __main__ INFO: \u001b[0mEpoch 36 Step 100/351 lr 0.100000 loss 0.9463 (0.9460) acc@1 0.6484 (0.6557) acc@5 0.9062 (0.8959)\n",
      "\u001b[32m[2020-07-15 06:00:28] __main__ INFO: \u001b[0mEpoch 36 Step 200/351 lr 0.100000 loss 1.0857 (0.9641) acc@1 0.5938 (0.6494) acc@5 0.8750 (0.8920)\n",
      "\u001b[32m[2020-07-15 06:02:12] __main__ INFO: \u001b[0mEpoch 36 Step 300/351 lr 0.100000 loss 1.0370 (0.9630) acc@1 0.6406 (0.6493) acc@5 0.8672 (0.8920)\n",
      "\u001b[32m[2020-07-15 06:03:05] __main__ INFO: \u001b[0mEpoch 36 Step 351/351 lr 0.100000 loss 1.0748 (0.9709) acc@1 0.5938 (0.6456) acc@5 0.8594 (0.8915)\n",
      "\u001b[32m[2020-07-15 06:03:05] __main__ INFO: \u001b[0mElapsed 365.42\n",
      "\u001b[32m[2020-07-15 06:03:05] __main__ INFO: \u001b[0mVal 36\n",
      "\u001b[32m[2020-07-15 06:03:18] __main__ INFO: \u001b[0mEpoch 36 loss 0.8445 acc@1 0.7370 acc@5 0.9864\n",
      "\u001b[32m[2020-07-15 06:03:18] __main__ INFO: \u001b[0mElapsed 12.26\n",
      "\u001b[32m[2020-07-15 06:03:18] __main__ INFO: \u001b[0mTrain 37 12636\n",
      "\u001b[32m[2020-07-15 06:05:02] __main__ INFO: \u001b[0mEpoch 37 Step 100/351 lr 0.100000 loss 0.6911 (0.9642) acc@1 0.7422 (0.6491) acc@5 0.9297 (0.8885)\n",
      "\u001b[32m[2020-07-15 06:06:47] __main__ INFO: \u001b[0mEpoch 37 Step 200/351 lr 0.100000 loss 1.1238 (0.9663) acc@1 0.5703 (0.6451) acc@5 0.8984 (0.8897)\n",
      "\u001b[32m[2020-07-15 06:08:31] __main__ INFO: \u001b[0mEpoch 37 Step 300/351 lr 0.100000 loss 0.8562 (0.9708) acc@1 0.6875 (0.6450) acc@5 0.9141 (0.8911)\n",
      "\u001b[32m[2020-07-15 06:09:25] __main__ INFO: \u001b[0mEpoch 37 Step 351/351 lr 0.100000 loss 1.0954 (0.9713) acc@1 0.5859 (0.6454) acc@5 0.8750 (0.8912)\n",
      "\u001b[32m[2020-07-15 06:09:25] __main__ INFO: \u001b[0mElapsed 367.23\n",
      "\u001b[32m[2020-07-15 06:09:25] __main__ INFO: \u001b[0mVal 37\n",
      "\u001b[32m[2020-07-15 06:09:37] __main__ INFO: \u001b[0mEpoch 37 loss 0.5552 acc@1 0.8220 acc@5 0.9886\n",
      "\u001b[32m[2020-07-15 06:09:37] __main__ INFO: \u001b[0mElapsed 12.28\n",
      "\u001b[32m[2020-07-15 06:09:37] __main__ INFO: \u001b[0mTrain 38 12987\n",
      "\u001b[32m[2020-07-15 06:11:21] __main__ INFO: \u001b[0mEpoch 38 Step 100/351 lr 0.100000 loss 1.0329 (0.9731) acc@1 0.5938 (0.6409) acc@5 0.9062 (0.8913)\n",
      "\u001b[32m[2020-07-15 06:13:05] __main__ INFO: \u001b[0mEpoch 38 Step 200/351 lr 0.100000 loss 0.9135 (0.9602) acc@1 0.6641 (0.6465) acc@5 0.8750 (0.8917)\n",
      "\u001b[32m[2020-07-15 06:14:49] __main__ INFO: \u001b[0mEpoch 38 Step 300/351 lr 0.100000 loss 0.9806 (0.9590) acc@1 0.6250 (0.6476) acc@5 0.8750 (0.8928)\n",
      "\u001b[32m[2020-07-15 06:15:42] __main__ INFO: \u001b[0mEpoch 38 Step 351/351 lr 0.100000 loss 0.9571 (0.9647) acc@1 0.6641 (0.6455) acc@5 0.9297 (0.8921)\n",
      "\u001b[32m[2020-07-15 06:15:42] __main__ INFO: \u001b[0mElapsed 365.26\n",
      "\u001b[32m[2020-07-15 06:15:42] __main__ INFO: \u001b[0mVal 38\n",
      "\u001b[32m[2020-07-15 06:15:55] __main__ INFO: \u001b[0mEpoch 38 loss 0.5543 acc@1 0.8008 acc@5 0.9916\n",
      "\u001b[32m[2020-07-15 06:15:55] __main__ INFO: \u001b[0mElapsed 12.27\n",
      "\u001b[32m[2020-07-15 06:15:55] __main__ INFO: \u001b[0mTrain 39 13338\n",
      "\u001b[32m[2020-07-15 06:17:39] __main__ INFO: \u001b[0mEpoch 39 Step 100/351 lr 0.100000 loss 0.9414 (0.9479) acc@1 0.6719 (0.6562) acc@5 0.8984 (0.8931)\n",
      "\u001b[32m[2020-07-15 06:19:24] __main__ INFO: \u001b[0mEpoch 39 Step 200/351 lr 0.100000 loss 0.9888 (0.9515) acc@1 0.6562 (0.6547) acc@5 0.9219 (0.8907)\n",
      "\u001b[32m[2020-07-15 06:21:08] __main__ INFO: \u001b[0mEpoch 39 Step 300/351 lr 0.100000 loss 1.0172 (0.9575) acc@1 0.6094 (0.6527) acc@5 0.8203 (0.8902)\n",
      "\u001b[32m[2020-07-15 06:22:02] __main__ INFO: \u001b[0mEpoch 39 Step 351/351 lr 0.100000 loss 1.0466 (0.9610) acc@1 0.6484 (0.6513) acc@5 0.8750 (0.8907)\n",
      "\u001b[32m[2020-07-15 06:22:02] __main__ INFO: \u001b[0mElapsed 367.05\n",
      "\u001b[32m[2020-07-15 06:22:02] __main__ INFO: \u001b[0mVal 39\n",
      "\u001b[32m[2020-07-15 06:22:14] __main__ INFO: \u001b[0mEpoch 39 loss 0.5295 acc@1 0.8208 acc@5 0.9890\n",
      "\u001b[32m[2020-07-15 06:22:14] __main__ INFO: \u001b[0mElapsed 12.27\n",
      "\u001b[32m[2020-07-15 06:22:14] __main__ INFO: \u001b[0mTrain 40 13689\n",
      "\u001b[32m[2020-07-15 06:23:58] __main__ INFO: \u001b[0mEpoch 40 Step 100/351 lr 0.100000 loss 1.0518 (0.9574) acc@1 0.6406 (0.6530) acc@5 0.8828 (0.8896)\n",
      "\u001b[32m[2020-07-15 06:25:42] __main__ INFO: \u001b[0mEpoch 40 Step 200/351 lr 0.100000 loss 0.9951 (0.9551) acc@1 0.6484 (0.6512) acc@5 0.8750 (0.8929)\n",
      "\u001b[32m[2020-07-15 06:27:26] __main__ INFO: \u001b[0mEpoch 40 Step 300/351 lr 0.100000 loss 1.0445 (0.9569) acc@1 0.6328 (0.6506) acc@5 0.8672 (0.8930)\n",
      "\u001b[32m[2020-07-15 06:28:19] __main__ INFO: \u001b[0mEpoch 40 Step 351/351 lr 0.100000 loss 0.9693 (0.9585) acc@1 0.6328 (0.6507) acc@5 0.8906 (0.8923)\n",
      "\u001b[32m[2020-07-15 06:28:19] __main__ INFO: \u001b[0mElapsed 365.32\n",
      "\u001b[32m[2020-07-15 06:28:19] __main__ INFO: \u001b[0mVal 40\n",
      "\u001b[32m[2020-07-15 06:28:32] __main__ INFO: \u001b[0mEpoch 40 loss 0.4723 acc@1 0.8384 acc@5 0.9906\n",
      "\u001b[32m[2020-07-15 06:28:32] __main__ INFO: \u001b[0mElapsed 12.26\n",
      "\u001b[32m[2020-07-15 06:28:32] __main__ INFO: \u001b[0mTrain 41 14040\n",
      "\u001b[32m[2020-07-15 06:30:16] __main__ INFO: \u001b[0mEpoch 41 Step 100/351 lr 0.100000 loss 1.0185 (0.9285) acc@1 0.6562 (0.6621) acc@5 0.9141 (0.8942)\n",
      "\u001b[32m[2020-07-15 06:32:01] __main__ INFO: \u001b[0mEpoch 41 Step 200/351 lr 0.100000 loss 0.9580 (0.9476) acc@1 0.6406 (0.6533) acc@5 0.8906 (0.8954)\n",
      "\u001b[32m[2020-07-15 06:33:45] __main__ INFO: \u001b[0mEpoch 41 Step 300/351 lr 0.100000 loss 1.0691 (0.9562) acc@1 0.5938 (0.6506) acc@5 0.8828 (0.8932)\n",
      "\u001b[32m[2020-07-15 06:34:38] __main__ INFO: \u001b[0mEpoch 41 Step 351/351 lr 0.100000 loss 0.9442 (0.9590) acc@1 0.6484 (0.6495) acc@5 0.8516 (0.8929)\n",
      "\u001b[32m[2020-07-15 06:34:38] __main__ INFO: \u001b[0mElapsed 366.87\n",
      "\u001b[32m[2020-07-15 06:34:38] __main__ INFO: \u001b[0mVal 41\n",
      "\u001b[32m[2020-07-15 06:34:51] __main__ INFO: \u001b[0mEpoch 41 loss 0.5945 acc@1 0.8060 acc@5 0.9876\n",
      "\u001b[32m[2020-07-15 06:34:51] __main__ INFO: \u001b[0mElapsed 12.27\n",
      "\u001b[32m[2020-07-15 06:34:51] __main__ INFO: \u001b[0mTrain 42 14391\n",
      "\u001b[32m[2020-07-15 06:36:35] __main__ INFO: \u001b[0mEpoch 42 Step 100/351 lr 0.100000 loss 0.8793 (0.9350) acc@1 0.6797 (0.6582) acc@5 0.8594 (0.8951)\n",
      "\u001b[32m[2020-07-15 06:38:19] __main__ INFO: \u001b[0mEpoch 42 Step 200/351 lr 0.100000 loss 1.0890 (0.9458) acc@1 0.5859 (0.6546) acc@5 0.8359 (0.8948)\n",
      "\u001b[32m[2020-07-15 06:40:03] __main__ INFO: \u001b[0mEpoch 42 Step 300/351 lr 0.100000 loss 0.9242 (0.9554) acc@1 0.6719 (0.6505) acc@5 0.9062 (0.8927)\n",
      "\u001b[32m[2020-07-15 06:40:56] __main__ INFO: \u001b[0mEpoch 42 Step 351/351 lr 0.100000 loss 1.0267 (0.9592) acc@1 0.6094 (0.6491) acc@5 0.8203 (0.8925)\n",
      "\u001b[32m[2020-07-15 06:40:56] __main__ INFO: \u001b[0mElapsed 365.27\n",
      "\u001b[32m[2020-07-15 06:40:56] __main__ INFO: \u001b[0mVal 42\n",
      "\u001b[32m[2020-07-15 06:41:08] __main__ INFO: \u001b[0mEpoch 42 loss 0.4860 acc@1 0.8402 acc@5 0.9914\n",
      "\u001b[32m[2020-07-15 06:41:08] __main__ INFO: \u001b[0mElapsed 12.29\n",
      "\u001b[32m[2020-07-15 06:41:08] __main__ INFO: \u001b[0mTrain 43 14742\n",
      "\u001b[32m[2020-07-15 06:42:53] __main__ INFO: \u001b[0mEpoch 43 Step 100/351 lr 0.100000 loss 0.8991 (0.9259) acc@1 0.6641 (0.6616) acc@5 0.8672 (0.8936)\n",
      "\u001b[32m[2020-07-15 06:44:37] __main__ INFO: \u001b[0mEpoch 43 Step 200/351 lr 0.100000 loss 0.7977 (0.9410) acc@1 0.7031 (0.6549) acc@5 0.9062 (0.8925)\n",
      "\u001b[32m[2020-07-15 06:46:22] __main__ INFO: \u001b[0mEpoch 43 Step 300/351 lr 0.100000 loss 1.0502 (0.9513) acc@1 0.5859 (0.6512) acc@5 0.9141 (0.8923)\n",
      "\u001b[32m[2020-07-15 06:47:15] __main__ INFO: \u001b[0mEpoch 43 Step 351/351 lr 0.100000 loss 1.0859 (0.9551) acc@1 0.6250 (0.6499) acc@5 0.8594 (0.8928)\n",
      "\u001b[32m[2020-07-15 06:47:15] __main__ INFO: \u001b[0mElapsed 366.97\n",
      "\u001b[32m[2020-07-15 06:47:15] __main__ INFO: \u001b[0mVal 43\n",
      "\u001b[32m[2020-07-15 06:47:27] __main__ INFO: \u001b[0mEpoch 43 loss 0.4895 acc@1 0.8260 acc@5 0.9924\n",
      "\u001b[32m[2020-07-15 06:47:27] __main__ INFO: \u001b[0mElapsed 12.25\n",
      "\u001b[32m[2020-07-15 06:47:27] __main__ INFO: \u001b[0mTrain 44 15093\n",
      "\u001b[32m[2020-07-15 06:49:12] __main__ INFO: \u001b[0mEpoch 44 Step 100/351 lr 0.100000 loss 1.0380 (0.9113) acc@1 0.6016 (0.6673) acc@5 0.8594 (0.8995)\n",
      "\u001b[32m[2020-07-15 06:50:56] __main__ INFO: \u001b[0mEpoch 44 Step 200/351 lr 0.100000 loss 0.8490 (0.9436) acc@1 0.6797 (0.6565) acc@5 0.9375 (0.8946)\n",
      "\u001b[32m[2020-07-15 06:52:40] __main__ INFO: \u001b[0mEpoch 44 Step 300/351 lr 0.100000 loss 1.0526 (0.9447) acc@1 0.6719 (0.6551) acc@5 0.9062 (0.8931)\n",
      "\u001b[32m[2020-07-15 06:53:33] __main__ INFO: \u001b[0mEpoch 44 Step 351/351 lr 0.100000 loss 0.9754 (0.9479) acc@1 0.6797 (0.6535) acc@5 0.8984 (0.8929)\n",
      "\u001b[32m[2020-07-15 06:53:33] __main__ INFO: \u001b[0mElapsed 365.48\n",
      "\u001b[32m[2020-07-15 06:53:33] __main__ INFO: \u001b[0mVal 44\n",
      "\u001b[32m[2020-07-15 06:53:45] __main__ INFO: \u001b[0mEpoch 44 loss 0.5195 acc@1 0.8198 acc@5 0.9916\n",
      "\u001b[32m[2020-07-15 06:53:45] __main__ INFO: \u001b[0mElapsed 12.26\n",
      "\u001b[32m[2020-07-15 06:53:45] __main__ INFO: \u001b[0mTrain 45 15444\n",
      "\u001b[32m[2020-07-15 06:55:30] __main__ INFO: \u001b[0mEpoch 45 Step 100/351 lr 0.100000 loss 0.9940 (0.9226) acc@1 0.6328 (0.6610) acc@5 0.8516 (0.8978)\n",
      "\u001b[32m[2020-07-15 06:57:14] __main__ INFO: \u001b[0mEpoch 45 Step 200/351 lr 0.100000 loss 0.9705 (0.9418) acc@1 0.6406 (0.6543) acc@5 0.8984 (0.8934)\n",
      "\u001b[32m[2020-07-15 06:58:59] __main__ INFO: \u001b[0mEpoch 45 Step 300/351 lr 0.100000 loss 1.0411 (0.9422) acc@1 0.6094 (0.6530) acc@5 0.8750 (0.8948)\n",
      "\u001b[32m[2020-07-15 06:59:52] __main__ INFO: \u001b[0mEpoch 45 Step 351/351 lr 0.100000 loss 1.0206 (0.9464) acc@1 0.6172 (0.6518) acc@5 0.8516 (0.8945)\n",
      "\u001b[32m[2020-07-15 06:59:52] __main__ INFO: \u001b[0mElapsed 367.22\n",
      "\u001b[32m[2020-07-15 06:59:52] __main__ INFO: \u001b[0mVal 45\n",
      "\u001b[32m[2020-07-15 07:00:05] __main__ INFO: \u001b[0mEpoch 45 loss 0.5124 acc@1 0.8378 acc@5 0.9928\n",
      "\u001b[32m[2020-07-15 07:00:05] __main__ INFO: \u001b[0mElapsed 12.26\n",
      "\u001b[32m[2020-07-15 07:00:05] __main__ INFO: \u001b[0mTrain 46 15795\n",
      "\u001b[32m[2020-07-15 07:01:49] __main__ INFO: \u001b[0mEpoch 46 Step 100/351 lr 0.100000 loss 0.8489 (0.9220) acc@1 0.7031 (0.6609) acc@5 0.9297 (0.8948)\n",
      "\u001b[32m[2020-07-15 07:03:33] __main__ INFO: \u001b[0mEpoch 46 Step 200/351 lr 0.100000 loss 0.9892 (0.9413) acc@1 0.6328 (0.6542) acc@5 0.8516 (0.8947)\n",
      "\u001b[32m[2020-07-15 07:05:17] __main__ INFO: \u001b[0mEpoch 46 Step 300/351 lr 0.100000 loss 0.8200 (0.9436) acc@1 0.6953 (0.6541) acc@5 0.9297 (0.8942)\n",
      "\u001b[32m[2020-07-15 07:06:10] __main__ INFO: \u001b[0mEpoch 46 Step 351/351 lr 0.100000 loss 1.2691 (0.9509) acc@1 0.5156 (0.6519) acc@5 0.8828 (0.8933)\n",
      "\u001b[32m[2020-07-15 07:06:10] __main__ INFO: \u001b[0mElapsed 365.48\n",
      "\u001b[32m[2020-07-15 07:06:10] __main__ INFO: \u001b[0mVal 46\n",
      "\u001b[32m[2020-07-15 07:06:22] __main__ INFO: \u001b[0mEpoch 46 loss 0.4448 acc@1 0.8512 acc@5 0.9946\n",
      "\u001b[32m[2020-07-15 07:06:22] __main__ INFO: \u001b[0mElapsed 12.27\n",
      "\u001b[32m[2020-07-15 07:06:22] __main__ INFO: \u001b[0mTrain 47 16146\n",
      "\u001b[32m[2020-07-15 07:08:07] __main__ INFO: \u001b[0mEpoch 47 Step 100/351 lr 0.100000 loss 1.0499 (0.9074) acc@1 0.5938 (0.6708) acc@5 0.8438 (0.8926)\n",
      "\u001b[32m[2020-07-15 07:09:52] __main__ INFO: \u001b[0mEpoch 47 Step 200/351 lr 0.100000 loss 0.9973 (0.9299) acc@1 0.6484 (0.6613) acc@5 0.9062 (0.8920)\n",
      "\u001b[32m[2020-07-15 07:11:36] __main__ INFO: \u001b[0mEpoch 47 Step 300/351 lr 0.100000 loss 1.1219 (0.9390) acc@1 0.5703 (0.6573) acc@5 0.8984 (0.8916)\n",
      "\u001b[32m[2020-07-15 07:12:30] __main__ INFO: \u001b[0mEpoch 47 Step 351/351 lr 0.100000 loss 0.9169 (0.9426) acc@1 0.6406 (0.6561) acc@5 0.9062 (0.8918)\n",
      "\u001b[32m[2020-07-15 07:12:30] __main__ INFO: \u001b[0mElapsed 367.25\n",
      "\u001b[32m[2020-07-15 07:12:30] __main__ INFO: \u001b[0mVal 47\n",
      "\u001b[32m[2020-07-15 07:12:42] __main__ INFO: \u001b[0mEpoch 47 loss 0.4423 acc@1 0.8590 acc@5 0.9948\n",
      "\u001b[32m[2020-07-15 07:12:42] __main__ INFO: \u001b[0mElapsed 12.28\n",
      "\u001b[32m[2020-07-15 07:12:42] __main__ INFO: \u001b[0mTrain 48 16497\n",
      "\u001b[32m[2020-07-15 07:14:26] __main__ INFO: \u001b[0mEpoch 48 Step 100/351 lr 0.100000 loss 0.9761 (0.9307) acc@1 0.6875 (0.6613) acc@5 0.8828 (0.8944)\n",
      "\u001b[32m[2020-07-15 07:16:10] __main__ INFO: \u001b[0mEpoch 48 Step 200/351 lr 0.100000 loss 0.9019 (0.9417) acc@1 0.6484 (0.6560) acc@5 0.9219 (0.8921)\n",
      "\u001b[32m[2020-07-15 07:17:54] __main__ INFO: \u001b[0mEpoch 48 Step 300/351 lr 0.100000 loss 0.8881 (0.9485) acc@1 0.6719 (0.6543) acc@5 0.8984 (0.8910)\n",
      "\u001b[32m[2020-07-15 07:18:47] __main__ INFO: \u001b[0mEpoch 48 Step 351/351 lr 0.100000 loss 0.8814 (0.9509) acc@1 0.7188 (0.6527) acc@5 0.9141 (0.8912)\n",
      "\u001b[32m[2020-07-15 07:18:47] __main__ INFO: \u001b[0mElapsed 365.35\n",
      "\u001b[32m[2020-07-15 07:18:47] __main__ INFO: \u001b[0mVal 48\n",
      "\u001b[32m[2020-07-15 07:19:00] __main__ INFO: \u001b[0mEpoch 48 loss 0.5860 acc@1 0.8066 acc@5 0.9922\n",
      "\u001b[32m[2020-07-15 07:19:00] __main__ INFO: \u001b[0mElapsed 12.24\n",
      "\u001b[32m[2020-07-15 07:19:00] __main__ INFO: \u001b[0mTrain 49 16848\n",
      "\u001b[32m[2020-07-15 07:20:44] __main__ INFO: \u001b[0mEpoch 49 Step 100/351 lr 0.100000 loss 0.9403 (0.9064) acc@1 0.6484 (0.6673) acc@5 0.8672 (0.8962)\n",
      "\u001b[32m[2020-07-15 07:22:29] __main__ INFO: \u001b[0mEpoch 49 Step 200/351 lr 0.100000 loss 0.8076 (0.9218) acc@1 0.7344 (0.6637) acc@5 0.9062 (0.8963)\n",
      "\u001b[32m[2020-07-15 07:24:13] __main__ INFO: \u001b[0mEpoch 49 Step 300/351 lr 0.100000 loss 0.9223 (0.9364) acc@1 0.6562 (0.6575) acc@5 0.9375 (0.8937)\n",
      "\u001b[32m[2020-07-15 07:25:07] __main__ INFO: \u001b[0mEpoch 49 Step 351/351 lr 0.100000 loss 0.9713 (0.9426) acc@1 0.6562 (0.6560) acc@5 0.9375 (0.8935)\n",
      "\u001b[32m[2020-07-15 07:25:07] __main__ INFO: \u001b[0mElapsed 366.99\n",
      "\u001b[32m[2020-07-15 07:25:07] __main__ INFO: \u001b[0mVal 49\n",
      "\u001b[32m[2020-07-15 07:25:19] __main__ INFO: \u001b[0mEpoch 49 loss 0.5968 acc@1 0.8072 acc@5 0.9818\n",
      "\u001b[32m[2020-07-15 07:25:19] __main__ INFO: \u001b[0mElapsed 12.27\n",
      "\u001b[32m[2020-07-15 07:25:19] __main__ INFO: \u001b[0mTrain 50 17199\n",
      "\u001b[32m[2020-07-15 07:27:03] __main__ INFO: \u001b[0mEpoch 50 Step 100/351 lr 0.100000 loss 0.7391 (0.9197) acc@1 0.7812 (0.6629) acc@5 0.9219 (0.8894)\n",
      "\u001b[32m[2020-07-15 07:28:47] __main__ INFO: \u001b[0mEpoch 50 Step 200/351 lr 0.100000 loss 0.9123 (0.9341) acc@1 0.6641 (0.6565) acc@5 0.9062 (0.8898)\n",
      "\u001b[32m[2020-07-15 07:30:31] __main__ INFO: \u001b[0mEpoch 50 Step 300/351 lr 0.100000 loss 1.0059 (0.9414) acc@1 0.6562 (0.6539) acc@5 0.8906 (0.8913)\n",
      "\u001b[32m[2020-07-15 07:31:24] __main__ INFO: \u001b[0mEpoch 50 Step 351/351 lr 0.100000 loss 1.0322 (0.9461) acc@1 0.6250 (0.6532) acc@5 0.8906 (0.8910)\n",
      "\u001b[32m[2020-07-15 07:31:24] __main__ INFO: \u001b[0mElapsed 365.45\n",
      "\u001b[32m[2020-07-15 07:31:24] __main__ INFO: \u001b[0mVal 50\n",
      "\u001b[32m[2020-07-15 07:31:37] __main__ INFO: \u001b[0mEpoch 50 loss 0.6377 acc@1 0.8116 acc@5 0.9896\n",
      "\u001b[32m[2020-07-15 07:31:37] __main__ INFO: \u001b[0mElapsed 12.26\n",
      "\u001b[32m[2020-07-15 07:31:37] __main__ INFO: \u001b[0mTrain 51 17550\n",
      "\u001b[32m[2020-07-15 07:33:21] __main__ INFO: \u001b[0mEpoch 51 Step 100/351 lr 0.100000 loss 0.9276 (0.9196) acc@1 0.6406 (0.6650) acc@5 0.8984 (0.8896)\n",
      "\u001b[32m[2020-07-15 07:35:06] __main__ INFO: \u001b[0mEpoch 51 Step 200/351 lr 0.100000 loss 0.9067 (0.9307) acc@1 0.6641 (0.6589) acc@5 0.8906 (0.8904)\n",
      "\u001b[32m[2020-07-15 07:36:50] __main__ INFO: \u001b[0mEpoch 51 Step 300/351 lr 0.100000 loss 0.9669 (0.9341) acc@1 0.6641 (0.6584) acc@5 0.9219 (0.8932)\n",
      "\u001b[32m[2020-07-15 07:37:44] __main__ INFO: \u001b[0mEpoch 51 Step 351/351 lr 0.100000 loss 0.9449 (0.9380) acc@1 0.6797 (0.6573) acc@5 0.8750 (0.8931)\n",
      "\u001b[32m[2020-07-15 07:37:44] __main__ INFO: \u001b[0mElapsed 367.17\n",
      "\u001b[32m[2020-07-15 07:37:44] __main__ INFO: \u001b[0mVal 51\n",
      "\u001b[32m[2020-07-15 07:37:56] __main__ INFO: \u001b[0mEpoch 51 loss 0.4531 acc@1 0.8448 acc@5 0.9952\n",
      "\u001b[32m[2020-07-15 07:37:56] __main__ INFO: \u001b[0mElapsed 12.26\n",
      "\u001b[32m[2020-07-15 07:37:56] __main__ INFO: \u001b[0mTrain 52 17901\n",
      "\u001b[32m[2020-07-15 07:39:40] __main__ INFO: \u001b[0mEpoch 52 Step 100/351 lr 0.100000 loss 1.0685 (0.9165) acc@1 0.5938 (0.6674) acc@5 0.8828 (0.8969)\n",
      "\u001b[32m[2020-07-15 07:41:24] __main__ INFO: \u001b[0mEpoch 52 Step 200/351 lr 0.100000 loss 0.9821 (0.9381) acc@1 0.6484 (0.6585) acc@5 0.8516 (0.8938)\n",
      "\u001b[32m[2020-07-15 07:43:08] __main__ INFO: \u001b[0mEpoch 52 Step 300/351 lr 0.100000 loss 1.1541 (0.9379) acc@1 0.6094 (0.6571) acc@5 0.8438 (0.8948)\n",
      "\u001b[32m[2020-07-15 07:44:01] __main__ INFO: \u001b[0mEpoch 52 Step 351/351 lr 0.100000 loss 0.9947 (0.9419) acc@1 0.6328 (0.6556) acc@5 0.8672 (0.8945)\n",
      "\u001b[32m[2020-07-15 07:44:01] __main__ INFO: \u001b[0mElapsed 365.50\n",
      "\u001b[32m[2020-07-15 07:44:01] __main__ INFO: \u001b[0mVal 52\n",
      "\u001b[32m[2020-07-15 07:44:14] __main__ INFO: \u001b[0mEpoch 52 loss 0.4534 acc@1 0.8486 acc@5 0.9936\n",
      "\u001b[32m[2020-07-15 07:44:14] __main__ INFO: \u001b[0mElapsed 12.27\n",
      "\u001b[32m[2020-07-15 07:44:14] __main__ INFO: \u001b[0mTrain 53 18252\n",
      "\u001b[32m[2020-07-15 07:45:58] __main__ INFO: \u001b[0mEpoch 53 Step 100/351 lr 0.100000 loss 1.0461 (0.9220) acc@1 0.6016 (0.6616) acc@5 0.8750 (0.8938)\n",
      "\u001b[32m[2020-07-15 07:47:43] __main__ INFO: \u001b[0mEpoch 53 Step 200/351 lr 0.100000 loss 0.8983 (0.9235) acc@1 0.6953 (0.6622) acc@5 0.9141 (0.8946)\n",
      "\u001b[32m[2020-07-15 07:49:28] __main__ INFO: \u001b[0mEpoch 53 Step 300/351 lr 0.100000 loss 1.1255 (0.9332) acc@1 0.5781 (0.6596) acc@5 0.8750 (0.8937)\n",
      "\u001b[32m[2020-07-15 07:50:21] __main__ INFO: \u001b[0mEpoch 53 Step 351/351 lr 0.100000 loss 0.9092 (0.9392) acc@1 0.7109 (0.6566) acc@5 0.9062 (0.8934)\n",
      "\u001b[32m[2020-07-15 07:50:21] __main__ INFO: \u001b[0mElapsed 367.14\n",
      "\u001b[32m[2020-07-15 07:50:21] __main__ INFO: \u001b[0mVal 53\n",
      "\u001b[32m[2020-07-15 07:50:33] __main__ INFO: \u001b[0mEpoch 53 loss 0.5003 acc@1 0.8332 acc@5 0.9916\n",
      "\u001b[32m[2020-07-15 07:50:33] __main__ INFO: \u001b[0mElapsed 12.26\n",
      "\u001b[32m[2020-07-15 07:50:33] __main__ INFO: \u001b[0mTrain 54 18603\n",
      "\u001b[32m[2020-07-15 07:52:17] __main__ INFO: \u001b[0mEpoch 54 Step 100/351 lr 0.100000 loss 0.9149 (0.8955) acc@1 0.6484 (0.6714) acc@5 0.8438 (0.8962)\n",
      "\u001b[32m[2020-07-15 07:54:01] __main__ INFO: \u001b[0mEpoch 54 Step 200/351 lr 0.100000 loss 1.1617 (0.9191) acc@1 0.5625 (0.6621) acc@5 0.8203 (0.8954)\n",
      "\u001b[32m[2020-07-15 07:55:45] __main__ INFO: \u001b[0mEpoch 54 Step 300/351 lr 0.100000 loss 0.8548 (0.9262) acc@1 0.6797 (0.6605) acc@5 0.9062 (0.8951)\n",
      "\u001b[32m[2020-07-15 07:56:38] __main__ INFO: \u001b[0mEpoch 54 Step 351/351 lr 0.100000 loss 0.8558 (0.9319) acc@1 0.6953 (0.6593) acc@5 0.8984 (0.8951)\n",
      "\u001b[32m[2020-07-15 07:56:38] __main__ INFO: \u001b[0mElapsed 365.33\n",
      "\u001b[32m[2020-07-15 07:56:38] __main__ INFO: \u001b[0mVal 54\n",
      "\u001b[32m[2020-07-15 07:56:51] __main__ INFO: \u001b[0mEpoch 54 loss 0.4748 acc@1 0.8410 acc@5 0.9944\n",
      "\u001b[32m[2020-07-15 07:56:51] __main__ INFO: \u001b[0mElapsed 12.28\n",
      "\u001b[32m[2020-07-15 07:56:51] __main__ INFO: \u001b[0mTrain 55 18954\n",
      "\u001b[32m[2020-07-15 07:58:35] __main__ INFO: \u001b[0mEpoch 55 Step 100/351 lr 0.100000 loss 0.8637 (0.9061) acc@1 0.6719 (0.6677) acc@5 0.8906 (0.8938)\n",
      "\u001b[32m[2020-07-15 08:00:20] __main__ INFO: \u001b[0mEpoch 55 Step 200/351 lr 0.100000 loss 0.7810 (0.9117) acc@1 0.6719 (0.6646) acc@5 0.9062 (0.8931)\n",
      "\u001b[32m[2020-07-15 08:02:04] __main__ INFO: \u001b[0mEpoch 55 Step 300/351 lr 0.100000 loss 1.1324 (0.9232) acc@1 0.5469 (0.6595) acc@5 0.8750 (0.8936)\n",
      "\u001b[32m[2020-07-15 08:02:58] __main__ INFO: \u001b[0mEpoch 55 Step 351/351 lr 0.100000 loss 0.8754 (0.9305) acc@1 0.6953 (0.6573) acc@5 0.8984 (0.8943)\n",
      "\u001b[32m[2020-07-15 08:02:58] __main__ INFO: \u001b[0mElapsed 366.93\n",
      "\u001b[32m[2020-07-15 08:02:58] __main__ INFO: \u001b[0mVal 55\n",
      "\u001b[32m[2020-07-15 08:03:10] __main__ INFO: \u001b[0mEpoch 55 loss 0.5203 acc@1 0.8232 acc@5 0.9880\n",
      "\u001b[32m[2020-07-15 08:03:10] __main__ INFO: \u001b[0mElapsed 12.27\n",
      "\u001b[32m[2020-07-15 08:03:10] __main__ INFO: \u001b[0mTrain 56 19305\n",
      "\u001b[32m[2020-07-15 08:04:54] __main__ INFO: \u001b[0mEpoch 56 Step 100/351 lr 0.100000 loss 0.9942 (0.9082) acc@1 0.6172 (0.6654) acc@5 0.8906 (0.8959)\n",
      "\u001b[32m[2020-07-15 08:06:38] __main__ INFO: \u001b[0mEpoch 56 Step 200/351 lr 0.100000 loss 0.7724 (0.9172) acc@1 0.7422 (0.6634) acc@5 0.9453 (0.8983)\n",
      "\u001b[32m[2020-07-15 08:08:22] __main__ INFO: \u001b[0mEpoch 56 Step 300/351 lr 0.100000 loss 0.9636 (0.9292) acc@1 0.6484 (0.6588) acc@5 0.8828 (0.8971)\n",
      "\u001b[32m[2020-07-15 08:09:15] __main__ INFO: \u001b[0mEpoch 56 Step 351/351 lr 0.100000 loss 0.9474 (0.9318) acc@1 0.6797 (0.6580) acc@5 0.8750 (0.8965)\n",
      "\u001b[32m[2020-07-15 08:09:15] __main__ INFO: \u001b[0mElapsed 365.24\n",
      "\u001b[32m[2020-07-15 08:09:15] __main__ INFO: \u001b[0mVal 56\n",
      "\u001b[32m[2020-07-15 08:09:27] __main__ INFO: \u001b[0mEpoch 56 loss 0.6388 acc@1 0.7954 acc@5 0.9918\n",
      "\u001b[32m[2020-07-15 08:09:27] __main__ INFO: \u001b[0mElapsed 12.26\n",
      "\u001b[32m[2020-07-15 08:09:27] __main__ INFO: \u001b[0mTrain 57 19656\n",
      "\u001b[32m[2020-07-15 08:11:12] __main__ INFO: \u001b[0mEpoch 57 Step 100/351 lr 0.100000 loss 0.9359 (0.9145) acc@1 0.6953 (0.6625) acc@5 0.9062 (0.8946)\n",
      "\u001b[32m[2020-07-15 08:12:57] __main__ INFO: \u001b[0mEpoch 57 Step 200/351 lr 0.100000 loss 0.9001 (0.9238) acc@1 0.6719 (0.6598) acc@5 0.9141 (0.8946)\n",
      "\u001b[32m[2020-07-15 08:14:41] __main__ INFO: \u001b[0mEpoch 57 Step 300/351 lr 0.100000 loss 0.8571 (0.9321) acc@1 0.6797 (0.6577) acc@5 0.9062 (0.8941)\n",
      "\u001b[32m[2020-07-15 08:15:34] __main__ INFO: \u001b[0mEpoch 57 Step 351/351 lr 0.100000 loss 0.9103 (0.9348) acc@1 0.6719 (0.6565) acc@5 0.9062 (0.8938)\n",
      "\u001b[32m[2020-07-15 08:15:34] __main__ INFO: \u001b[0mElapsed 366.84\n",
      "\u001b[32m[2020-07-15 08:15:34] __main__ INFO: \u001b[0mVal 57\n",
      "\u001b[32m[2020-07-15 08:15:47] __main__ INFO: \u001b[0mEpoch 57 loss 0.5181 acc@1 0.8312 acc@5 0.9924\n",
      "\u001b[32m[2020-07-15 08:15:47] __main__ INFO: \u001b[0mElapsed 12.27\n",
      "\u001b[32m[2020-07-15 08:15:47] __main__ INFO: \u001b[0mTrain 58 20007\n",
      "\u001b[32m[2020-07-15 08:17:31] __main__ INFO: \u001b[0mEpoch 58 Step 100/351 lr 0.100000 loss 1.1050 (0.9053) acc@1 0.5859 (0.6710) acc@5 0.8750 (0.8984)\n",
      "\u001b[32m[2020-07-15 08:19:15] __main__ INFO: \u001b[0mEpoch 58 Step 200/351 lr 0.100000 loss 1.1319 (0.9277) acc@1 0.5781 (0.6600) acc@5 0.8594 (0.8936)\n",
      "\u001b[32m[2020-07-15 08:20:59] __main__ INFO: \u001b[0mEpoch 58 Step 300/351 lr 0.100000 loss 0.9852 (0.9224) acc@1 0.6250 (0.6611) acc@5 0.8906 (0.8948)\n",
      "\u001b[32m[2020-07-15 08:21:52] __main__ INFO: \u001b[0mEpoch 58 Step 351/351 lr 0.100000 loss 0.8887 (0.9256) acc@1 0.6875 (0.6601) acc@5 0.8828 (0.8939)\n",
      "\u001b[32m[2020-07-15 08:21:52] __main__ INFO: \u001b[0mElapsed 365.33\n",
      "\u001b[32m[2020-07-15 08:21:52] __main__ INFO: \u001b[0mVal 58\n",
      "\u001b[32m[2020-07-15 08:22:04] __main__ INFO: \u001b[0mEpoch 58 loss 0.4865 acc@1 0.8438 acc@5 0.9940\n",
      "\u001b[32m[2020-07-15 08:22:04] __main__ INFO: \u001b[0mElapsed 12.26\n",
      "\u001b[32m[2020-07-15 08:22:04] __main__ INFO: \u001b[0mTrain 59 20358\n",
      "\u001b[32m[2020-07-15 08:23:49] __main__ INFO: \u001b[0mEpoch 59 Step 100/351 lr 0.100000 loss 0.9148 (0.9059) acc@1 0.6641 (0.6680) acc@5 0.9062 (0.8933)\n",
      "\u001b[32m[2020-07-15 08:25:33] __main__ INFO: \u001b[0mEpoch 59 Step 200/351 lr 0.100000 loss 0.7963 (0.9162) acc@1 0.7344 (0.6630) acc@5 0.9375 (0.8950)\n",
      "\u001b[32m[2020-07-15 08:27:18] __main__ INFO: \u001b[0mEpoch 59 Step 300/351 lr 0.100000 loss 0.9340 (0.9225) acc@1 0.6875 (0.6616) acc@5 0.9141 (0.8931)\n",
      "\u001b[32m[2020-07-15 08:28:11] __main__ INFO: \u001b[0mEpoch 59 Step 351/351 lr 0.100000 loss 0.9272 (0.9285) acc@1 0.6797 (0.6603) acc@5 0.8750 (0.8931)\n",
      "\u001b[32m[2020-07-15 08:28:11] __main__ INFO: \u001b[0mElapsed 367.01\n",
      "\u001b[32m[2020-07-15 08:28:11] __main__ INFO: \u001b[0mVal 59\n",
      "\u001b[32m[2020-07-15 08:28:23] __main__ INFO: \u001b[0mEpoch 59 loss 0.6527 acc@1 0.7846 acc@5 0.9894\n",
      "\u001b[32m[2020-07-15 08:28:23] __main__ INFO: \u001b[0mElapsed 12.27\n",
      "\u001b[32m[2020-07-15 08:28:23] __main__ INFO: \u001b[0mTrain 60 20709\n",
      "\u001b[32m[2020-07-15 08:30:08] __main__ INFO: \u001b[0mEpoch 60 Step 100/351 lr 0.100000 loss 0.8331 (0.8983) acc@1 0.6719 (0.6724) acc@5 0.9297 (0.8995)\n",
      "\u001b[32m[2020-07-15 08:31:52] __main__ INFO: \u001b[0mEpoch 60 Step 200/351 lr 0.100000 loss 0.8727 (0.9204) acc@1 0.7266 (0.6626) acc@5 0.9297 (0.8952)\n",
      "\u001b[32m[2020-07-15 08:33:36] __main__ INFO: \u001b[0mEpoch 60 Step 300/351 lr 0.100000 loss 1.0599 (0.9261) acc@1 0.6328 (0.6601) acc@5 0.9141 (0.8952)\n",
      "\u001b[32m[2020-07-15 08:34:29] __main__ INFO: \u001b[0mEpoch 60 Step 351/351 lr 0.100000 loss 0.9533 (0.9251) acc@1 0.6641 (0.6607) acc@5 0.8594 (0.8954)\n",
      "\u001b[32m[2020-07-15 08:34:29] __main__ INFO: \u001b[0mElapsed 365.26\n",
      "\u001b[32m[2020-07-15 08:34:29] __main__ INFO: \u001b[0mVal 60\n",
      "\u001b[32m[2020-07-15 08:34:41] __main__ INFO: \u001b[0mEpoch 60 loss 0.4064 acc@1 0.8662 acc@5 0.9918\n",
      "\u001b[32m[2020-07-15 08:34:41] __main__ INFO: \u001b[0mElapsed 12.24\n",
      "\u001b[32m[2020-07-15 08:34:41] __main__ INFO: \u001b[0mTrain 61 21060\n",
      "\u001b[32m[2020-07-15 08:36:26] __main__ INFO: \u001b[0mEpoch 61 Step 100/351 lr 0.020000 loss 0.8017 (0.7769) acc@1 0.6797 (0.7141) acc@5 0.8594 (0.8994)\n",
      "\u001b[32m[2020-07-15 08:38:10] __main__ INFO: \u001b[0mEpoch 61 Step 200/351 lr 0.020000 loss 0.6851 (0.7479) acc@1 0.7734 (0.7244) acc@5 0.8750 (0.9009)\n",
      "\u001b[32m[2020-07-15 08:39:54] __main__ INFO: \u001b[0mEpoch 61 Step 300/351 lr 0.020000 loss 0.6995 (0.7303) acc@1 0.7578 (0.7309) acc@5 0.9141 (0.9022)\n",
      "\u001b[32m[2020-07-15 08:40:48] __main__ INFO: \u001b[0mEpoch 61 Step 351/351 lr 0.020000 loss 0.5777 (0.7233) acc@1 0.7812 (0.7335) acc@5 0.9531 (0.9036)\n",
      "\u001b[32m[2020-07-15 08:40:48] __main__ INFO: \u001b[0mElapsed 366.80\n",
      "\u001b[32m[2020-07-15 08:40:48] __main__ INFO: \u001b[0mVal 61\n",
      "\u001b[32m[2020-07-15 08:41:00] __main__ INFO: \u001b[0mEpoch 61 loss 0.2074 acc@1 0.9312 acc@5 0.9982\n",
      "\u001b[32m[2020-07-15 08:41:00] __main__ INFO: \u001b[0mElapsed 12.26\n",
      "\u001b[32m[2020-07-15 08:41:00] __main__ INFO: \u001b[0mTrain 62 21411\n",
      "\u001b[32m[2020-07-15 08:42:44] __main__ INFO: \u001b[0mEpoch 62 Step 100/351 lr 0.020000 loss 0.6800 (0.6441) acc@1 0.7344 (0.7588) acc@5 0.9062 (0.9047)\n",
      "\u001b[32m[2020-07-15 08:44:28] __main__ INFO: \u001b[0mEpoch 62 Step 200/351 lr 0.020000 loss 0.7955 (0.6502) acc@1 0.7109 (0.7567) acc@5 0.8672 (0.9061)\n",
      "\u001b[32m[2020-07-15 08:46:12] __main__ INFO: \u001b[0mEpoch 62 Step 300/351 lr 0.020000 loss 0.5886 (0.6534) acc@1 0.7734 (0.7541) acc@5 0.9141 (0.9040)\n",
      "\u001b[32m[2020-07-15 08:47:05] __main__ INFO: \u001b[0mEpoch 62 Step 351/351 lr 0.020000 loss 0.7985 (0.6557) acc@1 0.7109 (0.7532) acc@5 0.9141 (0.9045)\n",
      "\u001b[32m[2020-07-15 08:47:05] __main__ INFO: \u001b[0mElapsed 365.24\n",
      "\u001b[32m[2020-07-15 08:47:05] __main__ INFO: \u001b[0mVal 62\n",
      "\u001b[32m[2020-07-15 08:47:18] __main__ INFO: \u001b[0mEpoch 62 loss 0.2055 acc@1 0.9324 acc@5 0.9978\n",
      "\u001b[32m[2020-07-15 08:47:18] __main__ INFO: \u001b[0mElapsed 12.25\n",
      "\u001b[32m[2020-07-15 08:47:18] __main__ INFO: \u001b[0mTrain 63 21762\n",
      "\u001b[32m[2020-07-15 08:49:02] __main__ INFO: \u001b[0mEpoch 63 Step 100/351 lr 0.020000 loss 0.6038 (0.6161) acc@1 0.7891 (0.7689) acc@5 0.8984 (0.9067)\n",
      "\u001b[32m[2020-07-15 08:50:47] __main__ INFO: \u001b[0mEpoch 63 Step 200/351 lr 0.020000 loss 0.5393 (0.6159) acc@1 0.7969 (0.7693) acc@5 0.9453 (0.9055)\n",
      "\u001b[32m[2020-07-15 08:52:31] __main__ INFO: \u001b[0mEpoch 63 Step 300/351 lr 0.020000 loss 0.3773 (0.6236) acc@1 0.8594 (0.7661) acc@5 0.9531 (0.9047)\n",
      "\u001b[32m[2020-07-15 08:53:24] __main__ INFO: \u001b[0mEpoch 63 Step 351/351 lr 0.020000 loss 0.6588 (0.6248) acc@1 0.7422 (0.7651) acc@5 0.8906 (0.9048)\n",
      "\u001b[32m[2020-07-15 08:53:24] __main__ INFO: \u001b[0mElapsed 366.75\n",
      "\u001b[32m[2020-07-15 08:53:24] __main__ INFO: \u001b[0mVal 63\n",
      "\u001b[32m[2020-07-15 08:53:37] __main__ INFO: \u001b[0mEpoch 63 loss 0.2097 acc@1 0.9304 acc@5 0.9982\n",
      "\u001b[32m[2020-07-15 08:53:37] __main__ INFO: \u001b[0mElapsed 12.28\n",
      "\u001b[32m[2020-07-15 08:53:37] __main__ INFO: \u001b[0mTrain 64 22113\n",
      "\u001b[32m[2020-07-15 08:55:21] __main__ INFO: \u001b[0mEpoch 64 Step 100/351 lr 0.020000 loss 0.6197 (0.6050) acc@1 0.7500 (0.7716) acc@5 0.9141 (0.9063)\n",
      "\u001b[32m[2020-07-15 08:57:05] __main__ INFO: \u001b[0mEpoch 64 Step 200/351 lr 0.020000 loss 0.7144 (0.6040) acc@1 0.7266 (0.7722) acc@5 0.8906 (0.9073)\n",
      "\u001b[32m[2020-07-15 08:58:48] __main__ INFO: \u001b[0mEpoch 64 Step 300/351 lr 0.020000 loss 0.7020 (0.6069) acc@1 0.7422 (0.7707) acc@5 0.8750 (0.9060)\n",
      "\u001b[32m[2020-07-15 08:59:41] __main__ INFO: \u001b[0mEpoch 64 Step 351/351 lr 0.020000 loss 0.5028 (0.6073) acc@1 0.8281 (0.7704) acc@5 0.9375 (0.9057)\n",
      "\u001b[32m[2020-07-15 08:59:42] __main__ INFO: \u001b[0mElapsed 364.95\n",
      "\u001b[32m[2020-07-15 08:59:42] __main__ INFO: \u001b[0mVal 64\n",
      "\u001b[32m[2020-07-15 08:59:54] __main__ INFO: \u001b[0mEpoch 64 loss 0.2079 acc@1 0.9348 acc@5 0.9986\n",
      "\u001b[32m[2020-07-15 08:59:54] __main__ INFO: \u001b[0mElapsed 12.24\n",
      "\u001b[32m[2020-07-15 08:59:54] __main__ INFO: \u001b[0mTrain 65 22464\n",
      "\u001b[32m[2020-07-15 09:01:38] __main__ INFO: \u001b[0mEpoch 65 Step 100/351 lr 0.020000 loss 0.3702 (0.5813) acc@1 0.8516 (0.7804) acc@5 0.9219 (0.9036)\n",
      "\u001b[32m[2020-07-15 09:03:23] __main__ INFO: \u001b[0mEpoch 65 Step 200/351 lr 0.020000 loss 0.4846 (0.5916) acc@1 0.8359 (0.7770) acc@5 0.9219 (0.9047)\n",
      "\u001b[32m[2020-07-15 09:05:07] __main__ INFO: \u001b[0mEpoch 65 Step 300/351 lr 0.020000 loss 0.5615 (0.5938) acc@1 0.7969 (0.7752) acc@5 0.9453 (0.9056)\n",
      "\u001b[32m[2020-07-15 09:06:00] __main__ INFO: \u001b[0mEpoch 65 Step 351/351 lr 0.020000 loss 0.6284 (0.5971) acc@1 0.7344 (0.7739) acc@5 0.8828 (0.9056)\n",
      "\u001b[32m[2020-07-15 09:06:00] __main__ INFO: \u001b[0mElapsed 366.50\n",
      "\u001b[32m[2020-07-15 09:06:00] __main__ INFO: \u001b[0mVal 65\n",
      "\u001b[32m[2020-07-15 09:06:12] __main__ INFO: \u001b[0mEpoch 65 loss 0.2107 acc@1 0.9318 acc@5 0.9984\n",
      "\u001b[32m[2020-07-15 09:06:12] __main__ INFO: \u001b[0mElapsed 12.23\n",
      "\u001b[32m[2020-07-15 09:06:12] __main__ INFO: \u001b[0mTrain 66 22815\n",
      "\u001b[32m[2020-07-15 09:07:57] __main__ INFO: \u001b[0mEpoch 66 Step 100/351 lr 0.020000 loss 0.7728 (0.5716) acc@1 0.6953 (0.7866) acc@5 0.8750 (0.9066)\n",
      "\u001b[32m[2020-07-15 09:09:40] __main__ INFO: \u001b[0mEpoch 66 Step 200/351 lr 0.020000 loss 0.6998 (0.5816) acc@1 0.7188 (0.7816) acc@5 0.9062 (0.9057)\n",
      "\u001b[32m[2020-07-15 09:11:24] __main__ INFO: \u001b[0mEpoch 66 Step 300/351 lr 0.020000 loss 0.7699 (0.5853) acc@1 0.7188 (0.7797) acc@5 0.8672 (0.9066)\n",
      "\u001b[32m[2020-07-15 09:12:17] __main__ INFO: \u001b[0mEpoch 66 Step 351/351 lr 0.020000 loss 0.6284 (0.5896) acc@1 0.7344 (0.7777) acc@5 0.9062 (0.9063)\n",
      "\u001b[32m[2020-07-15 09:12:17] __main__ INFO: \u001b[0mElapsed 364.93\n",
      "\u001b[32m[2020-07-15 09:12:17] __main__ INFO: \u001b[0mVal 66\n",
      "\u001b[32m[2020-07-15 09:12:30] __main__ INFO: \u001b[0mEpoch 66 loss 0.1947 acc@1 0.9420 acc@5 0.9992\n",
      "\u001b[32m[2020-07-15 09:12:30] __main__ INFO: \u001b[0mElapsed 12.26\n",
      "\u001b[32m[2020-07-15 09:12:30] __main__ INFO: \u001b[0mTrain 67 23166\n",
      "\u001b[32m[2020-07-15 09:14:14] __main__ INFO: \u001b[0mEpoch 67 Step 100/351 lr 0.020000 loss 0.4561 (0.5552) acc@1 0.8359 (0.7883) acc@5 0.9219 (0.9065)\n",
      "\u001b[32m[2020-07-15 09:15:59] __main__ INFO: \u001b[0mEpoch 67 Step 200/351 lr 0.020000 loss 0.5861 (0.5721) acc@1 0.7578 (0.7826) acc@5 0.9062 (0.9067)\n",
      "\u001b[32m[2020-07-15 09:17:43] __main__ INFO: \u001b[0mEpoch 67 Step 300/351 lr 0.020000 loss 0.6547 (0.5788) acc@1 0.7578 (0.7812) acc@5 0.9141 (0.9059)\n",
      "\u001b[32m[2020-07-15 09:18:36] __main__ INFO: \u001b[0mEpoch 67 Step 351/351 lr 0.020000 loss 0.7389 (0.5817) acc@1 0.7188 (0.7799) acc@5 0.8906 (0.9062)\n",
      "\u001b[32m[2020-07-15 09:18:36] __main__ INFO: \u001b[0mElapsed 366.65\n",
      "\u001b[32m[2020-07-15 09:18:36] __main__ INFO: \u001b[0mVal 67\n",
      "\u001b[32m[2020-07-15 09:18:49] __main__ INFO: \u001b[0mEpoch 67 loss 0.2168 acc@1 0.9344 acc@5 0.9984\n",
      "\u001b[32m[2020-07-15 09:18:49] __main__ INFO: \u001b[0mElapsed 12.25\n",
      "\u001b[32m[2020-07-15 09:18:49] __main__ INFO: \u001b[0mTrain 68 23517\n",
      "\u001b[32m[2020-07-15 09:20:33] __main__ INFO: \u001b[0mEpoch 68 Step 100/351 lr 0.020000 loss 0.5246 (0.5821) acc@1 0.7891 (0.7805) acc@5 0.9062 (0.9074)\n",
      "\u001b[32m[2020-07-15 09:22:17] __main__ INFO: \u001b[0mEpoch 68 Step 200/351 lr 0.020000 loss 0.5380 (0.5790) acc@1 0.8203 (0.7825) acc@5 0.9219 (0.9090)\n",
      "\u001b[32m[2020-07-15 09:24:00] __main__ INFO: \u001b[0mEpoch 68 Step 300/351 lr 0.020000 loss 0.7324 (0.5801) acc@1 0.7188 (0.7817) acc@5 0.8594 (0.9081)\n",
      "\u001b[32m[2020-07-15 09:24:53] __main__ INFO: \u001b[0mEpoch 68 Step 351/351 lr 0.020000 loss 0.6012 (0.5833) acc@1 0.7422 (0.7804) acc@5 0.8906 (0.9084)\n",
      "\u001b[32m[2020-07-15 09:24:53] __main__ INFO: \u001b[0mElapsed 364.80\n",
      "\u001b[32m[2020-07-15 09:24:53] __main__ INFO: \u001b[0mVal 68\n",
      "\u001b[32m[2020-07-15 09:25:06] __main__ INFO: \u001b[0mEpoch 68 loss 0.3161 acc@1 0.9060 acc@5 0.9982\n",
      "\u001b[32m[2020-07-15 09:25:06] __main__ INFO: \u001b[0mElapsed 12.26\n",
      "\u001b[32m[2020-07-15 09:25:06] __main__ INFO: \u001b[0mTrain 69 23868\n",
      "\u001b[32m[2020-07-15 09:26:50] __main__ INFO: \u001b[0mEpoch 69 Step 100/351 lr 0.020000 loss 0.5190 (0.5597) acc@1 0.8203 (0.7870) acc@5 0.9297 (0.9055)\n",
      "\u001b[32m[2020-07-15 09:28:35] __main__ INFO: \u001b[0mEpoch 69 Step 200/351 lr 0.020000 loss 0.5790 (0.5687) acc@1 0.7734 (0.7835) acc@5 0.9219 (0.9057)\n",
      "\u001b[32m[2020-07-15 09:30:19] __main__ INFO: \u001b[0mEpoch 69 Step 300/351 lr 0.020000 loss 0.5654 (0.5726) acc@1 0.8125 (0.7815) acc@5 0.9375 (0.9065)\n",
      "\u001b[32m[2020-07-15 09:31:12] __main__ INFO: \u001b[0mEpoch 69 Step 351/351 lr 0.020000 loss 0.6522 (0.5791) acc@1 0.7578 (0.7793) acc@5 0.9062 (0.9065)\n",
      "\u001b[32m[2020-07-15 09:31:12] __main__ INFO: \u001b[0mElapsed 366.42\n",
      "\u001b[32m[2020-07-15 09:31:12] __main__ INFO: \u001b[0mVal 69\n",
      "\u001b[32m[2020-07-15 09:31:24] __main__ INFO: \u001b[0mEpoch 69 loss 0.2387 acc@1 0.9284 acc@5 0.9976\n",
      "\u001b[32m[2020-07-15 09:31:24] __main__ INFO: \u001b[0mElapsed 12.26\n",
      "\u001b[32m[2020-07-15 09:31:24] __main__ INFO: \u001b[0mTrain 70 24219\n",
      "\u001b[32m[2020-07-15 09:33:08] __main__ INFO: \u001b[0mEpoch 70 Step 100/351 lr 0.020000 loss 0.6363 (0.5594) acc@1 0.7656 (0.7895) acc@5 0.9062 (0.9073)\n",
      "\u001b[32m[2020-07-15 09:34:52] __main__ INFO: \u001b[0mEpoch 70 Step 200/351 lr 0.020000 loss 0.4823 (0.5635) acc@1 0.8203 (0.7876) acc@5 0.9453 (0.9081)\n",
      "\u001b[32m[2020-07-15 09:36:36] __main__ INFO: \u001b[0mEpoch 70 Step 300/351 lr 0.020000 loss 0.5011 (0.5713) acc@1 0.8203 (0.7844) acc@5 0.8750 (0.9074)\n",
      "\u001b[32m[2020-07-15 09:37:29] __main__ INFO: \u001b[0mEpoch 70 Step 351/351 lr 0.020000 loss 0.6308 (0.5745) acc@1 0.7500 (0.7830) acc@5 0.8828 (0.9076)\n",
      "\u001b[32m[2020-07-15 09:37:29] __main__ INFO: \u001b[0mElapsed 364.76\n",
      "\u001b[32m[2020-07-15 09:37:29] __main__ INFO: \u001b[0mVal 70\n",
      "\u001b[32m[2020-07-15 09:37:41] __main__ INFO: \u001b[0mEpoch 70 loss 0.2374 acc@1 0.9290 acc@5 0.9978\n",
      "\u001b[32m[2020-07-15 09:37:41] __main__ INFO: \u001b[0mElapsed 12.24\n",
      "\u001b[32m[2020-07-15 09:37:41] __main__ INFO: \u001b[0mTrain 71 24570\n",
      "\u001b[32m[2020-07-15 09:39:26] __main__ INFO: \u001b[0mEpoch 71 Step 100/351 lr 0.020000 loss 0.5172 (0.5563) acc@1 0.8203 (0.7889) acc@5 0.9453 (0.9067)\n",
      "\u001b[32m[2020-07-15 09:41:10] __main__ INFO: \u001b[0mEpoch 71 Step 200/351 lr 0.020000 loss 0.6500 (0.5665) acc@1 0.7266 (0.7842) acc@5 0.8828 (0.9075)\n",
      "\u001b[32m[2020-07-15 09:42:54] __main__ INFO: \u001b[0mEpoch 71 Step 300/351 lr 0.020000 loss 0.6942 (0.5763) acc@1 0.7422 (0.7812) acc@5 0.9219 (0.9080)\n",
      "\u001b[32m[2020-07-15 09:43:48] __main__ INFO: \u001b[0mEpoch 71 Step 351/351 lr 0.020000 loss 0.6435 (0.5796) acc@1 0.7656 (0.7796) acc@5 0.8984 (0.9069)\n",
      "\u001b[32m[2020-07-15 09:43:48] __main__ INFO: \u001b[0mElapsed 366.40\n",
      "\u001b[32m[2020-07-15 09:43:48] __main__ INFO: \u001b[0mVal 71\n",
      "\u001b[32m[2020-07-15 09:44:00] __main__ INFO: \u001b[0mEpoch 71 loss 0.2172 acc@1 0.9326 acc@5 0.9990\n",
      "\u001b[32m[2020-07-15 09:44:00] __main__ INFO: \u001b[0mElapsed 12.25\n",
      "\u001b[32m[2020-07-15 09:44:00] __main__ INFO: \u001b[0mTrain 72 24921\n",
      "\u001b[32m[2020-07-15 09:45:44] __main__ INFO: \u001b[0mEpoch 72 Step 100/351 lr 0.020000 loss 0.5673 (0.5501) acc@1 0.7891 (0.7909) acc@5 0.9297 (0.9066)\n",
      "\u001b[32m[2020-07-15 09:47:28] __main__ INFO: \u001b[0mEpoch 72 Step 200/351 lr 0.020000 loss 0.6421 (0.5637) acc@1 0.7734 (0.7863) acc@5 0.9062 (0.9063)\n",
      "\u001b[32m[2020-07-15 09:49:12] __main__ INFO: \u001b[0mEpoch 72 Step 300/351 lr 0.020000 loss 0.6795 (0.5698) acc@1 0.7031 (0.7835) acc@5 0.9062 (0.9075)\n",
      "\u001b[32m[2020-07-15 09:50:05] __main__ INFO: \u001b[0mEpoch 72 Step 351/351 lr 0.020000 loss 0.5175 (0.5741) acc@1 0.8125 (0.7817) acc@5 0.9141 (0.9066)\n",
      "\u001b[32m[2020-07-15 09:50:05] __main__ INFO: \u001b[0mElapsed 364.80\n",
      "\u001b[32m[2020-07-15 09:50:05] __main__ INFO: \u001b[0mVal 72\n",
      "\u001b[32m[2020-07-15 09:50:17] __main__ INFO: \u001b[0mEpoch 72 loss 0.2845 acc@1 0.9176 acc@5 0.9976\n",
      "\u001b[32m[2020-07-15 09:50:17] __main__ INFO: \u001b[0mElapsed 12.24\n",
      "\u001b[32m[2020-07-15 09:50:17] __main__ INFO: \u001b[0mTrain 73 25272\n",
      "\u001b[32m[2020-07-15 09:52:02] __main__ INFO: \u001b[0mEpoch 73 Step 100/351 lr 0.020000 loss 0.4249 (0.5651) acc@1 0.8281 (0.7916) acc@5 0.9297 (0.9069)\n",
      "\u001b[32m[2020-07-15 09:53:46] __main__ INFO: \u001b[0mEpoch 73 Step 200/351 lr 0.020000 loss 0.4106 (0.5692) acc@1 0.8438 (0.7871) acc@5 0.9297 (0.9057)\n",
      "\u001b[32m[2020-07-15 09:55:30] __main__ INFO: \u001b[0mEpoch 73 Step 300/351 lr 0.020000 loss 0.5487 (0.5715) acc@1 0.8203 (0.7860) acc@5 0.9297 (0.9072)\n",
      "\u001b[32m[2020-07-15 09:56:24] __main__ INFO: \u001b[0mEpoch 73 Step 351/351 lr 0.020000 loss 0.6021 (0.5772) acc@1 0.7500 (0.7836) acc@5 0.9141 (0.9069)\n",
      "\u001b[32m[2020-07-15 09:56:24] __main__ INFO: \u001b[0mElapsed 366.49\n",
      "\u001b[32m[2020-07-15 09:56:24] __main__ INFO: \u001b[0mVal 73\n",
      "\u001b[32m[2020-07-15 09:56:36] __main__ INFO: \u001b[0mEpoch 73 loss 0.2314 acc@1 0.9340 acc@5 0.9978\n",
      "\u001b[32m[2020-07-15 09:56:36] __main__ INFO: \u001b[0mElapsed 12.26\n",
      "\u001b[32m[2020-07-15 09:56:36] __main__ INFO: \u001b[0mTrain 74 25623\n",
      "\u001b[32m[2020-07-15 09:58:20] __main__ INFO: \u001b[0mEpoch 74 Step 100/351 lr 0.020000 loss 0.7561 (0.5779) acc@1 0.6953 (0.7807) acc@5 0.8906 (0.9041)\n",
      "\u001b[32m[2020-07-15 10:00:04] __main__ INFO: \u001b[0mEpoch 74 Step 200/351 lr 0.020000 loss 0.5403 (0.5848) acc@1 0.8125 (0.7791) acc@5 0.9141 (0.9056)\n",
      "\u001b[32m[2020-07-15 10:01:48] __main__ INFO: \u001b[0mEpoch 74 Step 300/351 lr 0.020000 loss 0.5487 (0.5809) acc@1 0.8125 (0.7813) acc@5 0.9375 (0.9054)\n",
      "\u001b[32m[2020-07-15 10:02:41] __main__ INFO: \u001b[0mEpoch 74 Step 351/351 lr 0.020000 loss 0.5953 (0.5806) acc@1 0.7734 (0.7810) acc@5 0.8672 (0.9056)\n",
      "\u001b[32m[2020-07-15 10:02:41] __main__ INFO: \u001b[0mElapsed 364.95\n",
      "\u001b[32m[2020-07-15 10:02:41] __main__ INFO: \u001b[0mVal 74\n",
      "\u001b[32m[2020-07-15 10:02:53] __main__ INFO: \u001b[0mEpoch 74 loss 0.2193 acc@1 0.9346 acc@5 0.9988\n",
      "\u001b[32m[2020-07-15 10:02:53] __main__ INFO: \u001b[0mElapsed 12.24\n",
      "\u001b[32m[2020-07-15 10:02:53] __main__ INFO: \u001b[0mTrain 75 25974\n",
      "\u001b[32m[2020-07-15 10:04:38] __main__ INFO: \u001b[0mEpoch 75 Step 100/351 lr 0.020000 loss 0.5308 (0.5542) acc@1 0.7891 (0.7937) acc@5 0.8984 (0.9097)\n",
      "\u001b[32m[2020-07-15 10:06:22] __main__ INFO: \u001b[0mEpoch 75 Step 200/351 lr 0.020000 loss 0.4368 (0.5648) acc@1 0.8594 (0.7870) acc@5 0.9219 (0.9084)\n",
      "\u001b[32m[2020-07-15 10:08:06] __main__ INFO: \u001b[0mEpoch 75 Step 300/351 lr 0.020000 loss 0.8751 (0.5768) acc@1 0.6641 (0.7824) acc@5 0.8828 (0.9070)\n",
      "\u001b[32m[2020-07-15 10:09:00] __main__ INFO: \u001b[0mEpoch 75 Step 351/351 lr 0.020000 loss 0.4428 (0.5775) acc@1 0.8359 (0.7821) acc@5 0.9453 (0.9072)\n",
      "\u001b[32m[2020-07-15 10:09:00] __main__ INFO: \u001b[0mElapsed 366.65\n",
      "\u001b[32m[2020-07-15 10:09:00] __main__ INFO: \u001b[0mVal 75\n",
      "\u001b[32m[2020-07-15 10:09:12] __main__ INFO: \u001b[0mEpoch 75 loss 0.2864 acc@1 0.9162 acc@5 0.9988\n",
      "\u001b[32m[2020-07-15 10:09:12] __main__ INFO: \u001b[0mElapsed 12.24\n",
      "\u001b[32m[2020-07-15 10:09:12] __main__ INFO: \u001b[0mTrain 76 26325\n",
      "\u001b[32m[2020-07-15 10:10:56] __main__ INFO: \u001b[0mEpoch 76 Step 100/351 lr 0.020000 loss 0.4895 (0.5596) acc@1 0.8125 (0.7882) acc@5 0.8984 (0.9084)\n",
      "\u001b[32m[2020-07-15 10:12:40] __main__ INFO: \u001b[0mEpoch 76 Step 200/351 lr 0.020000 loss 0.5678 (0.5733) acc@1 0.7734 (0.7840) acc@5 0.9062 (0.9077)\n",
      "\u001b[32m[2020-07-15 10:14:24] __main__ INFO: \u001b[0mEpoch 76 Step 300/351 lr 0.020000 loss 0.5937 (0.5757) acc@1 0.7656 (0.7822) acc@5 0.9141 (0.9086)\n",
      "\u001b[32m[2020-07-15 10:15:17] __main__ INFO: \u001b[0mEpoch 76 Step 351/351 lr 0.020000 loss 0.5111 (0.5788) acc@1 0.8047 (0.7805) acc@5 0.8984 (0.9079)\n",
      "\u001b[32m[2020-07-15 10:15:17] __main__ INFO: \u001b[0mElapsed 364.91\n",
      "\u001b[32m[2020-07-15 10:15:17] __main__ INFO: \u001b[0mVal 76\n",
      "\u001b[32m[2020-07-15 10:15:29] __main__ INFO: \u001b[0mEpoch 76 loss 0.3378 acc@1 0.9072 acc@5 0.9980\n",
      "\u001b[32m[2020-07-15 10:15:29] __main__ INFO: \u001b[0mElapsed 12.25\n",
      "\u001b[32m[2020-07-15 10:15:29] __main__ INFO: \u001b[0mTrain 77 26676\n",
      "\u001b[32m[2020-07-15 10:17:14] __main__ INFO: \u001b[0mEpoch 77 Step 100/351 lr 0.020000 loss 0.5825 (0.5484) acc@1 0.7969 (0.7930) acc@5 0.9297 (0.9073)\n",
      "\u001b[32m[2020-07-15 10:18:58] __main__ INFO: \u001b[0mEpoch 77 Step 200/351 lr 0.020000 loss 0.6508 (0.5565) acc@1 0.7578 (0.7906) acc@5 0.9062 (0.9080)\n",
      "\u001b[32m[2020-07-15 10:20:42] __main__ INFO: \u001b[0mEpoch 77 Step 300/351 lr 0.020000 loss 0.5955 (0.5648) acc@1 0.7969 (0.7868) acc@5 0.9297 (0.9078)\n",
      "\u001b[32m[2020-07-15 10:21:36] __main__ INFO: \u001b[0mEpoch 77 Step 351/351 lr 0.020000 loss 0.5892 (0.5687) acc@1 0.7656 (0.7849) acc@5 0.8906 (0.9073)\n",
      "\u001b[32m[2020-07-15 10:21:36] __main__ INFO: \u001b[0mElapsed 366.56\n",
      "\u001b[32m[2020-07-15 10:21:36] __main__ INFO: \u001b[0mVal 77\n",
      "\u001b[32m[2020-07-15 10:21:48] __main__ INFO: \u001b[0mEpoch 77 loss 0.3002 acc@1 0.9110 acc@5 0.9974\n",
      "\u001b[32m[2020-07-15 10:21:48] __main__ INFO: \u001b[0mElapsed 12.27\n",
      "\u001b[32m[2020-07-15 10:21:48] __main__ INFO: \u001b[0mTrain 78 27027\n",
      "\u001b[32m[2020-07-15 10:23:32] __main__ INFO: \u001b[0mEpoch 78 Step 100/351 lr 0.020000 loss 0.4453 (0.5752) acc@1 0.8203 (0.7831) acc@5 0.9219 (0.9080)\n",
      "\u001b[32m[2020-07-15 10:25:16] __main__ INFO: \u001b[0mEpoch 78 Step 200/351 lr 0.020000 loss 0.6196 (0.5734) acc@1 0.7734 (0.7837) acc@5 0.8750 (0.9079)\n",
      "\u001b[32m[2020-07-15 10:27:00] __main__ INFO: \u001b[0mEpoch 78 Step 300/351 lr 0.020000 loss 0.5240 (0.5749) acc@1 0.8047 (0.7817) acc@5 0.9375 (0.9075)\n",
      "\u001b[32m[2020-07-15 10:27:53] __main__ INFO: \u001b[0mEpoch 78 Step 351/351 lr 0.020000 loss 0.6228 (0.5811) acc@1 0.7578 (0.7797) acc@5 0.9219 (0.9074)\n",
      "\u001b[32m[2020-07-15 10:27:53] __main__ INFO: \u001b[0mElapsed 364.87\n",
      "\u001b[32m[2020-07-15 10:27:53] __main__ INFO: \u001b[0mVal 78\n",
      "\u001b[32m[2020-07-15 10:28:05] __main__ INFO: \u001b[0mEpoch 78 loss 0.2393 acc@1 0.9280 acc@5 0.9986\n",
      "\u001b[32m[2020-07-15 10:28:05] __main__ INFO: \u001b[0mElapsed 12.24\n",
      "\u001b[32m[2020-07-15 10:28:05] __main__ INFO: \u001b[0mTrain 79 27378\n",
      "\u001b[32m[2020-07-15 10:29:49] __main__ INFO: \u001b[0mEpoch 79 Step 100/351 lr 0.020000 loss 0.5920 (0.5596) acc@1 0.7812 (0.7888) acc@5 0.8906 (0.9080)\n",
      "\u001b[32m[2020-07-15 10:31:34] __main__ INFO: \u001b[0mEpoch 79 Step 200/351 lr 0.020000 loss 0.5552 (0.5624) acc@1 0.7812 (0.7881) acc@5 0.8984 (0.9062)\n",
      "\u001b[32m[2020-07-15 10:33:18] __main__ INFO: \u001b[0mEpoch 79 Step 300/351 lr 0.020000 loss 0.8083 (0.5698) acc@1 0.6953 (0.7845) acc@5 0.8906 (0.9078)\n",
      "\u001b[32m[2020-07-15 10:34:12] __main__ INFO: \u001b[0mEpoch 79 Step 351/351 lr 0.020000 loss 0.6673 (0.5724) acc@1 0.7500 (0.7831) acc@5 0.8906 (0.9074)\n",
      "\u001b[32m[2020-07-15 10:34:12] __main__ INFO: \u001b[0mElapsed 366.56\n",
      "\u001b[32m[2020-07-15 10:34:12] __main__ INFO: \u001b[0mVal 79\n",
      "\u001b[32m[2020-07-15 10:34:24] __main__ INFO: \u001b[0mEpoch 79 loss 0.2820 acc@1 0.9204 acc@5 0.9978\n",
      "\u001b[32m[2020-07-15 10:34:24] __main__ INFO: \u001b[0mElapsed 12.25\n",
      "\u001b[32m[2020-07-15 10:34:24] __main__ INFO: \u001b[0mTrain 80 27729\n",
      "\u001b[32m[2020-07-15 10:36:08] __main__ INFO: \u001b[0mEpoch 80 Step 100/351 lr 0.020000 loss 0.4998 (0.5557) acc@1 0.7969 (0.7894) acc@5 0.9062 (0.9081)\n",
      "\u001b[32m[2020-07-15 10:37:52] __main__ INFO: \u001b[0mEpoch 80 Step 200/351 lr 0.020000 loss 0.5788 (0.5656) acc@1 0.7812 (0.7851) acc@5 0.8984 (0.9086)\n",
      "\u001b[32m[2020-07-15 10:39:36] __main__ INFO: \u001b[0mEpoch 80 Step 300/351 lr 0.020000 loss 0.6485 (0.5739) acc@1 0.7500 (0.7817) acc@5 0.9062 (0.9070)\n",
      "\u001b[32m[2020-07-15 10:40:29] __main__ INFO: \u001b[0mEpoch 80 Step 351/351 lr 0.020000 loss 0.5796 (0.5783) acc@1 0.7734 (0.7805) acc@5 0.8984 (0.9065)\n",
      "\u001b[32m[2020-07-15 10:40:29] __main__ INFO: \u001b[0mElapsed 365.00\n",
      "\u001b[32m[2020-07-15 10:40:29] __main__ INFO: \u001b[0mVal 80\n",
      "\u001b[32m[2020-07-15 10:40:41] __main__ INFO: \u001b[0mEpoch 80 loss 0.2511 acc@1 0.9258 acc@5 0.9986\n",
      "\u001b[32m[2020-07-15 10:40:41] __main__ INFO: \u001b[0mElapsed 12.27\n",
      "\u001b[32m[2020-07-15 10:40:41] __main__ INFO: \u001b[0mTrain 81 28080\n",
      "\u001b[32m[2020-07-15 10:42:26] __main__ INFO: \u001b[0mEpoch 81 Step 100/351 lr 0.020000 loss 0.4928 (0.5586) acc@1 0.7969 (0.7888) acc@5 0.9062 (0.9081)\n",
      "\u001b[32m[2020-07-15 10:44:10] __main__ INFO: \u001b[0mEpoch 81 Step 200/351 lr 0.020000 loss 0.4912 (0.5692) acc@1 0.8203 (0.7848) acc@5 0.9219 (0.9087)\n",
      "\u001b[32m[2020-07-15 10:45:54] __main__ INFO: \u001b[0mEpoch 81 Step 300/351 lr 0.020000 loss 0.6116 (0.5776) acc@1 0.7500 (0.7814) acc@5 0.9609 (0.9078)\n",
      "\u001b[32m[2020-07-15 10:46:48] __main__ INFO: \u001b[0mEpoch 81 Step 351/351 lr 0.020000 loss 0.4794 (0.5779) acc@1 0.8281 (0.7818) acc@5 0.9375 (0.9077)\n",
      "\u001b[32m[2020-07-15 10:46:48] __main__ INFO: \u001b[0mElapsed 366.74\n",
      "\u001b[32m[2020-07-15 10:46:48] __main__ INFO: \u001b[0mVal 81\n",
      "\u001b[32m[2020-07-15 10:47:00] __main__ INFO: \u001b[0mEpoch 81 loss 0.2379 acc@1 0.9292 acc@5 0.9986\n",
      "\u001b[32m[2020-07-15 10:47:00] __main__ INFO: \u001b[0mElapsed 12.27\n",
      "\u001b[32m[2020-07-15 10:47:00] __main__ INFO: \u001b[0mTrain 82 28431\n",
      "\u001b[32m[2020-07-15 10:48:44] __main__ INFO: \u001b[0mEpoch 82 Step 100/351 lr 0.020000 loss 0.6418 (0.5396) acc@1 0.7266 (0.7976) acc@5 0.8672 (0.9060)\n",
      "\u001b[32m[2020-07-15 10:50:28] __main__ INFO: \u001b[0mEpoch 82 Step 200/351 lr 0.020000 loss 0.5986 (0.5497) acc@1 0.7734 (0.7926) acc@5 0.8906 (0.9060)\n",
      "\u001b[32m[2020-07-15 10:52:12] __main__ INFO: \u001b[0mEpoch 82 Step 300/351 lr 0.020000 loss 0.6073 (0.5620) acc@1 0.7656 (0.7879) acc@5 0.9141 (0.9072)\n",
      "\u001b[32m[2020-07-15 10:53:05] __main__ INFO: \u001b[0mEpoch 82 Step 351/351 lr 0.020000 loss 0.5678 (0.5669) acc@1 0.7656 (0.7858) acc@5 0.9219 (0.9070)\n",
      "\u001b[32m[2020-07-15 10:53:05] __main__ INFO: \u001b[0mElapsed 365.22\n",
      "\u001b[32m[2020-07-15 10:53:05] __main__ INFO: \u001b[0mVal 82\n",
      "\u001b[32m[2020-07-15 10:53:18] __main__ INFO: \u001b[0mEpoch 82 loss 0.2616 acc@1 0.9258 acc@5 0.9986\n",
      "\u001b[32m[2020-07-15 10:53:18] __main__ INFO: \u001b[0mElapsed 12.26\n",
      "\u001b[32m[2020-07-15 10:53:18] __main__ INFO: \u001b[0mTrain 83 28782\n",
      "\u001b[32m[2020-07-15 10:55:02] __main__ INFO: \u001b[0mEpoch 83 Step 100/351 lr 0.020000 loss 0.4726 (0.5455) acc@1 0.8281 (0.7923) acc@5 0.9297 (0.9066)\n",
      "\u001b[32m[2020-07-15 10:56:47] __main__ INFO: \u001b[0mEpoch 83 Step 200/351 lr 0.020000 loss 0.5021 (0.5548) acc@1 0.8203 (0.7893) acc@5 0.9453 (0.9072)\n",
      "\u001b[32m[2020-07-15 10:58:31] __main__ INFO: \u001b[0mEpoch 83 Step 300/351 lr 0.020000 loss 0.5294 (0.5631) acc@1 0.7969 (0.7863) acc@5 0.9375 (0.9066)\n",
      "\u001b[32m[2020-07-15 10:59:25] __main__ INFO: \u001b[0mEpoch 83 Step 351/351 lr 0.020000 loss 0.6653 (0.5666) acc@1 0.7500 (0.7851) acc@5 0.9219 (0.9065)\n",
      "\u001b[32m[2020-07-15 10:59:25] __main__ INFO: \u001b[0mElapsed 367.09\n",
      "\u001b[32m[2020-07-15 10:59:25] __main__ INFO: \u001b[0mVal 83\n",
      "\u001b[32m[2020-07-15 10:59:37] __main__ INFO: \u001b[0mEpoch 83 loss 0.2465 acc@1 0.9308 acc@5 0.9980\n",
      "\u001b[32m[2020-07-15 10:59:37] __main__ INFO: \u001b[0mElapsed 12.27\n",
      "\u001b[32m[2020-07-15 10:59:37] __main__ INFO: \u001b[0mTrain 84 29133\n",
      "\u001b[32m[2020-07-15 11:01:21] __main__ INFO: \u001b[0mEpoch 84 Step 100/351 lr 0.020000 loss 0.5024 (0.5721) acc@1 0.8125 (0.7820) acc@5 0.8906 (0.9045)\n",
      "\u001b[32m[2020-07-15 11:03:05] __main__ INFO: \u001b[0mEpoch 84 Step 200/351 lr 0.020000 loss 0.5999 (0.5641) acc@1 0.7578 (0.7864) acc@5 0.9062 (0.9064)\n",
      "\u001b[32m[2020-07-15 11:04:49] __main__ INFO: \u001b[0mEpoch 84 Step 300/351 lr 0.020000 loss 0.6263 (0.5703) acc@1 0.7422 (0.7839) acc@5 0.8750 (0.9057)\n",
      "\u001b[32m[2020-07-15 11:05:42] __main__ INFO: \u001b[0mEpoch 84 Step 351/351 lr 0.020000 loss 0.5802 (0.5687) acc@1 0.7734 (0.7847) acc@5 0.9062 (0.9060)\n",
      "\u001b[32m[2020-07-15 11:05:42] __main__ INFO: \u001b[0mElapsed 365.55\n",
      "\u001b[32m[2020-07-15 11:05:43] __main__ INFO: \u001b[0mVal 84\n",
      "\u001b[32m[2020-07-15 11:05:55] __main__ INFO: \u001b[0mEpoch 84 loss 0.2567 acc@1 0.9258 acc@5 0.9992\n",
      "\u001b[32m[2020-07-15 11:05:55] __main__ INFO: \u001b[0mElapsed 12.26\n",
      "\u001b[32m[2020-07-15 11:05:55] __main__ INFO: \u001b[0mTrain 85 29484\n",
      "\u001b[32m[2020-07-15 11:07:39] __main__ INFO: \u001b[0mEpoch 85 Step 100/351 lr 0.020000 loss 0.4928 (0.5587) acc@1 0.8203 (0.7877) acc@5 0.9219 (0.9066)\n",
      "\u001b[32m[2020-07-15 11:09:24] __main__ INFO: \u001b[0mEpoch 85 Step 200/351 lr 0.020000 loss 0.7468 (0.5650) acc@1 0.7188 (0.7843) acc@5 0.9062 (0.9057)\n",
      "\u001b[32m[2020-07-15 11:11:09] __main__ INFO: \u001b[0mEpoch 85 Step 300/351 lr 0.020000 loss 0.5432 (0.5728) acc@1 0.7812 (0.7828) acc@5 0.9297 (0.9061)\n",
      "\u001b[32m[2020-07-15 11:12:02] __main__ INFO: \u001b[0mEpoch 85 Step 351/351 lr 0.020000 loss 0.5470 (0.5713) acc@1 0.7969 (0.7837) acc@5 0.9141 (0.9069)\n",
      "\u001b[32m[2020-07-15 11:12:02] __main__ INFO: \u001b[0mElapsed 367.27\n",
      "\u001b[32m[2020-07-15 11:12:02] __main__ INFO: \u001b[0mVal 85\n",
      "\u001b[32m[2020-07-15 11:12:14] __main__ INFO: \u001b[0mEpoch 85 loss 0.2675 acc@1 0.9260 acc@5 0.9980\n",
      "\u001b[32m[2020-07-15 11:12:14] __main__ INFO: \u001b[0mElapsed 12.26\n",
      "\u001b[32m[2020-07-15 11:12:14] __main__ INFO: \u001b[0mTrain 86 29835\n",
      "\u001b[32m[2020-07-15 11:13:59] __main__ INFO: \u001b[0mEpoch 86 Step 100/351 lr 0.020000 loss 0.5584 (0.5506) acc@1 0.7969 (0.7935) acc@5 0.8594 (0.9111)\n",
      "\u001b[32m[2020-07-15 11:15:43] __main__ INFO: \u001b[0mEpoch 86 Step 200/351 lr 0.020000 loss 0.5474 (0.5516) acc@1 0.7969 (0.7909) acc@5 0.9062 (0.9080)\n",
      "\u001b[32m[2020-07-15 11:17:27] __main__ INFO: \u001b[0mEpoch 86 Step 300/351 lr 0.020000 loss 0.5187 (0.5596) acc@1 0.7969 (0.7878) acc@5 0.9141 (0.9081)\n",
      "\u001b[32m[2020-07-15 11:18:20] __main__ INFO: \u001b[0mEpoch 86 Step 351/351 lr 0.020000 loss 0.5324 (0.5648) acc@1 0.7891 (0.7853) acc@5 0.8750 (0.9059)\n",
      "\u001b[32m[2020-07-15 11:18:20] __main__ INFO: \u001b[0mElapsed 365.61\n",
      "\u001b[32m[2020-07-15 11:18:20] __main__ INFO: \u001b[0mVal 86\n",
      "\u001b[32m[2020-07-15 11:18:32] __main__ INFO: \u001b[0mEpoch 86 loss 0.2227 acc@1 0.9306 acc@5 0.9976\n",
      "\u001b[32m[2020-07-15 11:18:32] __main__ INFO: \u001b[0mElapsed 12.27\n",
      "\u001b[32m[2020-07-15 11:18:32] __main__ INFO: \u001b[0mTrain 87 30186\n",
      "\u001b[32m[2020-07-15 11:20:17] __main__ INFO: \u001b[0mEpoch 87 Step 100/351 lr 0.020000 loss 0.6737 (0.5520) acc@1 0.7656 (0.7917) acc@5 0.8594 (0.9086)\n",
      "\u001b[32m[2020-07-15 11:22:01] __main__ INFO: \u001b[0mEpoch 87 Step 200/351 lr 0.020000 loss 0.5299 (0.5576) acc@1 0.7891 (0.7882) acc@5 0.9531 (0.9065)\n",
      "\u001b[32m[2020-07-15 11:23:46] __main__ INFO: \u001b[0mEpoch 87 Step 300/351 lr 0.020000 loss 0.7626 (0.5632) acc@1 0.7188 (0.7863) acc@5 0.8516 (0.9073)\n",
      "\u001b[32m[2020-07-15 11:24:39] __main__ INFO: \u001b[0mEpoch 87 Step 351/351 lr 0.020000 loss 0.5339 (0.5644) acc@1 0.8125 (0.7859) acc@5 0.8984 (0.9070)\n",
      "\u001b[32m[2020-07-15 11:24:39] __main__ INFO: \u001b[0mElapsed 367.25\n",
      "\u001b[32m[2020-07-15 11:24:39] __main__ INFO: \u001b[0mVal 87\n",
      "\u001b[32m[2020-07-15 11:24:52] __main__ INFO: \u001b[0mEpoch 87 loss 0.2956 acc@1 0.9196 acc@5 0.9986\n",
      "\u001b[32m[2020-07-15 11:24:52] __main__ INFO: \u001b[0mElapsed 12.26\n",
      "\u001b[32m[2020-07-15 11:24:52] __main__ INFO: \u001b[0mTrain 88 30537\n",
      "\u001b[32m[2020-07-15 11:26:36] __main__ INFO: \u001b[0mEpoch 88 Step 100/351 lr 0.020000 loss 0.6525 (0.5425) acc@1 0.7578 (0.7943) acc@5 0.8828 (0.9094)\n",
      "\u001b[32m[2020-07-15 11:28:20] __main__ INFO: \u001b[0mEpoch 88 Step 200/351 lr 0.020000 loss 0.5315 (0.5592) acc@1 0.8047 (0.7880) acc@5 0.8984 (0.9079)\n",
      "\u001b[32m[2020-07-15 11:30:04] __main__ INFO: \u001b[0mEpoch 88 Step 300/351 lr 0.020000 loss 0.5623 (0.5662) acc@1 0.7812 (0.7857) acc@5 0.9141 (0.9078)\n",
      "\u001b[32m[2020-07-15 11:30:57] __main__ INFO: \u001b[0mEpoch 88 Step 351/351 lr 0.020000 loss 0.4898 (0.5698) acc@1 0.7969 (0.7840) acc@5 0.9453 (0.9075)\n",
      "\u001b[32m[2020-07-15 11:30:57] __main__ INFO: \u001b[0mElapsed 365.43\n",
      "\u001b[32m[2020-07-15 11:30:57] __main__ INFO: \u001b[0mVal 88\n",
      "\u001b[32m[2020-07-15 11:31:09] __main__ INFO: \u001b[0mEpoch 88 loss 0.3293 acc@1 0.9072 acc@5 0.9988\n",
      "\u001b[32m[2020-07-15 11:31:09] __main__ INFO: \u001b[0mElapsed 12.28\n",
      "\u001b[32m[2020-07-15 11:31:09] __main__ INFO: \u001b[0mTrain 89 30888\n",
      "\u001b[32m[2020-07-15 11:32:54] __main__ INFO: \u001b[0mEpoch 89 Step 100/351 lr 0.020000 loss 0.5035 (0.5435) acc@1 0.8203 (0.7948) acc@5 0.9062 (0.9114)\n",
      "\u001b[32m[2020-07-15 11:34:39] __main__ INFO: \u001b[0mEpoch 89 Step 200/351 lr 0.020000 loss 0.7777 (0.5575) acc@1 0.7109 (0.7899) acc@5 0.9062 (0.9096)\n",
      "\u001b[32m[2020-07-15 11:36:23] __main__ INFO: \u001b[0mEpoch 89 Step 300/351 lr 0.020000 loss 0.3862 (0.5666) acc@1 0.8438 (0.7866) acc@5 0.9375 (0.9073)\n",
      "\u001b[32m[2020-07-15 11:37:16] __main__ INFO: \u001b[0mEpoch 89 Step 351/351 lr 0.020000 loss 0.6684 (0.5668) acc@1 0.7812 (0.7861) acc@5 0.9141 (0.9068)\n",
      "\u001b[32m[2020-07-15 11:37:16] __main__ INFO: \u001b[0mElapsed 367.07\n",
      "\u001b[32m[2020-07-15 11:37:16] __main__ INFO: \u001b[0mVal 89\n",
      "\u001b[32m[2020-07-15 11:37:29] __main__ INFO: \u001b[0mEpoch 89 loss 0.2901 acc@1 0.9186 acc@5 0.9988\n",
      "\u001b[32m[2020-07-15 11:37:29] __main__ INFO: \u001b[0mElapsed 12.26\n",
      "\u001b[32m[2020-07-15 11:37:29] __main__ INFO: \u001b[0mTrain 90 31239\n",
      "\u001b[32m[2020-07-15 11:39:13] __main__ INFO: \u001b[0mEpoch 90 Step 100/351 lr 0.020000 loss 0.6410 (0.5497) acc@1 0.7344 (0.7925) acc@5 0.9219 (0.9071)\n",
      "\u001b[32m[2020-07-15 11:40:57] __main__ INFO: \u001b[0mEpoch 90 Step 200/351 lr 0.020000 loss 0.5405 (0.5544) acc@1 0.7891 (0.7914) acc@5 0.9375 (0.9073)\n",
      "\u001b[32m[2020-07-15 11:42:41] __main__ INFO: \u001b[0mEpoch 90 Step 300/351 lr 0.020000 loss 0.5532 (0.5550) acc@1 0.7969 (0.7908) acc@5 0.8672 (0.9087)\n",
      "\u001b[32m[2020-07-15 11:43:34] __main__ INFO: \u001b[0mEpoch 90 Step 351/351 lr 0.020000 loss 0.6426 (0.5628) acc@1 0.7578 (0.7874) acc@5 0.8984 (0.9067)\n",
      "\u001b[32m[2020-07-15 11:43:34] __main__ INFO: \u001b[0mElapsed 365.49\n",
      "\u001b[32m[2020-07-15 11:43:34] __main__ INFO: \u001b[0mVal 90\n",
      "\u001b[32m[2020-07-15 11:43:46] __main__ INFO: \u001b[0mEpoch 90 loss 0.3299 acc@1 0.9086 acc@5 0.9982\n",
      "\u001b[32m[2020-07-15 11:43:46] __main__ INFO: \u001b[0mElapsed 12.26\n",
      "\u001b[32m[2020-07-15 11:43:46] __main__ INFO: \u001b[0mTrain 91 31590\n",
      "\u001b[32m[2020-07-15 11:45:31] __main__ INFO: \u001b[0mEpoch 91 Step 100/351 lr 0.020000 loss 0.4008 (0.5458) acc@1 0.8516 (0.7947) acc@5 0.9297 (0.9108)\n",
      "\u001b[32m[2020-07-15 11:47:16] __main__ INFO: \u001b[0mEpoch 91 Step 200/351 lr 0.020000 loss 0.7399 (0.5559) acc@1 0.7266 (0.7902) acc@5 0.8125 (0.9078)\n",
      "\u001b[32m[2020-07-15 11:49:00] __main__ INFO: \u001b[0mEpoch 91 Step 300/351 lr 0.020000 loss 0.4605 (0.5594) acc@1 0.8438 (0.7890) acc@5 0.9609 (0.9069)\n",
      "\u001b[32m[2020-07-15 11:49:54] __main__ INFO: \u001b[0mEpoch 91 Step 351/351 lr 0.020000 loss 0.5731 (0.5630) acc@1 0.7812 (0.7877) acc@5 0.8750 (0.9067)\n",
      "\u001b[32m[2020-07-15 11:49:54] __main__ INFO: \u001b[0mElapsed 367.19\n",
      "\u001b[32m[2020-07-15 11:49:54] __main__ INFO: \u001b[0mVal 91\n",
      "\u001b[32m[2020-07-15 11:50:06] __main__ INFO: \u001b[0mEpoch 91 loss 0.2748 acc@1 0.9242 acc@5 0.9982\n",
      "\u001b[32m[2020-07-15 11:50:06] __main__ INFO: \u001b[0mElapsed 12.27\n",
      "\u001b[32m[2020-07-15 11:50:06] __main__ INFO: \u001b[0mTrain 92 31941\n",
      "\u001b[32m[2020-07-15 11:51:50] __main__ INFO: \u001b[0mEpoch 92 Step 100/351 lr 0.020000 loss 0.5295 (0.5603) acc@1 0.7969 (0.7872) acc@5 0.9219 (0.9057)\n",
      "\u001b[32m[2020-07-15 11:53:34] __main__ INFO: \u001b[0mEpoch 92 Step 200/351 lr 0.020000 loss 0.5133 (0.5513) acc@1 0.8125 (0.7910) acc@5 0.9297 (0.9067)\n",
      "\u001b[32m[2020-07-15 11:55:18] __main__ INFO: \u001b[0mEpoch 92 Step 300/351 lr 0.020000 loss 0.5372 (0.5566) acc@1 0.8125 (0.7889) acc@5 0.8984 (0.9065)\n",
      "\u001b[32m[2020-07-15 11:56:11] __main__ INFO: \u001b[0mEpoch 92 Step 351/351 lr 0.020000 loss 0.5677 (0.5632) acc@1 0.7656 (0.7861) acc@5 0.9297 (0.9060)\n",
      "\u001b[32m[2020-07-15 11:56:11] __main__ INFO: \u001b[0mElapsed 365.45\n",
      "\u001b[32m[2020-07-15 11:56:11] __main__ INFO: \u001b[0mVal 92\n",
      "\u001b[32m[2020-07-15 11:56:24] __main__ INFO: \u001b[0mEpoch 92 loss 0.2760 acc@1 0.9202 acc@5 0.9982\n",
      "\u001b[32m[2020-07-15 11:56:24] __main__ INFO: \u001b[0mElapsed 12.26\n",
      "\u001b[32m[2020-07-15 11:56:24] __main__ INFO: \u001b[0mTrain 93 32292\n",
      "\u001b[32m[2020-07-15 11:58:08] __main__ INFO: \u001b[0mEpoch 93 Step 100/351 lr 0.020000 loss 0.4821 (0.5395) acc@1 0.8125 (0.7935) acc@5 0.9062 (0.9062)\n",
      "\u001b[32m[2020-07-15 11:59:53] __main__ INFO: \u001b[0mEpoch 93 Step 200/351 lr 0.020000 loss 0.4628 (0.5399) acc@1 0.8438 (0.7946) acc@5 0.9375 (0.9081)\n",
      "\u001b[32m[2020-07-15 12:01:38] __main__ INFO: \u001b[0mEpoch 93 Step 300/351 lr 0.020000 loss 0.7039 (0.5535) acc@1 0.7188 (0.7905) acc@5 0.8594 (0.9074)\n",
      "\u001b[32m[2020-07-15 12:02:31] __main__ INFO: \u001b[0mEpoch 93 Step 351/351 lr 0.020000 loss 0.5975 (0.5592) acc@1 0.7734 (0.7885) acc@5 0.9219 (0.9073)\n",
      "\u001b[32m[2020-07-15 12:02:31] __main__ INFO: \u001b[0mElapsed 367.32\n",
      "\u001b[32m[2020-07-15 12:02:31] __main__ INFO: \u001b[0mVal 93\n",
      "\u001b[32m[2020-07-15 12:02:43] __main__ INFO: \u001b[0mEpoch 93 loss 0.2507 acc@1 0.9248 acc@5 0.9972\n",
      "\u001b[32m[2020-07-15 12:02:43] __main__ INFO: \u001b[0mElapsed 12.28\n",
      "\u001b[32m[2020-07-15 12:02:43] __main__ INFO: \u001b[0mTrain 94 32643\n",
      "\u001b[32m[2020-07-15 12:04:28] __main__ INFO: \u001b[0mEpoch 94 Step 100/351 lr 0.020000 loss 0.3525 (0.5311) acc@1 0.8750 (0.8006) acc@5 0.9531 (0.9109)\n",
      "\u001b[32m[2020-07-15 12:06:12] __main__ INFO: \u001b[0mEpoch 94 Step 200/351 lr 0.020000 loss 0.4927 (0.5319) acc@1 0.8281 (0.7993) acc@5 0.9062 (0.9093)\n",
      "\u001b[32m[2020-07-15 12:07:56] __main__ INFO: \u001b[0mEpoch 94 Step 300/351 lr 0.020000 loss 0.5803 (0.5506) acc@1 0.7734 (0.7922) acc@5 0.8906 (0.9079)\n",
      "\u001b[32m[2020-07-15 12:08:49] __main__ INFO: \u001b[0mEpoch 94 Step 351/351 lr 0.020000 loss 0.4873 (0.5553) acc@1 0.8438 (0.7906) acc@5 0.9375 (0.9084)\n",
      "\u001b[32m[2020-07-15 12:08:49] __main__ INFO: \u001b[0mElapsed 365.72\n",
      "\u001b[32m[2020-07-15 12:08:49] __main__ INFO: \u001b[0mVal 94\n",
      "\u001b[32m[2020-07-15 12:09:01] __main__ INFO: \u001b[0mEpoch 94 loss 0.2576 acc@1 0.9254 acc@5 0.9974\n",
      "\u001b[32m[2020-07-15 12:09:01] __main__ INFO: \u001b[0mElapsed 12.27\n",
      "\u001b[32m[2020-07-15 12:09:01] __main__ INFO: \u001b[0mTrain 95 32994\n",
      "\u001b[32m[2020-07-15 12:10:46] __main__ INFO: \u001b[0mEpoch 95 Step 100/351 lr 0.020000 loss 0.4056 (0.5391) acc@1 0.8672 (0.7984) acc@5 0.9297 (0.9112)\n",
      "\u001b[32m[2020-07-15 12:12:31] __main__ INFO: \u001b[0mEpoch 95 Step 200/351 lr 0.020000 loss 0.5959 (0.5421) acc@1 0.7812 (0.7971) acc@5 0.9062 (0.9098)\n",
      "\u001b[32m[2020-07-15 12:14:16] __main__ INFO: \u001b[0mEpoch 95 Step 300/351 lr 0.020000 loss 0.6394 (0.5540) acc@1 0.7656 (0.7919) acc@5 0.9453 (0.9079)\n",
      "\u001b[32m[2020-07-15 12:15:09] __main__ INFO: \u001b[0mEpoch 95 Step 351/351 lr 0.020000 loss 0.5791 (0.5577) acc@1 0.7500 (0.7903) acc@5 0.9297 (0.9073)\n",
      "\u001b[32m[2020-07-15 12:15:09] __main__ INFO: \u001b[0mElapsed 367.71\n",
      "\u001b[32m[2020-07-15 12:15:09] __main__ INFO: \u001b[0mVal 95\n",
      "\u001b[32m[2020-07-15 12:15:21] __main__ INFO: \u001b[0mEpoch 95 loss 0.2432 acc@1 0.9328 acc@5 0.9984\n",
      "\u001b[32m[2020-07-15 12:15:21] __main__ INFO: \u001b[0mElapsed 12.31\n",
      "\u001b[32m[2020-07-15 12:15:21] __main__ INFO: \u001b[0mTrain 96 33345\n",
      "\u001b[32m[2020-07-15 12:17:06] __main__ INFO: \u001b[0mEpoch 96 Step 100/351 lr 0.020000 loss 0.6682 (0.5404) acc@1 0.7422 (0.7953) acc@5 0.9062 (0.9082)\n",
      "\u001b[32m[2020-07-15 12:18:50] __main__ INFO: \u001b[0mEpoch 96 Step 200/351 lr 0.020000 loss 0.5955 (0.5536) acc@1 0.7500 (0.7904) acc@5 0.8828 (0.9068)\n",
      "\u001b[32m[2020-07-15 12:20:34] __main__ INFO: \u001b[0mEpoch 96 Step 300/351 lr 0.020000 loss 0.4927 (0.5547) acc@1 0.8203 (0.7906) acc@5 0.8906 (0.9070)\n",
      "\u001b[32m[2020-07-15 12:21:28] __main__ INFO: \u001b[0mEpoch 96 Step 351/351 lr 0.020000 loss 0.7304 (0.5569) acc@1 0.7031 (0.7893) acc@5 0.8516 (0.9068)\n",
      "\u001b[32m[2020-07-15 12:21:28] __main__ INFO: \u001b[0mElapsed 366.37\n",
      "\u001b[32m[2020-07-15 12:21:28] __main__ INFO: \u001b[0mVal 96\n",
      "\u001b[32m[2020-07-15 12:21:40] __main__ INFO: \u001b[0mEpoch 96 loss 0.2835 acc@1 0.9188 acc@5 0.9982\n",
      "\u001b[32m[2020-07-15 12:21:40] __main__ INFO: \u001b[0mElapsed 12.31\n",
      "\u001b[32m[2020-07-15 12:21:40] __main__ INFO: \u001b[0mTrain 97 33696\n",
      "\u001b[32m[2020-07-15 12:23:25] __main__ INFO: \u001b[0mEpoch 97 Step 100/351 lr 0.020000 loss 0.5165 (0.5499) acc@1 0.7969 (0.7923) acc@5 0.9219 (0.9097)\n",
      "\u001b[32m[2020-07-15 12:25:10] __main__ INFO: \u001b[0mEpoch 97 Step 200/351 lr 0.020000 loss 0.5693 (0.5458) acc@1 0.7969 (0.7939) acc@5 0.9219 (0.9089)\n",
      "\u001b[32m[2020-07-15 12:26:55] __main__ INFO: \u001b[0mEpoch 97 Step 300/351 lr 0.020000 loss 0.5247 (0.5568) acc@1 0.8125 (0.7895) acc@5 0.8906 (0.9077)\n",
      "\u001b[32m[2020-07-15 12:27:48] __main__ INFO: \u001b[0mEpoch 97 Step 351/351 lr 0.020000 loss 0.3989 (0.5590) acc@1 0.8438 (0.7884) acc@5 0.9219 (0.9076)\n",
      "\u001b[32m[2020-07-15 12:27:48] __main__ INFO: \u001b[0mElapsed 368.42\n",
      "\u001b[32m[2020-07-15 12:27:48] __main__ INFO: \u001b[0mVal 97\n",
      "\u001b[32m[2020-07-15 12:28:01] __main__ INFO: \u001b[0mEpoch 97 loss 0.2443 acc@1 0.9280 acc@5 0.9984\n",
      "\u001b[32m[2020-07-15 12:28:01] __main__ INFO: \u001b[0mElapsed 12.30\n",
      "\u001b[32m[2020-07-15 12:28:01] __main__ INFO: \u001b[0mTrain 98 34047\n",
      "\u001b[32m[2020-07-15 12:29:45] __main__ INFO: \u001b[0mEpoch 98 Step 100/351 lr 0.020000 loss 0.5339 (0.5340) acc@1 0.7734 (0.7965) acc@5 0.9141 (0.9073)\n",
      "\u001b[32m[2020-07-15 12:31:30] __main__ INFO: \u001b[0mEpoch 98 Step 200/351 lr 0.020000 loss 0.5054 (0.5469) acc@1 0.8047 (0.7921) acc@5 0.8750 (0.9066)\n",
      "\u001b[32m[2020-07-15 12:33:14] __main__ INFO: \u001b[0mEpoch 98 Step 300/351 lr 0.020000 loss 0.5456 (0.5541) acc@1 0.7656 (0.7897) acc@5 0.9297 (0.9070)\n",
      "\u001b[32m[2020-07-15 12:34:08] __main__ INFO: \u001b[0mEpoch 98 Step 351/351 lr 0.020000 loss 0.5308 (0.5537) acc@1 0.8047 (0.7898) acc@5 0.9297 (0.9071)\n",
      "\u001b[32m[2020-07-15 12:34:08] __main__ INFO: \u001b[0mElapsed 366.82\n",
      "\u001b[32m[2020-07-15 12:34:08] __main__ INFO: \u001b[0mVal 98\n",
      "\u001b[32m[2020-07-15 12:34:20] __main__ INFO: \u001b[0mEpoch 98 loss 0.2489 acc@1 0.9270 acc@5 0.9990\n",
      "\u001b[32m[2020-07-15 12:34:20] __main__ INFO: \u001b[0mElapsed 12.31\n",
      "\u001b[32m[2020-07-15 12:34:20] __main__ INFO: \u001b[0mTrain 99 34398\n",
      "\u001b[32m[2020-07-15 12:36:05] __main__ INFO: \u001b[0mEpoch 99 Step 100/351 lr 0.020000 loss 0.5347 (0.5317) acc@1 0.7812 (0.7992) acc@5 0.8984 (0.9065)\n",
      "\u001b[32m[2020-07-15 12:37:50] __main__ INFO: \u001b[0mEpoch 99 Step 200/351 lr 0.020000 loss 0.5866 (0.5412) acc@1 0.8047 (0.7956) acc@5 0.9219 (0.9072)\n",
      "\u001b[32m[2020-07-15 12:39:35] __main__ INFO: \u001b[0mEpoch 99 Step 300/351 lr 0.020000 loss 0.5267 (0.5467) acc@1 0.7969 (0.7936) acc@5 0.9141 (0.9061)\n",
      "\u001b[32m[2020-07-15 12:40:28] __main__ INFO: \u001b[0mEpoch 99 Step 351/351 lr 0.020000 loss 0.5292 (0.5522) acc@1 0.7891 (0.7912) acc@5 0.9062 (0.9055)\n",
      "\u001b[32m[2020-07-15 12:40:28] __main__ INFO: \u001b[0mElapsed 368.51\n",
      "\u001b[32m[2020-07-15 12:40:28] __main__ INFO: \u001b[0mVal 99\n",
      "\u001b[32m[2020-07-15 12:40:41] __main__ INFO: \u001b[0mEpoch 99 loss 0.2562 acc@1 0.9252 acc@5 0.9980\n",
      "\u001b[32m[2020-07-15 12:40:41] __main__ INFO: \u001b[0mElapsed 12.32\n",
      "\u001b[32m[2020-07-15 12:40:41] __main__ INFO: \u001b[0mTrain 100 34749\n",
      "\u001b[32m[2020-07-15 12:42:25] __main__ INFO: \u001b[0mEpoch 100 Step 100/351 lr 0.020000 loss 0.6021 (0.5323) acc@1 0.7656 (0.8002) acc@5 0.8984 (0.9104)\n",
      "\u001b[32m[2020-07-15 12:44:10] __main__ INFO: \u001b[0mEpoch 100 Step 200/351 lr 0.020000 loss 0.4062 (0.5403) acc@1 0.8516 (0.7948) acc@5 0.9453 (0.9084)\n",
      "\u001b[32m[2020-07-15 12:45:54] __main__ INFO: \u001b[0mEpoch 100 Step 300/351 lr 0.020000 loss 0.5708 (0.5473) acc@1 0.8047 (0.7928) acc@5 0.9219 (0.9083)\n",
      "\u001b[32m[2020-07-15 12:46:47] __main__ INFO: \u001b[0mEpoch 100 Step 351/351 lr 0.020000 loss 0.4181 (0.5478) acc@1 0.8438 (0.7926) acc@5 0.9375 (0.9085)\n",
      "\u001b[32m[2020-07-15 12:46:47] __main__ INFO: \u001b[0mElapsed 366.62\n",
      "\u001b[32m[2020-07-15 12:46:47] __main__ INFO: \u001b[0mVal 100\n",
      "\u001b[32m[2020-07-15 12:47:00] __main__ INFO: \u001b[0mEpoch 100 loss 0.2618 acc@1 0.9308 acc@5 0.9986\n",
      "\u001b[32m[2020-07-15 12:47:00] __main__ INFO: \u001b[0mElapsed 12.33\n",
      "\u001b[32m[2020-07-15 12:47:00] fvcore.common.checkpoint INFO: \u001b[0mSaving checkpoint to /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_1_20_c10val/exp00/checkpoint_00100.pth\n",
      "\u001b[32m[2020-07-15 12:47:00] __main__ INFO: \u001b[0mTrain 101 35100\n",
      "\u001b[32m[2020-07-15 12:48:45] __main__ INFO: \u001b[0mEpoch 101 Step 100/351 lr 0.020000 loss 0.5058 (0.5486) acc@1 0.8047 (0.7924) acc@5 0.9062 (0.9089)\n",
      "\u001b[32m[2020-07-15 12:50:30] __main__ INFO: \u001b[0mEpoch 101 Step 200/351 lr 0.020000 loss 0.4607 (0.5469) acc@1 0.8281 (0.7928) acc@5 0.9375 (0.9092)\n",
      "\u001b[32m[2020-07-15 12:52:15] __main__ INFO: \u001b[0mEpoch 101 Step 300/351 lr 0.020000 loss 0.3742 (0.5541) acc@1 0.8672 (0.7897) acc@5 0.9453 (0.9090)\n",
      "\u001b[32m[2020-07-15 12:53:08] __main__ INFO: \u001b[0mEpoch 101 Step 351/351 lr 0.020000 loss 0.6109 (0.5547) acc@1 0.7656 (0.7893) acc@5 0.9141 (0.9081)\n",
      "\u001b[32m[2020-07-15 12:53:08] __main__ INFO: \u001b[0mElapsed 368.31\n",
      "\u001b[32m[2020-07-15 12:53:08] __main__ INFO: \u001b[0mVal 101\n",
      "\u001b[32m[2020-07-15 12:53:20] __main__ INFO: \u001b[0mEpoch 101 loss 0.2505 acc@1 0.9280 acc@5 0.9982\n",
      "\u001b[32m[2020-07-15 12:53:20] __main__ INFO: \u001b[0mElapsed 12.30\n",
      "\u001b[32m[2020-07-15 12:53:20] __main__ INFO: \u001b[0mTrain 102 35451\n",
      "\u001b[32m[2020-07-15 12:55:05] __main__ INFO: \u001b[0mEpoch 102 Step 100/351 lr 0.020000 loss 0.6788 (0.5298) acc@1 0.7422 (0.7987) acc@5 0.8828 (0.9086)\n",
      "\u001b[32m[2020-07-15 12:56:49] __main__ INFO: \u001b[0mEpoch 102 Step 200/351 lr 0.020000 loss 0.5141 (0.5384) acc@1 0.7891 (0.7959) acc@5 0.8750 (0.9086)\n",
      "\u001b[32m[2020-07-15 12:58:34] __main__ INFO: \u001b[0mEpoch 102 Step 300/351 lr 0.020000 loss 0.7268 (0.5451) acc@1 0.7500 (0.7938) acc@5 0.8984 (0.9079)\n",
      "\u001b[32m[2020-07-15 12:59:27] __main__ INFO: \u001b[0mEpoch 102 Step 351/351 lr 0.020000 loss 0.6733 (0.5476) acc@1 0.7344 (0.7931) acc@5 0.9062 (0.9079)\n",
      "\u001b[32m[2020-07-15 12:59:27] __main__ INFO: \u001b[0mElapsed 366.45\n",
      "\u001b[32m[2020-07-15 12:59:27] __main__ INFO: \u001b[0mVal 102\n",
      "\u001b[32m[2020-07-15 12:59:39] __main__ INFO: \u001b[0mEpoch 102 loss 0.2502 acc@1 0.9246 acc@5 0.9984\n",
      "\u001b[32m[2020-07-15 12:59:39] __main__ INFO: \u001b[0mElapsed 12.30\n",
      "\u001b[32m[2020-07-15 12:59:39] __main__ INFO: \u001b[0mTrain 103 35802\n",
      "\u001b[32m[2020-07-15 13:01:24] __main__ INFO: \u001b[0mEpoch 103 Step 100/351 lr 0.020000 loss 0.5987 (0.5476) acc@1 0.8125 (0.7920) acc@5 0.8906 (0.9084)\n",
      "\u001b[32m[2020-07-15 13:03:09] __main__ INFO: \u001b[0mEpoch 103 Step 200/351 lr 0.020000 loss 0.5370 (0.5405) acc@1 0.7969 (0.7947) acc@5 0.9297 (0.9089)\n",
      "\u001b[32m[2020-07-15 13:04:54] __main__ INFO: \u001b[0mEpoch 103 Step 300/351 lr 0.020000 loss 0.5469 (0.5495) acc@1 0.7812 (0.7921) acc@5 0.9297 (0.9080)\n",
      "\u001b[32m[2020-07-15 13:05:47] __main__ INFO: \u001b[0mEpoch 103 Step 351/351 lr 0.020000 loss 0.7481 (0.5549) acc@1 0.7031 (0.7903) acc@5 0.8672 (0.9077)\n",
      "\u001b[32m[2020-07-15 13:05:47] __main__ INFO: \u001b[0mElapsed 367.79\n",
      "\u001b[32m[2020-07-15 13:05:47] __main__ INFO: \u001b[0mVal 103\n",
      "\u001b[32m[2020-07-15 13:05:59] __main__ INFO: \u001b[0mEpoch 103 loss 0.2814 acc@1 0.9206 acc@5 0.9974\n",
      "\u001b[32m[2020-07-15 13:05:59] __main__ INFO: \u001b[0mElapsed 12.27\n",
      "\u001b[32m[2020-07-15 13:05:59] __main__ INFO: \u001b[0mTrain 104 36153\n",
      "\u001b[32m[2020-07-15 13:07:44] __main__ INFO: \u001b[0mEpoch 104 Step 100/351 lr 0.020000 loss 0.5380 (0.5418) acc@1 0.8125 (0.7922) acc@5 0.9062 (0.9062)\n",
      "\u001b[32m[2020-07-15 13:09:28] __main__ INFO: \u001b[0mEpoch 104 Step 200/351 lr 0.020000 loss 0.6839 (0.5443) acc@1 0.7109 (0.7923) acc@5 0.8984 (0.9044)\n",
      "\u001b[32m[2020-07-15 13:11:12] __main__ INFO: \u001b[0mEpoch 104 Step 300/351 lr 0.020000 loss 0.7954 (0.5435) acc@1 0.6797 (0.7930) acc@5 0.8750 (0.9068)\n",
      "\u001b[32m[2020-07-15 13:12:05] __main__ INFO: \u001b[0mEpoch 104 Step 351/351 lr 0.020000 loss 0.5622 (0.5470) acc@1 0.7656 (0.7921) acc@5 0.8672 (0.9062)\n",
      "\u001b[32m[2020-07-15 13:12:05] __main__ INFO: \u001b[0mElapsed 365.61\n",
      "\u001b[32m[2020-07-15 13:12:05] __main__ INFO: \u001b[0mVal 104\n",
      "\u001b[32m[2020-07-15 13:12:17] __main__ INFO: \u001b[0mEpoch 104 loss 0.3017 acc@1 0.9134 acc@5 0.9982\n",
      "\u001b[32m[2020-07-15 13:12:17] __main__ INFO: \u001b[0mElapsed 12.26\n",
      "\u001b[32m[2020-07-15 13:12:17] __main__ INFO: \u001b[0mTrain 105 36504\n",
      "\u001b[32m[2020-07-15 13:14:02] __main__ INFO: \u001b[0mEpoch 105 Step 100/351 lr 0.020000 loss 0.7572 (0.5436) acc@1 0.7422 (0.7931) acc@5 0.8828 (0.9083)\n",
      "\u001b[32m[2020-07-15 13:15:46] __main__ INFO: \u001b[0mEpoch 105 Step 200/351 lr 0.020000 loss 0.5180 (0.5542) acc@1 0.7891 (0.7904) acc@5 0.9141 (0.9083)\n",
      "\u001b[32m[2020-07-15 13:17:31] __main__ INFO: \u001b[0mEpoch 105 Step 300/351 lr 0.020000 loss 0.5941 (0.5579) acc@1 0.7578 (0.7888) acc@5 0.8750 (0.9077)\n",
      "\u001b[32m[2020-07-15 13:18:24] __main__ INFO: \u001b[0mEpoch 105 Step 351/351 lr 0.020000 loss 0.5282 (0.5588) acc@1 0.8203 (0.7884) acc@5 0.9375 (0.9070)\n",
      "\u001b[32m[2020-07-15 13:18:24] __main__ INFO: \u001b[0mElapsed 367.12\n",
      "\u001b[32m[2020-07-15 13:18:24] __main__ INFO: \u001b[0mVal 105\n",
      "\u001b[32m[2020-07-15 13:18:37] __main__ INFO: \u001b[0mEpoch 105 loss 0.2320 acc@1 0.9308 acc@5 0.9984\n",
      "\u001b[32m[2020-07-15 13:18:37] __main__ INFO: \u001b[0mElapsed 12.25\n",
      "\u001b[32m[2020-07-15 13:18:37] __main__ INFO: \u001b[0mTrain 106 36855\n",
      "\u001b[32m[2020-07-15 13:20:21] __main__ INFO: \u001b[0mEpoch 106 Step 100/351 lr 0.020000 loss 0.4373 (0.5347) acc@1 0.8359 (0.8000) acc@5 0.9375 (0.9096)\n",
      "\u001b[32m[2020-07-15 13:22:05] __main__ INFO: \u001b[0mEpoch 106 Step 200/351 lr 0.020000 loss 0.5373 (0.5419) acc@1 0.7969 (0.7966) acc@5 0.8750 (0.9090)\n",
      "\u001b[32m[2020-07-15 13:23:49] __main__ INFO: \u001b[0mEpoch 106 Step 300/351 lr 0.020000 loss 0.5322 (0.5463) acc@1 0.8047 (0.7930) acc@5 0.8906 (0.9075)\n",
      "\u001b[32m[2020-07-15 13:24:42] __main__ INFO: \u001b[0mEpoch 106 Step 351/351 lr 0.020000 loss 0.5510 (0.5486) acc@1 0.7734 (0.7920) acc@5 0.8594 (0.9076)\n",
      "\u001b[32m[2020-07-15 13:24:42] __main__ INFO: \u001b[0mElapsed 365.34\n",
      "\u001b[32m[2020-07-15 13:24:42] __main__ INFO: \u001b[0mVal 106\n",
      "\u001b[32m[2020-07-15 13:24:54] __main__ INFO: \u001b[0mEpoch 106 loss 0.3457 acc@1 0.9054 acc@5 0.9976\n",
      "\u001b[32m[2020-07-15 13:24:54] __main__ INFO: \u001b[0mElapsed 12.26\n",
      "\u001b[32m[2020-07-15 13:24:54] __main__ INFO: \u001b[0mTrain 107 37206\n",
      "\u001b[32m[2020-07-15 13:26:39] __main__ INFO: \u001b[0mEpoch 107 Step 100/351 lr 0.020000 loss 0.6396 (0.5454) acc@1 0.7500 (0.7937) acc@5 0.8984 (0.9095)\n",
      "\u001b[32m[2020-07-15 13:28:23] __main__ INFO: \u001b[0mEpoch 107 Step 200/351 lr 0.020000 loss 0.5065 (0.5480) acc@1 0.8281 (0.7929) acc@5 0.9297 (0.9073)\n",
      "\u001b[32m[2020-07-15 13:30:08] __main__ INFO: \u001b[0mEpoch 107 Step 300/351 lr 0.020000 loss 0.5284 (0.5500) acc@1 0.7891 (0.7923) acc@5 0.9219 (0.9077)\n",
      "\u001b[32m[2020-07-15 13:31:01] __main__ INFO: \u001b[0mEpoch 107 Step 351/351 lr 0.020000 loss 0.5736 (0.5527) acc@1 0.7734 (0.7914) acc@5 0.9297 (0.9067)\n",
      "\u001b[32m[2020-07-15 13:31:01] __main__ INFO: \u001b[0mElapsed 366.89\n",
      "\u001b[32m[2020-07-15 13:31:01] __main__ INFO: \u001b[0mVal 107\n",
      "\u001b[32m[2020-07-15 13:31:13] __main__ INFO: \u001b[0mEpoch 107 loss 0.2425 acc@1 0.9328 acc@5 0.9986\n",
      "\u001b[32m[2020-07-15 13:31:13] __main__ INFO: \u001b[0mElapsed 12.27\n",
      "\u001b[32m[2020-07-15 13:31:13] __main__ INFO: \u001b[0mTrain 108 37557\n",
      "\u001b[32m[2020-07-15 13:32:57] __main__ INFO: \u001b[0mEpoch 108 Step 100/351 lr 0.020000 loss 0.5124 (0.5314) acc@1 0.8047 (0.7991) acc@5 0.9141 (0.9058)\n",
      "\u001b[32m[2020-07-15 13:34:41] __main__ INFO: \u001b[0mEpoch 108 Step 200/351 lr 0.020000 loss 0.4034 (0.5358) acc@1 0.8594 (0.7983) acc@5 0.9297 (0.9060)\n",
      "\u001b[32m[2020-07-15 13:36:25] __main__ INFO: \u001b[0mEpoch 108 Step 300/351 lr 0.020000 loss 0.4144 (0.5463) acc@1 0.8359 (0.7943) acc@5 0.9219 (0.9059)\n",
      "\u001b[32m[2020-07-15 13:37:18] __main__ INFO: \u001b[0mEpoch 108 Step 351/351 lr 0.020000 loss 0.5232 (0.5460) acc@1 0.7891 (0.7941) acc@5 0.8906 (0.9063)\n",
      "\u001b[32m[2020-07-15 13:37:18] __main__ INFO: \u001b[0mElapsed 365.20\n",
      "\u001b[32m[2020-07-15 13:37:18] __main__ INFO: \u001b[0mVal 108\n",
      "\u001b[32m[2020-07-15 13:37:31] __main__ INFO: \u001b[0mEpoch 108 loss 0.2688 acc@1 0.9228 acc@5 0.9984\n",
      "\u001b[32m[2020-07-15 13:37:31] __main__ INFO: \u001b[0mElapsed 12.24\n",
      "\u001b[32m[2020-07-15 13:37:31] __main__ INFO: \u001b[0mTrain 109 37908\n",
      "\u001b[32m[2020-07-15 13:39:15] __main__ INFO: \u001b[0mEpoch 109 Step 100/351 lr 0.020000 loss 0.4165 (0.5369) acc@1 0.8281 (0.7957) acc@5 0.9375 (0.9060)\n",
      "\u001b[32m[2020-07-15 13:41:00] __main__ INFO: \u001b[0mEpoch 109 Step 200/351 lr 0.020000 loss 0.5608 (0.5505) acc@1 0.7812 (0.7909) acc@5 0.9141 (0.9073)\n",
      "\u001b[32m[2020-07-15 13:42:44] __main__ INFO: \u001b[0mEpoch 109 Step 300/351 lr 0.020000 loss 0.5190 (0.5487) acc@1 0.7812 (0.7916) acc@5 0.8828 (0.9082)\n",
      "\u001b[32m[2020-07-15 13:43:38] __main__ INFO: \u001b[0mEpoch 109 Step 351/351 lr 0.020000 loss 0.7069 (0.5505) acc@1 0.7344 (0.7909) acc@5 0.8906 (0.9075)\n",
      "\u001b[32m[2020-07-15 13:43:38] __main__ INFO: \u001b[0mElapsed 366.93\n",
      "\u001b[32m[2020-07-15 13:43:38] __main__ INFO: \u001b[0mVal 109\n",
      "\u001b[32m[2020-07-15 13:43:50] __main__ INFO: \u001b[0mEpoch 109 loss 0.3118 acc@1 0.9132 acc@5 0.9958\n",
      "\u001b[32m[2020-07-15 13:43:50] __main__ INFO: \u001b[0mElapsed 12.25\n",
      "\u001b[32m[2020-07-15 13:43:50] __main__ INFO: \u001b[0mTrain 110 38259\n",
      "\u001b[32m[2020-07-15 13:45:34] __main__ INFO: \u001b[0mEpoch 110 Step 100/351 lr 0.020000 loss 0.4308 (0.5519) acc@1 0.8281 (0.7927) acc@5 0.9453 (0.9064)\n",
      "\u001b[32m[2020-07-15 13:47:18] __main__ INFO: \u001b[0mEpoch 110 Step 200/351 lr 0.020000 loss 0.5949 (0.5487) acc@1 0.7891 (0.7929) acc@5 0.9141 (0.9076)\n",
      "\u001b[32m[2020-07-15 13:49:02] __main__ INFO: \u001b[0mEpoch 110 Step 300/351 lr 0.020000 loss 0.5239 (0.5474) acc@1 0.8047 (0.7930) acc@5 0.9141 (0.9074)\n",
      "\u001b[32m[2020-07-15 13:49:55] __main__ INFO: \u001b[0mEpoch 110 Step 351/351 lr 0.020000 loss 0.7003 (0.5501) acc@1 0.7109 (0.7922) acc@5 0.8438 (0.9072)\n",
      "\u001b[32m[2020-07-15 13:49:55] __main__ INFO: \u001b[0mElapsed 365.40\n",
      "\u001b[32m[2020-07-15 13:49:55] __main__ INFO: \u001b[0mVal 110\n",
      "\u001b[32m[2020-07-15 13:50:08] __main__ INFO: \u001b[0mEpoch 110 loss 0.2645 acc@1 0.9250 acc@5 0.9978\n",
      "\u001b[32m[2020-07-15 13:50:08] __main__ INFO: \u001b[0mElapsed 12.29\n",
      "\u001b[32m[2020-07-15 13:50:08] __main__ INFO: \u001b[0mTrain 111 38610\n"
     ]
    }
   ],
   "source": [
    "# Train the model per the settings specified in the original paper\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "!python train.py --config configs/cifar/wrn.yaml \\\n",
    "    model.wrn.depth 28 \\\n",
    "    model.wrn.widening_factor 10 \\\n",
    "    train.batch_size 128 \\\n",
    "    train.base_lr 0.1 \\\n",
    "    dataset.name CIFAR10_RA_1_20 \\\n",
    "    train.output_dir /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_1_20_c10val/exp00 \\\n",
    "    scheduler.epochs 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Refine the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-16 21:36:05] __main__ INFO: \u001b[0mdevice: cuda\n",
      "cudnn:\n",
      "  benchmark: True\n",
      "  deterministic: False\n",
      "dataset:\n",
      "  name: CIFAR10\n",
      "  dataset_dir: ~/.torch/datasets/CIFAR10\n",
      "  image_size: 32\n",
      "  n_channels: 3\n",
      "  n_classes: 10\n",
      "model:\n",
      "  type: cifar\n",
      "  name: wrn\n",
      "  init_mode: kaiming_fan_in\n",
      "  vgg:\n",
      "    n_channels: [64, 128, 256, 512, 512]\n",
      "    n_layers: [2, 2, 3, 3, 3]\n",
      "    use_bn: True\n",
      "  resnet:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "  resnet_preact:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "  wrn:\n",
      "    depth: 28\n",
      "    initial_channels: 16\n",
      "    widening_factor: 10\n",
      "    drop_rate: 0.0\n",
      "  densenet:\n",
      "    depth: 100\n",
      "    n_blocks: [6, 12, 24, 16]\n",
      "    block_type: bottleneck\n",
      "    growth_rate: 12\n",
      "    drop_rate: 0.0\n",
      "    compression_rate: 0.5\n",
      "  pyramidnet:\n",
      "    depth: 272\n",
      "    n_blocks: [3, 24, 36, 3]\n",
      "    initial_channels: 16\n",
      "    block_type: bottleneck\n",
      "    alpha: 200\n",
      "  resnext:\n",
      "    depth: 29\n",
      "    n_blocks: [3, 4, 6, 3]\n",
      "    initial_channels: 64\n",
      "    cardinality: 8\n",
      "    base_channels: 4\n",
      "  shake_shake:\n",
      "    depth: 26\n",
      "    initial_channels: 96\n",
      "    shake_forward: True\n",
      "    shake_backward: True\n",
      "    shake_image: True\n",
      "  se_resnet_preact:\n",
      "    depth: 110\n",
      "    initial_channels: 16\n",
      "    se_reduction: 16\n",
      "    block_type: basic\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "train:\n",
      "  checkpoint: /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_1_20_c10val/exp00/checkpoint_00400.pth\n",
      "  resume: False\n",
      "  precision: O0\n",
      "  batch_size: 128\n",
      "  subdivision: 1\n",
      "  optimizer: sgd\n",
      "  base_lr: 0.0008\n",
      "  momentum: 0.9\n",
      "  nesterov: True\n",
      "  weight_decay: 0.0005\n",
      "  no_weight_decay_on_bn: False\n",
      "  gradient_clip: 0\n",
      "  start_epoch: 0\n",
      "  seed: 0\n",
      "  val_first: True\n",
      "  val_period: 1\n",
      "  val_ratio: 0.1\n",
      "  use_test_as_val: False\n",
      "  output_dir: /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_1_20_c10val/exp00_resume400_50\n",
      "  log_period: 100\n",
      "  checkpoint_period: 100\n",
      "  use_tensorboard: True\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: True\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "  distributed: False\n",
      "  dist:\n",
      "    backend: nccl\n",
      "    init_method: env://\n",
      "    world_size: -1\n",
      "    node_rank: -1\n",
      "    local_rank: 0\n",
      "    use_sync_bn: False\n",
      "tensorboard:\n",
      "  train_images: False\n",
      "  val_images: False\n",
      "  model_params: False\n",
      "optim:\n",
      "  adam:\n",
      "    betas: (0.9, 0.999)\n",
      "  lars:\n",
      "    eps: 1e-09\n",
      "    threshold: 0.01\n",
      "  adabound:\n",
      "    betas: (0.9, 0.999)\n",
      "    final_lr: 0.1\n",
      "    gamma: 0.001\n",
      "scheduler:\n",
      "  epochs: 50\n",
      "  warmup:\n",
      "    type: none\n",
      "    epochs: 0\n",
      "    start_factor: 0.001\n",
      "    exponent: 4\n",
      "  type: multistep\n",
      "  milestones: [60, 120, 160]\n",
      "  lr_decay: 0.2\n",
      "  lr_min_factor: 0.001\n",
      "  T0: 10\n",
      "  T_mul: 1.0\n",
      "validation:\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: False\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "augmentation:\n",
      "  use_random_crop: True\n",
      "  use_random_horizontal_flip: True\n",
      "  use_cutout: False\n",
      "  use_random_erasing: False\n",
      "  use_dual_cutout: False\n",
      "  use_mixup: False\n",
      "  use_ricap: False\n",
      "  use_cutmix: False\n",
      "  use_label_smoothing: False\n",
      "  random_crop:\n",
      "    padding: 4\n",
      "    fill: 0\n",
      "    padding_mode: constant\n",
      "  random_horizontal_flip:\n",
      "    prob: 0.5\n",
      "  cutout:\n",
      "    prob: 1.0\n",
      "    mask_size: 16\n",
      "    cut_inside: False\n",
      "    mask_color: 0\n",
      "    dual_cutout_alpha: 0.1\n",
      "  random_erasing:\n",
      "    prob: 0.5\n",
      "    area_ratio_range: [0.02, 0.4]\n",
      "    min_aspect_ratio: 0.3\n",
      "    max_attempt: 20\n",
      "  mixup:\n",
      "    alpha: 1.0\n",
      "  ricap:\n",
      "    beta: 0.3\n",
      "  cutmix:\n",
      "    alpha: 1.0\n",
      "  label_smoothing:\n",
      "    epsilon: 0.1\n",
      "tta:\n",
      "  use_resize: False\n",
      "  use_center_crop: False\n",
      "  resize: 256\n",
      "test:\n",
      "  checkpoint: ''\n",
      "  output_dir: ''\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    pin_memory: False\n",
      "\u001b[32m[2020-07-16 21:36:05] __main__ INFO: \u001b[0menv_info:\n",
      "  pytorch_version: 1.4.0\n",
      "  cuda_version: 10.1\n",
      "  cudnn_version: 7603\n",
      "  num_gpus: 1\n",
      "  gpu_name: Tesla K80\n",
      "  gpu_capability: 3.7\n",
      "Files already downloaded and verified\n",
      "\u001b[32m[2020-07-16 21:36:09] __main__ INFO: \u001b[0mMACs  : 5.25G\n",
      "\u001b[32m[2020-07-16 21:36:09] __main__ INFO: \u001b[0m#params: 36.48M\n",
      "Selected optimization level O0:  Pure FP32 training.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\",)\n",
      "\u001b[32m[2020-07-16 21:36:09] __main__ INFO: \u001b[0mVal 0\n",
      "\u001b[32m[2020-07-16 21:36:27] __main__ INFO: \u001b[0mEpoch 0 loss 0.4380 acc@1 0.9014 acc@5 0.9946\n",
      "\u001b[32m[2020-07-16 21:36:27] __main__ INFO: \u001b[0mElapsed 18.29\n",
      "\u001b[32m[2020-07-16 21:36:27] __main__ INFO: \u001b[0mTrain 1 0\n",
      "\u001b[32m[2020-07-16 21:38:17] __main__ INFO: \u001b[0mEpoch 1 Step 100/351 lr 0.000800 loss 0.0534 (0.1394) acc@1 0.9766 (0.9618) acc@5 1.0000 (0.9991)\n",
      "\u001b[32m[2020-07-16 21:40:02] __main__ INFO: \u001b[0mEpoch 1 Step 200/351 lr 0.000800 loss 0.1064 (0.1313) acc@1 0.9766 (0.9631) acc@5 1.0000 (0.9991)\n",
      "\u001b[32m[2020-07-16 21:41:46] __main__ INFO: \u001b[0mEpoch 1 Step 300/351 lr 0.000800 loss 0.0656 (0.1257) acc@1 0.9844 (0.9645) acc@5 1.0000 (0.9991)\n",
      "\u001b[32m[2020-07-16 21:42:39] __main__ INFO: \u001b[0mEpoch 1 Step 351/351 lr 0.000800 loss 0.1060 (0.1256) acc@1 0.9766 (0.9643) acc@5 1.0000 (0.9991)\n",
      "\u001b[32m[2020-07-16 21:42:39] __main__ INFO: \u001b[0mElapsed 371.94\n",
      "\u001b[32m[2020-07-16 21:42:39] __main__ INFO: \u001b[0mVal 1\n",
      "\u001b[32m[2020-07-16 21:42:52] __main__ INFO: \u001b[0mEpoch 1 loss 0.2205 acc@1 0.9328 acc@5 0.9982\n",
      "\u001b[32m[2020-07-16 21:42:52] __main__ INFO: \u001b[0mElapsed 12.28\n",
      "\u001b[32m[2020-07-16 21:42:52] __main__ INFO: \u001b[0mTrain 2 351\n",
      "\u001b[32m[2020-07-16 21:44:36] __main__ INFO: \u001b[0mEpoch 2 Step 100/351 lr 0.000800 loss 0.0466 (0.0805) acc@1 0.9766 (0.9765) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-07-16 21:46:21] __main__ INFO: \u001b[0mEpoch 2 Step 200/351 lr 0.000800 loss 0.0273 (0.0807) acc@1 1.0000 (0.9764) acc@5 1.0000 (0.9995)\n",
      "\u001b[32m[2020-07-16 21:48:05] __main__ INFO: \u001b[0mEpoch 2 Step 300/351 lr 0.000800 loss 0.1132 (0.0815) acc@1 0.9531 (0.9755) acc@5 0.9922 (0.9996)\n",
      "\u001b[32m[2020-07-16 21:48:59] __main__ INFO: \u001b[0mEpoch 2 Step 351/351 lr 0.000800 loss 0.0872 (0.0813) acc@1 0.9844 (0.9756) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-07-16 21:48:59] __main__ INFO: \u001b[0mElapsed 367.02\n",
      "\u001b[32m[2020-07-16 21:48:59] __main__ INFO: \u001b[0mVal 2\n",
      "\u001b[32m[2020-07-16 21:49:11] __main__ INFO: \u001b[0mEpoch 2 loss 0.2217 acc@1 0.9342 acc@5 0.9976\n",
      "\u001b[32m[2020-07-16 21:49:11] __main__ INFO: \u001b[0mElapsed 12.28\n",
      "\u001b[32m[2020-07-16 21:49:11] __main__ INFO: \u001b[0mTrain 3 702\n",
      "\u001b[32m[2020-07-16 21:50:56] __main__ INFO: \u001b[0mEpoch 3 Step 100/351 lr 0.000800 loss 0.0381 (0.0617) acc@1 0.9922 (0.9813) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-16 21:52:40] __main__ INFO: \u001b[0mEpoch 3 Step 200/351 lr 0.000800 loss 0.0436 (0.0619) acc@1 0.9922 (0.9814) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-07-16 21:54:25] __main__ INFO: \u001b[0mEpoch 3 Step 300/351 lr 0.000800 loss 0.1416 (0.0619) acc@1 0.9609 (0.9819) acc@5 0.9922 (0.9997)\n",
      "\u001b[32m[2020-07-16 21:55:18] __main__ INFO: \u001b[0mEpoch 3 Step 351/351 lr 0.000800 loss 0.0856 (0.0628) acc@1 0.9766 (0.9816) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-07-16 21:55:18] __main__ INFO: \u001b[0mElapsed 367.01\n",
      "\u001b[32m[2020-07-16 21:55:18] __main__ INFO: \u001b[0mVal 3\n",
      "\u001b[32m[2020-07-16 21:55:30] __main__ INFO: \u001b[0mEpoch 3 loss 0.2256 acc@1 0.9324 acc@5 0.9980\n",
      "\u001b[32m[2020-07-16 21:55:30] __main__ INFO: \u001b[0mElapsed 12.24\n",
      "\u001b[32m[2020-07-16 21:55:30] __main__ INFO: \u001b[0mTrain 4 1053\n",
      "\u001b[32m[2020-07-16 21:57:15] __main__ INFO: \u001b[0mEpoch 4 Step 100/351 lr 0.000800 loss 0.0230 (0.0459) acc@1 1.0000 (0.9862) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-16 21:58:59] __main__ INFO: \u001b[0mEpoch 4 Step 200/351 lr 0.000800 loss 0.0967 (0.0459) acc@1 0.9844 (0.9870) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-16 22:00:44] __main__ INFO: \u001b[0mEpoch 4 Step 300/351 lr 0.000800 loss 0.0168 (0.0459) acc@1 1.0000 (0.9872) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-16 22:01:37] __main__ INFO: \u001b[0mEpoch 4 Step 351/351 lr 0.000800 loss 0.0742 (0.0466) acc@1 0.9688 (0.9870) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-16 22:01:37] __main__ INFO: \u001b[0mElapsed 367.05\n",
      "\u001b[32m[2020-07-16 22:01:37] __main__ INFO: \u001b[0mVal 4\n",
      "\u001b[32m[2020-07-16 22:01:50] __main__ INFO: \u001b[0mEpoch 4 loss 0.2337 acc@1 0.9326 acc@5 0.9986\n",
      "\u001b[32m[2020-07-16 22:01:50] __main__ INFO: \u001b[0mElapsed 12.28\n",
      "\u001b[32m[2020-07-16 22:01:50] __main__ INFO: \u001b[0mTrain 5 1404\n",
      "\u001b[32m[2020-07-16 22:03:34] __main__ INFO: \u001b[0mEpoch 5 Step 100/351 lr 0.000800 loss 0.0302 (0.0368) acc@1 0.9922 (0.9904) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-16 22:05:19] __main__ INFO: \u001b[0mEpoch 5 Step 200/351 lr 0.000800 loss 0.0203 (0.0367) acc@1 0.9922 (0.9904) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-16 22:07:03] __main__ INFO: \u001b[0mEpoch 5 Step 300/351 lr 0.000800 loss 0.0673 (0.0379) acc@1 0.9688 (0.9901) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-16 22:07:57] __main__ INFO: \u001b[0mEpoch 5 Step 351/351 lr 0.000800 loss 0.0572 (0.0376) acc@1 0.9844 (0.9901) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-16 22:07:57] __main__ INFO: \u001b[0mElapsed 367.23\n",
      "\u001b[32m[2020-07-16 22:07:57] __main__ INFO: \u001b[0mVal 5\n",
      "\u001b[32m[2020-07-16 22:08:09] __main__ INFO: \u001b[0mEpoch 5 loss 0.2194 acc@1 0.9340 acc@5 0.9984\n",
      "\u001b[32m[2020-07-16 22:08:09] __main__ INFO: \u001b[0mElapsed 12.25\n",
      "\u001b[32m[2020-07-16 22:08:09] __main__ INFO: \u001b[0mTrain 6 1755\n",
      "\u001b[32m[2020-07-16 22:09:54] __main__ INFO: \u001b[0mEpoch 6 Step 100/351 lr 0.000800 loss 0.0086 (0.0303) acc@1 1.0000 (0.9927) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-16 22:11:38] __main__ INFO: \u001b[0mEpoch 6 Step 200/351 lr 0.000800 loss 0.0554 (0.0314) acc@1 0.9844 (0.9913) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-16 22:13:23] __main__ INFO: \u001b[0mEpoch 6 Step 300/351 lr 0.000800 loss 0.0140 (0.0323) acc@1 1.0000 (0.9914) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-16 22:14:16] __main__ INFO: \u001b[0mEpoch 6 Step 351/351 lr 0.000800 loss 0.0084 (0.0322) acc@1 1.0000 (0.9913) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-16 22:14:16] __main__ INFO: \u001b[0mElapsed 367.07\n",
      "\u001b[32m[2020-07-16 22:14:16] __main__ INFO: \u001b[0mVal 6\n",
      "\u001b[32m[2020-07-16 22:14:28] __main__ INFO: \u001b[0mEpoch 6 loss 0.2244 acc@1 0.9336 acc@5 0.9982\n",
      "\u001b[32m[2020-07-16 22:14:28] __main__ INFO: \u001b[0mElapsed 12.29\n",
      "\u001b[32m[2020-07-16 22:14:28] __main__ INFO: \u001b[0mTrain 7 2106\n",
      "\u001b[32m[2020-07-16 22:16:13] __main__ INFO: \u001b[0mEpoch 7 Step 100/351 lr 0.000800 loss 0.0480 (0.0271) acc@1 0.9844 (0.9934) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-16 22:17:58] __main__ INFO: \u001b[0mEpoch 7 Step 200/351 lr 0.000800 loss 0.0435 (0.0283) acc@1 0.9844 (0.9930) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-16 22:19:42] __main__ INFO: \u001b[0mEpoch 7 Step 300/351 lr 0.000800 loss 0.0265 (0.0277) acc@1 0.9922 (0.9929) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-16 22:20:36] __main__ INFO: \u001b[0mEpoch 7 Step 351/351 lr 0.000800 loss 0.0316 (0.0281) acc@1 0.9922 (0.9929) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-16 22:20:36] __main__ INFO: \u001b[0mElapsed 367.34\n",
      "\u001b[32m[2020-07-16 22:20:36] __main__ INFO: \u001b[0mVal 7\n",
      "\u001b[32m[2020-07-16 22:20:48] __main__ INFO: \u001b[0mEpoch 7 loss 0.2232 acc@1 0.9378 acc@5 0.9984\n",
      "\u001b[32m[2020-07-16 22:20:48] __main__ INFO: \u001b[0mElapsed 12.27\n",
      "\u001b[32m[2020-07-16 22:20:48] __main__ INFO: \u001b[0mTrain 8 2457\n",
      "\u001b[32m[2020-07-16 22:22:33] __main__ INFO: \u001b[0mEpoch 8 Step 100/351 lr 0.000800 loss 0.0259 (0.0219) acc@1 0.9844 (0.9947) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-16 22:24:17] __main__ INFO: \u001b[0mEpoch 8 Step 200/351 lr 0.000800 loss 0.0219 (0.0217) acc@1 0.9922 (0.9952) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-16 22:26:02] __main__ INFO: \u001b[0mEpoch 8 Step 300/351 lr 0.000800 loss 0.0392 (0.0222) acc@1 0.9922 (0.9948) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-16 22:26:55] __main__ INFO: \u001b[0mEpoch 8 Step 351/351 lr 0.000800 loss 0.0228 (0.0226) acc@1 0.9922 (0.9945) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-16 22:26:55] __main__ INFO: \u001b[0mElapsed 367.37\n",
      "\u001b[32m[2020-07-16 22:26:55] __main__ INFO: \u001b[0mVal 8\n",
      "\u001b[32m[2020-07-16 22:27:08] __main__ INFO: \u001b[0mEpoch 8 loss 0.2285 acc@1 0.9368 acc@5 0.9980\n",
      "\u001b[32m[2020-07-16 22:27:08] __main__ INFO: \u001b[0mElapsed 12.29\n",
      "\u001b[32m[2020-07-16 22:27:08] __main__ INFO: \u001b[0mTrain 9 2808\n",
      "\u001b[32m[2020-07-16 22:28:52] __main__ INFO: \u001b[0mEpoch 9 Step 100/351 lr 0.000800 loss 0.0141 (0.0187) acc@1 1.0000 (0.9962) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-16 22:30:37] __main__ INFO: \u001b[0mEpoch 9 Step 200/351 lr 0.000800 loss 0.0072 (0.0194) acc@1 1.0000 (0.9954) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-16 22:32:22] __main__ INFO: \u001b[0mEpoch 9 Step 300/351 lr 0.000800 loss 0.0181 (0.0191) acc@1 1.0000 (0.9956) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-16 22:33:15] __main__ INFO: \u001b[0mEpoch 9 Step 351/351 lr 0.000800 loss 0.0233 (0.0194) acc@1 0.9922 (0.9954) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-16 22:33:15] __main__ INFO: \u001b[0mElapsed 367.52\n",
      "\u001b[32m[2020-07-16 22:33:15] __main__ INFO: \u001b[0mVal 9\n",
      "\u001b[32m[2020-07-16 22:33:27] __main__ INFO: \u001b[0mEpoch 9 loss 0.2232 acc@1 0.9368 acc@5 0.9982\n",
      "\u001b[32m[2020-07-16 22:33:27] __main__ INFO: \u001b[0mElapsed 12.26\n",
      "\u001b[32m[2020-07-16 22:33:27] __main__ INFO: \u001b[0mTrain 10 3159\n",
      "\u001b[32m[2020-07-16 22:35:12] __main__ INFO: \u001b[0mEpoch 10 Step 100/351 lr 0.000800 loss 0.0156 (0.0152) acc@1 1.0000 (0.9970) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-16 22:36:57] __main__ INFO: \u001b[0mEpoch 10 Step 200/351 lr 0.000800 loss 0.0116 (0.0157) acc@1 1.0000 (0.9968) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-16 22:38:42] __main__ INFO: \u001b[0mEpoch 10 Step 300/351 lr 0.000800 loss 0.0192 (0.0166) acc@1 1.0000 (0.9965) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-16 22:39:35] __main__ INFO: \u001b[0mEpoch 10 Step 351/351 lr 0.000800 loss 0.0542 (0.0163) acc@1 0.9922 (0.9966) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-16 22:39:35] __main__ INFO: \u001b[0mElapsed 367.44\n",
      "\u001b[32m[2020-07-16 22:39:35] __main__ INFO: \u001b[0mVal 10\n",
      "\u001b[32m[2020-07-16 22:39:47] __main__ INFO: \u001b[0mEpoch 10 loss 0.2340 acc@1 0.9368 acc@5 0.9986\n",
      "\u001b[32m[2020-07-16 22:39:47] __main__ INFO: \u001b[0mElapsed 12.28\n",
      "\u001b[32m[2020-07-16 22:39:47] __main__ INFO: \u001b[0mTrain 11 3510\n",
      "\u001b[32m[2020-07-16 22:41:32] __main__ INFO: \u001b[0mEpoch 11 Step 100/351 lr 0.000800 loss 0.0105 (0.0135) acc@1 1.0000 (0.9975) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-16 22:43:17] __main__ INFO: \u001b[0mEpoch 11 Step 200/351 lr 0.000800 loss 0.0047 (0.0149) acc@1 1.0000 (0.9971) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-16 22:45:01] __main__ INFO: \u001b[0mEpoch 11 Step 300/351 lr 0.000800 loss 0.0263 (0.0149) acc@1 0.9922 (0.9970) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-16 22:45:55] __main__ INFO: \u001b[0mEpoch 11 Step 351/351 lr 0.000800 loss 0.0069 (0.0147) acc@1 1.0000 (0.9971) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-16 22:45:55] __main__ INFO: \u001b[0mElapsed 367.44\n",
      "\u001b[32m[2020-07-16 22:45:55] __main__ INFO: \u001b[0mVal 11\n",
      "\u001b[32m[2020-07-16 22:46:07] __main__ INFO: \u001b[0mEpoch 11 loss 0.2399 acc@1 0.9392 acc@5 0.9988\n",
      "\u001b[32m[2020-07-16 22:46:07] __main__ INFO: \u001b[0mElapsed 12.27\n",
      "\u001b[32m[2020-07-16 22:46:07] __main__ INFO: \u001b[0mTrain 12 3861\n",
      "\u001b[32m[2020-07-16 22:47:52] __main__ INFO: \u001b[0mEpoch 12 Step 100/351 lr 0.000800 loss 0.0169 (0.0106) acc@1 0.9922 (0.9980) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-16 22:49:36] __main__ INFO: \u001b[0mEpoch 12 Step 200/351 lr 0.000800 loss 0.0054 (0.0122) acc@1 1.0000 (0.9977) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-16 22:51:21] __main__ INFO: \u001b[0mEpoch 12 Step 300/351 lr 0.000800 loss 0.0070 (0.0125) acc@1 1.0000 (0.9978) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-16 22:52:14] __main__ INFO: \u001b[0mEpoch 12 Step 351/351 lr 0.000800 loss 0.0090 (0.0126) acc@1 1.0000 (0.9976) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-16 22:52:14] __main__ INFO: \u001b[0mElapsed 367.37\n",
      "\u001b[32m[2020-07-16 22:52:14] __main__ INFO: \u001b[0mVal 12\n",
      "\u001b[32m[2020-07-16 22:52:27] __main__ INFO: \u001b[0mEpoch 12 loss 0.2394 acc@1 0.9370 acc@5 0.9986\n",
      "\u001b[32m[2020-07-16 22:52:27] __main__ INFO: \u001b[0mElapsed 12.28\n",
      "\u001b[32m[2020-07-16 22:52:27] __main__ INFO: \u001b[0mTrain 13 4212\n",
      "\u001b[32m[2020-07-16 22:54:11] __main__ INFO: \u001b[0mEpoch 13 Step 100/351 lr 0.000800 loss 0.0330 (0.0117) acc@1 0.9922 (0.9983) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-16 22:55:56] __main__ INFO: \u001b[0mEpoch 13 Step 200/351 lr 0.000800 loss 0.0184 (0.0123) acc@1 0.9922 (0.9980) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-16 22:57:41] __main__ INFO: \u001b[0mEpoch 13 Step 300/351 lr 0.000800 loss 0.0046 (0.0124) acc@1 1.0000 (0.9979) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-16 22:58:34] __main__ INFO: \u001b[0mEpoch 13 Step 351/351 lr 0.000800 loss 0.0065 (0.0123) acc@1 1.0000 (0.9978) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-16 22:58:34] __main__ INFO: \u001b[0mElapsed 367.49\n",
      "\u001b[32m[2020-07-16 22:58:34] __main__ INFO: \u001b[0mVal 13\n",
      "\u001b[32m[2020-07-16 22:58:46] __main__ INFO: \u001b[0mEpoch 13 loss 0.2384 acc@1 0.9364 acc@5 0.9988\n",
      "\u001b[32m[2020-07-16 22:58:46] __main__ INFO: \u001b[0mElapsed 12.28\n",
      "\u001b[32m[2020-07-16 22:58:46] __main__ INFO: \u001b[0mTrain 14 4563\n",
      "\u001b[32m[2020-07-16 23:00:31] __main__ INFO: \u001b[0mEpoch 14 Step 100/351 lr 0.000800 loss 0.0074 (0.0108) acc@1 1.0000 (0.9981) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-16 23:02:16] __main__ INFO: \u001b[0mEpoch 14 Step 200/351 lr 0.000800 loss 0.0046 (0.0107) acc@1 1.0000 (0.9982) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-16 23:04:00] __main__ INFO: \u001b[0mEpoch 14 Step 300/351 lr 0.000800 loss 0.0068 (0.0107) acc@1 1.0000 (0.9983) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-16 23:04:54] __main__ INFO: \u001b[0mEpoch 14 Step 351/351 lr 0.000800 loss 0.0096 (0.0107) acc@1 1.0000 (0.9982) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-16 23:04:54] __main__ INFO: \u001b[0mElapsed 367.47\n",
      "\u001b[32m[2020-07-16 23:04:54] __main__ INFO: \u001b[0mVal 14\n",
      "\u001b[32m[2020-07-16 23:05:06] __main__ INFO: \u001b[0mEpoch 14 loss 0.2314 acc@1 0.9412 acc@5 0.9986\n",
      "\u001b[32m[2020-07-16 23:05:06] __main__ INFO: \u001b[0mElapsed 12.29\n",
      "\u001b[32m[2020-07-16 23:05:06] __main__ INFO: \u001b[0mTrain 15 4914\n",
      "\u001b[32m[2020-07-16 23:06:51] __main__ INFO: \u001b[0mEpoch 15 Step 100/351 lr 0.000800 loss 0.0048 (0.0092) acc@1 1.0000 (0.9986) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-16 23:08:35] __main__ INFO: \u001b[0mEpoch 15 Step 200/351 lr 0.000800 loss 0.0054 (0.0091) acc@1 1.0000 (0.9985) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-16 23:10:20] __main__ INFO: \u001b[0mEpoch 15 Step 300/351 lr 0.000800 loss 0.0081 (0.0089) acc@1 1.0000 (0.9985) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-16 23:11:13] __main__ INFO: \u001b[0mEpoch 15 Step 351/351 lr 0.000800 loss 0.0179 (0.0090) acc@1 0.9922 (0.9985) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-16 23:11:14] __main__ INFO: \u001b[0mElapsed 367.44\n",
      "\u001b[32m[2020-07-16 23:11:14] __main__ INFO: \u001b[0mVal 15\n",
      "\u001b[32m[2020-07-16 23:11:26] __main__ INFO: \u001b[0mEpoch 15 loss 0.2357 acc@1 0.9402 acc@5 0.9980\n",
      "\u001b[32m[2020-07-16 23:11:26] __main__ INFO: \u001b[0mElapsed 12.30\n",
      "\u001b[32m[2020-07-16 23:11:26] __main__ INFO: \u001b[0mTrain 16 5265\n",
      "\u001b[32m[2020-07-16 23:13:11] __main__ INFO: \u001b[0mEpoch 16 Step 100/351 lr 0.000800 loss 0.0055 (0.0081) acc@1 1.0000 (0.9986) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-16 23:14:55] __main__ INFO: \u001b[0mEpoch 16 Step 200/351 lr 0.000800 loss 0.0045 (0.0085) acc@1 1.0000 (0.9985) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-16 23:16:40] __main__ INFO: \u001b[0mEpoch 16 Step 300/351 lr 0.000800 loss 0.0087 (0.0080) acc@1 1.0000 (0.9987) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-16 23:17:33] __main__ INFO: \u001b[0mEpoch 16 Step 351/351 lr 0.000800 loss 0.0032 (0.0081) acc@1 1.0000 (0.9987) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-16 23:17:33] __main__ INFO: \u001b[0mElapsed 367.25\n",
      "\u001b[32m[2020-07-16 23:17:33] __main__ INFO: \u001b[0mVal 16\n",
      "\u001b[32m[2020-07-16 23:17:45] __main__ INFO: \u001b[0mEpoch 16 loss 0.2353 acc@1 0.9388 acc@5 0.9984\n",
      "\u001b[32m[2020-07-16 23:17:45] __main__ INFO: \u001b[0mElapsed 12.28\n",
      "\u001b[32m[2020-07-16 23:17:45] __main__ INFO: \u001b[0mTrain 17 5616\n",
      "\u001b[32m[2020-07-16 23:19:30] __main__ INFO: \u001b[0mEpoch 17 Step 100/351 lr 0.000800 loss 0.0083 (0.0064) acc@1 1.0000 (0.9989) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-16 23:21:15] __main__ INFO: \u001b[0mEpoch 17 Step 200/351 lr 0.000800 loss 0.0230 (0.0078) acc@1 0.9922 (0.9986) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-16 23:22:59] __main__ INFO: \u001b[0mEpoch 17 Step 300/351 lr 0.000800 loss 0.0041 (0.0076) acc@1 1.0000 (0.9988) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-16 23:23:53] __main__ INFO: \u001b[0mEpoch 17 Step 351/351 lr 0.000800 loss 0.0110 (0.0079) acc@1 1.0000 (0.9987) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-16 23:23:53] __main__ INFO: \u001b[0mElapsed 367.33\n",
      "\u001b[32m[2020-07-16 23:23:53] __main__ INFO: \u001b[0mVal 17\n",
      "\u001b[32m[2020-07-16 23:24:05] __main__ INFO: \u001b[0mEpoch 17 loss 0.2356 acc@1 0.9386 acc@5 0.9992\n",
      "\u001b[32m[2020-07-16 23:24:05] __main__ INFO: \u001b[0mElapsed 12.28\n",
      "\u001b[32m[2020-07-16 23:24:05] __main__ INFO: \u001b[0mTrain 18 5967\n",
      "\u001b[32m[2020-07-16 23:25:50] __main__ INFO: \u001b[0mEpoch 18 Step 100/351 lr 0.000800 loss 0.0034 (0.0096) acc@1 1.0000 (0.9980) acc@5 1.0000 (1.0000)\n"
     ]
    }
   ],
   "source": [
    "# Resume training with the un-augmented data\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "!python train.py --config /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_1_20_c10val/exp00/config.yaml \\\n",
    "    train.checkpoint /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_1_20_c10val/exp00/checkpoint_00400.pth \\\n",
    "    dataset.name CIFAR10 \\\n",
    "    train.base_lr 0.0008 \\\n",
    "    train.output_dir /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_1_20_c10val/exp00_resume400_50 \\\n",
    "    scheduler.epochs 50\n",
    "\n",
    "#### Set LEARNING RATE based on ending LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-17 02:59:44] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_1_20_c10val/exp00/checkpoint_00400.pth\n",
      "Files already downloaded and verified\n",
      "100%|| 79/79 [00:25<00:00,  3.16it/s]\n",
      "\u001b[32m[2020-07-17 03:00:10] __main__ INFO: \u001b[0mElapsed 25.01\n",
      "\u001b[32m[2020-07-17 03:00:10] __main__ INFO: \u001b[0mLoss 0.4061 Accuracy 0.9057\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10 test dataset - without training on unaugmented data\n",
    "# Right the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/wrn.yaml \\\n",
    "   test.batch_size 128 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_1_20_c10val/exp00/checkpoint_00400.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_1_20_c10val/exp00/test_results_0400_cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-17 03:00:21] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_1_20_c10val/exp00/checkpoint_00400.pth\n",
      "CIFAR 10.1\n",
      "100%|| 16/16 [00:05<00:00,  2.99it/s]\n",
      "\u001b[32m[2020-07-17 03:00:27] __main__ INFO: \u001b[0mElapsed 5.35\n",
      "\u001b[32m[2020-07-17 03:00:27] __main__ INFO: \u001b[0mLoss 0.8234 Accuracy 0.8220\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10 test dataset - without training on unaugmented data\n",
    "# Write the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/wrn.yaml \\\n",
    "   test.batch_size 128 \\\n",
    "   dataset.name CIFAR101 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_1_20_c10val/exp00/checkpoint_00400.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_1_20_c10val/exp00/test_results_0400_cifar101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-17 03:00:45] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_1_20_c10val/exp00_resume400_50/checkpoint_00050.pth\n",
      "Files already downloaded and verified\n",
      "100%|| 79/79 [00:25<00:00,  3.16it/s]\n",
      "\u001b[32m[2020-07-17 03:01:11] __main__ INFO: \u001b[0mElapsed 25.02\n",
      "\u001b[32m[2020-07-17 03:01:11] __main__ INFO: \u001b[0mLoss 0.2403 Accuracy 0.9430\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10 test dataset \n",
    "# Write the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/wrn.yaml \\\n",
    "   test.batch_size 128 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_1_20_c10val/exp00_resume400_50/checkpoint_00050.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_1_20_c10val/exp00_resume400_50/test_results_0050_cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-17 03:01:22] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_1_20_c10val/exp00_resume400_50/checkpoint_00050.pth\n",
      "CIFAR 10.1\n",
      "100%|| 16/16 [00:05<00:00,  2.99it/s]\n",
      "\u001b[32m[2020-07-17 03:01:28] __main__ INFO: \u001b[0mElapsed 5.36\n",
      "\u001b[32m[2020-07-17 03:01:28] __main__ INFO: \u001b[0mLoss 0.5465 Accuracy 0.8755\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10.1 test dataset \n",
    "# Write the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/wrn.yaml \\\n",
    "   test.batch_size 128 \\\n",
    "   dataset.name CIFAR101 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_1_20_c10val/exp00_resume400_50/checkpoint_00050.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_1_20_c10val/exp00_resume400_50/test_results_0050_cifar101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Record the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Testset</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Original_Accuracy</th>\n",
       "      <th>Original_CI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wrn_28_10_ra_1_20_c10val</td>\n",
       "      <td>400</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.4061</td>\n",
       "      <td>0.9057</td>\n",
       "      <td>92.5</td>\n",
       "      <td>(92.0, 93.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wrn_28_10_ra_1_20_c10val</td>\n",
       "      <td>400</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>0.8234</td>\n",
       "      <td>0.822</td>\n",
       "      <td>84.9</td>\n",
       "      <td>(83.2, 86.4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wrn_28_10_ra_1_20_c10val_refined400</td>\n",
       "      <td>50</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>0.5465</td>\n",
       "      <td>0.8755</td>\n",
       "      <td>84.9</td>\n",
       "      <td>(83.2, 86.4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wrn_28_10_ra_1_20_c10val_refined400</td>\n",
       "      <td>50</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.2403</td>\n",
       "      <td>0.943</td>\n",
       "      <td>92.5</td>\n",
       "      <td>(92.0, 93.0)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model Epoch    Testset    Loss Accuracy  \\\n",
       "0             wrn_28_10_ra_1_20_c10val   400    cifar10  0.4061   0.9057   \n",
       "1             wrn_28_10_ra_1_20_c10val   400  cifar10.1  0.8234    0.822   \n",
       "2  wrn_28_10_ra_1_20_c10val_refined400    50  cifar10.1  0.5465   0.8755   \n",
       "3  wrn_28_10_ra_1_20_c10val_refined400    50    cifar10  0.2403    0.943   \n",
       "\n",
       "   Original_Accuracy   Original_CI  \n",
       "0               92.5  (92.0, 93.0)  \n",
       "1               84.9  (83.2, 86.4)  \n",
       "2               84.9  (83.2, 86.4)  \n",
       "3               92.5  (92.0, 93.0)  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "model = 'wrn_28_10_ra_1_20_c10val'\n",
    "model_refined = model + '_refined400'\n",
    "\n",
    "a = pd.Series([model, 400, 'cifar10', 0.4061, 0.9057])\n",
    "c = pd.Series([model, 400, 'cifar10.1', 0.8234, 0.8220])\n",
    "\n",
    "e = pd.Series([model_refined, 50, 'cifar10.1', 0.5465, 0.8755])\n",
    "f = pd.Series([model_refined, 50, 'cifar10', 0.2403, 0.9430])\n",
    "               \n",
    "df_results = pd.concat([a,c,e,f], axis=1).T\n",
    "df_results.columns = ['Model', 'Epoch', 'Testset', 'Loss', 'Accuracy']\n",
    "\n",
    "df_results['Original_Accuracy'] = df_results.apply((lambda row: 92.5 if row[2] == 'cifar10' else 84.9), axis=1)\n",
    "df_results['Original_CI'] = df_results.apply((lambda row: (92.0, 93.0) if row[2] == 'cifar10' else (83.2, 86.4)), axis=1)\n",
    "\n",
    "df_results.to_csv('/home/ec2-user/SageMaker/experiments/' + model + '/results.csv')\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the model checkpoints, configs, and results to S3 \n",
    "bucket='sagemaker-june29'\n",
    "prefix = 'sagemaker/results/original-models/wrn_28_10_ra_1_20_c10val'\n",
    "path = '/home/ec2-user/SageMaker/experiments/wrn_28_10_ra_1_20_c10val'\n",
    "\n",
    "s3_resource = boto3.resource(\"s3\", region_name=\"us-east-2\")\n",
    "\n",
    "def uploadDirectory(local_path,bucket_name,s3_prefix):\n",
    "\n",
    "    my_bucket = s3_resource.Bucket(bucket_name)\n",
    "    \n",
    "    for path, subdirs, files in os.walk(local_path):\n",
    "        path = path.replace(\"\\\\\",\"/\")\n",
    "        directory_name = path.replace(local_path,\"\")\n",
    "        for file in files:\n",
    "            #print(\"Local File:\", os.path.join(path, file))\n",
    "            #print(\"      Dest:\", s3_prefix+directory_name+'/'+file)\n",
    "            my_bucket.upload_file(os.path.join(path, file), s3_prefix+directory_name+'/'+file)\n",
    "    \n",
    "uploadDirectory(path,bucket,prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
