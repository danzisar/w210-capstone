{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wide Residual Net\n",
    "\n",
    " - Training Dataset:  RandAugment, N=2, M=5\n",
    "   Validation with Unaugmented Data\n",
    " - Sagemaker Notebook must be of type, conda_pytorch_p36\n",
    " \n",
    "#### Install Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import numpy \n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 1)) (1.18.1)\n",
      "Requirement already satisfied: torch>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 2)) (1.4.0)\n",
      "Requirement already satisfied: torchvision in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (0.5.0)\n",
      "Requirement already satisfied: fvcore in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (0.1.1.post20200716)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 5)) (4.44.1)\n",
      "Requirement already satisfied: yacs in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 6)) (0.1.7)\n",
      "Requirement already satisfied: apex from git+https://github.com/NVIDIA/apex.git#egg=apex in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 7)) (0.1)\n",
      "Requirement already satisfied: termcolor in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 8)) (1.1.0)\n",
      "Requirement already satisfied: thop<0.0.31.post2004070130 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 9)) (0.0.31.post2001170342)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (1.14.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (7.0.0)\n",
      "Requirement already satisfied: tabulate in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (0.8.7)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (5.3.1)\n",
      "Requirement already satisfied: portalocker in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (1.7.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: tensorboard in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (2.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (3.2.2)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.30.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (0.9.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.19.1)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (3.12.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.0.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.7.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.14.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (0.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (2.23.0)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (0.34.2)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.18.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (46.1.3.post20200330)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard) (1.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (4.1.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (3.4.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (2020.4.5.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard) (2.2.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r '/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt'\n",
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-16 21:55:09] __main__ INFO: \u001b[0mdevice: cuda\n",
      "cudnn:\n",
      "  benchmark: True\n",
      "  deterministic: False\n",
      "dataset:\n",
      "  name: CIFAR10_RA_2_5\n",
      "  dataset_dir: ''\n",
      "  image_size: 32\n",
      "  n_channels: 3\n",
      "  n_classes: 10\n",
      "model:\n",
      "  type: cifar\n",
      "  name: wrn\n",
      "  init_mode: kaiming_fan_in\n",
      "  vgg:\n",
      "    n_channels: [64, 128, 256, 512, 512]\n",
      "    n_layers: [2, 2, 3, 3, 3]\n",
      "    use_bn: True\n",
      "  resnet:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "  resnet_preact:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "  wrn:\n",
      "    depth: 28\n",
      "    initial_channels: 16\n",
      "    widening_factor: 10\n",
      "    drop_rate: 0.0\n",
      "  densenet:\n",
      "    depth: 100\n",
      "    n_blocks: [6, 12, 24, 16]\n",
      "    block_type: bottleneck\n",
      "    growth_rate: 12\n",
      "    drop_rate: 0.0\n",
      "    compression_rate: 0.5\n",
      "  pyramidnet:\n",
      "    depth: 272\n",
      "    n_blocks: [3, 24, 36, 3]\n",
      "    initial_channels: 16\n",
      "    block_type: bottleneck\n",
      "    alpha: 200\n",
      "  resnext:\n",
      "    depth: 29\n",
      "    n_blocks: [3, 4, 6, 3]\n",
      "    initial_channels: 64\n",
      "    cardinality: 8\n",
      "    base_channels: 4\n",
      "  shake_shake:\n",
      "    depth: 26\n",
      "    initial_channels: 96\n",
      "    shake_forward: True\n",
      "    shake_backward: True\n",
      "    shake_image: True\n",
      "  se_resnet_preact:\n",
      "    depth: 110\n",
      "    initial_channels: 16\n",
      "    se_reduction: 16\n",
      "    block_type: basic\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "train:\n",
      "  checkpoint: ''\n",
      "  resume: False\n",
      "  precision: O0\n",
      "  batch_size: 128\n",
      "  subdivision: 1\n",
      "  optimizer: sgd\n",
      "  base_lr: 0.1\n",
      "  momentum: 0.9\n",
      "  nesterov: True\n",
      "  weight_decay: 0.0005\n",
      "  no_weight_decay_on_bn: False\n",
      "  gradient_clip: 0\n",
      "  start_epoch: 0\n",
      "  seed: 0\n",
      "  val_first: True\n",
      "  val_period: 1\n",
      "  val_ratio: 0.1\n",
      "  use_test_as_val: False\n",
      "  output_dir: /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_5_c10val/exp00\n",
      "  log_period: 100\n",
      "  checkpoint_period: 100\n",
      "  use_tensorboard: True\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: True\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "  distributed: False\n",
      "  dist:\n",
      "    backend: nccl\n",
      "    init_method: env://\n",
      "    world_size: -1\n",
      "    node_rank: -1\n",
      "    local_rank: 0\n",
      "    use_sync_bn: False\n",
      "tensorboard:\n",
      "  train_images: False\n",
      "  val_images: False\n",
      "  model_params: False\n",
      "optim:\n",
      "  adam:\n",
      "    betas: (0.9, 0.999)\n",
      "  lars:\n",
      "    eps: 1e-09\n",
      "    threshold: 0.01\n",
      "  adabound:\n",
      "    betas: (0.9, 0.999)\n",
      "    final_lr: 0.1\n",
      "    gamma: 0.001\n",
      "scheduler:\n",
      "  epochs: 400\n",
      "  warmup:\n",
      "    type: none\n",
      "    epochs: 0\n",
      "    start_factor: 0.001\n",
      "    exponent: 4\n",
      "  type: multistep\n",
      "  milestones: [60, 120, 160]\n",
      "  lr_decay: 0.2\n",
      "  lr_min_factor: 0.001\n",
      "  T0: 10\n",
      "  T_mul: 1.0\n",
      "validation:\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: False\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "augmentation:\n",
      "  use_random_crop: True\n",
      "  use_random_horizontal_flip: True\n",
      "  use_cutout: False\n",
      "  use_random_erasing: False\n",
      "  use_dual_cutout: False\n",
      "  use_mixup: False\n",
      "  use_ricap: False\n",
      "  use_cutmix: False\n",
      "  use_label_smoothing: False\n",
      "  random_crop:\n",
      "    padding: 4\n",
      "    fill: 0\n",
      "    padding_mode: constant\n",
      "  random_horizontal_flip:\n",
      "    prob: 0.5\n",
      "  cutout:\n",
      "    prob: 1.0\n",
      "    mask_size: 16\n",
      "    cut_inside: False\n",
      "    mask_color: 0\n",
      "    dual_cutout_alpha: 0.1\n",
      "  random_erasing:\n",
      "    prob: 0.5\n",
      "    area_ratio_range: [0.02, 0.4]\n",
      "    min_aspect_ratio: 0.3\n",
      "    max_attempt: 20\n",
      "  mixup:\n",
      "    alpha: 1.0\n",
      "  ricap:\n",
      "    beta: 0.3\n",
      "  cutmix:\n",
      "    alpha: 1.0\n",
      "  label_smoothing:\n",
      "    epsilon: 0.1\n",
      "tta:\n",
      "  use_resize: False\n",
      "  use_center_crop: False\n",
      "  resize: 256\n",
      "test:\n",
      "  checkpoint: ''\n",
      "  output_dir: ''\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    pin_memory: False\n",
      "\u001b[32m[2020-07-16 21:55:09] __main__ INFO: \u001b[0menv_info:\n",
      "  pytorch_version: 1.4.0\n",
      "  cuda_version: 10.1\n",
      "  cudnn_version: 7603\n",
      "  num_gpus: 1\n",
      "  gpu_name: Tesla K80\n",
      "  gpu_capability: 3.7\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /home/ec2-user/.torch/datasets/CIFAR10/cifar-10-python.tar.gz\n",
      "170500096it [00:02, 69882071.58it/s]                                            \n",
      "Extracting /home/ec2-user/.torch/datasets/CIFAR10/cifar-10-python.tar.gz to /home/ec2-user/.torch/datasets/CIFAR10\n",
      "\u001b[32m[2020-07-16 21:55:43] __main__ INFO: \u001b[0mMACs  : 5.25G\n",
      "\u001b[32m[2020-07-16 21:55:43] __main__ INFO: \u001b[0m#params: 36.48M\n",
      "Selected optimization level O0:  Pure FP32 training.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\",)\n",
      "\u001b[32m[2020-07-16 21:55:43] __main__ INFO: \u001b[0mVal 0\n",
      "\u001b[32m[2020-07-16 21:56:04] __main__ INFO: \u001b[0mEpoch 0 loss 142.2038 acc@1 0.0968 acc@5 0.5496\n",
      "\u001b[32m[2020-07-16 21:56:04] __main__ INFO: \u001b[0mElapsed 20.95\n",
      "\u001b[32m[2020-07-16 21:56:04] __main__ INFO: \u001b[0mTrain 1 0\n",
      "\u001b[32m[2020-07-16 21:57:57] __main__ INFO: \u001b[0mEpoch 1 Step 100/351 lr 0.100000 loss 2.1632 (2.3628) acc@1 0.2109 (0.1376) acc@5 0.7109 (0.5986)\n",
      "\u001b[32m[2020-07-16 21:59:46] __main__ INFO: \u001b[0mEpoch 1 Step 200/351 lr 0.100000 loss 1.9584 (2.2400) acc@1 0.2422 (0.1686) acc@5 0.7656 (0.6475)\n",
      "\u001b[32m[2020-07-16 22:01:35] __main__ INFO: \u001b[0mEpoch 1 Step 300/351 lr 0.100000 loss 1.9544 (2.1656) acc@1 0.2812 (0.1943) acc@5 0.7266 (0.6738)\n",
      "\u001b[32m[2020-07-16 22:02:31] __main__ INFO: \u001b[0mEpoch 1 Step 351/351 lr 0.100000 loss 1.9151 (2.1364) acc@1 0.2109 (0.2039) acc@5 0.7812 (0.6840)\n",
      "\u001b[32m[2020-07-16 22:02:31] __main__ INFO: \u001b[0mElapsed 386.85\n",
      "\u001b[32m[2020-07-16 22:02:31] __main__ INFO: \u001b[0mVal 1\n",
      "\u001b[32m[2020-07-16 22:02:44] __main__ INFO: \u001b[0mEpoch 1 loss 1.5801 acc@1 0.4154 acc@5 0.9056\n",
      "\u001b[32m[2020-07-16 22:02:44] __main__ INFO: \u001b[0mElapsed 12.87\n",
      "\u001b[32m[2020-07-16 22:02:44] __main__ INFO: \u001b[0mTrain 2 351\n",
      "\u001b[32m[2020-07-16 22:04:32] __main__ INFO: \u001b[0mEpoch 2 Step 100/351 lr 0.100000 loss 1.8251 (1.9045) acc@1 0.3594 (0.2908) acc@5 0.7500 (0.7610)\n",
      "\u001b[32m[2020-07-16 22:06:21] __main__ INFO: \u001b[0mEpoch 2 Step 200/351 lr 0.100000 loss 1.8188 (1.8568) acc@1 0.3203 (0.3071) acc@5 0.7578 (0.7694)\n",
      "\u001b[32m[2020-07-16 22:08:10] __main__ INFO: \u001b[0mEpoch 2 Step 300/351 lr 0.100000 loss 1.7025 (1.8233) acc@1 0.4062 (0.3232) acc@5 0.7969 (0.7741)\n",
      "\u001b[32m[2020-07-16 22:09:05] __main__ INFO: \u001b[0mEpoch 2 Step 351/351 lr 0.100000 loss 1.5906 (1.8044) acc@1 0.3594 (0.3312) acc@5 0.8203 (0.7776)\n",
      "\u001b[32m[2020-07-16 22:09:05] __main__ INFO: \u001b[0mElapsed 381.63\n",
      "\u001b[32m[2020-07-16 22:09:05] __main__ INFO: \u001b[0mVal 2\n",
      "\u001b[32m[2020-07-16 22:09:18] __main__ INFO: \u001b[0mEpoch 2 loss 1.4212 acc@1 0.4912 acc@5 0.9354\n",
      "\u001b[32m[2020-07-16 22:09:18] __main__ INFO: \u001b[0mElapsed 12.88\n",
      "\u001b[32m[2020-07-16 22:09:18] __main__ INFO: \u001b[0mTrain 3 702\n",
      "\u001b[32m[2020-07-16 22:11:07] __main__ INFO: \u001b[0mEpoch 3 Step 100/351 lr 0.100000 loss 1.5881 (1.6807) acc@1 0.4062 (0.3748) acc@5 0.8203 (0.7957)\n",
      "\u001b[32m[2020-07-16 22:12:56] __main__ INFO: \u001b[0mEpoch 3 Step 200/351 lr 0.100000 loss 1.5056 (1.6589) acc@1 0.4531 (0.3855) acc@5 0.8359 (0.8007)\n",
      "\u001b[32m[2020-07-16 22:14:45] __main__ INFO: \u001b[0mEpoch 3 Step 300/351 lr 0.100000 loss 1.7123 (1.6372) acc@1 0.3750 (0.3933) acc@5 0.7500 (0.8056)\n",
      "\u001b[32m[2020-07-16 22:15:41] __main__ INFO: \u001b[0mEpoch 3 Step 351/351 lr 0.100000 loss 1.7196 (1.6242) acc@1 0.3828 (0.3983) acc@5 0.7578 (0.8076)\n",
      "\u001b[32m[2020-07-16 22:15:41] __main__ INFO: \u001b[0mElapsed 382.94\n",
      "\u001b[32m[2020-07-16 22:15:41] __main__ INFO: \u001b[0mVal 3\n",
      "\u001b[32m[2020-07-16 22:15:54] __main__ INFO: \u001b[0mEpoch 3 loss 1.4063 acc@1 0.5072 acc@5 0.9632\n",
      "\u001b[32m[2020-07-16 22:15:54] __main__ INFO: \u001b[0mElapsed 12.88\n",
      "\u001b[32m[2020-07-16 22:15:54] __main__ INFO: \u001b[0mTrain 4 1053\n",
      "\u001b[32m[2020-07-16 22:17:43] __main__ INFO: \u001b[0mEpoch 4 Step 100/351 lr 0.100000 loss 1.4958 (1.5416) acc@1 0.4375 (0.4332) acc@5 0.7969 (0.8160)\n",
      "\u001b[32m[2020-07-16 22:19:32] __main__ INFO: \u001b[0mEpoch 4 Step 200/351 lr 0.100000 loss 1.5089 (1.5244) acc@1 0.4766 (0.4407) acc@5 0.7578 (0.8179)\n",
      "\u001b[32m[2020-07-16 22:21:20] __main__ INFO: \u001b[0mEpoch 4 Step 300/351 lr 0.100000 loss 1.5715 (1.5099) acc@1 0.4141 (0.4468) acc@5 0.7656 (0.8198)\n",
      "\u001b[32m[2020-07-16 22:22:16] __main__ INFO: \u001b[0mEpoch 4 Step 351/351 lr 0.100000 loss 1.6325 (1.5021) acc@1 0.4453 (0.4496) acc@5 0.7969 (0.8208)\n",
      "\u001b[32m[2020-07-16 22:22:16] __main__ INFO: \u001b[0mElapsed 381.89\n",
      "\u001b[32m[2020-07-16 22:22:16] __main__ INFO: \u001b[0mVal 4\n",
      "\u001b[32m[2020-07-16 22:22:29] __main__ INFO: \u001b[0mEpoch 4 loss 0.9658 acc@1 0.6568 acc@5 0.9708\n",
      "\u001b[32m[2020-07-16 22:22:29] __main__ INFO: \u001b[0mElapsed 12.86\n",
      "\u001b[32m[2020-07-16 22:22:29] __main__ INFO: \u001b[0mTrain 5 1404\n",
      "\u001b[32m[2020-07-16 22:24:18] __main__ INFO: \u001b[0mEpoch 5 Step 100/351 lr 0.100000 loss 1.3183 (1.4373) acc@1 0.4922 (0.4738) acc@5 0.8594 (0.8267)\n",
      "\u001b[32m[2020-07-16 22:26:06] __main__ INFO: \u001b[0mEpoch 5 Step 200/351 lr 0.100000 loss 1.4508 (1.4266) acc@1 0.4922 (0.4779) acc@5 0.8203 (0.8271)\n",
      "\u001b[32m[2020-07-16 22:27:56] __main__ INFO: \u001b[0mEpoch 5 Step 300/351 lr 0.100000 loss 1.3301 (1.4209) acc@1 0.5391 (0.4811) acc@5 0.8203 (0.8300)\n",
      "\u001b[32m[2020-07-16 22:28:52] __main__ INFO: \u001b[0mEpoch 5 Step 351/351 lr 0.100000 loss 1.2259 (1.4227) acc@1 0.5859 (0.4804) acc@5 0.8281 (0.8298)\n",
      "\u001b[32m[2020-07-16 22:28:52] __main__ INFO: \u001b[0mElapsed 382.85\n",
      "\u001b[32m[2020-07-16 22:28:52] __main__ INFO: \u001b[0mVal 5\n",
      "\u001b[32m[2020-07-16 22:29:04] __main__ INFO: \u001b[0mEpoch 5 loss 0.9765 acc@1 0.6598 acc@5 0.9704\n",
      "\u001b[32m[2020-07-16 22:29:04] __main__ INFO: \u001b[0mElapsed 12.96\n",
      "\u001b[32m[2020-07-16 22:29:04] __main__ INFO: \u001b[0mTrain 6 1755\n",
      "\u001b[32m[2020-07-16 22:30:54] __main__ INFO: \u001b[0mEpoch 6 Step 100/351 lr 0.100000 loss 1.2437 (1.3785) acc@1 0.5781 (0.4909) acc@5 0.8672 (0.8345)\n",
      "\u001b[32m[2020-07-16 22:32:42] __main__ INFO: \u001b[0mEpoch 6 Step 200/351 lr 0.100000 loss 1.4067 (1.3788) acc@1 0.4766 (0.4927) acc@5 0.8047 (0.8358)\n",
      "\u001b[32m[2020-07-16 22:34:31] __main__ INFO: \u001b[0mEpoch 6 Step 300/351 lr 0.100000 loss 1.4390 (1.3728) acc@1 0.4766 (0.4954) acc@5 0.8750 (0.8372)\n",
      "\u001b[32m[2020-07-16 22:35:27] __main__ INFO: \u001b[0mEpoch 6 Step 351/351 lr 0.100000 loss 1.4171 (1.3668) acc@1 0.5000 (0.4965) acc@5 0.8125 (0.8378)\n",
      "\u001b[32m[2020-07-16 22:35:27] __main__ INFO: \u001b[0mElapsed 382.23\n",
      "\u001b[32m[2020-07-16 22:35:27] __main__ INFO: \u001b[0mVal 6\n",
      "\u001b[32m[2020-07-16 22:35:40] __main__ INFO: \u001b[0mEpoch 6 loss 0.8265 acc@1 0.6988 acc@5 0.9798\n",
      "\u001b[32m[2020-07-16 22:35:40] __main__ INFO: \u001b[0mElapsed 12.87\n",
      "\u001b[32m[2020-07-16 22:35:40] __main__ INFO: \u001b[0mTrain 7 2106\n",
      "\u001b[32m[2020-07-16 22:37:29] __main__ INFO: \u001b[0mEpoch 7 Step 100/351 lr 0.100000 loss 1.3189 (1.3335) acc@1 0.5078 (0.5087) acc@5 0.8125 (0.8363)\n",
      "\u001b[32m[2020-07-16 22:39:18] __main__ INFO: \u001b[0mEpoch 7 Step 200/351 lr 0.100000 loss 1.1766 (1.3361) acc@1 0.5781 (0.5070) acc@5 0.9453 (0.8352)\n",
      "\u001b[32m[2020-07-16 22:41:07] __main__ INFO: \u001b[0mEpoch 7 Step 300/351 lr 0.100000 loss 1.1481 (1.3370) acc@1 0.5938 (0.5064) acc@5 0.9297 (0.8369)\n",
      "\u001b[32m[2020-07-16 22:42:03] __main__ INFO: \u001b[0mEpoch 7 Step 351/351 lr 0.100000 loss 1.3229 (1.3381) acc@1 0.5312 (0.5058) acc@5 0.8672 (0.8374)\n",
      "\u001b[32m[2020-07-16 22:42:03] __main__ INFO: \u001b[0mElapsed 382.99\n",
      "\u001b[32m[2020-07-16 22:42:03] __main__ INFO: \u001b[0mVal 7\n",
      "\u001b[32m[2020-07-16 22:42:15] __main__ INFO: \u001b[0mEpoch 7 loss 0.7637 acc@1 0.7362 acc@5 0.9836\n",
      "\u001b[32m[2020-07-16 22:42:15] __main__ INFO: \u001b[0mElapsed 12.87\n",
      "\u001b[32m[2020-07-16 22:42:15] __main__ INFO: \u001b[0mTrain 8 2457\n",
      "\u001b[32m[2020-07-16 22:44:04] __main__ INFO: \u001b[0mEpoch 8 Step 100/351 lr 0.100000 loss 1.3725 (1.2982) acc@1 0.4766 (0.5246) acc@5 0.8047 (0.8405)\n",
      "\u001b[32m[2020-07-16 22:45:53] __main__ INFO: \u001b[0mEpoch 8 Step 200/351 lr 0.100000 loss 1.3639 (1.2987) acc@1 0.4766 (0.5242) acc@5 0.8828 (0.8412)\n",
      "\u001b[32m[2020-07-16 22:47:42] __main__ INFO: \u001b[0mEpoch 8 Step 300/351 lr 0.100000 loss 1.1997 (1.2988) acc@1 0.5781 (0.5240) acc@5 0.8984 (0.8402)\n",
      "\u001b[32m[2020-07-16 22:48:37] __main__ INFO: \u001b[0mEpoch 8 Step 351/351 lr 0.100000 loss 1.2941 (1.3023) acc@1 0.5781 (0.5235) acc@5 0.8906 (0.8401)\n",
      "\u001b[32m[2020-07-16 22:48:37] __main__ INFO: \u001b[0mElapsed 381.52\n",
      "\u001b[32m[2020-07-16 22:48:37] __main__ INFO: \u001b[0mVal 8\n",
      "\u001b[32m[2020-07-16 22:48:50] __main__ INFO: \u001b[0mEpoch 8 loss 0.7180 acc@1 0.7576 acc@5 0.9850\n",
      "\u001b[32m[2020-07-16 22:48:50] __main__ INFO: \u001b[0mElapsed 12.88\n",
      "\u001b[32m[2020-07-16 22:48:50] __main__ INFO: \u001b[0mTrain 9 2808\n",
      "\u001b[32m[2020-07-16 22:50:39] __main__ INFO: \u001b[0mEpoch 9 Step 100/351 lr 0.100000 loss 1.1355 (1.2842) acc@1 0.6406 (0.5236) acc@5 0.8750 (0.8370)\n",
      "\u001b[32m[2020-07-16 22:52:28] __main__ INFO: \u001b[0mEpoch 9 Step 200/351 lr 0.100000 loss 1.2315 (1.2790) acc@1 0.5391 (0.5261) acc@5 0.8281 (0.8373)\n",
      "\u001b[32m[2020-07-16 22:54:17] __main__ INFO: \u001b[0mEpoch 9 Step 300/351 lr 0.100000 loss 1.2606 (1.2775) acc@1 0.5625 (0.5279) acc@5 0.8359 (0.8387)\n",
      "\u001b[32m[2020-07-16 22:55:12] __main__ INFO: \u001b[0mEpoch 9 Step 351/351 lr 0.100000 loss 1.4822 (1.2788) acc@1 0.4531 (0.5280) acc@5 0.8672 (0.8391)\n",
      "\u001b[32m[2020-07-16 22:55:12] __main__ INFO: \u001b[0mElapsed 382.34\n",
      "\u001b[32m[2020-07-16 22:55:12] __main__ INFO: \u001b[0mVal 9\n",
      "\u001b[32m[2020-07-16 22:55:25] __main__ INFO: \u001b[0mEpoch 9 loss 0.6952 acc@1 0.7634 acc@5 0.9848\n",
      "\u001b[32m[2020-07-16 22:55:25] __main__ INFO: \u001b[0mElapsed 12.87\n",
      "\u001b[32m[2020-07-16 22:55:25] __main__ INFO: \u001b[0mTrain 10 3159\n",
      "\u001b[32m[2020-07-16 22:57:14] __main__ INFO: \u001b[0mEpoch 10 Step 100/351 lr 0.100000 loss 1.1033 (1.2598) acc@1 0.6172 (0.5374) acc@5 0.8906 (0.8424)\n",
      "\u001b[32m[2020-07-16 22:59:03] __main__ INFO: \u001b[0mEpoch 10 Step 200/351 lr 0.100000 loss 1.2349 (1.2613) acc@1 0.5234 (0.5364) acc@5 0.8516 (0.8428)\n",
      "\u001b[32m[2020-07-16 23:00:51] __main__ INFO: \u001b[0mEpoch 10 Step 300/351 lr 0.100000 loss 1.2882 (1.2643) acc@1 0.5469 (0.5358) acc@5 0.9141 (0.8434)\n",
      "\u001b[32m[2020-07-16 23:01:47] __main__ INFO: \u001b[0mEpoch 10 Step 351/351 lr 0.100000 loss 1.2790 (1.2669) acc@1 0.5234 (0.5353) acc@5 0.7500 (0.8427)\n",
      "\u001b[32m[2020-07-16 23:01:47] __main__ INFO: \u001b[0mElapsed 381.57\n",
      "\u001b[32m[2020-07-16 23:01:47] __main__ INFO: \u001b[0mVal 10\n",
      "\u001b[32m[2020-07-16 23:02:00] __main__ INFO: \u001b[0mEpoch 10 loss 0.9199 acc@1 0.7046 acc@5 0.9766\n",
      "\u001b[32m[2020-07-16 23:02:00] __main__ INFO: \u001b[0mElapsed 12.88\n",
      "\u001b[32m[2020-07-16 23:02:00] __main__ INFO: \u001b[0mTrain 11 3510\n",
      "\u001b[32m[2020-07-16 23:03:48] __main__ INFO: \u001b[0mEpoch 11 Step 100/351 lr 0.100000 loss 1.2586 (1.2385) acc@1 0.5391 (0.5429) acc@5 0.8672 (0.8470)\n",
      "\u001b[32m[2020-07-16 23:05:37] __main__ INFO: \u001b[0mEpoch 11 Step 200/351 lr 0.100000 loss 1.2667 (1.2476) acc@1 0.5078 (0.5404) acc@5 0.8438 (0.8425)\n",
      "\u001b[32m[2020-07-16 23:07:26] __main__ INFO: \u001b[0mEpoch 11 Step 300/351 lr 0.100000 loss 1.2170 (1.2433) acc@1 0.5547 (0.5408) acc@5 0.8359 (0.8434)\n",
      "\u001b[32m[2020-07-16 23:08:22] __main__ INFO: \u001b[0mEpoch 11 Step 351/351 lr 0.100000 loss 1.2019 (1.2458) acc@1 0.5781 (0.5402) acc@5 0.8203 (0.8426)\n",
      "\u001b[32m[2020-07-16 23:08:22] __main__ INFO: \u001b[0mElapsed 382.46\n",
      "\u001b[32m[2020-07-16 23:08:22] __main__ INFO: \u001b[0mVal 11\n",
      "\u001b[32m[2020-07-16 23:08:35] __main__ INFO: \u001b[0mEpoch 11 loss 0.7589 acc@1 0.7396 acc@5 0.9856\n",
      "\u001b[32m[2020-07-16 23:08:35] __main__ INFO: \u001b[0mElapsed 12.92\n",
      "\u001b[32m[2020-07-16 23:08:35] __main__ INFO: \u001b[0mTrain 12 3861\n",
      "\u001b[32m[2020-07-16 23:10:24] __main__ INFO: \u001b[0mEpoch 12 Step 100/351 lr 0.100000 loss 1.3492 (1.2277) acc@1 0.4453 (0.5455) acc@5 0.8359 (0.8457)\n",
      "\u001b[32m[2020-07-16 23:12:14] __main__ INFO: \u001b[0mEpoch 12 Step 200/351 lr 0.100000 loss 1.1839 (1.2413) acc@1 0.5469 (0.5418) acc@5 0.8984 (0.8454)\n",
      "\u001b[32m[2020-07-16 23:14:03] __main__ INFO: \u001b[0mEpoch 12 Step 300/351 lr 0.100000 loss 1.2018 (1.2410) acc@1 0.5625 (0.5434) acc@5 0.8594 (0.8460)\n",
      "\u001b[32m[2020-07-16 23:14:59] __main__ INFO: \u001b[0mEpoch 12 Step 351/351 lr 0.100000 loss 0.9899 (1.2373) acc@1 0.6484 (0.5453) acc@5 0.8906 (0.8454)\n",
      "\u001b[32m[2020-07-16 23:14:59] __main__ INFO: \u001b[0mElapsed 383.76\n",
      "\u001b[32m[2020-07-16 23:14:59] __main__ INFO: \u001b[0mVal 12\n",
      "\u001b[32m[2020-07-16 23:15:12] __main__ INFO: \u001b[0mEpoch 12 loss 0.7137 acc@1 0.7494 acc@5 0.9818\n",
      "\u001b[32m[2020-07-16 23:15:12] __main__ INFO: \u001b[0mElapsed 12.97\n",
      "\u001b[32m[2020-07-16 23:15:12] __main__ INFO: \u001b[0mTrain 13 4212\n",
      "\u001b[32m[2020-07-16 23:17:01] __main__ INFO: \u001b[0mEpoch 13 Step 100/351 lr 0.100000 loss 1.3532 (1.1998) acc@1 0.5078 (0.5575) acc@5 0.8359 (0.8488)\n",
      "\u001b[32m[2020-07-16 23:18:51] __main__ INFO: \u001b[0mEpoch 13 Step 200/351 lr 0.100000 loss 1.1252 (1.2208) acc@1 0.5703 (0.5500) acc@5 0.8125 (0.8467)\n",
      "\u001b[32m[2020-07-16 23:20:41] __main__ INFO: \u001b[0mEpoch 13 Step 300/351 lr 0.100000 loss 1.0546 (1.2208) acc@1 0.6172 (0.5505) acc@5 0.8828 (0.8460)\n",
      "\u001b[32m[2020-07-16 23:21:37] __main__ INFO: \u001b[0mEpoch 13 Step 351/351 lr 0.100000 loss 1.0762 (1.2207) acc@1 0.6250 (0.5507) acc@5 0.9219 (0.8454)\n",
      "\u001b[32m[2020-07-16 23:21:37] __main__ INFO: \u001b[0mElapsed 385.02\n",
      "\u001b[32m[2020-07-16 23:21:37] __main__ INFO: \u001b[0mVal 13\n",
      "\u001b[32m[2020-07-16 23:21:50] __main__ INFO: \u001b[0mEpoch 13 loss 0.6723 acc@1 0.7610 acc@5 0.9852\n",
      "\u001b[32m[2020-07-16 23:21:50] __main__ INFO: \u001b[0mElapsed 12.99\n",
      "\u001b[32m[2020-07-16 23:21:50] __main__ INFO: \u001b[0mTrain 14 4563\n",
      "\u001b[32m[2020-07-16 23:23:39] __main__ INFO: \u001b[0mEpoch 14 Step 100/351 lr 0.100000 loss 1.1980 (1.1944) acc@1 0.5234 (0.5584) acc@5 0.8828 (0.8498)\n",
      "\u001b[32m[2020-07-16 23:25:28] __main__ INFO: \u001b[0mEpoch 14 Step 200/351 lr 0.100000 loss 1.1818 (1.2045) acc@1 0.5547 (0.5544) acc@5 0.8672 (0.8478)\n",
      "\u001b[32m[2020-07-16 23:27:18] __main__ INFO: \u001b[0mEpoch 14 Step 300/351 lr 0.100000 loss 1.2210 (1.2124) acc@1 0.5625 (0.5504) acc@5 0.8672 (0.8463)\n",
      "\u001b[32m[2020-07-16 23:28:13] __main__ INFO: \u001b[0mEpoch 14 Step 351/351 lr 0.100000 loss 1.2824 (1.2148) acc@1 0.5469 (0.5493) acc@5 0.8438 (0.8462)\n",
      "\u001b[32m[2020-07-16 23:28:13] __main__ INFO: \u001b[0mElapsed 383.62\n",
      "\u001b[32m[2020-07-16 23:28:13] __main__ INFO: \u001b[0mVal 14\n",
      "\u001b[32m[2020-07-16 23:28:26] __main__ INFO: \u001b[0mEpoch 14 loss 0.6424 acc@1 0.7844 acc@5 0.9868\n",
      "\u001b[32m[2020-07-16 23:28:26] __main__ INFO: \u001b[0mElapsed 12.96\n",
      "\u001b[32m[2020-07-16 23:28:26] __main__ INFO: \u001b[0mTrain 15 4914\n",
      "\u001b[32m[2020-07-16 23:30:16] __main__ INFO: \u001b[0mEpoch 15 Step 100/351 lr 0.100000 loss 1.4241 (1.1969) acc@1 0.4609 (0.5617) acc@5 0.7891 (0.8441)\n",
      "\u001b[32m[2020-07-16 23:32:05] __main__ INFO: \u001b[0mEpoch 15 Step 200/351 lr 0.100000 loss 1.1774 (1.1974) acc@1 0.5312 (0.5573) acc@5 0.8438 (0.8462)\n",
      "\u001b[32m[2020-07-16 23:33:55] __main__ INFO: \u001b[0mEpoch 15 Step 300/351 lr 0.100000 loss 1.0451 (1.2001) acc@1 0.6172 (0.5574) acc@5 0.8359 (0.8446)\n",
      "\u001b[32m[2020-07-16 23:34:51] __main__ INFO: \u001b[0mEpoch 15 Step 351/351 lr 0.100000 loss 1.3427 (1.2004) acc@1 0.5078 (0.5571) acc@5 0.7969 (0.8453)\n",
      "\u001b[32m[2020-07-16 23:34:51] __main__ INFO: \u001b[0mElapsed 384.33\n",
      "\u001b[32m[2020-07-16 23:34:51] __main__ INFO: \u001b[0mVal 15\n",
      "\u001b[32m[2020-07-16 23:35:04] __main__ INFO: \u001b[0mEpoch 15 loss 0.8510 acc@1 0.7090 acc@5 0.9844\n",
      "\u001b[32m[2020-07-16 23:35:04] __main__ INFO: \u001b[0mElapsed 12.95\n",
      "\u001b[32m[2020-07-16 23:35:04] __main__ INFO: \u001b[0mTrain 16 5265\n",
      "\u001b[32m[2020-07-16 23:36:53] __main__ INFO: \u001b[0mEpoch 16 Step 100/351 lr 0.100000 loss 1.1784 (1.1872) acc@1 0.5625 (0.5584) acc@5 0.8203 (0.8420)\n",
      "\u001b[32m[2020-07-16 23:38:42] __main__ INFO: \u001b[0mEpoch 16 Step 200/351 lr 0.100000 loss 1.3269 (1.1956) acc@1 0.4609 (0.5566) acc@5 0.8125 (0.8439)\n",
      "\u001b[32m[2020-07-16 23:40:31] __main__ INFO: \u001b[0mEpoch 16 Step 300/351 lr 0.100000 loss 1.1195 (1.1951) acc@1 0.5859 (0.5568) acc@5 0.8438 (0.8443)\n",
      "\u001b[32m[2020-07-16 23:41:27] __main__ INFO: \u001b[0mEpoch 16 Step 351/351 lr 0.100000 loss 1.1341 (1.1941) acc@1 0.6016 (0.5585) acc@5 0.8906 (0.8452)\n",
      "\u001b[32m[2020-07-16 23:41:27] __main__ INFO: \u001b[0mElapsed 383.23\n",
      "\u001b[32m[2020-07-16 23:41:27] __main__ INFO: \u001b[0mVal 16\n",
      "\u001b[32m[2020-07-16 23:41:40] __main__ INFO: \u001b[0mEpoch 16 loss 0.7274 acc@1 0.7578 acc@5 0.9864\n",
      "\u001b[32m[2020-07-16 23:41:40] __main__ INFO: \u001b[0mElapsed 12.91\n",
      "\u001b[32m[2020-07-16 23:41:40] __main__ INFO: \u001b[0mTrain 17 5616\n",
      "\u001b[32m[2020-07-16 23:43:29] __main__ INFO: \u001b[0mEpoch 17 Step 100/351 lr 0.100000 loss 1.2332 (1.1878) acc@1 0.5703 (0.5616) acc@5 0.8359 (0.8435)\n",
      "\u001b[32m[2020-07-16 23:45:18] __main__ INFO: \u001b[0mEpoch 17 Step 200/351 lr 0.100000 loss 1.2654 (1.1829) acc@1 0.5469 (0.5648) acc@5 0.8516 (0.8480)\n",
      "\u001b[32m[2020-07-16 23:47:06] __main__ INFO: \u001b[0mEpoch 17 Step 300/351 lr 0.100000 loss 1.0519 (1.1893) acc@1 0.6328 (0.5612) acc@5 0.8672 (0.8462)\n",
      "\u001b[32m[2020-07-16 23:48:02] __main__ INFO: \u001b[0mEpoch 17 Step 351/351 lr 0.100000 loss 1.2886 (1.1880) acc@1 0.5625 (0.5613) acc@5 0.8750 (0.8464)\n",
      "\u001b[32m[2020-07-16 23:48:02] __main__ INFO: \u001b[0mElapsed 382.01\n",
      "\u001b[32m[2020-07-16 23:48:02] __main__ INFO: \u001b[0mVal 17\n",
      "\u001b[32m[2020-07-16 23:48:15] __main__ INFO: \u001b[0mEpoch 17 loss 0.9467 acc@1 0.7012 acc@5 0.9658\n",
      "\u001b[32m[2020-07-16 23:48:15] __main__ INFO: \u001b[0mElapsed 12.83\n",
      "\u001b[32m[2020-07-16 23:48:15] __main__ INFO: \u001b[0mTrain 18 5967\n",
      "\u001b[32m[2020-07-16 23:50:03] __main__ INFO: \u001b[0mEpoch 18 Step 100/351 lr 0.100000 loss 1.2351 (1.1771) acc@1 0.5156 (0.5635) acc@5 0.8672 (0.8460)\n",
      "\u001b[32m[2020-07-16 23:51:51] __main__ INFO: \u001b[0mEpoch 18 Step 200/351 lr 0.100000 loss 1.1221 (1.1837) acc@1 0.6094 (0.5609) acc@5 0.8594 (0.8455)\n",
      "\u001b[32m[2020-07-16 23:53:40] __main__ INFO: \u001b[0mEpoch 18 Step 300/351 lr 0.100000 loss 1.2284 (1.1825) acc@1 0.5859 (0.5624) acc@5 0.8672 (0.8467)\n",
      "\u001b[32m[2020-07-16 23:54:35] __main__ INFO: \u001b[0mEpoch 18 Step 351/351 lr 0.100000 loss 1.1188 (1.1814) acc@1 0.6016 (0.5631) acc@5 0.8750 (0.8468)\n",
      "\u001b[32m[2020-07-16 23:54:35] __main__ INFO: \u001b[0mElapsed 380.70\n",
      "\u001b[32m[2020-07-16 23:54:35] __main__ INFO: \u001b[0mVal 18\n",
      "\u001b[32m[2020-07-16 23:54:48] __main__ INFO: \u001b[0mEpoch 18 loss 0.7358 acc@1 0.7554 acc@5 0.9822\n",
      "\u001b[32m[2020-07-16 23:54:48] __main__ INFO: \u001b[0mElapsed 12.82\n",
      "\u001b[32m[2020-07-16 23:54:48] __main__ INFO: \u001b[0mTrain 19 6318\n",
      "\u001b[32m[2020-07-16 23:56:37] __main__ INFO: \u001b[0mEpoch 19 Step 100/351 lr 0.100000 loss 1.3188 (1.1611) acc@1 0.5078 (0.5702) acc@5 0.8359 (0.8485)\n",
      "\u001b[32m[2020-07-16 23:58:26] __main__ INFO: \u001b[0mEpoch 19 Step 200/351 lr 0.100000 loss 1.2598 (1.1709) acc@1 0.5234 (0.5660) acc@5 0.8438 (0.8489)\n",
      "\u001b[32m[2020-07-17 00:00:14] __main__ INFO: \u001b[0mEpoch 19 Step 300/351 lr 0.100000 loss 1.2020 (1.1691) acc@1 0.5469 (0.5675) acc@5 0.8203 (0.8499)\n",
      "\u001b[32m[2020-07-17 00:01:10] __main__ INFO: \u001b[0mEpoch 19 Step 351/351 lr 0.100000 loss 1.1807 (1.1673) acc@1 0.5703 (0.5680) acc@5 0.8438 (0.8493)\n",
      "\u001b[32m[2020-07-17 00:01:10] __main__ INFO: \u001b[0mElapsed 381.71\n",
      "\u001b[32m[2020-07-17 00:01:10] __main__ INFO: \u001b[0mVal 19\n",
      "\u001b[32m[2020-07-17 00:01:23] __main__ INFO: \u001b[0mEpoch 19 loss 0.5576 acc@1 0.8108 acc@5 0.9882\n",
      "\u001b[32m[2020-07-17 00:01:23] __main__ INFO: \u001b[0mElapsed 12.85\n",
      "\u001b[32m[2020-07-17 00:01:23] __main__ INFO: \u001b[0mTrain 20 6669\n",
      "\u001b[32m[2020-07-17 00:03:11] __main__ INFO: \u001b[0mEpoch 20 Step 100/351 lr 0.100000 loss 1.1992 (1.1449) acc@1 0.5234 (0.5780) acc@5 0.8125 (0.8495)\n"
     ]
    }
   ],
   "source": [
    "# Train the model per the settings specified in the original paper\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "!python train.py --config configs/cifar/wrn.yaml \\\n",
    "    model.wrn.depth 28 \\\n",
    "    model.wrn.widening_factor 10 \\\n",
    "    train.batch_size 128 \\\n",
    "    train.base_lr 0.1 \\\n",
    "    dataset.name CIFAR10_RA_2_5 \\\n",
    "    train.output_dir /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_5_c10val/exp00 \\\n",
    "    scheduler.epochs 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Refine the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-18 22:09:44] __main__ INFO: \u001b[0mdevice: cuda\n",
      "cudnn:\n",
      "  benchmark: True\n",
      "  deterministic: False\n",
      "dataset:\n",
      "  name: CIFAR10\n",
      "  dataset_dir: ~/.torch/datasets/CIFAR10\n",
      "  image_size: 32\n",
      "  n_channels: 3\n",
      "  n_classes: 10\n",
      "model:\n",
      "  type: cifar\n",
      "  name: wrn\n",
      "  init_mode: kaiming_fan_in\n",
      "  vgg:\n",
      "    n_channels: [64, 128, 256, 512, 512]\n",
      "    n_layers: [2, 2, 3, 3, 3]\n",
      "    use_bn: True\n",
      "  resnet:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "  resnet_preact:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "  wrn:\n",
      "    depth: 28\n",
      "    initial_channels: 16\n",
      "    widening_factor: 10\n",
      "    drop_rate: 0.0\n",
      "  densenet:\n",
      "    depth: 100\n",
      "    n_blocks: [6, 12, 24, 16]\n",
      "    block_type: bottleneck\n",
      "    growth_rate: 12\n",
      "    drop_rate: 0.0\n",
      "    compression_rate: 0.5\n",
      "  pyramidnet:\n",
      "    depth: 272\n",
      "    n_blocks: [3, 24, 36, 3]\n",
      "    initial_channels: 16\n",
      "    block_type: bottleneck\n",
      "    alpha: 200\n",
      "  resnext:\n",
      "    depth: 29\n",
      "    n_blocks: [3, 4, 6, 3]\n",
      "    initial_channels: 64\n",
      "    cardinality: 8\n",
      "    base_channels: 4\n",
      "  shake_shake:\n",
      "    depth: 26\n",
      "    initial_channels: 96\n",
      "    shake_forward: True\n",
      "    shake_backward: True\n",
      "    shake_image: True\n",
      "  se_resnet_preact:\n",
      "    depth: 110\n",
      "    initial_channels: 16\n",
      "    se_reduction: 16\n",
      "    block_type: basic\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "train:\n",
      "  checkpoint: /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_5_c10val/exp00/checkpoint_00400.pth\n",
      "  resume: False\n",
      "  precision: O0\n",
      "  batch_size: 128\n",
      "  subdivision: 1\n",
      "  optimizer: sgd\n",
      "  base_lr: 0.001\n",
      "  momentum: 0.9\n",
      "  nesterov: True\n",
      "  weight_decay: 0.0005\n",
      "  no_weight_decay_on_bn: False\n",
      "  gradient_clip: 0\n",
      "  start_epoch: 0\n",
      "  seed: 0\n",
      "  val_first: True\n",
      "  val_period: 1\n",
      "  val_ratio: 0.1\n",
      "  use_test_as_val: False\n",
      "  output_dir: /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_5_c10val/exp00_resume400_50\n",
      "  log_period: 100\n",
      "  checkpoint_period: 100\n",
      "  use_tensorboard: True\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: True\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "  distributed: False\n",
      "  dist:\n",
      "    backend: nccl\n",
      "    init_method: env://\n",
      "    world_size: -1\n",
      "    node_rank: -1\n",
      "    local_rank: 0\n",
      "    use_sync_bn: False\n",
      "tensorboard:\n",
      "  train_images: False\n",
      "  val_images: False\n",
      "  model_params: False\n",
      "optim:\n",
      "  adam:\n",
      "    betas: (0.9, 0.999)\n",
      "  lars:\n",
      "    eps: 1e-09\n",
      "    threshold: 0.01\n",
      "  adabound:\n",
      "    betas: (0.9, 0.999)\n",
      "    final_lr: 0.1\n",
      "    gamma: 0.001\n",
      "scheduler:\n",
      "  epochs: 50\n",
      "  warmup:\n",
      "    type: none\n",
      "    epochs: 0\n",
      "    start_factor: 0.001\n",
      "    exponent: 4\n",
      "  type: multistep\n",
      "  milestones: [60, 120, 160]\n",
      "  lr_decay: 0.2\n",
      "  lr_min_factor: 0.001\n",
      "  T0: 10\n",
      "  T_mul: 1.0\n",
      "validation:\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: False\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "augmentation:\n",
      "  use_random_crop: True\n",
      "  use_random_horizontal_flip: True\n",
      "  use_cutout: False\n",
      "  use_random_erasing: False\n",
      "  use_dual_cutout: False\n",
      "  use_mixup: False\n",
      "  use_ricap: False\n",
      "  use_cutmix: False\n",
      "  use_label_smoothing: False\n",
      "  random_crop:\n",
      "    padding: 4\n",
      "    fill: 0\n",
      "    padding_mode: constant\n",
      "  random_horizontal_flip:\n",
      "    prob: 0.5\n",
      "  cutout:\n",
      "    prob: 1.0\n",
      "    mask_size: 16\n",
      "    cut_inside: False\n",
      "    mask_color: 0\n",
      "    dual_cutout_alpha: 0.1\n",
      "  random_erasing:\n",
      "    prob: 0.5\n",
      "    area_ratio_range: [0.02, 0.4]\n",
      "    min_aspect_ratio: 0.3\n",
      "    max_attempt: 20\n",
      "  mixup:\n",
      "    alpha: 1.0\n",
      "  ricap:\n",
      "    beta: 0.3\n",
      "  cutmix:\n",
      "    alpha: 1.0\n",
      "  label_smoothing:\n",
      "    epsilon: 0.1\n",
      "tta:\n",
      "  use_resize: False\n",
      "  use_center_crop: False\n",
      "  resize: 256\n",
      "test:\n",
      "  checkpoint: ''\n",
      "  output_dir: ''\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    pin_memory: False\n",
      "\u001b[32m[2020-07-18 22:09:44] __main__ INFO: \u001b[0menv_info:\n",
      "  pytorch_version: 1.4.0\n",
      "  cuda_version: 10.1\n",
      "  cudnn_version: 7603\n",
      "  num_gpus: 1\n",
      "  gpu_name: Tesla K80\n",
      "  gpu_capability: 3.7\n",
      "Files already downloaded and verified\n",
      "\u001b[32m[2020-07-18 22:09:48] __main__ INFO: \u001b[0mMACs  : 5.25G\n",
      "\u001b[32m[2020-07-18 22:09:48] __main__ INFO: \u001b[0m#params: 36.48M\n",
      "Selected optimization level O0:  Pure FP32 training.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\",)\n",
      "\u001b[32m[2020-07-18 22:09:48] __main__ INFO: \u001b[0mVal 0\n",
      "\u001b[32m[2020-07-18 22:10:07] __main__ INFO: \u001b[0mEpoch 0 loss 0.4238 acc@1 0.9060 acc@5 0.9934\n",
      "\u001b[32m[2020-07-18 22:10:07] __main__ INFO: \u001b[0mElapsed 19.13\n",
      "\u001b[32m[2020-07-18 22:10:07] __main__ INFO: \u001b[0mTrain 1 0\n",
      "\u001b[32m[2020-07-18 22:12:02] __main__ INFO: \u001b[0mEpoch 1 Step 100/351 lr 0.001000 loss 0.0505 (0.1458) acc@1 0.9844 (0.9580) acc@5 1.0000 (0.9986)\n",
      "\u001b[32m[2020-07-18 22:13:52] __main__ INFO: \u001b[0mEpoch 1 Step 200/351 lr 0.001000 loss 0.1410 (0.1391) acc@1 0.9531 (0.9599) acc@5 1.0000 (0.9985)\n",
      "\u001b[32m[2020-07-18 22:15:42] __main__ INFO: \u001b[0mEpoch 1 Step 300/351 lr 0.001000 loss 0.0589 (0.1363) acc@1 0.9844 (0.9599) acc@5 1.0000 (0.9986)\n",
      "\u001b[32m[2020-07-18 22:16:38] __main__ INFO: \u001b[0mEpoch 1 Step 351/351 lr 0.001000 loss 0.0826 (0.1350) acc@1 0.9609 (0.9604) acc@5 1.0000 (0.9987)\n",
      "\u001b[32m[2020-07-18 22:16:38] __main__ INFO: \u001b[0mElapsed 390.74\n",
      "\u001b[32m[2020-07-18 22:16:38] __main__ INFO: \u001b[0mVal 1\n",
      "\u001b[32m[2020-07-18 22:16:51] __main__ INFO: \u001b[0mEpoch 1 loss 0.2388 acc@1 0.9300 acc@5 0.9970\n",
      "\u001b[32m[2020-07-18 22:16:51] __main__ INFO: \u001b[0mElapsed 12.96\n",
      "\u001b[32m[2020-07-18 22:16:51] __main__ INFO: \u001b[0mTrain 2 351\n",
      "\u001b[32m[2020-07-18 22:18:41] __main__ INFO: \u001b[0mEpoch 2 Step 100/351 lr 0.001000 loss 0.0858 (0.0897) acc@1 0.9688 (0.9741) acc@5 1.0000 (0.9993)\n",
      "\u001b[32m[2020-07-18 22:20:31] __main__ INFO: \u001b[0mEpoch 2 Step 200/351 lr 0.001000 loss 0.0605 (0.0890) acc@1 0.9844 (0.9739) acc@5 1.0000 (0.9994)\n",
      "\u001b[32m[2020-07-18 22:22:21] __main__ INFO: \u001b[0mEpoch 2 Step 300/351 lr 0.001000 loss 0.1141 (0.0900) acc@1 0.9766 (0.9734) acc@5 1.0000 (0.9994)\n",
      "\u001b[32m[2020-07-18 22:23:17] __main__ INFO: \u001b[0mEpoch 2 Step 351/351 lr 0.001000 loss 0.0960 (0.0911) acc@1 0.9766 (0.9731) acc@5 1.0000 (0.9994)\n",
      "\u001b[32m[2020-07-18 22:23:17] __main__ INFO: \u001b[0mElapsed 386.39\n",
      "\u001b[32m[2020-07-18 22:23:17] __main__ INFO: \u001b[0mVal 2\n",
      "\u001b[32m[2020-07-18 22:23:30] __main__ INFO: \u001b[0mEpoch 2 loss 0.2220 acc@1 0.9344 acc@5 0.9976\n",
      "\u001b[32m[2020-07-18 22:23:30] __main__ INFO: \u001b[0mElapsed 12.93\n",
      "\u001b[32m[2020-07-18 22:23:30] __main__ INFO: \u001b[0mTrain 3 702\n",
      "\u001b[32m[2020-07-18 22:25:20] __main__ INFO: \u001b[0mEpoch 3 Step 100/351 lr 0.001000 loss 0.0253 (0.0610) acc@1 1.0000 (0.9827) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-18 22:27:11] __main__ INFO: \u001b[0mEpoch 3 Step 200/351 lr 0.001000 loss 0.0438 (0.0660) acc@1 0.9844 (0.9809) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-07-18 22:29:01] __main__ INFO: \u001b[0mEpoch 3 Step 300/351 lr 0.001000 loss 0.1418 (0.0645) acc@1 0.9531 (0.9816) acc@5 0.9922 (0.9997)\n",
      "\u001b[32m[2020-07-18 22:29:57] __main__ INFO: \u001b[0mEpoch 3 Step 351/351 lr 0.001000 loss 0.0999 (0.0649) acc@1 0.9688 (0.9816) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-07-18 22:29:57] __main__ INFO: \u001b[0mElapsed 386.59\n",
      "\u001b[32m[2020-07-18 22:29:57] __main__ INFO: \u001b[0mVal 3\n",
      "\u001b[32m[2020-07-18 22:30:10] __main__ INFO: \u001b[0mEpoch 3 loss 0.2234 acc@1 0.9374 acc@5 0.9976\n",
      "\u001b[32m[2020-07-18 22:30:10] __main__ INFO: \u001b[0mElapsed 12.98\n",
      "\u001b[32m[2020-07-18 22:30:10] __main__ INFO: \u001b[0mTrain 4 1053\n",
      "\u001b[32m[2020-07-18 22:32:00] __main__ INFO: \u001b[0mEpoch 4 Step 100/351 lr 0.001000 loss 0.0355 (0.0465) acc@1 0.9844 (0.9873) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-18 22:33:50] __main__ INFO: \u001b[0mEpoch 4 Step 200/351 lr 0.001000 loss 0.0877 (0.0503) acc@1 0.9844 (0.9862) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-07-18 22:35:40] __main__ INFO: \u001b[0mEpoch 4 Step 300/351 lr 0.001000 loss 0.0504 (0.0507) acc@1 0.9844 (0.9860) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-07-18 22:36:36] __main__ INFO: \u001b[0mEpoch 4 Step 351/351 lr 0.001000 loss 0.1078 (0.0523) acc@1 0.9688 (0.9855) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-07-18 22:36:36] __main__ INFO: \u001b[0mElapsed 386.33\n",
      "\u001b[32m[2020-07-18 22:36:36] __main__ INFO: \u001b[0mVal 4\n",
      "\u001b[32m[2020-07-18 22:36:49] __main__ INFO: \u001b[0mEpoch 4 loss 0.2267 acc@1 0.9338 acc@5 0.9978\n",
      "\u001b[32m[2020-07-18 22:36:49] __main__ INFO: \u001b[0mElapsed 12.92\n",
      "\u001b[32m[2020-07-18 22:36:49] __main__ INFO: \u001b[0mTrain 5 1404\n",
      "\u001b[32m[2020-07-18 22:38:39] __main__ INFO: \u001b[0mEpoch 5 Step 100/351 lr 0.001000 loss 0.0151 (0.0414) acc@1 1.0000 (0.9890) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-18 22:40:30] __main__ INFO: \u001b[0mEpoch 5 Step 200/351 lr 0.001000 loss 0.0524 (0.0404) acc@1 0.9766 (0.9891) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-18 22:42:20] __main__ INFO: \u001b[0mEpoch 5 Step 300/351 lr 0.001000 loss 0.0506 (0.0416) acc@1 0.9922 (0.9885) acc@5 0.9922 (0.9998)\n",
      "\u001b[32m[2020-07-18 22:43:16] __main__ INFO: \u001b[0mEpoch 5 Step 351/351 lr 0.001000 loss 0.0511 (0.0416) acc@1 0.9844 (0.9885) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-18 22:43:16] __main__ INFO: \u001b[0mElapsed 386.77\n",
      "\u001b[32m[2020-07-18 22:43:16] __main__ INFO: \u001b[0mVal 5\n",
      "\u001b[32m[2020-07-18 22:43:29] __main__ INFO: \u001b[0mEpoch 5 loss 0.2332 acc@1 0.9376 acc@5 0.9974\n",
      "\u001b[32m[2020-07-18 22:43:29] __main__ INFO: \u001b[0mElapsed 12.98\n",
      "\u001b[32m[2020-07-18 22:43:29] __main__ INFO: \u001b[0mTrain 6 1755\n",
      "\u001b[32m[2020-07-18 22:45:19] __main__ INFO: \u001b[0mEpoch 6 Step 100/351 lr 0.001000 loss 0.0183 (0.0293) acc@1 1.0000 (0.9930) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-18 22:47:09] __main__ INFO: \u001b[0mEpoch 6 Step 200/351 lr 0.001000 loss 0.0229 (0.0320) acc@1 0.9922 (0.9918) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-18 22:48:59] __main__ INFO: \u001b[0mEpoch 6 Step 300/351 lr 0.001000 loss 0.0184 (0.0318) acc@1 0.9922 (0.9920) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-18 22:49:56] __main__ INFO: \u001b[0mEpoch 6 Step 351/351 lr 0.001000 loss 0.0085 (0.0325) acc@1 1.0000 (0.9918) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-18 22:49:56] __main__ INFO: \u001b[0mElapsed 386.71\n",
      "\u001b[32m[2020-07-18 22:49:56] __main__ INFO: \u001b[0mVal 6\n",
      "\u001b[32m[2020-07-18 22:50:09] __main__ INFO: \u001b[0mEpoch 6 loss 0.2394 acc@1 0.9394 acc@5 0.9974\n",
      "\u001b[32m[2020-07-18 22:50:09] __main__ INFO: \u001b[0mElapsed 13.01\n",
      "\u001b[32m[2020-07-18 22:50:09] __main__ INFO: \u001b[0mTrain 7 2106\n",
      "\u001b[32m[2020-07-18 22:51:59] __main__ INFO: \u001b[0mEpoch 7 Step 100/351 lr 0.001000 loss 0.0216 (0.0261) acc@1 0.9922 (0.9943) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-18 22:53:49] __main__ INFO: \u001b[0mEpoch 7 Step 200/351 lr 0.001000 loss 0.0281 (0.0281) acc@1 0.9922 (0.9932) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-18 22:55:39] __main__ INFO: \u001b[0mEpoch 7 Step 300/351 lr 0.001000 loss 0.0167 (0.0271) acc@1 1.0000 (0.9933) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-18 22:56:35] __main__ INFO: \u001b[0mEpoch 7 Step 351/351 lr 0.001000 loss 0.0280 (0.0279) acc@1 0.9922 (0.9931) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-18 22:56:35] __main__ INFO: \u001b[0mElapsed 386.31\n",
      "\u001b[32m[2020-07-18 22:56:35] __main__ INFO: \u001b[0mVal 7\n",
      "\u001b[32m[2020-07-18 22:56:48] __main__ INFO: \u001b[0mEpoch 7 loss 0.2434 acc@1 0.9374 acc@5 0.9978\n",
      "\u001b[32m[2020-07-18 22:56:48] __main__ INFO: \u001b[0mElapsed 12.99\n",
      "\u001b[32m[2020-07-18 22:56:48] __main__ INFO: \u001b[0mTrain 8 2457\n",
      "\u001b[32m[2020-07-18 22:58:38] __main__ INFO: \u001b[0mEpoch 8 Step 100/351 lr 0.001000 loss 0.0067 (0.0227) acc@1 1.0000 (0.9940) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-18 23:00:28] __main__ INFO: \u001b[0mEpoch 8 Step 200/351 lr 0.001000 loss 0.0314 (0.0242) acc@1 0.9922 (0.9938) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-18 23:38:33] __main__ INFO: \u001b[0mEpoch 14 Step 100/351 lr 0.001000 loss 0.0088 (0.0121) acc@1 1.0000 (0.9972) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-19 01:20:03] __main__ INFO: \u001b[0mEpoch 29 Step 200/351 lr 0.001000 loss 0.0099 (0.0059) acc@1 1.0000 (0.9990) acc@5 1.0000 (1.0000)\n"
     ]
    }
   ],
   "source": [
    "# Resume training with the un-augmented data\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "!python train.py --config /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_5_c10val/exp00/config.yaml \\\n",
    "    train.checkpoint /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_5_c10val/exp00/checkpoint_00400.pth \\\n",
    "    dataset.name CIFAR10 \\\n",
    "    train.base_lr .001 \\\n",
    "    train.output_dir /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_5_c10val/exp00_resume400_50 \\\n",
    "    scheduler.epochs 50\n",
    "\n",
    "#### Set LEARNING RATE based on ending LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-19 10:55:43] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_5_c10val/exp00/checkpoint_00400.pth\n",
      "Files already downloaded and verified\n",
      "100%|| 79/79 [00:25<00:00,  3.04it/s]\n",
      "\u001b[32m[2020-07-19 10:56:10] __main__ INFO: \u001b[0mElapsed 25.96\n",
      "\u001b[32m[2020-07-19 10:56:10] __main__ INFO: \u001b[0mLoss 0.3968 Accuracy 0.9109\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10 test dataset - without training on unaugmented data\n",
    "# Right the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/wrn.yaml \\\n",
    "   test.batch_size 128 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_5_c10val/exp00/checkpoint_00400.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_5_c10val/exp00/test_results_0400_cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-19 10:56:23] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_5_c10val/exp00/checkpoint_00400.pth\n",
      "CIFAR 10.1\n",
      "100%|| 16/16 [00:05<00:00,  2.86it/s]\n",
      "\u001b[32m[2020-07-19 10:56:29] __main__ INFO: \u001b[0mElapsed 5.60\n",
      "\u001b[32m[2020-07-19 10:56:29] __main__ INFO: \u001b[0mLoss 0.8234 Accuracy 0.8180\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10 test dataset - without training on unaugmented data\n",
    "# Write the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/wrn.yaml \\\n",
    "   test.batch_size 128 \\\n",
    "   dataset.name CIFAR101 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_5_c10val/exp00/checkpoint_00400.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_5_c10val/exp00/test_results_0400_cifar101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-19 10:56:43] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_5_c10val/exp00_resume400_50/checkpoint_00050.pth\n",
      "Files already downloaded and verified\n",
      "100%|| 79/79 [00:26<00:00,  3.03it/s]\n",
      "\u001b[32m[2020-07-19 10:57:11] __main__ INFO: \u001b[0mElapsed 26.11\n",
      "\u001b[32m[2020-07-19 10:57:11] __main__ INFO: \u001b[0mLoss 0.2752 Accuracy 0.9376\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10 test dataset \n",
    "# Write the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/wrn.yaml \\\n",
    "   test.batch_size 128 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_5_c10val/exp00_resume400_50/checkpoint_00050.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_5_c10val/exp00_resume400_50/test_results_0050_cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-19 10:57:27] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_5_c10val/exp00_resume400_50/checkpoint_00050.pth\n",
      "CIFAR 10.1\n",
      "100%|| 16/16 [00:05<00:00,  2.83it/s]\n",
      "\u001b[32m[2020-07-19 10:57:34] __main__ INFO: \u001b[0mElapsed 5.65\n",
      "\u001b[32m[2020-07-19 10:57:34] __main__ INFO: \u001b[0mLoss 0.6268 Accuracy 0.8600\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10.1 test dataset \n",
    "# Write the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/wrn.yaml \\\n",
    "   test.batch_size 128 \\\n",
    "   dataset.name CIFAR101 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_5_c10val/exp00_resume400_50/checkpoint_00050.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_5_c10val/exp00_resume400_50/test_results_0050_cifar101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Record the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Testset</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Original_Accuracy</th>\n",
       "      <th>Original_CI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wrn_28_10_ra_2_5_c10val</td>\n",
       "      <td>400</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.3968</td>\n",
       "      <td>0.9109</td>\n",
       "      <td>92.5</td>\n",
       "      <td>(92.0, 93.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wrn_28_10_ra_2_5_c10val</td>\n",
       "      <td>400</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>0.8234</td>\n",
       "      <td>0.818</td>\n",
       "      <td>84.9</td>\n",
       "      <td>(83.2, 86.4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wrn_28_10_ra_2_5_c10val_refined400</td>\n",
       "      <td>50</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>0.6268</td>\n",
       "      <td>0.86</td>\n",
       "      <td>84.9</td>\n",
       "      <td>(83.2, 86.4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wrn_28_10_ra_2_5_c10val_refined400</td>\n",
       "      <td>50</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.2752</td>\n",
       "      <td>0.9376</td>\n",
       "      <td>92.5</td>\n",
       "      <td>(92.0, 93.0)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Model Epoch    Testset    Loss Accuracy  \\\n",
       "0             wrn_28_10_ra_2_5_c10val   400    cifar10  0.3968   0.9109   \n",
       "1             wrn_28_10_ra_2_5_c10val   400  cifar10.1  0.8234    0.818   \n",
       "2  wrn_28_10_ra_2_5_c10val_refined400    50  cifar10.1  0.6268     0.86   \n",
       "3  wrn_28_10_ra_2_5_c10val_refined400    50    cifar10  0.2752   0.9376   \n",
       "\n",
       "   Original_Accuracy   Original_CI  \n",
       "0               92.5  (92.0, 93.0)  \n",
       "1               84.9  (83.2, 86.4)  \n",
       "2               84.9  (83.2, 86.4)  \n",
       "3               92.5  (92.0, 93.0)  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "model = 'wrn_28_10_ra_2_5_c10val'\n",
    "model_refined = model + '_refined400'\n",
    "\n",
    "a = pd.Series([model, 400, 'cifar10', 0.3968, 0.9109])\n",
    "c = pd.Series([model, 400, 'cifar10.1', 0.8234, 0.8180])\n",
    "\n",
    "e = pd.Series([model_refined, 50, 'cifar10.1', 0.6268, 0.8600])\n",
    "f = pd.Series([model_refined, 50, 'cifar10', 0.2752, 0.9376])\n",
    "               \n",
    "df_results = pd.concat([a,c,e,f], axis=1).T\n",
    "df_results.columns = ['Model', 'Epoch', 'Testset', 'Loss', 'Accuracy']\n",
    "\n",
    "df_results['Original_Accuracy'] = df_results.apply((lambda row: 92.5 if row[2] == 'cifar10' else 84.9), axis=1)\n",
    "df_results['Original_CI'] = df_results.apply((lambda row: (92.0, 93.0) if row[2] == 'cifar10' else (83.2, 86.4)), axis=1)\n",
    "\n",
    "df_results.to_csv('/home/ec2-user/SageMaker/experiments/' + model + '/results.csv')\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the model checkpoints, configs, and results to S3 \n",
    "bucket='sagemaker-may29'\n",
    "prefix = 'sagemaker/results/original-models/wrn_28_10_ra_2_5_c10val'\n",
    "path = '/home/ec2-user/SageMaker/experiments/wrn_28_10_ra_2_5_c10val'\n",
    "\n",
    "s3_resource = boto3.resource(\"s3\", region_name=\"us-east-2\")\n",
    "\n",
    "def uploadDirectory(local_path,bucket_name,s3_prefix):\n",
    "\n",
    "    my_bucket = s3_resource.Bucket(bucket_name)\n",
    "    \n",
    "    for path, subdirs, files in os.walk(local_path):\n",
    "        path = path.replace(\"\\\\\",\"/\")\n",
    "        directory_name = path.replace(local_path,\"\")\n",
    "        for file in files:\n",
    "            #print(\"Local File:\", os.path.join(path, file))\n",
    "            #print(\"      Dest:\", s3_prefix+directory_name+'/'+file)\n",
    "            my_bucket.upload_file(os.path.join(path, file), s3_prefix+directory_name+'/'+file)\n",
    "    \n",
    "uploadDirectory(path,bucket,prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
