{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import numpy \n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESNEXT_29_8x64D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without Containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 1)) (1.18.1)\n",
      "Requirement already satisfied: torch>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 2)) (1.4.0)\n",
      "Requirement already satisfied: torchvision in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (0.5.0)\n",
      "Requirement already satisfied: fvcore in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (0.1.1.post20200608)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 5)) (4.44.1)\n",
      "Requirement already satisfied: yacs in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 6)) (0.1.7)\n",
      "Requirement already satisfied: apex from git+https://github.com/NVIDIA/apex.git#egg=apex in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 7)) (0.1)\n",
      "Requirement already satisfied: termcolor in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 8)) (1.1.0)\n",
      "Requirement already satisfied: thop<0.0.31.post2004070130 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 9)) (0.0.31.post2001170342)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (1.14.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (7.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (5.3.1)\n",
      "Requirement already satisfied: portalocker in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (1.7.0)\n",
      "Requirement already satisfied: tabulate in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (0.8.7)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Sagemaker Notebook must be of type, conda_pytorch_p36\n",
    "\n",
    "!pip install -r '/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorboard in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (2.2.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (46.1.3.post20200330)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.18.1)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (3.12.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (2.23.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (0.9.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.29.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (0.4.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (3.2.2)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.14.0)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (0.34.2)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.16.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.6.0.post3)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.0.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (2020.4.5.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (1.25.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard) (1.5.0)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (3.4.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (4.1.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (0.2.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard) (2.2.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard) (0.4.8)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Need to add this to requirements.txt\n",
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-11 02:12:08] __main__ INFO: \u001b[0mdevice: cuda\n",
      "cudnn:\n",
      "  benchmark: True\n",
      "  deterministic: False\n",
      "dataset:\n",
      "  name: CIFAR10\n",
      "  dataset_dir: ~/.torch/datasets/CIFAR10\n",
      "  image_size: 32\n",
      "  n_channels: 3\n",
      "  n_classes: 10\n",
      "model:\n",
      "  type: cifar\n",
      "  name: resnext\n",
      "  init_mode: kaiming_fan_out\n",
      "  vgg:\n",
      "    n_channels: [64, 128, 256, 512, 512]\n",
      "    n_layers: [2, 2, 3, 3, 3]\n",
      "    use_bn: True\n",
      "  resnet:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "  resnet_preact:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "  wrn:\n",
      "    depth: 28\n",
      "    initial_channels: 16\n",
      "    widening_factor: 10\n",
      "    drop_rate: 0.0\n",
      "  densenet:\n",
      "    depth: 100\n",
      "    n_blocks: [6, 12, 24, 16]\n",
      "    block_type: bottleneck\n",
      "    growth_rate: 12\n",
      "    drop_rate: 0.0\n",
      "    compression_rate: 0.5\n",
      "  pyramidnet:\n",
      "    depth: 272\n",
      "    n_blocks: [3, 24, 36, 3]\n",
      "    initial_channels: 16\n",
      "    block_type: bottleneck\n",
      "    alpha: 200\n",
      "  resnext:\n",
      "    depth: 29\n",
      "    n_blocks: [3, 4, 6, 3]\n",
      "    initial_channels: 64\n",
      "    cardinality: 8\n",
      "    base_channels: 64\n",
      "  shake_shake:\n",
      "    depth: 26\n",
      "    initial_channels: 96\n",
      "    shake_forward: True\n",
      "    shake_backward: True\n",
      "    shake_image: True\n",
      "  se_resnet_preact:\n",
      "    depth: 110\n",
      "    initial_channels: 16\n",
      "    se_reduction: 16\n",
      "    block_type: basic\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "train:\n",
      "  checkpoint: ''\n",
      "  resume: False\n",
      "  precision: O0\n",
      "  batch_size: 128\n",
      "  subdivision: 1\n",
      "  optimizer: sgd\n",
      "  base_lr: 0.1\n",
      "  momentum: 0.9\n",
      "  nesterov: True\n",
      "  weight_decay: 0.0005\n",
      "  no_weight_decay_on_bn: False\n",
      "  gradient_clip: 0\n",
      "  start_epoch: 0\n",
      "  seed: 0\n",
      "  val_first: True\n",
      "  val_period: 1\n",
      "  val_ratio: 0.1\n",
      "  use_test_as_val: False\n",
      "  output_dir: /home/ec2-user/SageMaker/experiments/resnext_29_8x64d/exp00\n",
      "  log_period: 100\n",
      "  checkpoint_period: 100\n",
      "  use_tensorboard: True\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: True\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "  distributed: False\n",
      "  dist:\n",
      "    backend: nccl\n",
      "    init_method: env://\n",
      "    world_size: -1\n",
      "    node_rank: -1\n",
      "    local_rank: 0\n",
      "    use_sync_bn: False\n",
      "tensorboard:\n",
      "  train_images: False\n",
      "  val_images: False\n",
      "  model_params: False\n",
      "optim:\n",
      "  adam:\n",
      "    betas: (0.9, 0.999)\n",
      "  lars:\n",
      "    eps: 1e-09\n",
      "    threshold: 0.01\n",
      "  adabound:\n",
      "    betas: (0.9, 0.999)\n",
      "    final_lr: 0.1\n",
      "    gamma: 0.001\n",
      "scheduler:\n",
      "  epochs: 300\n",
      "  warmup:\n",
      "    type: none\n",
      "    epochs: 0\n",
      "    start_factor: 0.001\n",
      "    exponent: 4\n",
      "  type: multistep\n",
      "  milestones: [150, 225]\n",
      "  lr_decay: 0.1\n",
      "  lr_min_factor: 0.001\n",
      "  T0: 10\n",
      "  T_mul: 1.0\n",
      "validation:\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: False\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "augmentation:\n",
      "  use_random_crop: True\n",
      "  use_random_horizontal_flip: True\n",
      "  use_cutout: False\n",
      "  use_random_erasing: False\n",
      "  use_dual_cutout: False\n",
      "  use_mixup: False\n",
      "  use_ricap: False\n",
      "  use_cutmix: False\n",
      "  use_label_smoothing: False\n",
      "  random_crop:\n",
      "    padding: 4\n",
      "    fill: 0\n",
      "    padding_mode: constant\n",
      "  random_horizontal_flip:\n",
      "    prob: 0.5\n",
      "  cutout:\n",
      "    prob: 1.0\n",
      "    mask_size: 16\n",
      "    cut_inside: False\n",
      "    mask_color: 0\n",
      "    dual_cutout_alpha: 0.1\n",
      "  random_erasing:\n",
      "    prob: 0.5\n",
      "    area_ratio_range: [0.02, 0.4]\n",
      "    min_aspect_ratio: 0.3\n",
      "    max_attempt: 20\n",
      "  mixup:\n",
      "    alpha: 1.0\n",
      "  ricap:\n",
      "    beta: 0.3\n",
      "  cutmix:\n",
      "    alpha: 1.0\n",
      "  label_smoothing:\n",
      "    epsilon: 0.1\n",
      "tta:\n",
      "  use_resize: False\n",
      "  use_center_crop: False\n",
      "  resize: 256\n",
      "test:\n",
      "  checkpoint: ''\n",
      "  output_dir: ''\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    pin_memory: False\n",
      "\u001b[32m[2020-06-11 02:12:08] __main__ INFO: \u001b[0menv_info:\n",
      "  pytorch_version: 1.4.0\n",
      "  cuda_version: 10.1\n",
      "  cudnn_version: 7603\n",
      "  num_gpus: 1\n",
      "  gpu_name: Tesla K80\n",
      "  gpu_capability: 3.7\n",
      "Files already downloaded and verified\n",
      "\u001b[32m[2020-06-11 02:12:14] __main__ INFO: \u001b[0mMACs  : 5.40G\n",
      "\u001b[32m[2020-06-11 02:12:14] __main__ INFO: \u001b[0m#params: 34.43M\n",
      "Selected optimization level O0:  Pure FP32 training.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\",)\n",
      "\u001b[32m[2020-06-11 02:12:14] __main__ INFO: \u001b[0mVal 0\n",
      "\u001b[32m[2020-06-11 02:13:04] __main__ INFO: \u001b[0mEpoch 0 loss 3.2139 acc@1 0.1014 acc@5 0.5160\n",
      "\u001b[32m[2020-06-11 02:13:04] __main__ INFO: \u001b[0mElapsed 49.43\n",
      "\u001b[32m[2020-06-11 02:13:04] __main__ INFO: \u001b[0mTrain 1 0\n",
      "\u001b[32m[2020-06-11 02:17:31] __main__ INFO: \u001b[0mEpoch 1 Step 100/351 lr 0.100000 loss 2.6399 (8.6813) acc@1 0.1641 (0.1105) acc@5 0.6484 (0.5364)\n",
      "\u001b[32m[2020-06-11 02:21:44] __main__ INFO: \u001b[0mEpoch 1 Step 200/351 lr 0.100000 loss 2.3403 (5.5713) acc@1 0.1875 (0.1455) acc@5 0.7578 (0.6068)\n",
      "\u001b[32m[2020-06-11 02:25:56] __main__ INFO: \u001b[0mEpoch 1 Step 300/351 lr 0.100000 loss 1.9053 (4.4101) acc@1 0.2266 (0.1812) acc@5 0.8516 (0.6696)\n",
      "\u001b[32m[2020-06-11 02:28:05] __main__ INFO: \u001b[0mEpoch 1 Step 351/351 lr 0.100000 loss 1.8136 (4.0548) acc@1 0.2969 (0.1965) acc@5 0.8516 (0.6931)\n",
      "\u001b[32m[2020-06-11 02:28:05] __main__ INFO: \u001b[0mElapsed 901.48\n",
      "\u001b[32m[2020-06-11 02:28:05] __main__ INFO: \u001b[0mVal 1\n",
      "\u001b[32m[2020-06-11 02:28:37] __main__ INFO: \u001b[0mEpoch 1 loss 1.9752 acc@1 0.2800 acc@5 0.7988\n",
      "\u001b[32m[2020-06-11 02:28:37] __main__ INFO: \u001b[0mElapsed 31.63\n",
      "\u001b[32m[2020-06-11 02:28:37] __main__ INFO: \u001b[0mTrain 2 351\n",
      "\u001b[32m[2020-06-11 02:32:50] __main__ INFO: \u001b[0mEpoch 2 Step 100/351 lr 0.100000 loss 1.8183 (1.8302) acc@1 0.3516 (0.3227) acc@5 0.8281 (0.8558)\n",
      "\u001b[32m[2020-06-11 02:37:03] __main__ INFO: \u001b[0mEpoch 2 Step 200/351 lr 0.100000 loss 1.6674 (1.7979) acc@1 0.3594 (0.3324) acc@5 0.9219 (0.8645)\n",
      "\u001b[32m[2020-06-11 02:41:16] __main__ INFO: \u001b[0mEpoch 2 Step 300/351 lr 0.100000 loss 1.8177 (1.7687) acc@1 0.3359 (0.3430) acc@5 0.7891 (0.8699)\n",
      "\u001b[32m[2020-06-11 02:43:25] __main__ INFO: \u001b[0mEpoch 2 Step 351/351 lr 0.100000 loss 1.6848 (1.7589) acc@1 0.3594 (0.3474) acc@5 0.9062 (0.8709)\n",
      "\u001b[32m[2020-06-11 02:43:25] __main__ INFO: \u001b[0mElapsed 887.62\n",
      "\u001b[32m[2020-06-11 02:43:25] __main__ INFO: \u001b[0mVal 2\n",
      "\u001b[32m[2020-06-11 02:43:55] __main__ INFO: \u001b[0mEpoch 2 loss 1.6695 acc@1 0.3832 acc@5 0.8868\n",
      "\u001b[32m[2020-06-11 02:43:55] __main__ INFO: \u001b[0mElapsed 30.76\n",
      "\u001b[32m[2020-06-11 02:43:55] __main__ INFO: \u001b[0mTrain 3 702\n",
      "\u001b[32m[2020-06-11 02:48:08] __main__ INFO: \u001b[0mEpoch 3 Step 100/351 lr 0.100000 loss 1.6073 (1.6064) acc@1 0.3984 (0.4080) acc@5 0.8906 (0.9015)\n",
      "\u001b[32m[2020-06-11 02:52:21] __main__ INFO: \u001b[0mEpoch 3 Step 200/351 lr 0.100000 loss 1.5679 (1.5912) acc@1 0.4531 (0.4147) acc@5 0.8906 (0.9036)\n",
      "\u001b[32m[2020-06-11 02:56:34] __main__ INFO: \u001b[0mEpoch 3 Step 300/351 lr 0.100000 loss 1.2789 (1.5649) acc@1 0.5234 (0.4248) acc@5 0.9609 (0.9077)\n",
      "\u001b[32m[2020-06-11 02:58:43] __main__ INFO: \u001b[0mEpoch 3 Step 351/351 lr 0.100000 loss 1.3677 (1.5508) acc@1 0.5234 (0.4296) acc@5 0.9375 (0.9103)\n",
      "\u001b[32m[2020-06-11 02:58:43] __main__ INFO: \u001b[0mElapsed 887.82\n",
      "\u001b[32m[2020-06-11 02:58:43] __main__ INFO: \u001b[0mVal 3\n",
      "\u001b[32m[2020-06-11 02:59:14] __main__ INFO: \u001b[0mEpoch 3 loss 1.4440 acc@1 0.4786 acc@5 0.9246\n",
      "\u001b[32m[2020-06-11 02:59:14] __main__ INFO: \u001b[0mElapsed 30.78\n",
      "\u001b[32m[2020-06-11 02:59:14] __main__ INFO: \u001b[0mTrain 4 1053\n",
      "\u001b[32m[2020-06-11 03:03:27] __main__ INFO: \u001b[0mEpoch 4 Step 100/351 lr 0.100000 loss 1.3919 (1.4118) acc@1 0.4922 (0.4838) acc@5 0.8984 (0.9270)\n",
      "\u001b[32m[2020-06-11 03:07:40] __main__ INFO: \u001b[0mEpoch 4 Step 200/351 lr 0.100000 loss 1.2670 (1.3785) acc@1 0.5391 (0.4960) acc@5 0.9453 (0.9311)\n",
      "\u001b[32m[2020-06-11 03:11:53] __main__ INFO: \u001b[0mEpoch 4 Step 300/351 lr 0.100000 loss 1.3188 (1.3540) acc@1 0.5234 (0.5082) acc@5 0.9141 (0.9336)\n",
      "\u001b[32m[2020-06-11 03:14:02] __main__ INFO: \u001b[0mEpoch 4 Step 351/351 lr 0.100000 loss 1.2083 (1.3364) acc@1 0.5312 (0.5153) acc@5 0.9688 (0.9354)\n",
      "\u001b[32m[2020-06-11 03:14:02] __main__ INFO: \u001b[0mElapsed 888.27\n",
      "\u001b[32m[2020-06-11 03:14:02] __main__ INFO: \u001b[0mVal 4\n",
      "\u001b[32m[2020-06-11 03:14:33] __main__ INFO: \u001b[0mEpoch 4 loss 1.4052 acc@1 0.5214 acc@5 0.9250\n",
      "\u001b[32m[2020-06-11 03:14:33] __main__ INFO: \u001b[0mElapsed 30.74\n",
      "\u001b[32m[2020-06-11 03:14:33] __main__ INFO: \u001b[0mTrain 5 1404\n",
      "\u001b[32m[2020-06-11 03:18:46] __main__ INFO: \u001b[0mEpoch 5 Step 100/351 lr 0.100000 loss 1.1055 (1.1801) acc@1 0.5781 (0.5725) acc@5 0.9453 (0.9546)\n",
      "\u001b[32m[2020-06-11 03:22:59] __main__ INFO: \u001b[0mEpoch 5 Step 200/351 lr 0.100000 loss 1.2471 (1.1561) acc@1 0.5703 (0.5872) acc@5 0.9375 (0.9542)\n",
      "\u001b[32m[2020-06-11 03:27:13] __main__ INFO: \u001b[0mEpoch 5 Step 300/351 lr 0.100000 loss 0.9364 (1.1302) acc@1 0.6406 (0.5964) acc@5 0.9922 (0.9574)\n",
      "\u001b[32m[2020-06-11 03:29:22] __main__ INFO: \u001b[0mEpoch 5 Step 351/351 lr 0.100000 loss 1.0331 (1.1187) acc@1 0.6328 (0.6003) acc@5 0.9609 (0.9584)\n",
      "\u001b[32m[2020-06-11 03:29:22] __main__ INFO: \u001b[0mElapsed 888.78\n",
      "\u001b[32m[2020-06-11 03:29:22] __main__ INFO: \u001b[0mVal 5\n",
      "\u001b[32m[2020-06-11 03:29:53] __main__ INFO: \u001b[0mEpoch 5 loss 1.0463 acc@1 0.6214 acc@5 0.9632\n",
      "\u001b[32m[2020-06-11 03:29:53] __main__ INFO: \u001b[0mElapsed 30.75\n",
      "\u001b[32m[2020-06-11 03:29:53] __main__ INFO: \u001b[0mTrain 6 1755\n",
      "\u001b[32m[2020-06-11 03:34:06] __main__ INFO: \u001b[0mEpoch 6 Step 100/351 lr 0.100000 loss 1.2224 (1.0088) acc@1 0.6562 (0.6446) acc@5 0.9375 (0.9655)\n",
      "\u001b[32m[2020-06-11 03:38:19] __main__ INFO: \u001b[0mEpoch 6 Step 200/351 lr 0.100000 loss 0.8483 (0.9876) acc@1 0.6719 (0.6524) acc@5 0.9766 (0.9673)\n",
      "\u001b[32m[2020-06-11 03:42:33] __main__ INFO: \u001b[0mEpoch 6 Step 300/351 lr 0.100000 loss 0.9017 (0.9728) acc@1 0.6641 (0.6564) acc@5 0.9766 (0.9693)\n",
      "\u001b[32m[2020-06-11 03:44:42] __main__ INFO: \u001b[0mEpoch 6 Step 351/351 lr 0.100000 loss 0.8653 (0.9644) acc@1 0.6953 (0.6589) acc@5 0.9609 (0.9694)\n",
      "\u001b[32m[2020-06-11 03:44:42] __main__ INFO: \u001b[0mElapsed 889.27\n",
      "\u001b[32m[2020-06-11 03:44:42] __main__ INFO: \u001b[0mVal 6\n",
      "\u001b[32m[2020-06-11 03:45:13] __main__ INFO: \u001b[0mEpoch 6 loss 0.9657 acc@1 0.6642 acc@5 0.9674\n",
      "\u001b[32m[2020-06-11 03:45:13] __main__ INFO: \u001b[0mElapsed 30.78\n",
      "\u001b[32m[2020-06-11 03:45:13] __main__ INFO: \u001b[0mTrain 7 2106\n",
      "\u001b[32m[2020-06-11 03:49:26] __main__ INFO: \u001b[0mEpoch 7 Step 100/351 lr 0.100000 loss 0.8086 (0.8717) acc@1 0.7188 (0.6927) acc@5 0.9766 (0.9762)\n",
      "\u001b[32m[2020-06-11 03:53:39] __main__ INFO: \u001b[0mEpoch 7 Step 200/351 lr 0.100000 loss 0.8519 (0.8673) acc@1 0.6953 (0.6946) acc@5 0.9844 (0.9755)\n",
      "\u001b[32m[2020-06-11 03:57:52] __main__ INFO: \u001b[0mEpoch 7 Step 300/351 lr 0.100000 loss 0.7675 (0.8619) acc@1 0.7656 (0.6971) acc@5 0.9609 (0.9760)\n",
      "\u001b[32m[2020-06-11 04:00:02] __main__ INFO: \u001b[0mEpoch 7 Step 351/351 lr 0.100000 loss 0.6780 (0.8563) acc@1 0.7812 (0.6999) acc@5 0.9922 (0.9759)\n",
      "\u001b[32m[2020-06-11 04:00:02] __main__ INFO: \u001b[0mElapsed 889.01\n",
      "\u001b[32m[2020-06-11 04:00:02] __main__ INFO: \u001b[0mVal 7\n",
      "\u001b[32m[2020-06-11 04:00:32] __main__ INFO: \u001b[0mEpoch 7 loss 0.8249 acc@1 0.7140 acc@5 0.9750\n",
      "\u001b[32m[2020-06-11 04:00:32] __main__ INFO: \u001b[0mElapsed 30.76\n",
      "\u001b[32m[2020-06-11 04:00:32] __main__ INFO: \u001b[0mTrain 8 2457\n",
      "\u001b[32m[2020-06-11 04:04:46] __main__ INFO: \u001b[0mEpoch 8 Step 100/351 lr 0.100000 loss 0.6113 (0.7881) acc@1 0.8125 (0.7235) acc@5 0.9844 (0.9809)\n",
      "\u001b[32m[2020-06-11 04:08:59] __main__ INFO: \u001b[0mEpoch 8 Step 200/351 lr 0.100000 loss 0.7815 (0.7826) acc@1 0.7266 (0.7252) acc@5 0.9844 (0.9805)\n",
      "\u001b[32m[2020-06-11 04:13:12] __main__ INFO: \u001b[0mEpoch 8 Step 300/351 lr 0.100000 loss 0.8261 (0.7821) acc@1 0.6875 (0.7257) acc@5 0.9844 (0.9804)\n",
      "\u001b[32m[2020-06-11 04:15:21] __main__ INFO: \u001b[0mEpoch 8 Step 351/351 lr 0.100000 loss 0.6028 (0.7775) acc@1 0.8125 (0.7279) acc@5 0.9844 (0.9805)\n",
      "\u001b[32m[2020-06-11 04:15:21] __main__ INFO: \u001b[0mElapsed 889.09\n",
      "\u001b[32m[2020-06-11 04:15:21] __main__ INFO: \u001b[0mVal 8\n",
      "\u001b[32m[2020-06-11 04:15:52] __main__ INFO: \u001b[0mEpoch 8 loss 0.9861 acc@1 0.6612 acc@5 0.9724\n",
      "\u001b[32m[2020-06-11 04:15:52] __main__ INFO: \u001b[0mElapsed 30.81\n",
      "\u001b[32m[2020-06-11 04:15:52] __main__ INFO: \u001b[0mTrain 9 2808\n",
      "\u001b[32m[2020-06-11 04:20:06] __main__ INFO: \u001b[0mEpoch 9 Step 100/351 lr 0.100000 loss 0.7477 (0.7179) acc@1 0.7656 (0.7455) acc@5 0.9688 (0.9849)\n",
      "\u001b[32m[2020-06-11 04:24:19] __main__ INFO: \u001b[0mEpoch 9 Step 200/351 lr 0.100000 loss 0.8120 (0.7130) acc@1 0.7422 (0.7485) acc@5 0.9609 (0.9851)\n",
      "\u001b[32m[2020-06-11 04:28:32] __main__ INFO: \u001b[0mEpoch 9 Step 300/351 lr 0.100000 loss 0.6985 (0.7071) acc@1 0.7344 (0.7529) acc@5 0.9844 (0.9845)\n",
      "\u001b[32m[2020-06-11 04:30:41] __main__ INFO: \u001b[0mEpoch 9 Step 351/351 lr 0.100000 loss 0.5541 (0.7077) acc@1 0.7891 (0.7528) acc@5 0.9922 (0.9845)\n",
      "\u001b[32m[2020-06-11 04:30:41] __main__ INFO: \u001b[0mElapsed 888.84\n",
      "\u001b[32m[2020-06-11 04:30:41] __main__ INFO: \u001b[0mVal 9\n",
      "\u001b[32m[2020-06-11 04:31:12] __main__ INFO: \u001b[0mEpoch 9 loss 0.8980 acc@1 0.7072 acc@5 0.9698\n",
      "\u001b[32m[2020-06-11 04:31:12] __main__ INFO: \u001b[0mElapsed 30.76\n",
      "\u001b[32m[2020-06-11 04:31:12] __main__ INFO: \u001b[0mTrain 10 3159\n",
      "\u001b[32m[2020-06-11 04:35:25] __main__ INFO: \u001b[0mEpoch 10 Step 100/351 lr 0.100000 loss 0.8044 (0.6531) acc@1 0.7422 (0.7741) acc@5 0.9609 (0.9880)\n",
      "\u001b[32m[2020-06-11 04:39:38] __main__ INFO: \u001b[0mEpoch 10 Step 200/351 lr 0.100000 loss 0.5346 (0.6567) acc@1 0.8281 (0.7730) acc@5 0.9844 (0.9882)\n",
      "\u001b[32m[2020-06-11 04:43:52] __main__ INFO: \u001b[0mEpoch 10 Step 300/351 lr 0.100000 loss 0.5344 (0.6476) acc@1 0.8516 (0.7764) acc@5 0.9766 (0.9874)\n",
      "\u001b[32m[2020-06-11 04:46:01] __main__ INFO: \u001b[0mEpoch 10 Step 351/351 lr 0.100000 loss 0.6383 (0.6471) acc@1 0.7812 (0.7761) acc@5 0.9922 (0.9876)\n",
      "\u001b[32m[2020-06-11 04:46:01] __main__ INFO: \u001b[0mElapsed 889.21\n",
      "\u001b[32m[2020-06-11 04:46:01] __main__ INFO: \u001b[0mVal 10\n",
      "\u001b[32m[2020-06-11 04:46:32] __main__ INFO: \u001b[0mEpoch 10 loss 0.7133 acc@1 0.7562 acc@5 0.9808\n",
      "\u001b[32m[2020-06-11 04:46:32] __main__ INFO: \u001b[0mElapsed 30.81\n",
      "\u001b[32m[2020-06-11 04:46:32] __main__ INFO: \u001b[0mTrain 11 3510\n",
      "\u001b[32m[2020-06-11 04:50:45] __main__ INFO: \u001b[0mEpoch 11 Step 100/351 lr 0.100000 loss 0.6728 (0.5930) acc@1 0.7578 (0.7977) acc@5 0.9844 (0.9886)\n",
      "\u001b[32m[2020-06-11 04:54:59] __main__ INFO: \u001b[0mEpoch 11 Step 200/351 lr 0.100000 loss 0.6560 (0.5936) acc@1 0.7812 (0.7948) acc@5 0.9844 (0.9888)\n",
      "\u001b[32m[2020-06-11 04:59:12] __main__ INFO: \u001b[0mEpoch 11 Step 300/351 lr 0.100000 loss 0.5468 (0.5967) acc@1 0.7969 (0.7939) acc@5 0.9844 (0.9889)\n",
      "\u001b[32m[2020-06-11 05:01:21] __main__ INFO: \u001b[0mEpoch 11 Step 351/351 lr 0.100000 loss 0.4743 (0.5935) acc@1 0.8203 (0.7952) acc@5 1.0000 (0.9891)\n",
      "\u001b[32m[2020-06-11 05:01:21] __main__ INFO: \u001b[0mElapsed 889.42\n",
      "\u001b[32m[2020-06-11 05:01:21] __main__ INFO: \u001b[0mVal 11\n",
      "\u001b[32m[2020-06-11 05:01:52] __main__ INFO: \u001b[0mEpoch 11 loss 0.7185 acc@1 0.7630 acc@5 0.9862\n",
      "\u001b[32m[2020-06-11 05:01:52] __main__ INFO: \u001b[0mElapsed 30.83\n",
      "\u001b[32m[2020-06-11 05:01:52] __main__ INFO: \u001b[0mTrain 12 3861\n",
      "\u001b[32m[2020-06-11 05:06:06] __main__ INFO: \u001b[0mEpoch 12 Step 100/351 lr 0.100000 loss 0.5190 (0.5546) acc@1 0.8438 (0.8079) acc@5 0.9766 (0.9911)\n",
      "\u001b[32m[2020-06-11 05:10:19] __main__ INFO: \u001b[0mEpoch 12 Step 200/351 lr 0.100000 loss 0.5591 (0.5575) acc@1 0.7734 (0.8093) acc@5 0.9922 (0.9900)\n",
      "\u001b[32m[2020-06-11 05:14:33] __main__ INFO: \u001b[0mEpoch 12 Step 300/351 lr 0.100000 loss 0.4859 (0.5532) acc@1 0.8594 (0.8091) acc@5 1.0000 (0.9901)\n",
      "\u001b[32m[2020-06-11 05:16:42] __main__ INFO: \u001b[0mEpoch 12 Step 351/351 lr 0.100000 loss 0.3792 (0.5497) acc@1 0.8672 (0.8108) acc@5 1.0000 (0.9903)\n",
      "\u001b[32m[2020-06-11 05:16:42] __main__ INFO: \u001b[0mElapsed 889.92\n",
      "\u001b[32m[2020-06-11 05:16:42] __main__ INFO: \u001b[0mVal 12\n",
      "\u001b[32m[2020-06-11 05:17:13] __main__ INFO: \u001b[0mEpoch 12 loss 0.6969 acc@1 0.7676 acc@5 0.9862\n",
      "\u001b[32m[2020-06-11 05:17:13] __main__ INFO: \u001b[0mElapsed 30.83\n",
      "\u001b[32m[2020-06-11 05:17:13] __main__ INFO: \u001b[0mTrain 13 4212\n",
      "\u001b[32m[2020-06-11 05:21:26] __main__ INFO: \u001b[0mEpoch 13 Step 100/351 lr 0.100000 loss 0.6877 (0.5139) acc@1 0.7500 (0.8198) acc@5 0.9688 (0.9920)\n",
      "\u001b[32m[2020-06-11 05:25:40] __main__ INFO: \u001b[0mEpoch 13 Step 200/351 lr 0.100000 loss 0.6656 (0.5200) acc@1 0.7734 (0.8177) acc@5 0.9922 (0.9925)\n",
      "\u001b[32m[2020-06-11 05:29:53] __main__ INFO: \u001b[0mEpoch 13 Step 300/351 lr 0.100000 loss 0.5640 (0.5190) acc@1 0.8047 (0.8198) acc@5 1.0000 (0.9922)\n",
      "\u001b[32m[2020-06-11 05:32:02] __main__ INFO: \u001b[0mEpoch 13 Step 351/351 lr 0.100000 loss 0.5100 (0.5172) acc@1 0.8125 (0.8209) acc@5 0.9922 (0.9920)\n",
      "\u001b[32m[2020-06-11 05:32:02] __main__ INFO: \u001b[0mElapsed 889.45\n",
      "\u001b[32m[2020-06-11 05:32:02] __main__ INFO: \u001b[0mVal 13\n",
      "\u001b[32m[2020-06-11 05:32:33] __main__ INFO: \u001b[0mEpoch 13 loss 0.6445 acc@1 0.7852 acc@5 0.9834\n",
      "\u001b[32m[2020-06-11 05:32:33] __main__ INFO: \u001b[0mElapsed 30.77\n",
      "\u001b[32m[2020-06-11 05:32:33] __main__ INFO: \u001b[0mTrain 14 4563\n",
      "\u001b[32m[2020-06-11 05:36:47] __main__ INFO: \u001b[0mEpoch 14 Step 100/351 lr 0.100000 loss 0.4891 (0.4789) acc@1 0.8438 (0.8352) acc@5 0.9922 (0.9933)\n",
      "\u001b[32m[2020-06-11 05:41:00] __main__ INFO: \u001b[0mEpoch 14 Step 200/351 lr 0.100000 loss 0.5208 (0.4856) acc@1 0.8359 (0.8319) acc@5 0.9844 (0.9936)\n",
      "\u001b[32m[2020-06-11 05:45:13] __main__ INFO: \u001b[0mEpoch 14 Step 300/351 lr 0.100000 loss 0.5881 (0.4909) acc@1 0.7891 (0.8301) acc@5 0.9766 (0.9930)\n",
      "\u001b[32m[2020-06-11 05:47:22] __main__ INFO: \u001b[0mEpoch 14 Step 351/351 lr 0.100000 loss 0.5162 (0.4887) acc@1 0.8203 (0.8305) acc@5 0.9922 (0.9929)\n",
      "\u001b[32m[2020-06-11 05:47:22] __main__ INFO: \u001b[0mElapsed 889.30\n",
      "\u001b[32m[2020-06-11 05:47:22] __main__ INFO: \u001b[0mVal 14\n",
      "\u001b[32m[2020-06-11 05:47:53] __main__ INFO: \u001b[0mEpoch 14 loss 0.8568 acc@1 0.7406 acc@5 0.9688\n",
      "\u001b[32m[2020-06-11 05:47:53] __main__ INFO: \u001b[0mElapsed 30.83\n",
      "\u001b[32m[2020-06-11 05:47:53] __main__ INFO: \u001b[0mTrain 15 4914\n",
      "\u001b[32m[2020-06-11 05:52:07] __main__ INFO: \u001b[0mEpoch 15 Step 100/351 lr 0.100000 loss 0.4317 (0.4793) acc@1 0.8203 (0.8329) acc@5 1.0000 (0.9920)\n",
      "\u001b[32m[2020-06-11 05:56:20] __main__ INFO: \u001b[0mEpoch 15 Step 200/351 lr 0.100000 loss 0.4740 (0.4781) acc@1 0.8516 (0.8342) acc@5 1.0000 (0.9924)\n",
      "\u001b[32m[2020-06-11 06:00:33] __main__ INFO: \u001b[0mEpoch 15 Step 300/351 lr 0.100000 loss 0.4966 (0.4810) acc@1 0.8359 (0.8339) acc@5 0.9922 (0.9925)\n",
      "\u001b[32m[2020-06-11 06:02:43] __main__ INFO: \u001b[0mEpoch 15 Step 351/351 lr 0.100000 loss 0.4165 (0.4807) acc@1 0.8438 (0.8340) acc@5 0.9922 (0.9926)\n",
      "\u001b[32m[2020-06-11 06:02:43] __main__ INFO: \u001b[0mElapsed 889.33\n",
      "\u001b[32m[2020-06-11 06:02:43] __main__ INFO: \u001b[0mVal 15\n",
      "\u001b[32m[2020-06-11 06:03:13] __main__ INFO: \u001b[0mEpoch 15 loss 0.7892 acc@1 0.7426 acc@5 0.9802\n",
      "\u001b[32m[2020-06-11 06:03:13] __main__ INFO: \u001b[0mElapsed 30.82\n",
      "\u001b[32m[2020-06-11 06:03:13] __main__ INFO: \u001b[0mTrain 16 5265\n",
      "\u001b[32m[2020-06-11 06:07:27] __main__ INFO: \u001b[0mEpoch 16 Step 100/351 lr 0.100000 loss 0.3143 (0.4565) acc@1 0.8984 (0.8420) acc@5 0.9922 (0.9928)\n",
      "\u001b[32m[2020-06-11 06:11:40] __main__ INFO: \u001b[0mEpoch 16 Step 200/351 lr 0.100000 loss 0.5087 (0.4489) acc@1 0.8359 (0.8458) acc@5 1.0000 (0.9929)\n",
      "\u001b[32m[2020-06-11 06:15:53] __main__ INFO: \u001b[0mEpoch 16 Step 300/351 lr 0.100000 loss 0.3857 (0.4525) acc@1 0.8594 (0.8442) acc@5 0.9922 (0.9933)\n",
      "\u001b[32m[2020-06-11 06:18:03] __main__ INFO: \u001b[0mEpoch 16 Step 351/351 lr 0.100000 loss 0.3489 (0.4564) acc@1 0.8906 (0.8430) acc@5 1.0000 (0.9932)\n",
      "\u001b[32m[2020-06-11 06:18:03] __main__ INFO: \u001b[0mElapsed 889.26\n",
      "\u001b[32m[2020-06-11 06:18:03] __main__ INFO: \u001b[0mVal 16\n",
      "\u001b[32m[2020-06-11 06:18:33] __main__ INFO: \u001b[0mEpoch 16 loss 0.7206 acc@1 0.7630 acc@5 0.9888\n",
      "\u001b[32m[2020-06-11 06:18:33] __main__ INFO: \u001b[0mElapsed 30.81\n",
      "\u001b[32m[2020-06-11 06:18:33] __main__ INFO: \u001b[0mTrain 17 5616\n",
      "\u001b[32m[2020-06-11 06:22:47] __main__ INFO: \u001b[0mEpoch 17 Step 100/351 lr 0.100000 loss 0.5368 (0.4329) acc@1 0.8281 (0.8503) acc@5 0.9922 (0.9951)\n",
      "\u001b[32m[2020-06-11 06:27:00] __main__ INFO: \u001b[0mEpoch 17 Step 200/351 lr 0.100000 loss 0.4564 (0.4467) acc@1 0.8438 (0.8451) acc@5 0.9766 (0.9942)\n",
      "\u001b[32m[2020-06-11 06:31:13] __main__ INFO: \u001b[0mEpoch 17 Step 300/351 lr 0.100000 loss 0.5324 (0.4513) acc@1 0.8359 (0.8444) acc@5 0.9922 (0.9937)\n",
      "\u001b[32m[2020-06-11 06:33:23] __main__ INFO: \u001b[0mEpoch 17 Step 351/351 lr 0.100000 loss 0.6252 (0.4483) acc@1 0.7891 (0.8458) acc@5 0.9766 (0.9937)\n",
      "\u001b[32m[2020-06-11 06:33:23] __main__ INFO: \u001b[0mElapsed 889.13\n",
      "\u001b[32m[2020-06-11 06:33:23] __main__ INFO: \u001b[0mVal 17\n",
      "\u001b[32m[2020-06-11 06:33:53] __main__ INFO: \u001b[0mEpoch 17 loss 0.7616 acc@1 0.7562 acc@5 0.9838\n",
      "\u001b[32m[2020-06-11 06:33:53] __main__ INFO: \u001b[0mElapsed 30.78\n",
      "\u001b[32m[2020-06-11 06:33:53] __main__ INFO: \u001b[0mTrain 18 5967\n",
      "\u001b[32m[2020-06-11 06:38:07] __main__ INFO: \u001b[0mEpoch 18 Step 100/351 lr 0.100000 loss 0.4156 (0.4369) acc@1 0.8359 (0.8491) acc@5 1.0000 (0.9942)\n",
      "\u001b[32m[2020-06-11 06:42:20] __main__ INFO: \u001b[0mEpoch 18 Step 200/351 lr 0.100000 loss 0.4865 (0.4363) acc@1 0.8438 (0.8480) acc@5 0.9922 (0.9942)\n",
      "\u001b[32m[2020-06-11 06:46:34] __main__ INFO: \u001b[0mEpoch 18 Step 300/351 lr 0.100000 loss 0.5313 (0.4415) acc@1 0.8438 (0.8471) acc@5 1.0000 (0.9944)\n",
      "\u001b[32m[2020-06-11 06:48:43] __main__ INFO: \u001b[0mEpoch 18 Step 351/351 lr 0.100000 loss 0.3707 (0.4398) acc@1 0.8516 (0.8477) acc@5 1.0000 (0.9944)\n",
      "\u001b[32m[2020-06-11 06:48:43] __main__ INFO: \u001b[0mElapsed 889.37\n",
      "\u001b[32m[2020-06-11 06:48:43] __main__ INFO: \u001b[0mVal 18\n",
      "\u001b[32m[2020-06-11 06:49:14] __main__ INFO: \u001b[0mEpoch 18 loss 0.8158 acc@1 0.7304 acc@5 0.9812\n",
      "\u001b[32m[2020-06-11 06:49:14] __main__ INFO: \u001b[0mElapsed 30.79\n",
      "\u001b[32m[2020-06-11 06:49:14] __main__ INFO: \u001b[0mTrain 19 6318\n",
      "\u001b[32m[2020-06-11 06:53:27] __main__ INFO: \u001b[0mEpoch 19 Step 100/351 lr 0.100000 loss 0.4352 (0.3960) acc@1 0.8516 (0.8665) acc@5 0.9844 (0.9955)\n",
      "\u001b[32m[2020-06-11 06:57:40] __main__ INFO: \u001b[0mEpoch 19 Step 200/351 lr 0.100000 loss 0.5388 (0.4121) acc@1 0.7969 (0.8611) acc@5 0.9922 (0.9945)\n",
      "\u001b[32m[2020-06-11 07:01:53] __main__ INFO: \u001b[0mEpoch 19 Step 300/351 lr 0.100000 loss 0.5598 (0.4173) acc@1 0.8047 (0.8582) acc@5 0.9844 (0.9942)\n",
      "\u001b[32m[2020-06-11 07:04:03] __main__ INFO: \u001b[0mEpoch 19 Step 351/351 lr 0.100000 loss 0.5055 (0.4225) acc@1 0.8125 (0.8556) acc@5 0.9922 (0.9941)\n",
      "\u001b[32m[2020-06-11 07:04:03] __main__ INFO: \u001b[0mElapsed 889.04\n",
      "\u001b[32m[2020-06-11 07:04:03] __main__ INFO: \u001b[0mVal 19\n",
      "\u001b[32m[2020-06-11 07:04:33] __main__ INFO: \u001b[0mEpoch 19 loss 0.5953 acc@1 0.8010 acc@5 0.9842\n",
      "\u001b[32m[2020-06-11 07:04:33] __main__ INFO: \u001b[0mElapsed 30.78\n",
      "\u001b[32m[2020-06-11 07:04:33] __main__ INFO: \u001b[0mTrain 20 6669\n",
      "\u001b[32m[2020-06-11 07:08:47] __main__ INFO: \u001b[0mEpoch 20 Step 100/351 lr 0.100000 loss 0.4721 (0.3999) acc@1 0.8672 (0.8627) acc@5 1.0000 (0.9963)\n",
      "\u001b[32m[2020-06-11 07:13:00] __main__ INFO: \u001b[0mEpoch 20 Step 200/351 lr 0.100000 loss 0.4662 (0.4106) acc@1 0.8594 (0.8583) acc@5 0.9844 (0.9952)\n",
      "\u001b[32m[2020-06-11 07:17:13] __main__ INFO: \u001b[0mEpoch 20 Step 300/351 lr 0.100000 loss 0.6602 (0.4115) acc@1 0.8281 (0.8591) acc@5 0.9844 (0.9949)\n",
      "\u001b[32m[2020-06-11 07:19:23] __main__ INFO: \u001b[0mEpoch 20 Step 351/351 lr 0.100000 loss 0.3791 (0.4120) acc@1 0.8828 (0.8587) acc@5 1.0000 (0.9948)\n",
      "\u001b[32m[2020-06-11 07:19:23] __main__ INFO: \u001b[0mElapsed 889.18\n",
      "\u001b[32m[2020-06-11 07:19:23] __main__ INFO: \u001b[0mVal 20\n",
      "\u001b[32m[2020-06-11 07:19:53] __main__ INFO: \u001b[0mEpoch 20 loss 0.6200 acc@1 0.8022 acc@5 0.9882\n",
      "\u001b[32m[2020-06-11 07:19:53] __main__ INFO: \u001b[0mElapsed 30.82\n",
      "\u001b[32m[2020-06-11 07:19:53] __main__ INFO: \u001b[0mTrain 21 7020\n",
      "\u001b[32m[2020-06-11 07:24:07] __main__ INFO: \u001b[0mEpoch 21 Step 100/351 lr 0.100000 loss 0.4641 (0.3995) acc@1 0.8281 (0.8625) acc@5 1.0000 (0.9945)\n",
      "\u001b[32m[2020-06-11 07:28:20] __main__ INFO: \u001b[0mEpoch 21 Step 200/351 lr 0.100000 loss 0.4429 (0.4007) acc@1 0.8438 (0.8605) acc@5 0.9922 (0.9949)\n",
      "\u001b[32m[2020-06-11 07:32:33] __main__ INFO: \u001b[0mEpoch 21 Step 300/351 lr 0.100000 loss 0.4376 (0.4030) acc@1 0.8516 (0.8599) acc@5 0.9922 (0.9948)\n",
      "\u001b[32m[2020-06-11 07:34:42] __main__ INFO: \u001b[0mEpoch 21 Step 351/351 lr 0.100000 loss 0.3361 (0.4067) acc@1 0.8438 (0.8583) acc@5 1.0000 (0.9950)\n",
      "\u001b[32m[2020-06-11 07:34:42] __main__ INFO: \u001b[0mElapsed 888.87\n",
      "\u001b[32m[2020-06-11 07:34:42] __main__ INFO: \u001b[0mVal 21\n",
      "\u001b[32m[2020-06-11 07:35:13] __main__ INFO: \u001b[0mEpoch 21 loss 0.7059 acc@1 0.7756 acc@5 0.9818\n",
      "\u001b[32m[2020-06-11 07:35:13] __main__ INFO: \u001b[0mElapsed 30.76\n",
      "\u001b[32m[2020-06-11 07:35:13] __main__ INFO: \u001b[0mTrain 22 7371\n",
      "\u001b[32m[2020-06-11 07:39:26] __main__ INFO: \u001b[0mEpoch 22 Step 100/351 lr 0.100000 loss 0.2739 (0.3817) acc@1 0.9141 (0.8673) acc@5 1.0000 (0.9959)\n",
      "\u001b[32m[2020-06-11 07:43:40] __main__ INFO: \u001b[0mEpoch 22 Step 200/351 lr 0.100000 loss 0.3468 (0.3976) acc@1 0.8672 (0.8647) acc@5 0.9922 (0.9954)\n",
      "\u001b[32m[2020-06-11 07:47:53] __main__ INFO: \u001b[0mEpoch 22 Step 300/351 lr 0.100000 loss 0.4683 (0.3978) acc@1 0.8516 (0.8638) acc@5 0.9922 (0.9952)\n",
      "\u001b[32m[2020-06-11 07:50:02] __main__ INFO: \u001b[0mEpoch 22 Step 351/351 lr 0.100000 loss 0.4757 (0.3995) acc@1 0.8438 (0.8626) acc@5 1.0000 (0.9954)\n",
      "\u001b[32m[2020-06-11 07:50:02] __main__ INFO: \u001b[0mElapsed 888.99\n",
      "\u001b[32m[2020-06-11 07:50:02] __main__ INFO: \u001b[0mVal 22\n",
      "\u001b[32m[2020-06-11 07:50:33] __main__ INFO: \u001b[0mEpoch 22 loss 0.6280 acc@1 0.7936 acc@5 0.9852\n",
      "\u001b[32m[2020-06-11 07:50:33] __main__ INFO: \u001b[0mElapsed 30.83\n",
      "\u001b[32m[2020-06-11 07:50:33] __main__ INFO: \u001b[0mTrain 23 7722\n",
      "\u001b[32m[2020-06-11 07:54:46] __main__ INFO: \u001b[0mEpoch 23 Step 100/351 lr 0.100000 loss 0.2594 (0.3922) acc@1 0.8984 (0.8649) acc@5 0.9922 (0.9970)\n",
      "\u001b[32m[2020-06-11 07:58:59] __main__ INFO: \u001b[0mEpoch 23 Step 200/351 lr 0.100000 loss 0.4794 (0.3975) acc@1 0.8047 (0.8611) acc@5 1.0000 (0.9958)\n",
      "\u001b[32m[2020-06-11 08:03:13] __main__ INFO: \u001b[0mEpoch 23 Step 300/351 lr 0.100000 loss 0.5108 (0.3972) acc@1 0.7969 (0.8609) acc@5 0.9922 (0.9954)\n",
      "\u001b[32m[2020-06-11 08:05:22] __main__ INFO: \u001b[0mEpoch 23 Step 351/351 lr 0.100000 loss 0.4001 (0.3953) acc@1 0.8906 (0.8622) acc@5 0.9922 (0.9956)\n",
      "\u001b[32m[2020-06-11 08:05:22] __main__ INFO: \u001b[0mElapsed 888.84\n",
      "\u001b[32m[2020-06-11 08:05:22] __main__ INFO: \u001b[0mVal 23\n",
      "\u001b[32m[2020-06-11 08:05:52] __main__ INFO: \u001b[0mEpoch 23 loss 0.7980 acc@1 0.7528 acc@5 0.9790\n",
      "\u001b[32m[2020-06-11 08:05:52] __main__ INFO: \u001b[0mElapsed 30.81\n",
      "\u001b[32m[2020-06-11 08:05:52] __main__ INFO: \u001b[0mTrain 24 8073\n",
      "\u001b[32m[2020-06-11 08:10:06] __main__ INFO: \u001b[0mEpoch 24 Step 100/351 lr 0.100000 loss 0.3428 (0.3698) acc@1 0.8828 (0.8740) acc@5 1.0000 (0.9961)\n",
      "\u001b[32m[2020-06-11 08:14:19] __main__ INFO: \u001b[0mEpoch 24 Step 200/351 lr 0.100000 loss 0.3966 (0.3791) acc@1 0.8672 (0.8701) acc@5 1.0000 (0.9959)\n",
      "\u001b[32m[2020-06-11 08:18:33] __main__ INFO: \u001b[0mEpoch 24 Step 300/351 lr 0.100000 loss 0.3358 (0.3814) acc@1 0.8750 (0.8704) acc@5 0.9922 (0.9959)\n",
      "\u001b[32m[2020-06-11 08:20:42] __main__ INFO: \u001b[0mEpoch 24 Step 351/351 lr 0.100000 loss 0.2921 (0.3828) acc@1 0.8984 (0.8698) acc@5 1.0000 (0.9959)\n",
      "\u001b[32m[2020-06-11 08:20:42] __main__ INFO: \u001b[0mElapsed 889.35\n",
      "\u001b[32m[2020-06-11 08:20:42] __main__ INFO: \u001b[0mVal 24\n",
      "\u001b[32m[2020-06-11 08:21:13] __main__ INFO: \u001b[0mEpoch 24 loss 0.6162 acc@1 0.8096 acc@5 0.9880\n",
      "\u001b[32m[2020-06-11 08:21:13] __main__ INFO: \u001b[0mElapsed 30.85\n",
      "\u001b[32m[2020-06-11 08:21:13] __main__ INFO: \u001b[0mTrain 25 8424\n",
      "\u001b[32m[2020-06-11 08:25:26] __main__ INFO: \u001b[0mEpoch 25 Step 100/351 lr 0.100000 loss 0.4976 (0.3687) acc@1 0.8203 (0.8722) acc@5 0.9922 (0.9965)\n",
      "\u001b[32m[2020-06-11 08:29:39] __main__ INFO: \u001b[0mEpoch 25 Step 200/351 lr 0.100000 loss 0.3618 (0.3800) acc@1 0.8984 (0.8685) acc@5 1.0000 (0.9958)\n",
      "\u001b[32m[2020-06-11 08:33:52] __main__ INFO: \u001b[0mEpoch 25 Step 300/351 lr 0.100000 loss 0.3630 (0.3835) acc@1 0.8906 (0.8675) acc@5 1.0000 (0.9957)\n",
      "\u001b[32m[2020-06-11 08:36:01] __main__ INFO: \u001b[0mEpoch 25 Step 351/351 lr 0.100000 loss 0.3995 (0.3849) acc@1 0.8672 (0.8680) acc@5 1.0000 (0.9956)\n",
      "\u001b[32m[2020-06-11 08:36:01] __main__ INFO: \u001b[0mElapsed 888.60\n",
      "\u001b[32m[2020-06-11 08:36:01] __main__ INFO: \u001b[0mVal 25\n",
      "\u001b[32m[2020-06-11 08:36:32] __main__ INFO: \u001b[0mEpoch 25 loss 0.5696 acc@1 0.8012 acc@5 0.9914\n",
      "\u001b[32m[2020-06-11 08:36:32] __main__ INFO: \u001b[0mElapsed 30.79\n",
      "\u001b[32m[2020-06-11 08:36:32] __main__ INFO: \u001b[0mTrain 26 8775\n",
      "\u001b[32m[2020-06-11 08:40:45] __main__ INFO: \u001b[0mEpoch 26 Step 100/351 lr 0.100000 loss 0.3388 (0.3493) acc@1 0.8828 (0.8804) acc@5 1.0000 (0.9966)\n",
      "\u001b[32m[2020-06-11 08:44:58] __main__ INFO: \u001b[0mEpoch 26 Step 200/351 lr 0.100000 loss 0.4340 (0.3620) acc@1 0.8438 (0.8739) acc@5 1.0000 (0.9962)\n",
      "\u001b[32m[2020-06-11 08:49:12] __main__ INFO: \u001b[0mEpoch 26 Step 300/351 lr 0.100000 loss 0.2747 (0.3713) acc@1 0.8672 (0.8720) acc@5 1.0000 (0.9959)\n",
      "\u001b[32m[2020-06-11 08:51:21] __main__ INFO: \u001b[0mEpoch 26 Step 351/351 lr 0.100000 loss 0.3944 (0.3719) acc@1 0.8516 (0.8719) acc@5 0.9922 (0.9959)\n",
      "\u001b[32m[2020-06-11 08:51:21] __main__ INFO: \u001b[0mElapsed 888.61\n",
      "\u001b[32m[2020-06-11 08:51:21] __main__ INFO: \u001b[0mVal 26\n",
      "\u001b[32m[2020-06-11 08:51:52] __main__ INFO: \u001b[0mEpoch 26 loss 0.5828 acc@1 0.8164 acc@5 0.9882\n",
      "\u001b[32m[2020-06-11 08:51:52] __main__ INFO: \u001b[0mElapsed 30.83\n",
      "\u001b[32m[2020-06-11 08:51:52] __main__ INFO: \u001b[0mTrain 27 9126\n",
      "\u001b[32m[2020-06-11 08:56:05] __main__ INFO: \u001b[0mEpoch 27 Step 100/351 lr 0.100000 loss 0.3483 (0.3554) acc@1 0.8516 (0.8760) acc@5 1.0000 (0.9963)\n",
      "\u001b[32m[2020-06-11 09:00:18] __main__ INFO: \u001b[0mEpoch 27 Step 200/351 lr 0.100000 loss 0.3731 (0.3606) acc@1 0.8516 (0.8757) acc@5 0.9922 (0.9959)\n",
      "\u001b[32m[2020-06-11 09:04:31] __main__ INFO: \u001b[0mEpoch 27 Step 300/351 lr 0.100000 loss 0.3102 (0.3663) acc@1 0.8984 (0.8739) acc@5 1.0000 (0.9959)\n",
      "\u001b[32m[2020-06-11 09:06:40] __main__ INFO: \u001b[0mEpoch 27 Step 351/351 lr 0.100000 loss 0.4180 (0.3682) acc@1 0.8984 (0.8730) acc@5 0.9922 (0.9959)\n",
      "\u001b[32m[2020-06-11 09:06:40] __main__ INFO: \u001b[0mElapsed 888.76\n",
      "\u001b[32m[2020-06-11 09:06:40] __main__ INFO: \u001b[0mVal 27\n",
      "\u001b[32m[2020-06-11 09:07:11] __main__ INFO: \u001b[0mEpoch 27 loss 0.5387 acc@1 0.8220 acc@5 0.9908\n",
      "\u001b[32m[2020-06-11 09:07:11] __main__ INFO: \u001b[0mElapsed 30.76\n",
      "\u001b[32m[2020-06-11 09:07:11] __main__ INFO: \u001b[0mTrain 28 9477\n",
      "\u001b[32m[2020-06-11 09:11:24] __main__ INFO: \u001b[0mEpoch 28 Step 100/351 lr 0.100000 loss 0.4038 (0.3562) acc@1 0.8594 (0.8784) acc@5 1.0000 (0.9966)\n",
      "\u001b[32m[2020-06-11 09:15:38] __main__ INFO: \u001b[0mEpoch 28 Step 200/351 lr 0.100000 loss 0.3658 (0.3592) acc@1 0.8594 (0.8777) acc@5 1.0000 (0.9967)\n",
      "\u001b[32m[2020-06-11 09:19:51] __main__ INFO: \u001b[0mEpoch 28 Step 300/351 lr 0.100000 loss 0.3938 (0.3639) acc@1 0.8516 (0.8764) acc@5 1.0000 (0.9964)\n",
      "\u001b[32m[2020-06-11 09:22:00] __main__ INFO: \u001b[0mEpoch 28 Step 351/351 lr 0.100000 loss 0.3072 (0.3657) acc@1 0.8984 (0.8759) acc@5 1.0000 (0.9963)\n",
      "\u001b[32m[2020-06-11 09:22:00] __main__ INFO: \u001b[0mElapsed 888.89\n",
      "\u001b[32m[2020-06-11 09:22:00] __main__ INFO: \u001b[0mVal 28\n",
      "\u001b[32m[2020-06-11 09:22:31] __main__ INFO: \u001b[0mEpoch 28 loss 0.5050 acc@1 0.8332 acc@5 0.9936\n",
      "\u001b[32m[2020-06-11 09:22:31] __main__ INFO: \u001b[0mElapsed 30.76\n",
      "\u001b[32m[2020-06-11 09:22:31] __main__ INFO: \u001b[0mTrain 29 9828\n",
      "\u001b[32m[2020-06-11 09:26:44] __main__ INFO: \u001b[0mEpoch 29 Step 100/351 lr 0.100000 loss 0.2961 (0.3513) acc@1 0.8984 (0.8796) acc@5 1.0000 (0.9967)\n",
      "\u001b[32m[2020-06-11 09:30:57] __main__ INFO: \u001b[0mEpoch 29 Step 200/351 lr 0.100000 loss 0.3635 (0.3567) acc@1 0.8984 (0.8770) acc@5 0.9922 (0.9963)\n",
      "\u001b[32m[2020-06-11 09:35:11] __main__ INFO: \u001b[0mEpoch 29 Step 300/351 lr 0.100000 loss 0.3451 (0.3587) acc@1 0.8828 (0.8757) acc@5 1.0000 (0.9962)\n",
      "\u001b[32m[2020-06-11 09:37:20] __main__ INFO: \u001b[0mEpoch 29 Step 351/351 lr 0.100000 loss 0.4112 (0.3590) acc@1 0.8594 (0.8765) acc@5 1.0000 (0.9961)\n",
      "\u001b[32m[2020-06-11 09:37:20] __main__ INFO: \u001b[0mElapsed 889.02\n",
      "\u001b[32m[2020-06-11 09:37:20] __main__ INFO: \u001b[0mVal 29\n",
      "\u001b[32m[2020-06-11 09:37:51] __main__ INFO: \u001b[0mEpoch 29 loss 0.5551 acc@1 0.8242 acc@5 0.9916\n",
      "\u001b[32m[2020-06-11 09:37:51] __main__ INFO: \u001b[0mElapsed 30.80\n",
      "\u001b[32m[2020-06-11 09:37:51] __main__ INFO: \u001b[0mTrain 30 10179\n",
      "\u001b[32m[2020-06-11 09:42:04] __main__ INFO: \u001b[0mEpoch 30 Step 100/351 lr 0.100000 loss 0.2345 (0.3301) acc@1 0.9453 (0.8854) acc@5 1.0000 (0.9955)\n",
      "\u001b[32m[2020-06-11 09:46:17] __main__ INFO: \u001b[0mEpoch 30 Step 200/351 lr 0.100000 loss 0.2774 (0.3461) acc@1 0.9062 (0.8799) acc@5 0.9922 (0.9959)\n",
      "\u001b[32m[2020-06-11 09:50:31] __main__ INFO: \u001b[0mEpoch 30 Step 300/351 lr 0.100000 loss 0.4189 (0.3496) acc@1 0.8672 (0.8783) acc@5 0.9844 (0.9959)\n",
      "\u001b[32m[2020-06-11 09:52:40] __main__ INFO: \u001b[0mEpoch 30 Step 351/351 lr 0.100000 loss 0.4034 (0.3510) acc@1 0.8594 (0.8772) acc@5 1.0000 (0.9961)\n",
      "\u001b[32m[2020-06-11 09:52:40] __main__ INFO: \u001b[0mElapsed 889.22\n",
      "\u001b[32m[2020-06-11 09:52:40] __main__ INFO: \u001b[0mVal 30\n",
      "\u001b[32m[2020-06-11 09:53:11] __main__ INFO: \u001b[0mEpoch 30 loss 0.6655 acc@1 0.7936 acc@5 0.9890\n",
      "\u001b[32m[2020-06-11 09:53:11] __main__ INFO: \u001b[0mElapsed 30.79\n",
      "\u001b[32m[2020-06-11 09:53:11] __main__ INFO: \u001b[0mTrain 31 10530\n",
      "\u001b[32m[2020-06-11 09:57:24] __main__ INFO: \u001b[0mEpoch 31 Step 100/351 lr 0.100000 loss 0.4329 (0.3471) acc@1 0.8359 (0.8804) acc@5 0.9844 (0.9959)\n",
      "\u001b[32m[2020-06-11 10:01:37] __main__ INFO: \u001b[0mEpoch 31 Step 200/351 lr 0.100000 loss 0.3282 (0.3495) acc@1 0.8750 (0.8785) acc@5 1.0000 (0.9957)\n",
      "\u001b[32m[2020-06-11 10:05:51] __main__ INFO: \u001b[0mEpoch 31 Step 300/351 lr 0.100000 loss 0.3112 (0.3533) acc@1 0.8906 (0.8771) acc@5 1.0000 (0.9955)\n",
      "\u001b[32m[2020-06-11 10:08:00] __main__ INFO: \u001b[0mEpoch 31 Step 351/351 lr 0.100000 loss 0.4177 (0.3533) acc@1 0.8672 (0.8773) acc@5 1.0000 (0.9955)\n",
      "\u001b[32m[2020-06-11 10:08:00] __main__ INFO: \u001b[0mElapsed 889.30\n",
      "\u001b[32m[2020-06-11 10:08:00] __main__ INFO: \u001b[0mVal 31\n",
      "\u001b[32m[2020-06-11 10:08:31] __main__ INFO: \u001b[0mEpoch 31 loss 0.5625 acc@1 0.8126 acc@5 0.9896\n",
      "\u001b[32m[2020-06-11 10:08:31] __main__ INFO: \u001b[0mElapsed 30.84\n",
      "\u001b[32m[2020-06-11 10:08:31] __main__ INFO: \u001b[0mTrain 32 10881\n",
      "\u001b[32m[2020-06-11 10:12:44] __main__ INFO: \u001b[0mEpoch 32 Step 100/351 lr 0.100000 loss 0.2561 (0.3369) acc@1 0.8906 (0.8856) acc@5 1.0000 (0.9962)\n",
      "\u001b[32m[2020-06-11 10:16:57] __main__ INFO: \u001b[0mEpoch 32 Step 200/351 lr 0.100000 loss 0.2196 (0.3421) acc@1 0.9375 (0.8830) acc@5 1.0000 (0.9961)\n",
      "\u001b[32m[2020-06-11 10:21:10] __main__ INFO: \u001b[0mEpoch 32 Step 300/351 lr 0.100000 loss 0.3200 (0.3463) acc@1 0.8750 (0.8812) acc@5 1.0000 (0.9962)\n",
      "\u001b[32m[2020-06-11 10:23:20] __main__ INFO: \u001b[0mEpoch 32 Step 351/351 lr 0.100000 loss 0.2570 (0.3464) acc@1 0.9141 (0.8809) acc@5 0.9922 (0.9962)\n",
      "\u001b[32m[2020-06-11 10:23:20] __main__ INFO: \u001b[0mElapsed 888.91\n",
      "\u001b[32m[2020-06-11 10:23:20] __main__ INFO: \u001b[0mVal 32\n",
      "\u001b[32m[2020-06-11 10:23:50] __main__ INFO: \u001b[0mEpoch 32 loss 0.5377 acc@1 0.8252 acc@5 0.9898\n",
      "\u001b[32m[2020-06-11 10:23:50] __main__ INFO: \u001b[0mElapsed 30.83\n",
      "\u001b[32m[2020-06-11 10:23:50] __main__ INFO: \u001b[0mTrain 33 11232\n",
      "\u001b[32m[2020-06-11 10:28:04] __main__ INFO: \u001b[0mEpoch 33 Step 100/351 lr 0.100000 loss 0.3582 (0.3403) acc@1 0.8828 (0.8837) acc@5 1.0000 (0.9967)\n",
      "\u001b[32m[2020-06-11 10:32:17] __main__ INFO: \u001b[0mEpoch 33 Step 200/351 lr 0.100000 loss 0.3799 (0.3369) acc@1 0.8750 (0.8845) acc@5 1.0000 (0.9970)\n",
      "\u001b[32m[2020-06-11 10:36:30] __main__ INFO: \u001b[0mEpoch 33 Step 300/351 lr 0.100000 loss 0.4651 (0.3396) acc@1 0.8203 (0.8828) acc@5 1.0000 (0.9971)\n",
      "\u001b[32m[2020-06-11 10:38:39] __main__ INFO: \u001b[0mEpoch 33 Step 351/351 lr 0.100000 loss 0.3390 (0.3397) acc@1 0.8828 (0.8832) acc@5 1.0000 (0.9971)\n",
      "\u001b[32m[2020-06-11 10:38:39] __main__ INFO: \u001b[0mElapsed 888.76\n",
      "\u001b[32m[2020-06-11 10:38:39] __main__ INFO: \u001b[0mVal 33\n",
      "\u001b[32m[2020-06-11 10:39:10] __main__ INFO: \u001b[0mEpoch 33 loss 0.5936 acc@1 0.8142 acc@5 0.9886\n",
      "\u001b[32m[2020-06-11 10:39:10] __main__ INFO: \u001b[0mElapsed 30.82\n",
      "\u001b[32m[2020-06-11 10:39:10] __main__ INFO: \u001b[0mTrain 34 11583\n",
      "\u001b[32m[2020-06-11 10:43:23] __main__ INFO: \u001b[0mEpoch 34 Step 100/351 lr 0.100000 loss 0.4218 (0.3236) acc@1 0.8906 (0.8881) acc@5 1.0000 (0.9972)\n",
      "\u001b[32m[2020-06-11 10:47:36] __main__ INFO: \u001b[0mEpoch 34 Step 200/351 lr 0.100000 loss 0.3122 (0.3252) acc@1 0.8906 (0.8880) acc@5 1.0000 (0.9969)\n",
      "\u001b[32m[2020-06-11 10:51:49] __main__ INFO: \u001b[0mEpoch 34 Step 300/351 lr 0.100000 loss 0.4835 (0.3302) acc@1 0.8125 (0.8865) acc@5 1.0000 (0.9966)\n",
      "\u001b[32m[2020-06-11 10:53:59] __main__ INFO: \u001b[0mEpoch 34 Step 351/351 lr 0.100000 loss 0.2695 (0.3339) acc@1 0.9141 (0.8856) acc@5 1.0000 (0.9965)\n",
      "\u001b[32m[2020-06-11 10:53:59] __main__ INFO: \u001b[0mElapsed 888.54\n",
      "\u001b[32m[2020-06-11 10:53:59] __main__ INFO: \u001b[0mVal 34\n",
      "\u001b[32m[2020-06-11 10:54:29] __main__ INFO: \u001b[0mEpoch 34 loss 0.6493 acc@1 0.8074 acc@5 0.9888\n",
      "\u001b[32m[2020-06-11 10:54:29] __main__ INFO: \u001b[0mElapsed 30.85\n",
      "\u001b[32m[2020-06-11 10:54:29] __main__ INFO: \u001b[0mTrain 35 11934\n",
      "\u001b[32m[2020-06-11 10:58:43] __main__ INFO: \u001b[0mEpoch 35 Step 100/351 lr 0.100000 loss 0.2984 (0.3154) acc@1 0.9141 (0.8909) acc@5 1.0000 (0.9966)\n",
      "\u001b[32m[2020-06-11 11:02:56] __main__ INFO: \u001b[0mEpoch 35 Step 200/351 lr 0.100000 loss 0.2475 (0.3263) acc@1 0.9141 (0.8873) acc@5 1.0000 (0.9968)\n",
      "\u001b[32m[2020-06-11 11:07:09] __main__ INFO: \u001b[0mEpoch 35 Step 300/351 lr 0.100000 loss 0.4357 (0.3379) acc@1 0.8594 (0.8837) acc@5 1.0000 (0.9964)\n",
      "\u001b[32m[2020-06-11 11:09:18] __main__ INFO: \u001b[0mEpoch 35 Step 351/351 lr 0.100000 loss 0.3241 (0.3391) acc@1 0.8594 (0.8830) acc@5 1.0000 (0.9965)\n",
      "\u001b[32m[2020-06-11 11:09:18] __main__ INFO: \u001b[0mElapsed 888.89\n",
      "\u001b[32m[2020-06-11 11:09:18] __main__ INFO: \u001b[0mVal 35\n",
      "\u001b[32m[2020-06-11 11:09:49] __main__ INFO: \u001b[0mEpoch 35 loss 0.6967 acc@1 0.7928 acc@5 0.9898\n",
      "\u001b[32m[2020-06-11 11:09:49] __main__ INFO: \u001b[0mElapsed 30.84\n",
      "\u001b[32m[2020-06-11 11:09:49] __main__ INFO: \u001b[0mTrain 36 12285\n",
      "\u001b[32m[2020-06-11 11:14:02] __main__ INFO: \u001b[0mEpoch 36 Step 100/351 lr 0.100000 loss 0.3654 (0.3148) acc@1 0.8359 (0.8902) acc@5 1.0000 (0.9971)\n",
      "\u001b[32m[2020-06-11 11:18:16] __main__ INFO: \u001b[0mEpoch 36 Step 200/351 lr 0.100000 loss 0.4236 (0.3214) acc@1 0.8828 (0.8886) acc@5 1.0000 (0.9970)\n",
      "\u001b[32m[2020-06-11 11:22:29] __main__ INFO: \u001b[0mEpoch 36 Step 300/351 lr 0.100000 loss 0.2529 (0.3277) acc@1 0.9141 (0.8857) acc@5 1.0000 (0.9967)\n",
      "\u001b[32m[2020-06-11 11:24:38] __main__ INFO: \u001b[0mEpoch 36 Step 351/351 lr 0.100000 loss 0.3462 (0.3290) acc@1 0.8828 (0.8856) acc@5 1.0000 (0.9966)\n",
      "\u001b[32m[2020-06-11 11:24:38] __main__ INFO: \u001b[0mElapsed 888.63\n",
      "\u001b[32m[2020-06-11 11:24:38] __main__ INFO: \u001b[0mVal 36\n",
      "\u001b[32m[2020-06-11 11:25:09] __main__ INFO: \u001b[0mEpoch 36 loss 0.5111 acc@1 0.8364 acc@5 0.9924\n",
      "\u001b[32m[2020-06-11 11:25:09] __main__ INFO: \u001b[0mElapsed 30.84\n",
      "\u001b[32m[2020-06-11 11:25:09] __main__ INFO: \u001b[0mTrain 37 12636\n",
      "\u001b[32m[2020-06-11 11:29:22] __main__ INFO: \u001b[0mEpoch 37 Step 100/351 lr 0.100000 loss 0.2958 (0.3188) acc@1 0.8828 (0.8880) acc@5 0.9922 (0.9971)\n",
      "\u001b[32m[2020-06-11 11:33:35] __main__ INFO: \u001b[0mEpoch 37 Step 200/351 lr 0.100000 loss 0.3222 (0.3288) acc@1 0.8828 (0.8848) acc@5 0.9922 (0.9965)\n",
      "\u001b[32m[2020-06-11 11:37:48] __main__ INFO: \u001b[0mEpoch 37 Step 300/351 lr 0.100000 loss 0.2737 (0.3381) acc@1 0.9062 (0.8830) acc@5 1.0000 (0.9964)\n",
      "\u001b[32m[2020-06-11 11:39:57] __main__ INFO: \u001b[0mEpoch 37 Step 351/351 lr 0.100000 loss 0.3670 (0.3387) acc@1 0.8594 (0.8827) acc@5 1.0000 (0.9965)\n",
      "\u001b[32m[2020-06-11 11:39:57] __main__ INFO: \u001b[0mElapsed 888.64\n",
      "\u001b[32m[2020-06-11 11:39:57] __main__ INFO: \u001b[0mVal 37\n",
      "\u001b[32m[2020-06-11 11:40:28] __main__ INFO: \u001b[0mEpoch 37 loss 0.7566 acc@1 0.7740 acc@5 0.9870\n",
      "\u001b[32m[2020-06-11 11:40:28] __main__ INFO: \u001b[0mElapsed 30.84\n",
      "\u001b[32m[2020-06-11 11:40:28] __main__ INFO: \u001b[0mTrain 38 12987\n",
      "\u001b[32m[2020-06-11 11:44:41] __main__ INFO: \u001b[0mEpoch 38 Step 100/351 lr 0.100000 loss 0.4581 (0.3109) acc@1 0.8672 (0.8944) acc@5 0.9922 (0.9966)\n",
      "\u001b[32m[2020-06-11 11:48:55] __main__ INFO: \u001b[0mEpoch 38 Step 200/351 lr 0.100000 loss 0.4295 (0.3161) acc@1 0.8281 (0.8926) acc@5 0.9922 (0.9964)\n",
      "\u001b[32m[2020-06-11 11:53:08] __main__ INFO: \u001b[0mEpoch 38 Step 300/351 lr 0.100000 loss 0.2849 (0.3252) acc@1 0.9062 (0.8886) acc@5 1.0000 (0.9964)\n",
      "\u001b[32m[2020-06-11 11:55:17] __main__ INFO: \u001b[0mEpoch 38 Step 351/351 lr 0.100000 loss 0.2119 (0.3281) acc@1 0.9375 (0.8871) acc@5 1.0000 (0.9963)\n",
      "\u001b[32m[2020-06-11 11:55:17] __main__ INFO: \u001b[0mElapsed 889.26\n",
      "\u001b[32m[2020-06-11 11:55:17] __main__ INFO: \u001b[0mVal 38\n",
      "\u001b[32m[2020-06-11 11:55:48] __main__ INFO: \u001b[0mEpoch 38 loss 0.4752 acc@1 0.8450 acc@5 0.9908\n",
      "\u001b[32m[2020-06-11 11:55:48] __main__ INFO: \u001b[0mElapsed 30.83\n",
      "\u001b[32m[2020-06-11 11:55:48] __main__ INFO: \u001b[0mTrain 39 13338\n",
      "\u001b[32m[2020-06-11 12:00:02] __main__ INFO: \u001b[0mEpoch 39 Step 100/351 lr 0.100000 loss 0.2954 (0.3112) acc@1 0.8906 (0.8919) acc@5 1.0000 (0.9972)\n",
      "\u001b[32m[2020-06-11 12:04:15] __main__ INFO: \u001b[0mEpoch 39 Step 200/351 lr 0.100000 loss 0.2913 (0.3231) acc@1 0.9141 (0.8880) acc@5 0.9922 (0.9971)\n",
      "\u001b[32m[2020-06-11 12:08:29] __main__ INFO: \u001b[0mEpoch 39 Step 300/351 lr 0.100000 loss 0.4231 (0.3267) acc@1 0.8750 (0.8871) acc@5 0.9922 (0.9967)\n",
      "\u001b[32m[2020-06-11 12:10:38] __main__ INFO: \u001b[0mEpoch 39 Step 351/351 lr 0.100000 loss 0.3540 (0.3291) acc@1 0.8828 (0.8860) acc@5 1.0000 (0.9965)\n",
      "\u001b[32m[2020-06-11 12:10:38] __main__ INFO: \u001b[0mElapsed 890.21\n",
      "\u001b[32m[2020-06-11 12:10:38] __main__ INFO: \u001b[0mVal 39\n",
      "\u001b[32m[2020-06-11 12:11:09] __main__ INFO: \u001b[0mEpoch 39 loss 0.4352 acc@1 0.8528 acc@5 0.9954\n",
      "\u001b[32m[2020-06-11 12:11:09] __main__ INFO: \u001b[0mElapsed 30.86\n",
      "\u001b[32m[2020-06-11 12:11:09] __main__ INFO: \u001b[0mTrain 40 13689\n",
      "\u001b[32m[2020-06-11 12:15:23] __main__ INFO: \u001b[0mEpoch 40 Step 100/351 lr 0.100000 loss 0.3083 (0.2953) acc@1 0.8906 (0.8988) acc@5 0.9922 (0.9980)\n",
      "\u001b[32m[2020-06-11 12:19:37] __main__ INFO: \u001b[0mEpoch 40 Step 200/351 lr 0.100000 loss 0.2183 (0.3117) acc@1 0.9297 (0.8929) acc@5 1.0000 (0.9974)\n",
      "\u001b[32m[2020-06-11 12:23:50] __main__ INFO: \u001b[0mEpoch 40 Step 300/351 lr 0.100000 loss 0.2783 (0.3150) acc@1 0.9062 (0.8929) acc@5 0.9922 (0.9973)\n",
      "\u001b[32m[2020-06-11 12:26:00] __main__ INFO: \u001b[0mEpoch 40 Step 351/351 lr 0.100000 loss 0.2936 (0.3191) acc@1 0.8828 (0.8913) acc@5 1.0000 (0.9970)\n",
      "\u001b[32m[2020-06-11 12:26:00] __main__ INFO: \u001b[0mElapsed 890.64\n",
      "\u001b[32m[2020-06-11 12:26:00] __main__ INFO: \u001b[0mVal 40\n",
      "\u001b[32m[2020-06-11 12:26:31] __main__ INFO: \u001b[0mEpoch 40 loss 0.4619 acc@1 0.8424 acc@5 0.9954\n",
      "\u001b[32m[2020-06-11 12:26:31] __main__ INFO: \u001b[0mElapsed 30.90\n",
      "\u001b[32m[2020-06-11 12:26:31] __main__ INFO: \u001b[0mTrain 41 14040\n",
      "\u001b[32m[2020-06-11 12:30:45] __main__ INFO: \u001b[0mEpoch 41 Step 100/351 lr 0.100000 loss 0.3661 (0.3062) acc@1 0.8828 (0.8939) acc@5 0.9844 (0.9976)\n",
      "\u001b[32m[2020-06-11 12:34:59] __main__ INFO: \u001b[0mEpoch 41 Step 200/351 lr 0.100000 loss 0.2150 (0.3093) acc@1 0.9141 (0.8939) acc@5 1.0000 (0.9971)\n"
     ]
    }
   ],
   "source": [
    "# Train the model per the settings specified for ResNext 29_4x64d in the original paper\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "!python train.py --config configs/cifar/resnext.yaml \\\n",
    "    model.resnext.cardinality 8 \\\n",
    "    train.batch_size 128 \\\n",
    "    train.base_lr 0.1 \\\n",
    "    train.output_dir /home/ec2-user/SageMaker/experiments/resnext_29_8x64d/exp00 \\\n",
    "    scheduler.epochs 300\n",
    "\n",
    "# Number of epochs should be 300!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-14 17:22:43] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/resnext_29_8x64d/exp00/checkpoint_00300.pth\n",
      "Files already downloaded and verified\n",
      "100%|███████████████████████████████████████████| 79/79 [01:02<00:00,  1.27it/s]\n",
      "\u001b[32m[2020-06-14 17:23:46] __main__ INFO: \u001b[0mElapsed 62.31\n",
      "\u001b[32m[2020-06-14 17:23:46] __main__ INFO: \u001b[0mLoss 0.1457 Accuracy 0.9568\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/resnext.yaml \\\n",
    "   model.resnext.cardinality 8 \\\n",
    "   test.batch_size 128 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/resnext_29_8x64d/exp00/checkpoint_00300.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/resnext_29_8x64d/exp00/test_results_0300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-14 17:24:16] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/resnext_29_8x64d/exp00/checkpoint_00200.pth\n",
      "Files already downloaded and verified\n",
      "100%|███████████████████████████████████████████| 79/79 [01:02<00:00,  1.26it/s]\n",
      "\u001b[32m[2020-06-14 17:25:20] __main__ INFO: \u001b[0mElapsed 62.76\n",
      "\u001b[32m[2020-06-14 17:25:20] __main__ INFO: \u001b[0mLoss 0.2246 Accuracy 0.9324\n"
     ]
    }
   ],
   "source": [
    "!python evaluate.py --config configs/cifar/resnext.yaml \\\n",
    "   model.resnext.cardinality 8 \\\n",
    "   test.batch_size 128 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/resnext_29_8x64d/exp00/checkpoint_00200.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/resnext_29_8x64d/exp00/test_results_0200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-14 17:25:30] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/resnext_29_8x64d/exp00/checkpoint_00100.pth\n",
      "Files already downloaded and verified\n",
      "100%|███████████████████████████████████████████| 79/79 [01:03<00:00,  1.25it/s]\n",
      "\u001b[32m[2020-06-14 17:26:34] __main__ INFO: \u001b[0mElapsed 63.14\n",
      "\u001b[32m[2020-06-14 17:26:34] __main__ INFO: \u001b[0mLoss 0.4673 Accuracy 0.8432\n"
     ]
    }
   ],
   "source": [
    "!python evaluate.py --config configs/cifar/resnext.yaml \\\n",
    "   model.resnext.cardinality 8 \\\n",
    "   test.batch_size 128 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/resnext_29_8x64d/exp00/checkpoint_00100.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/resnext_29_8x64d/exp00/test_results_0100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-14 17:34:46] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/resnext_29_8x64d/exp00/checkpoint_00300.pth\n",
      "CIFAR 10.1\n",
      "100%|███████████████████████████████████████████| 16/16 [00:12<00:00,  1.26it/s]\n",
      "\u001b[32m[2020-06-14 17:34:59] __main__ INFO: \u001b[0mElapsed 12.72\n",
      "\u001b[32m[2020-06-14 17:34:59] __main__ INFO: \u001b[0mLoss 0.3615 Accuracy 0.8930\n"
     ]
    }
   ],
   "source": [
    "!python evaluate.py --config configs/cifar/resnext.yaml \\\n",
    "   model.resnext.cardinality 8 \\\n",
    "   test.batch_size 128 \\\n",
    "   dataset.name CIFAR101 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/resnext_29_8x64d/exp00/checkpoint_00300.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/resnext_29_8x64d/exp00/test_results_0300_CIFAR101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Testset</th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Original_Accuracy</th>\n",
       "      <th>Original_CI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>resnext_29_8x64d</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>100</td>\n",
       "      <td>0.4673</td>\n",
       "      <td>0.8432</td>\n",
       "      <td>96.2</td>\n",
       "      <td>(95.8, 96.6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>resnext_29_8x64d</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>200</td>\n",
       "      <td>0.2246</td>\n",
       "      <td>0.9324</td>\n",
       "      <td>96.2</td>\n",
       "      <td>(95.8, 96.6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>resnext_29_8x64d</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>300</td>\n",
       "      <td>0.1457</td>\n",
       "      <td>0.9568</td>\n",
       "      <td>96.2</td>\n",
       "      <td>(95.8, 96.6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>resnext_29_8x64d</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>300</td>\n",
       "      <td>0.3615</td>\n",
       "      <td>0.8930</td>\n",
       "      <td>90.0</td>\n",
       "      <td>(88.6, 91.2)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Model    Testset  Epoch    Loss  Accuracy  Original_Accuracy  \\\n",
       "0  resnext_29_8x64d    cifar10    100  0.4673    0.8432               96.2   \n",
       "1  resnext_29_8x64d    cifar10    200  0.2246    0.9324               96.2   \n",
       "2  resnext_29_8x64d    cifar10    300  0.1457    0.9568               96.2   \n",
       "3  resnext_29_8x64d  cifar10.1    300  0.3615    0.8930               90.0   \n",
       "\n",
       "    Original_CI  \n",
       "0  (95.8, 96.6)  \n",
       "1  (95.8, 96.6)  \n",
       "2  (95.8, 96.6)  \n",
       "3  (88.6, 91.2)  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write the results to a CSV file so that we can analyze later.\n",
    "import pandas as pd\n",
    "\n",
    "results = {'Model': ['resnext_29_8x64d', 'resnext_29_8x64d', 'resnext_29_8x64d', 'resnext_29_8x64d'],\n",
    "           'Testset': ['cifar10', 'cifar10', 'cifar10', 'cifar10.1'],\n",
    "           'Epoch': [100, 200, 300, 300],\n",
    "           'Loss': [0.4673, 0.2246, 0.1457, 0.3615],\n",
    "           'Accuracy': [0.8432, 0.9324, 0.9568, 0.8930],\n",
    "           'Original_Accuracy': [96.2, 96.2, 96.2, 90.0],\n",
    "           'Original_CI': [(95.8, 96.6), (95.8, 96.6), (95.8, 96.6), (88.6, 91.2)]\n",
    "           }\n",
    "\n",
    "df = pd.DataFrame(results, columns = ['Model', 'Testset', 'Epoch', 'Loss', 'Accuracy', \n",
    "                                      'Original_Accuracy', 'Original_CI'])\n",
    "\n",
    "\n",
    "df.to_csv('/home/ec2-user/SageMaker/experiments/resnext_29_8x64d/exp00/results.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['preds', 'probs', 'labels', 'loss', 'acc']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-1.4072659 , -1.7579162 , -0.0632572 , ..., -1.604979  ,\n",
       "        -2.1179168 , -2.3709297 ],\n",
       "       [-0.5318509 ,  2.3948684 , -1.824476  , ..., -1.735688  ,\n",
       "         9.22153   , -1.438901  ],\n",
       "       [-0.8268721 ,  1.7920238 , -2.0168679 , ..., -1.9178003 ,\n",
       "        11.20963   , -1.209595  ],\n",
       "       ...,\n",
       "       [-2.3609014 , -1.2094655 ,  1.1702778 , ..., -0.8153744 ,\n",
       "        -2.0398674 , -1.3504086 ],\n",
       "       [-0.30113205, 10.91866   , -1.1114168 , ..., -2.3269584 ,\n",
       "        -0.7837641 , -2.0046387 ],\n",
       "       [-1.7235601 , -1.396374  , -1.2323154 , ..., 13.169274  ,\n",
       "        -2.241554  , -1.0829642 ]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Peak inside the output file for predictions\n",
    "import numpy as np\n",
    "output = '/home/ec2-user/SageMaker/experiments/resnext_29_8x64d/exp00/test_results_0300/predictions.npz'\n",
    "npzfile = np.load(output)\n",
    "print(npzfile.files)\n",
    "npzfile['preds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the model checkpoints, configs, and results to S3 \n",
    "bucket='sagemaker-may29'\n",
    "prefix = 'sagemaker/results/original-models/resnext_29_8x64d'\n",
    "path = '/home/ec2-user/SageMaker/experiments/resnext_29_8x64d'\n",
    "\n",
    "s3_resource = boto3.resource(\"s3\", region_name=\"us-east-2\")\n",
    "\n",
    "def uploadDirectory(local_path,bucket_name,s3_prefix):\n",
    "\n",
    "    my_bucket = s3_resource.Bucket(bucket_name)\n",
    "    \n",
    "    for path, subdirs, files in os.walk(local_path):\n",
    "        path = path.replace(\"\\\\\",\"/\")\n",
    "        directory_name = path.replace(local_path,\"\")\n",
    "        for file in files:\n",
    "            #print(\"Local File:\", os.path.join(path, file))\n",
    "            #print(\"      Dest:\", s3_prefix+directory_name+'/'+file)\n",
    "            my_bucket.upload_file(os.path.join(path, file), s3_prefix+directory_name+'/'+file)\n",
    "    \n",
    "uploadDirectory(path,bucket,prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
