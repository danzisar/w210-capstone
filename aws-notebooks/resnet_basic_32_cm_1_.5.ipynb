{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESNET 32\n",
    "\n",
    " - Training Dataset:  CutMix, beta=1, cutmix_prob=0.5\n",
    " - Sagemaker Notebook must be of type, conda_pytorch_p36\n",
    " \n",
    "#### Install Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import numpy \n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 1)) (1.18.1)\n",
      "Requirement already satisfied: torch>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 2)) (1.4.0)\n",
      "Requirement already satisfied: torchvision in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (0.5.0)\n",
      "Requirement already satisfied: fvcore in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (0.1.1.post20200704)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 5)) (4.44.1)\n",
      "Requirement already satisfied: yacs in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 6)) (0.1.7)\n",
      "Requirement already satisfied: apex from git+https://github.com/NVIDIA/apex.git#egg=apex in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 7)) (0.1)\n",
      "Requirement already satisfied: termcolor in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 8)) (1.1.0)\n",
      "Requirement already satisfied: thop<0.0.31.post2004070130 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 9)) (0.0.31.post2001170342)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (1.14.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (7.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (5.3.1)\n",
      "Requirement already satisfied: tabulate in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (0.8.7)\n",
      "Requirement already satisfied: portalocker in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (1.7.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: tensorboard in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (2.2.2)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (3.12.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (46.1.3.post20200330)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.7.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (3.2.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (0.9.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.0.1)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (0.34.2)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.18.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.14.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (2.23.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.18.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.30.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (0.4.1)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard) (1.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (3.4.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (4.1.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (2020.4.5.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (3.0.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard) (2.2.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r '/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt'\n",
    "\n",
    "# Need to add this to requirements.txt\n",
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-10 17:10:32] __main__ INFO: \u001b[0mdevice: cuda\n",
      "cudnn:\n",
      "  benchmark: True\n",
      "  deterministic: False\n",
      "dataset:\n",
      "  name: CIFAR10_CM_1\n",
      "  dataset_dir: ''\n",
      "  image_size: 32\n",
      "  n_channels: 3\n",
      "  n_classes: 10\n",
      "model:\n",
      "  type: cifar\n",
      "  name: resnet\n",
      "  init_mode: kaiming_fan_out\n",
      "  vgg:\n",
      "    n_channels: [64, 128, 256, 512, 512]\n",
      "    n_layers: [2, 2, 3, 3, 3]\n",
      "    use_bn: True\n",
      "  resnet:\n",
      "    depth: 32\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "  resnet_preact:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "  wrn:\n",
      "    depth: 28\n",
      "    initial_channels: 16\n",
      "    widening_factor: 10\n",
      "    drop_rate: 0.0\n",
      "  densenet:\n",
      "    depth: 100\n",
      "    n_blocks: [6, 12, 24, 16]\n",
      "    block_type: bottleneck\n",
      "    growth_rate: 12\n",
      "    drop_rate: 0.0\n",
      "    compression_rate: 0.5\n",
      "  pyramidnet:\n",
      "    depth: 272\n",
      "    n_blocks: [3, 24, 36, 3]\n",
      "    initial_channels: 16\n",
      "    block_type: bottleneck\n",
      "    alpha: 200\n",
      "  resnext:\n",
      "    depth: 29\n",
      "    n_blocks: [3, 4, 6, 3]\n",
      "    initial_channels: 64\n",
      "    cardinality: 8\n",
      "    base_channels: 4\n",
      "  shake_shake:\n",
      "    depth: 26\n",
      "    initial_channels: 96\n",
      "    shake_forward: True\n",
      "    shake_backward: True\n",
      "    shake_image: True\n",
      "  se_resnet_preact:\n",
      "    depth: 110\n",
      "    initial_channels: 16\n",
      "    se_reduction: 16\n",
      "    block_type: basic\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "train:\n",
      "  checkpoint: ''\n",
      "  resume: False\n",
      "  precision: O0\n",
      "  batch_size: 128\n",
      "  subdivision: 1\n",
      "  optimizer: sgd\n",
      "  base_lr: 0.1\n",
      "  momentum: 0.9\n",
      "  nesterov: True\n",
      "  weight_decay: 0.0001\n",
      "  no_weight_decay_on_bn: False\n",
      "  gradient_clip: 0\n",
      "  start_epoch: 0\n",
      "  seed: 0\n",
      "  val_first: True\n",
      "  val_period: 1\n",
      "  val_ratio: 0.1\n",
      "  use_test_as_val: False\n",
      "  output_dir: /home/ec2-user/SageMaker/experiments/resnet_basic_32_cm_1_.11111/exp00\n",
      "  log_period: 100\n",
      "  checkpoint_period: 100\n",
      "  use_tensorboard: True\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: True\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "  distributed: False\n",
      "  dist:\n",
      "    backend: nccl\n",
      "    init_method: env://\n",
      "    world_size: -1\n",
      "    node_rank: -1\n",
      "    local_rank: 0\n",
      "    use_sync_bn: False\n",
      "tensorboard:\n",
      "  train_images: False\n",
      "  val_images: False\n",
      "  model_params: False\n",
      "optim:\n",
      "  adam:\n",
      "    betas: (0.9, 0.999)\n",
      "  lars:\n",
      "    eps: 1e-09\n",
      "    threshold: 0.01\n",
      "  adabound:\n",
      "    betas: (0.9, 0.999)\n",
      "    final_lr: 0.1\n",
      "    gamma: 0.001\n",
      "scheduler:\n",
      "  epochs: 1\n",
      "  warmup:\n",
      "    type: none\n",
      "    epochs: 0\n",
      "    start_factor: 0.001\n",
      "    exponent: 4\n",
      "  type: multistep\n",
      "  milestones: [80, 120]\n",
      "  lr_decay: 0.1\n",
      "  lr_min_factor: 0.001\n",
      "  T0: 10\n",
      "  T_mul: 1.0\n",
      "validation:\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: False\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "augmentation:\n",
      "  use_random_crop: True\n",
      "  use_random_horizontal_flip: True\n",
      "  use_cutout: False\n",
      "  use_random_erasing: False\n",
      "  use_dual_cutout: False\n",
      "  use_mixup: False\n",
      "  use_ricap: False\n",
      "  use_cutmix: False\n",
      "  use_label_smoothing: False\n",
      "  random_crop:\n",
      "    padding: 4\n",
      "    fill: 0\n",
      "    padding_mode: constant\n",
      "  random_horizontal_flip:\n",
      "    prob: 0.5\n",
      "  cutout:\n",
      "    prob: 1.0\n",
      "    mask_size: 16\n",
      "    cut_inside: False\n",
      "    mask_color: 0\n",
      "    dual_cutout_alpha: 0.1\n",
      "  random_erasing:\n",
      "    prob: 0.5\n",
      "    area_ratio_range: [0.02, 0.4]\n",
      "    min_aspect_ratio: 0.3\n",
      "    max_attempt: 20\n",
      "  mixup:\n",
      "    alpha: 1.0\n",
      "  ricap:\n",
      "    beta: 0.3\n",
      "  cutmix:\n",
      "    alpha: 1.0\n",
      "  label_smoothing:\n",
      "    epsilon: 0.1\n",
      "tta:\n",
      "  use_resize: False\n",
      "  use_center_crop: False\n",
      "  resize: 256\n",
      "test:\n",
      "  checkpoint: ''\n",
      "  output_dir: ''\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    pin_memory: False\n",
      "\u001b[32m[2020-07-10 17:10:32] __main__ INFO: \u001b[0menv_info:\n",
      "  pytorch_version: 1.4.0\n",
      "  cuda_version: 10.1\n",
      "  cudnn_version: 7603\n",
      "  num_gpus: 1\n",
      "  gpu_name: Tesla K80\n",
      "  gpu_capability: 3.7\n",
      "Files already downloaded and verified\n",
      "\u001b[32m[2020-07-10 17:10:55] __main__ INFO: \u001b[0mMACs  : 69.76M\n",
      "\u001b[32m[2020-07-10 17:10:55] __main__ INFO: \u001b[0m#params: 466.91K\n",
      "Selected optimization level O0:  Pure FP32 training.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\",)\n",
      "\u001b[32m[2020-07-10 17:10:55] __main__ INFO: \u001b[0mVal 0\n",
      "\u001b[32m[2020-07-10 17:10:56] __main__ INFO: \u001b[0mEpoch 0 loss 1481.8519 acc@1 0.1050 acc@5 0.5012\n",
      "\u001b[32m[2020-07-10 17:10:56] __main__ INFO: \u001b[0mElapsed 1.45\n",
      "\u001b[32m[2020-07-10 17:10:56] __main__ INFO: \u001b[0mTrain 1 0\n",
      "\u001b[32m[2020-07-10 17:11:06] __main__ INFO: \u001b[0mEpoch 1 Step 100/351 lr 0.100000 loss 2.3133 (2.8535) acc@1 0.1094 (0.0988) acc@5 0.5099 (0.5067)\n",
      "\u001b[32m[2020-07-10 17:11:15] __main__ INFO: \u001b[0mEpoch 1 Step 200/351 lr 0.100000 loss 2.3332 (2.5856) acc@1 0.0774 (0.1010) acc@5 0.4815 (0.5081)\n",
      "\u001b[32m[2020-07-10 17:11:25] __main__ INFO: \u001b[0mEpoch 1 Step 300/351 lr 0.100000 loss 2.3194 (2.4946) acc@1 0.1124 (0.1031) acc@5 0.5530 (0.5112)\n",
      "\u001b[32m[2020-07-10 17:11:29] __main__ INFO: \u001b[0mEpoch 1 Step 351/351 lr 0.100000 loss 2.2538 (2.4634) acc@1 0.1768 (0.1075) acc@5 0.6033 (0.5226)\n",
      "\u001b[32m[2020-07-10 17:11:29] __main__ INFO: \u001b[0mElapsed 32.94\n",
      "\u001b[32m[2020-07-10 17:11:29] __main__ INFO: \u001b[0mVal 1\n",
      "\u001b[32m[2020-07-10 17:11:30] __main__ INFO: \u001b[0mEpoch 1 loss 2.2961 acc@1 0.1806 acc@5 0.6846\n",
      "\u001b[32m[2020-07-10 17:11:30] __main__ INFO: \u001b[0mElapsed 1.07\n",
      "\u001b[32m[2020-07-10 17:11:30] fvcore.common.checkpoint INFO: \u001b[0mSaving checkpoint to /home/ec2-user/SageMaker/experiments/resnet_basic_32_cm_1_.11111/exp00/checkpoint_00001.pth\n"
     ]
    }
   ],
   "source": [
    "# Train the model per the settings specified in the original paper\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "!python train.py --config configs/cifar/resnet.yaml \\\n",
    "    model.resnet.depth 32 \\\n",
    "    train.batch_size 128 \\\n",
    "    dataset.name CIFAR10_CM_1 \\\n",
    "    train.base_lr 0.1 \\\n",
    "    train.output_dir /home/ec2-user/SageMaker/experiments/resnet_basic_32_cm_1_.5/exp00 \\\n",
    "    scheduler.epochs 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Refine the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-10 13:39:42] __main__ INFO: \u001b[0mdevice: cuda\n",
      "cudnn:\n",
      "  benchmark: True\n",
      "  deterministic: False\n",
      "dataset:\n",
      "  name: CIFAR10\n",
      "  dataset_dir: ~/.torch/datasets/CIFAR10\n",
      "  image_size: 32\n",
      "  n_channels: 3\n",
      "  n_classes: 10\n",
      "model:\n",
      "  type: cifar\n",
      "  name: resnet\n",
      "  init_mode: kaiming_fan_out\n",
      "  vgg:\n",
      "    n_channels: [64, 128, 256, 512, 512]\n",
      "    n_layers: [2, 2, 3, 3, 3]\n",
      "    use_bn: True\n",
      "  resnet:\n",
      "    depth: 32\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "  resnet_preact:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "  wrn:\n",
      "    depth: 28\n",
      "    initial_channels: 16\n",
      "    widening_factor: 10\n",
      "    drop_rate: 0.0\n",
      "  densenet:\n",
      "    depth: 100\n",
      "    n_blocks: [6, 12, 24, 16]\n",
      "    block_type: bottleneck\n",
      "    growth_rate: 12\n",
      "    drop_rate: 0.0\n",
      "    compression_rate: 0.5\n",
      "  pyramidnet:\n",
      "    depth: 272\n",
      "    n_blocks: [3, 24, 36, 3]\n",
      "    initial_channels: 16\n",
      "    block_type: bottleneck\n",
      "    alpha: 200\n",
      "  resnext:\n",
      "    depth: 29\n",
      "    n_blocks: [3, 4, 6, 3]\n",
      "    initial_channels: 64\n",
      "    cardinality: 8\n",
      "    base_channels: 4\n",
      "  shake_shake:\n",
      "    depth: 26\n",
      "    initial_channels: 96\n",
      "    shake_forward: True\n",
      "    shake_backward: True\n",
      "    shake_image: True\n",
      "  se_resnet_preact:\n",
      "    depth: 110\n",
      "    initial_channels: 16\n",
      "    se_reduction: 16\n",
      "    block_type: basic\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "train:\n",
      "  checkpoint: /home/ec2-user/SageMaker/experiments/resnet_basic_32_cm_1_1/exp00/checkpoint_00400.pth\n",
      "  resume: False\n",
      "  precision: O0\n",
      "  batch_size: 128\n",
      "  subdivision: 1\n",
      "  optimizer: sgd\n",
      "  base_lr: 0.001\n",
      "  momentum: 0.9\n",
      "  nesterov: True\n",
      "  weight_decay: 0.0001\n",
      "  no_weight_decay_on_bn: False\n",
      "  gradient_clip: 0\n",
      "  start_epoch: 0\n",
      "  seed: 0\n",
      "  val_first: True\n",
      "  val_period: 1\n",
      "  val_ratio: 0.1\n",
      "  use_test_as_val: False\n",
      "  output_dir: /home/ec2-user/SageMaker/experiments/resnet_basic_32_cm_1_1/exp00_resume400_50\n",
      "  log_period: 100\n",
      "  checkpoint_period: 100\n",
      "  use_tensorboard: True\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: True\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "  distributed: False\n",
      "  dist:\n",
      "    backend: nccl\n",
      "    init_method: env://\n",
      "    world_size: -1\n",
      "    node_rank: -1\n",
      "    local_rank: 0\n",
      "    use_sync_bn: False\n",
      "tensorboard:\n",
      "  train_images: False\n",
      "  val_images: False\n",
      "  model_params: False\n",
      "optim:\n",
      "  adam:\n",
      "    betas: (0.9, 0.999)\n",
      "  lars:\n",
      "    eps: 1e-09\n",
      "    threshold: 0.01\n",
      "  adabound:\n",
      "    betas: (0.9, 0.999)\n",
      "    final_lr: 0.1\n",
      "    gamma: 0.001\n",
      "scheduler:\n",
      "  epochs: 50\n",
      "  warmup:\n",
      "    type: none\n",
      "    epochs: 0\n",
      "    start_factor: 0.001\n",
      "    exponent: 4\n",
      "  type: multistep\n",
      "  milestones: [80, 120]\n",
      "  lr_decay: 0.1\n",
      "  lr_min_factor: 0.001\n",
      "  T0: 10\n",
      "  T_mul: 1.0\n",
      "validation:\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: False\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "augmentation:\n",
      "  use_random_crop: True\n",
      "  use_random_horizontal_flip: True\n",
      "  use_cutout: False\n",
      "  use_random_erasing: False\n",
      "  use_dual_cutout: False\n",
      "  use_mixup: False\n",
      "  use_ricap: False\n",
      "  use_cutmix: False\n",
      "  use_label_smoothing: False\n",
      "  random_crop:\n",
      "    padding: 4\n",
      "    fill: 0\n",
      "    padding_mode: constant\n",
      "  random_horizontal_flip:\n",
      "    prob: 0.5\n",
      "  cutout:\n",
      "    prob: 1.0\n",
      "    mask_size: 16\n",
      "    cut_inside: False\n",
      "    mask_color: 0\n",
      "    dual_cutout_alpha: 0.1\n",
      "  random_erasing:\n",
      "    prob: 0.5\n",
      "    area_ratio_range: [0.02, 0.4]\n",
      "    min_aspect_ratio: 0.3\n",
      "    max_attempt: 20\n",
      "  mixup:\n",
      "    alpha: 1.0\n",
      "  ricap:\n",
      "    beta: 0.3\n",
      "  cutmix:\n",
      "    alpha: 1.0\n",
      "  label_smoothing:\n",
      "    epsilon: 0.1\n",
      "tta:\n",
      "  use_resize: False\n",
      "  use_center_crop: False\n",
      "  resize: 256\n",
      "test:\n",
      "  checkpoint: ''\n",
      "  output_dir: ''\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    pin_memory: False\n",
      "\u001b[32m[2020-07-10 13:39:42] __main__ INFO: \u001b[0menv_info:\n",
      "  pytorch_version: 1.4.0\n",
      "  cuda_version: 10.1\n",
      "  cudnn_version: 7603\n",
      "  num_gpus: 1\n",
      "  gpu_name: Tesla K80\n",
      "  gpu_capability: 3.7\n",
      "Files already downloaded and verified\n",
      "\u001b[32m[2020-07-10 13:39:45] __main__ INFO: \u001b[0mMACs  : 69.76M\n",
      "\u001b[32m[2020-07-10 13:39:45] __main__ INFO: \u001b[0m#params: 466.91K\n",
      "Selected optimization level O0:  Pure FP32 training.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\",)\n",
      "\u001b[32m[2020-07-10 13:39:45] __main__ INFO: \u001b[0mVal 0\n",
      "\u001b[32m[2020-07-10 13:39:47] __main__ INFO: \u001b[0mEpoch 0 loss 0.5501 acc@1 0.8698 acc@5 0.9796\n",
      "\u001b[32m[2020-07-10 13:39:47] __main__ INFO: \u001b[0mElapsed 1.42\n",
      "\u001b[32m[2020-07-10 13:39:47] __main__ INFO: \u001b[0mTrain 1 0\n",
      "\u001b[32m[2020-07-10 13:39:56] __main__ INFO: \u001b[0mEpoch 1 Step 100/351 lr 0.001000 loss 0.2423 (0.2862) acc@1 0.9297 (0.9212) acc@5 1.0000 (0.9962)\n",
      "\u001b[32m[2020-07-10 13:40:06] __main__ INFO: \u001b[0mEpoch 1 Step 200/351 lr 0.001000 loss 0.2956 (0.2553) acc@1 0.9219 (0.9270) acc@5 0.9922 (0.9969)\n",
      "\u001b[32m[2020-07-10 13:40:15] __main__ INFO: \u001b[0mEpoch 1 Step 300/351 lr 0.001000 loss 0.2155 (0.2415) acc@1 0.9453 (0.9284) acc@5 1.0000 (0.9972)\n",
      "\u001b[32m[2020-07-10 13:40:20] __main__ INFO: \u001b[0mEpoch 1 Step 351/351 lr 0.001000 loss 0.2429 (0.2372) acc@1 0.9062 (0.9290) acc@5 1.0000 (0.9972)\n",
      "\u001b[32m[2020-07-10 13:40:20] __main__ INFO: \u001b[0mElapsed 33.15\n",
      "\u001b[32m[2020-07-10 13:40:20] __main__ INFO: \u001b[0mVal 1\n",
      "\u001b[32m[2020-07-10 13:40:21] __main__ INFO: \u001b[0mEpoch 1 loss 0.3264 acc@1 0.9016 acc@5 0.9958\n",
      "\u001b[32m[2020-07-10 13:40:21] __main__ INFO: \u001b[0mElapsed 1.07\n",
      "\u001b[32m[2020-07-10 13:40:21] __main__ INFO: \u001b[0mTrain 2 351\n",
      "\u001b[32m[2020-07-10 13:40:30] __main__ INFO: \u001b[0mEpoch 2 Step 100/351 lr 0.001000 loss 0.2068 (0.2028) acc@1 0.9609 (0.9352) acc@5 0.9922 (0.9980)\n",
      "\u001b[32m[2020-07-10 13:40:40] __main__ INFO: \u001b[0mEpoch 2 Step 200/351 lr 0.001000 loss 0.2627 (0.2009) acc@1 0.9375 (0.9354) acc@5 0.9844 (0.9983)\n",
      "\u001b[32m[2020-07-10 13:40:49] __main__ INFO: \u001b[0mEpoch 2 Step 300/351 lr 0.001000 loss 0.2231 (0.1995) acc@1 0.9297 (0.9351) acc@5 1.0000 (0.9983)\n",
      "\u001b[32m[2020-07-10 13:40:54] __main__ INFO: \u001b[0mEpoch 2 Step 351/351 lr 0.001000 loss 0.1768 (0.1977) acc@1 0.9453 (0.9360) acc@5 1.0000 (0.9983)\n",
      "\u001b[32m[2020-07-10 13:40:54] __main__ INFO: \u001b[0mElapsed 33.22\n",
      "\u001b[32m[2020-07-10 13:40:54] __main__ INFO: \u001b[0mVal 2\n",
      "\u001b[32m[2020-07-10 13:40:55] __main__ INFO: \u001b[0mEpoch 2 loss 0.3108 acc@1 0.9064 acc@5 0.9970\n",
      "\u001b[32m[2020-07-10 13:40:55] __main__ INFO: \u001b[0mElapsed 1.07\n",
      "\u001b[32m[2020-07-10 13:40:55] __main__ INFO: \u001b[0mTrain 3 702\n",
      "\u001b[32m[2020-07-10 13:41:05] __main__ INFO: \u001b[0mEpoch 3 Step 100/351 lr 0.001000 loss 0.1866 (0.1746) acc@1 0.9297 (0.9418) acc@5 1.0000 (0.9990)\n",
      "\u001b[32m[2020-07-10 13:41:14] __main__ INFO: \u001b[0mEpoch 3 Step 200/351 lr 0.001000 loss 0.1195 (0.1799) acc@1 0.9453 (0.9408) acc@5 1.0000 (0.9990)\n",
      "\u001b[32m[2020-07-10 13:41:24] __main__ INFO: \u001b[0mEpoch 3 Step 300/351 lr 0.001000 loss 0.1551 (0.1825) acc@1 0.9375 (0.9399) acc@5 1.0000 (0.9988)\n",
      "\u001b[32m[2020-07-10 13:41:29] __main__ INFO: \u001b[0mEpoch 3 Step 351/351 lr 0.001000 loss 0.1272 (0.1837) acc@1 0.9609 (0.9395) acc@5 1.0000 (0.9988)\n",
      "\u001b[32m[2020-07-10 13:41:29] __main__ INFO: \u001b[0mElapsed 33.47\n",
      "\u001b[32m[2020-07-10 13:41:29] __main__ INFO: \u001b[0mVal 3\n",
      "\u001b[32m[2020-07-10 13:41:30] __main__ INFO: \u001b[0mEpoch 3 loss 0.3057 acc@1 0.9018 acc@5 0.9968\n",
      "\u001b[32m[2020-07-10 13:41:30] __main__ INFO: \u001b[0mElapsed 1.10\n",
      "\u001b[32m[2020-07-10 13:41:30] __main__ INFO: \u001b[0mTrain 4 1053\n",
      "\u001b[32m[2020-07-10 13:41:39] __main__ INFO: \u001b[0mEpoch 4 Step 100/351 lr 0.001000 loss 0.2115 (0.1729) acc@1 0.9219 (0.9431) acc@5 0.9922 (0.9984)\n",
      "\u001b[32m[2020-07-10 13:41:49] __main__ INFO: \u001b[0mEpoch 4 Step 200/351 lr 0.001000 loss 0.1954 (0.1721) acc@1 0.9453 (0.9434) acc@5 1.0000 (0.9985)\n",
      "\u001b[32m[2020-07-10 13:41:58] __main__ INFO: \u001b[0mEpoch 4 Step 300/351 lr 0.001000 loss 0.1433 (0.1773) acc@1 0.9531 (0.9416) acc@5 1.0000 (0.9985)\n",
      "\u001b[32m[2020-07-10 13:42:03] __main__ INFO: \u001b[0mEpoch 4 Step 351/351 lr 0.001000 loss 0.1715 (0.1779) acc@1 0.9531 (0.9413) acc@5 1.0000 (0.9986)\n",
      "\u001b[32m[2020-07-10 13:42:03] __main__ INFO: \u001b[0mElapsed 33.56\n",
      "\u001b[32m[2020-07-10 13:42:03] __main__ INFO: \u001b[0mVal 4\n",
      "\u001b[32m[2020-07-10 13:42:04] __main__ INFO: \u001b[0mEpoch 4 loss 0.3041 acc@1 0.9054 acc@5 0.9970\n",
      "\u001b[32m[2020-07-10 13:42:04] __main__ INFO: \u001b[0mElapsed 1.09\n",
      "\u001b[32m[2020-07-10 13:42:04] __main__ INFO: \u001b[0mTrain 5 1404\n",
      "\u001b[32m[2020-07-10 13:42:14] __main__ INFO: \u001b[0mEpoch 5 Step 100/351 lr 0.001000 loss 0.1997 (0.1723) acc@1 0.9453 (0.9425) acc@5 1.0000 (0.9988)\n",
      "\u001b[32m[2020-07-10 13:42:24] __main__ INFO: \u001b[0mEpoch 5 Step 200/351 lr 0.001000 loss 0.2562 (0.1688) acc@1 0.9297 (0.9432) acc@5 1.0000 (0.9989)\n",
      "\u001b[32m[2020-07-10 13:42:33] __main__ INFO: \u001b[0mEpoch 5 Step 300/351 lr 0.001000 loss 0.1895 (0.1700) acc@1 0.9375 (0.9441) acc@5 1.0000 (0.9988)\n",
      "\u001b[32m[2020-07-10 13:42:38] __main__ INFO: \u001b[0mEpoch 5 Step 351/351 lr 0.001000 loss 0.2039 (0.1688) acc@1 0.9219 (0.9443) acc@5 1.0000 (0.9988)\n",
      "\u001b[32m[2020-07-10 13:42:38] __main__ INFO: \u001b[0mElapsed 33.68\n",
      "\u001b[32m[2020-07-10 13:42:38] __main__ INFO: \u001b[0mVal 5\n",
      "\u001b[32m[2020-07-10 13:42:39] __main__ INFO: \u001b[0mEpoch 5 loss 0.3030 acc@1 0.9084 acc@5 0.9974\n",
      "\u001b[32m[2020-07-10 13:42:39] __main__ INFO: \u001b[0mElapsed 1.09\n",
      "\u001b[32m[2020-07-10 13:42:39] __main__ INFO: \u001b[0mTrain 6 1755\n",
      "\u001b[32m[2020-07-10 13:42:49] __main__ INFO: \u001b[0mEpoch 6 Step 100/351 lr 0.001000 loss 0.1610 (0.1650) acc@1 0.9297 (0.9451) acc@5 1.0000 (0.9992)\n",
      "\u001b[32m[2020-07-10 13:42:58] __main__ INFO: \u001b[0mEpoch 6 Step 200/351 lr 0.001000 loss 0.1819 (0.1618) acc@1 0.9453 (0.9459) acc@5 0.9922 (0.9991)\n",
      "\u001b[32m[2020-07-10 13:43:08] __main__ INFO: \u001b[0mEpoch 6 Step 300/351 lr 0.001000 loss 0.1200 (0.1644) acc@1 0.9531 (0.9445) acc@5 0.9922 (0.9990)\n",
      "\u001b[32m[2020-07-10 13:43:13] __main__ INFO: \u001b[0mEpoch 6 Step 351/351 lr 0.001000 loss 0.1370 (0.1650) acc@1 0.9688 (0.9441) acc@5 1.0000 (0.9990)\n",
      "\u001b[32m[2020-07-10 13:43:13] __main__ INFO: \u001b[0mElapsed 33.78\n",
      "\u001b[32m[2020-07-10 13:43:13] __main__ INFO: \u001b[0mVal 6\n",
      "\u001b[32m[2020-07-10 13:43:14] __main__ INFO: \u001b[0mEpoch 6 loss 0.3051 acc@1 0.9076 acc@5 0.9970\n",
      "\u001b[32m[2020-07-10 13:43:14] __main__ INFO: \u001b[0mElapsed 1.06\n",
      "\u001b[32m[2020-07-10 13:43:14] __main__ INFO: \u001b[0mTrain 7 2106\n",
      "\u001b[32m[2020-07-10 13:43:24] __main__ INFO: \u001b[0mEpoch 7 Step 100/351 lr 0.001000 loss 0.0897 (0.1465) acc@1 0.9766 (0.9498) acc@5 1.0000 (0.9995)\n",
      "\u001b[32m[2020-07-10 13:43:33] __main__ INFO: \u001b[0mEpoch 7 Step 200/351 lr 0.001000 loss 0.1600 (0.1512) acc@1 0.9219 (0.9489) acc@5 1.0000 (0.9994)\n",
      "\u001b[32m[2020-07-10 13:43:43] __main__ INFO: \u001b[0mEpoch 7 Step 300/351 lr 0.001000 loss 0.1039 (0.1575) acc@1 0.9609 (0.9474) acc@5 1.0000 (0.9991)\n",
      "\u001b[32m[2020-07-10 13:43:48] __main__ INFO: \u001b[0mEpoch 7 Step 351/351 lr 0.001000 loss 0.1359 (0.1590) acc@1 0.9531 (0.9469) acc@5 1.0000 (0.9990)\n",
      "\u001b[32m[2020-07-10 13:43:48] __main__ INFO: \u001b[0mElapsed 33.71\n",
      "\u001b[32m[2020-07-10 13:43:48] __main__ INFO: \u001b[0mVal 7\n",
      "\u001b[32m[2020-07-10 13:43:49] __main__ INFO: \u001b[0mEpoch 7 loss 0.3023 acc@1 0.9094 acc@5 0.9968\n",
      "\u001b[32m[2020-07-10 13:43:49] __main__ INFO: \u001b[0mElapsed 1.10\n",
      "\u001b[32m[2020-07-10 13:43:49] __main__ INFO: \u001b[0mTrain 8 2457\n",
      "\u001b[32m[2020-07-10 13:43:59] __main__ INFO: \u001b[0mEpoch 8 Step 100/351 lr 0.001000 loss 0.0775 (0.1517) acc@1 0.9766 (0.9482) acc@5 1.0000 (0.9989)\n",
      "\u001b[32m[2020-07-10 13:44:08] __main__ INFO: \u001b[0mEpoch 8 Step 200/351 lr 0.001000 loss 0.1171 (0.1524) acc@1 0.9531 (0.9464) acc@5 1.0000 (0.9993)\n",
      "\u001b[32m[2020-07-10 13:44:18] __main__ INFO: \u001b[0mEpoch 8 Step 300/351 lr 0.001000 loss 0.1102 (0.1534) acc@1 0.9609 (0.9471) acc@5 1.0000 (0.9993)\n",
      "\u001b[32m[2020-07-10 13:44:23] __main__ INFO: \u001b[0mEpoch 8 Step 351/351 lr 0.001000 loss 0.1200 (0.1563) acc@1 0.9453 (0.9468) acc@5 1.0000 (0.9992)\n",
      "\u001b[32m[2020-07-10 13:44:23] __main__ INFO: \u001b[0mElapsed 33.81\n",
      "\u001b[32m[2020-07-10 13:44:23] __main__ INFO: \u001b[0mVal 8\n",
      "\u001b[32m[2020-07-10 13:44:24] __main__ INFO: \u001b[0mEpoch 8 loss 0.2992 acc@1 0.9096 acc@5 0.9972\n",
      "\u001b[32m[2020-07-10 13:44:24] __main__ INFO: \u001b[0mElapsed 1.08\n",
      "\u001b[32m[2020-07-10 13:44:24] __main__ INFO: \u001b[0mTrain 9 2808\n",
      "\u001b[32m[2020-07-10 13:44:33] __main__ INFO: \u001b[0mEpoch 9 Step 100/351 lr 0.001000 loss 0.1764 (0.1442) acc@1 0.9453 (0.9505) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-07-10 13:44:43] __main__ INFO: \u001b[0mEpoch 9 Step 200/351 lr 0.001000 loss 0.1477 (0.1498) acc@1 0.9453 (0.9481) acc@5 1.0000 (0.9994)\n",
      "\u001b[32m[2020-07-10 13:44:53] __main__ INFO: \u001b[0mEpoch 9 Step 300/351 lr 0.001000 loss 0.2224 (0.1519) acc@1 0.9375 (0.9475) acc@5 1.0000 (0.9994)\n",
      "\u001b[32m[2020-07-10 13:44:57] __main__ INFO: \u001b[0mEpoch 9 Step 351/351 lr 0.001000 loss 0.1233 (0.1529) acc@1 0.9766 (0.9474) acc@5 1.0000 (0.9993)\n",
      "\u001b[32m[2020-07-10 13:44:57] __main__ INFO: \u001b[0mElapsed 33.78\n",
      "\u001b[32m[2020-07-10 13:44:57] __main__ INFO: \u001b[0mVal 9\n",
      "\u001b[32m[2020-07-10 13:44:59] __main__ INFO: \u001b[0mEpoch 9 loss 0.3025 acc@1 0.9070 acc@5 0.9972\n",
      "\u001b[32m[2020-07-10 13:44:59] __main__ INFO: \u001b[0mElapsed 1.11\n",
      "\u001b[32m[2020-07-10 13:44:59] __main__ INFO: \u001b[0mTrain 10 3159\n",
      "\u001b[32m[2020-07-10 13:45:08] __main__ INFO: \u001b[0mEpoch 10 Step 100/351 lr 0.001000 loss 0.0545 (0.1290) acc@1 0.9766 (0.9556) acc@5 1.0000 (0.9994)\n",
      "\u001b[32m[2020-07-10 13:45:18] __main__ INFO: \u001b[0mEpoch 10 Step 200/351 lr 0.001000 loss 0.0983 (0.1417) acc@1 0.9844 (0.9521) acc@5 1.0000 (0.9994)\n",
      "\u001b[32m[2020-07-10 13:45:28] __main__ INFO: \u001b[0mEpoch 10 Step 300/351 lr 0.001000 loss 0.1208 (0.1453) acc@1 0.9375 (0.9506) acc@5 1.0000 (0.9993)\n",
      "\u001b[32m[2020-07-10 13:45:33] __main__ INFO: \u001b[0mEpoch 10 Step 351/351 lr 0.001000 loss 0.1219 (0.1447) acc@1 0.9609 (0.9509) acc@5 1.0000 (0.9994)\n",
      "\u001b[32m[2020-07-10 13:45:33] __main__ INFO: \u001b[0mElapsed 33.92\n",
      "\u001b[32m[2020-07-10 13:45:33] __main__ INFO: \u001b[0mVal 10\n",
      "\u001b[32m[2020-07-10 13:45:34] __main__ INFO: \u001b[0mEpoch 10 loss 0.3071 acc@1 0.9060 acc@5 0.9968\n",
      "\u001b[32m[2020-07-10 13:45:34] __main__ INFO: \u001b[0mElapsed 1.09\n",
      "\u001b[32m[2020-07-10 13:45:34] __main__ INFO: \u001b[0mTrain 11 3510\n",
      "\u001b[32m[2020-07-10 13:45:43] __main__ INFO: \u001b[0mEpoch 11 Step 100/351 lr 0.001000 loss 0.1158 (0.1462) acc@1 0.9766 (0.9516) acc@5 1.0000 (0.9991)\n",
      "\u001b[32m[2020-07-10 13:45:53] __main__ INFO: \u001b[0mEpoch 11 Step 200/351 lr 0.001000 loss 0.0801 (0.1431) acc@1 0.9609 (0.9506) acc@5 1.0000 (0.9993)\n",
      "\u001b[32m[2020-07-10 13:46:03] __main__ INFO: \u001b[0mEpoch 11 Step 300/351 lr 0.001000 loss 0.1018 (0.1432) acc@1 0.9609 (0.9502) acc@5 1.0000 (0.9993)\n",
      "\u001b[32m[2020-07-10 13:46:07] __main__ INFO: \u001b[0mEpoch 11 Step 351/351 lr 0.001000 loss 0.2054 (0.1444) acc@1 0.9219 (0.9500) acc@5 1.0000 (0.9994)\n",
      "\u001b[32m[2020-07-10 13:46:07] __main__ INFO: \u001b[0mElapsed 33.87\n",
      "\u001b[32m[2020-07-10 13:46:07] __main__ INFO: \u001b[0mVal 11\n",
      "\u001b[32m[2020-07-10 13:46:09] __main__ INFO: \u001b[0mEpoch 11 loss 0.3079 acc@1 0.9064 acc@5 0.9968\n",
      "\u001b[32m[2020-07-10 13:46:09] __main__ INFO: \u001b[0mElapsed 1.08\n",
      "\u001b[32m[2020-07-10 13:46:09] __main__ INFO: \u001b[0mTrain 12 3861\n",
      "\u001b[32m[2020-07-10 13:46:18] __main__ INFO: \u001b[0mEpoch 12 Step 100/351 lr 0.001000 loss 0.1550 (0.1409) acc@1 0.9609 (0.9516) acc@5 0.9922 (0.9993)\n",
      "\u001b[32m[2020-07-10 13:46:28] __main__ INFO: \u001b[0mEpoch 12 Step 200/351 lr 0.001000 loss 0.1537 (0.1393) acc@1 0.9531 (0.9518) acc@5 1.0000 (0.9993)\n",
      "\u001b[32m[2020-07-10 13:46:37] __main__ INFO: \u001b[0mEpoch 12 Step 300/351 lr 0.001000 loss 0.0903 (0.1390) acc@1 0.9766 (0.9520) acc@5 1.0000 (0.9995)\n",
      "\u001b[32m[2020-07-10 13:46:42] __main__ INFO: \u001b[0mEpoch 12 Step 351/351 lr 0.001000 loss 0.2631 (0.1393) acc@1 0.9297 (0.9522) acc@5 1.0000 (0.9994)\n",
      "\u001b[32m[2020-07-10 13:46:42] __main__ INFO: \u001b[0mElapsed 33.76\n",
      "\u001b[32m[2020-07-10 13:46:42] __main__ INFO: \u001b[0mVal 12\n",
      "\u001b[32m[2020-07-10 13:46:43] __main__ INFO: \u001b[0mEpoch 12 loss 0.3069 acc@1 0.9096 acc@5 0.9964\n",
      "\u001b[32m[2020-07-10 13:46:43] __main__ INFO: \u001b[0mElapsed 1.10\n",
      "\u001b[32m[2020-07-10 13:46:43] __main__ INFO: \u001b[0mTrain 13 4212\n",
      "\u001b[32m[2020-07-10 13:46:53] __main__ INFO: \u001b[0mEpoch 13 Step 100/351 lr 0.001000 loss 0.1697 (0.1282) acc@1 0.9453 (0.9545) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-07-10 13:47:03] __main__ INFO: \u001b[0mEpoch 13 Step 200/351 lr 0.001000 loss 0.0569 (0.1298) acc@1 0.9844 (0.9541) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-07-10 13:47:12] __main__ INFO: \u001b[0mEpoch 13 Step 300/351 lr 0.001000 loss 0.0795 (0.1330) acc@1 0.9766 (0.9534) acc@5 1.0000 (0.9995)\n",
      "\u001b[32m[2020-07-10 13:47:17] __main__ INFO: \u001b[0mEpoch 13 Step 351/351 lr 0.001000 loss 0.0968 (0.1347) acc@1 0.9609 (0.9530) acc@5 1.0000 (0.9995)\n",
      "\u001b[32m[2020-07-10 13:47:17] __main__ INFO: \u001b[0mElapsed 33.77\n",
      "\u001b[32m[2020-07-10 13:47:17] __main__ INFO: \u001b[0mVal 13\n",
      "\u001b[32m[2020-07-10 13:47:18] __main__ INFO: \u001b[0mEpoch 13 loss 0.3068 acc@1 0.9088 acc@5 0.9960\n",
      "\u001b[32m[2020-07-10 13:47:18] __main__ INFO: \u001b[0mElapsed 1.12\n",
      "\u001b[32m[2020-07-10 13:47:18] __main__ INFO: \u001b[0mTrain 14 4563\n",
      "\u001b[32m[2020-07-10 13:47:28] __main__ INFO: \u001b[0mEpoch 14 Step 100/351 lr 0.001000 loss 0.1165 (0.1324) acc@1 0.9688 (0.9554) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-07-10 13:47:38] __main__ INFO: \u001b[0mEpoch 14 Step 200/351 lr 0.001000 loss 0.1093 (0.1328) acc@1 0.9453 (0.9552) acc@5 1.0000 (0.9994)\n",
      "\u001b[32m[2020-07-10 13:47:47] __main__ INFO: \u001b[0mEpoch 14 Step 300/351 lr 0.001000 loss 0.1362 (0.1349) acc@1 0.9531 (0.9546) acc@5 1.0000 (0.9993)\n",
      "\u001b[32m[2020-07-10 13:47:52] __main__ INFO: \u001b[0mEpoch 14 Step 351/351 lr 0.001000 loss 0.1745 (0.1339) acc@1 0.9453 (0.9547) acc@5 1.0000 (0.9994)\n",
      "\u001b[32m[2020-07-10 13:47:52] __main__ INFO: \u001b[0mElapsed 33.69\n",
      "\u001b[32m[2020-07-10 13:47:52] __main__ INFO: \u001b[0mVal 14\n",
      "\u001b[32m[2020-07-10 13:47:53] __main__ INFO: \u001b[0mEpoch 14 loss 0.3121 acc@1 0.9068 acc@5 0.9960\n",
      "\u001b[32m[2020-07-10 13:47:53] __main__ INFO: \u001b[0mElapsed 1.09\n",
      "\u001b[32m[2020-07-10 13:47:53] __main__ INFO: \u001b[0mTrain 15 4914\n",
      "\u001b[32m[2020-07-10 13:48:03] __main__ INFO: \u001b[0mEpoch 15 Step 100/351 lr 0.001000 loss 0.1584 (0.1354) acc@1 0.9219 (0.9544) acc@5 1.0000 (0.9992)\n",
      "\u001b[32m[2020-07-10 13:48:12] __main__ INFO: \u001b[0mEpoch 15 Step 200/351 lr 0.001000 loss 0.0724 (0.1297) acc@1 0.9766 (0.9567) acc@5 1.0000 (0.9993)\n",
      "\u001b[32m[2020-07-10 13:48:22] __main__ INFO: \u001b[0mEpoch 15 Step 300/351 lr 0.001000 loss 0.0757 (0.1302) acc@1 0.9766 (0.9564) acc@5 1.0000 (0.9993)\n",
      "\u001b[32m[2020-07-10 13:48:27] __main__ INFO: \u001b[0mEpoch 15 Step 351/351 lr 0.001000 loss 0.1667 (0.1307) acc@1 0.9297 (0.9561) acc@5 1.0000 (0.9994)\n",
      "\u001b[32m[2020-07-10 13:48:27] __main__ INFO: \u001b[0mElapsed 33.73\n",
      "\u001b[32m[2020-07-10 13:48:27] __main__ INFO: \u001b[0mVal 15\n",
      "\u001b[32m[2020-07-10 13:48:28] __main__ INFO: \u001b[0mEpoch 15 loss 0.3228 acc@1 0.9066 acc@5 0.9960\n",
      "\u001b[32m[2020-07-10 13:48:28] __main__ INFO: \u001b[0mElapsed 1.10\n",
      "\u001b[32m[2020-07-10 13:48:28] __main__ INFO: \u001b[0mTrain 16 5265\n",
      "\u001b[32m[2020-07-10 13:48:38] __main__ INFO: \u001b[0mEpoch 16 Step 100/351 lr 0.001000 loss 0.1312 (0.1299) acc@1 0.9688 (0.9546) acc@5 1.0000 (0.9994)\n",
      "\u001b[32m[2020-07-10 13:48:47] __main__ INFO: \u001b[0mEpoch 16 Step 200/351 lr 0.001000 loss 0.1181 (0.1286) acc@1 0.9453 (0.9561) acc@5 1.0000 (0.9994)\n",
      "\u001b[32m[2020-07-10 13:48:57] __main__ INFO: \u001b[0mEpoch 16 Step 300/351 lr 0.001000 loss 0.1258 (0.1280) acc@1 0.9453 (0.9566) acc@5 1.0000 (0.9995)\n",
      "\u001b[32m[2020-07-10 13:49:02] __main__ INFO: \u001b[0mEpoch 16 Step 351/351 lr 0.001000 loss 0.0812 (0.1299) acc@1 0.9766 (0.9561) acc@5 1.0000 (0.9994)\n",
      "\u001b[32m[2020-07-10 13:49:02] __main__ INFO: \u001b[0mElapsed 33.73\n",
      "\u001b[32m[2020-07-10 13:49:02] __main__ INFO: \u001b[0mVal 16\n",
      "\u001b[32m[2020-07-10 13:49:03] __main__ INFO: \u001b[0mEpoch 16 loss 0.3152 acc@1 0.9100 acc@5 0.9960\n",
      "\u001b[32m[2020-07-10 13:49:03] __main__ INFO: \u001b[0mElapsed 1.08\n",
      "\u001b[32m[2020-07-10 13:49:03] __main__ INFO: \u001b[0mTrain 17 5616\n",
      "\u001b[32m[2020-07-10 13:49:12] __main__ INFO: \u001b[0mEpoch 17 Step 100/351 lr 0.001000 loss 0.0759 (0.1213) acc@1 0.9844 (0.9574) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-07-10 13:49:22] __main__ INFO: \u001b[0mEpoch 17 Step 200/351 lr 0.001000 loss 0.2448 (0.1222) acc@1 0.9453 (0.9578) acc@5 1.0000 (0.9995)\n",
      "\u001b[32m[2020-07-10 13:49:32] __main__ INFO: \u001b[0mEpoch 17 Step 300/351 lr 0.001000 loss 0.1062 (0.1245) acc@1 0.9531 (0.9570) acc@5 1.0000 (0.9994)\n",
      "\u001b[32m[2020-07-10 13:49:36] __main__ INFO: \u001b[0mEpoch 17 Step 351/351 lr 0.001000 loss 0.0505 (0.1256) acc@1 0.9766 (0.9566) acc@5 1.0000 (0.9995)\n",
      "\u001b[32m[2020-07-10 13:49:36] __main__ INFO: \u001b[0mElapsed 33.66\n",
      "\u001b[32m[2020-07-10 13:49:36] __main__ INFO: \u001b[0mVal 17\n",
      "\u001b[32m[2020-07-10 13:49:37] __main__ INFO: \u001b[0mEpoch 17 loss 0.3227 acc@1 0.9070 acc@5 0.9960\n",
      "\u001b[32m[2020-07-10 13:49:37] __main__ INFO: \u001b[0mElapsed 1.06\n",
      "\u001b[32m[2020-07-10 13:49:37] __main__ INFO: \u001b[0mTrain 18 5967\n",
      "\u001b[32m[2020-07-10 13:49:47] __main__ INFO: \u001b[0mEpoch 18 Step 100/351 lr 0.001000 loss 0.2364 (0.1262) acc@1 0.9062 (0.9547) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-07-10 13:49:57] __main__ INFO: \u001b[0mEpoch 18 Step 200/351 lr 0.001000 loss 0.1001 (0.1231) acc@1 0.9531 (0.9566) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-07-10 13:50:06] __main__ INFO: \u001b[0mEpoch 18 Step 300/351 lr 0.001000 loss 0.1086 (0.1232) acc@1 0.9766 (0.9569) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-07-10 13:50:11] __main__ INFO: \u001b[0mEpoch 18 Step 351/351 lr 0.001000 loss 0.0757 (0.1232) acc@1 0.9766 (0.9571) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-07-10 13:50:11] __main__ INFO: \u001b[0mElapsed 33.70\n",
      "\u001b[32m[2020-07-10 13:50:11] __main__ INFO: \u001b[0mVal 18\n",
      "\u001b[32m[2020-07-10 13:50:12] __main__ INFO: \u001b[0mEpoch 18 loss 0.3152 acc@1 0.9080 acc@5 0.9970\n",
      "\u001b[32m[2020-07-10 13:50:12] __main__ INFO: \u001b[0mElapsed 1.08\n",
      "\u001b[32m[2020-07-10 13:50:12] __main__ INFO: \u001b[0mTrain 19 6318\n",
      "\u001b[32m[2020-07-10 13:50:22] __main__ INFO: \u001b[0mEpoch 19 Step 100/351 lr 0.001000 loss 0.0767 (0.1231) acc@1 0.9766 (0.9587) acc@5 1.0000 (0.9995)\n",
      "\u001b[32m[2020-07-10 13:50:32] __main__ INFO: \u001b[0mEpoch 19 Step 200/351 lr 0.001000 loss 0.1892 (0.1226) acc@1 0.9297 (0.9582) acc@5 1.0000 (0.9995)\n",
      "\u001b[32m[2020-07-10 13:50:41] __main__ INFO: \u001b[0mEpoch 19 Step 300/351 lr 0.001000 loss 0.2645 (0.1239) acc@1 0.9062 (0.9578) acc@5 1.0000 (0.9995)\n",
      "\u001b[32m[2020-07-10 13:50:46] __main__ INFO: \u001b[0mEpoch 19 Step 351/351 lr 0.001000 loss 0.1083 (0.1240) acc@1 0.9609 (0.9576) acc@5 1.0000 (0.9995)\n",
      "\u001b[32m[2020-07-10 13:50:46] __main__ INFO: \u001b[0mElapsed 33.71\n",
      "\u001b[32m[2020-07-10 13:50:46] __main__ INFO: \u001b[0mVal 19\n",
      "\u001b[32m[2020-07-10 13:50:47] __main__ INFO: \u001b[0mEpoch 19 loss 0.3178 acc@1 0.9090 acc@5 0.9970\n",
      "\u001b[32m[2020-07-10 13:50:47] __main__ INFO: \u001b[0mElapsed 1.08\n",
      "\u001b[32m[2020-07-10 13:50:47] __main__ INFO: \u001b[0mTrain 20 6669\n",
      "\u001b[32m[2020-07-10 13:50:57] __main__ INFO: \u001b[0mEpoch 20 Step 100/351 lr 0.001000 loss 0.0922 (0.1134) acc@1 0.9688 (0.9620) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-10 13:51:06] __main__ INFO: \u001b[0mEpoch 20 Step 200/351 lr 0.001000 loss 0.0961 (0.1127) acc@1 0.9688 (0.9620) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-07-10 13:51:16] __main__ INFO: \u001b[0mEpoch 20 Step 300/351 lr 0.001000 loss 0.1039 (0.1144) acc@1 0.9531 (0.9612) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-07-10 13:51:21] __main__ INFO: \u001b[0mEpoch 20 Step 351/351 lr 0.001000 loss 0.0999 (0.1163) acc@1 0.9688 (0.9604) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-07-10 13:51:21] __main__ INFO: \u001b[0mElapsed 33.73\n",
      "\u001b[32m[2020-07-10 13:51:21] __main__ INFO: \u001b[0mVal 20\n",
      "\u001b[32m[2020-07-10 13:51:22] __main__ INFO: \u001b[0mEpoch 20 loss 0.3189 acc@1 0.9092 acc@5 0.9964\n",
      "\u001b[32m[2020-07-10 13:51:22] __main__ INFO: \u001b[0mElapsed 1.10\n",
      "\u001b[32m[2020-07-10 13:51:22] __main__ INFO: \u001b[0mTrain 21 7020\n",
      "\u001b[32m[2020-07-10 13:51:32] __main__ INFO: \u001b[0mEpoch 21 Step 100/351 lr 0.001000 loss 0.1866 (0.1142) acc@1 0.9375 (0.9613) acc@5 1.0000 (0.9994)\n",
      "\u001b[32m[2020-07-10 13:51:41] __main__ INFO: \u001b[0mEpoch 21 Step 200/351 lr 0.001000 loss 0.1598 (0.1106) acc@1 0.9375 (0.9628) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-07-10 13:51:51] __main__ INFO: \u001b[0mEpoch 21 Step 300/351 lr 0.001000 loss 0.2139 (0.1144) acc@1 0.9219 (0.9619) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-07-10 13:51:56] __main__ INFO: \u001b[0mEpoch 21 Step 351/351 lr 0.001000 loss 0.1040 (0.1146) acc@1 0.9531 (0.9615) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-07-10 13:51:56] __main__ INFO: \u001b[0mElapsed 33.68\n",
      "\u001b[32m[2020-07-10 13:51:56] __main__ INFO: \u001b[0mVal 21\n",
      "\u001b[32m[2020-07-10 13:51:57] __main__ INFO: \u001b[0mEpoch 21 loss 0.3177 acc@1 0.9090 acc@5 0.9968\n",
      "\u001b[32m[2020-07-10 13:51:57] __main__ INFO: \u001b[0mElapsed 1.09\n",
      "\u001b[32m[2020-07-10 13:51:57] __main__ INFO: \u001b[0mTrain 22 7371\n",
      "\u001b[32m[2020-07-10 13:52:06] __main__ INFO: \u001b[0mEpoch 22 Step 100/351 lr 0.001000 loss 0.0972 (0.1135) acc@1 0.9531 (0.9614) acc@5 1.0000 (0.9995)\n",
      "\u001b[32m[2020-07-10 13:52:16] __main__ INFO: \u001b[0mEpoch 22 Step 200/351 lr 0.001000 loss 0.1649 (0.1142) acc@1 0.9531 (0.9611) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-07-10 13:52:26] __main__ INFO: \u001b[0mEpoch 22 Step 300/351 lr 0.001000 loss 0.1456 (0.1140) acc@1 0.9531 (0.9614) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-07-10 13:52:30] __main__ INFO: \u001b[0mEpoch 22 Step 351/351 lr 0.001000 loss 0.1139 (0.1122) acc@1 0.9844 (0.9624) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-07-10 13:52:30] __main__ INFO: \u001b[0mElapsed 33.75\n",
      "\u001b[32m[2020-07-10 13:52:30] __main__ INFO: \u001b[0mVal 22\n",
      "\u001b[32m[2020-07-10 13:52:31] __main__ INFO: \u001b[0mEpoch 22 loss 0.3209 acc@1 0.9094 acc@5 0.9966\n",
      "\u001b[32m[2020-07-10 13:52:32] __main__ INFO: \u001b[0mElapsed 1.08\n",
      "\u001b[32m[2020-07-10 13:52:32] __main__ INFO: \u001b[0mTrain 23 7722\n",
      "\u001b[32m[2020-07-10 13:52:41] __main__ INFO: \u001b[0mEpoch 23 Step 100/351 lr 0.001000 loss 0.1000 (0.1091) acc@1 0.9688 (0.9613) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-07-10 13:52:51] __main__ INFO: \u001b[0mEpoch 23 Step 200/351 lr 0.001000 loss 0.1743 (0.1125) acc@1 0.9219 (0.9615) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-07-10 13:53:00] __main__ INFO: \u001b[0mEpoch 23 Step 300/351 lr 0.001000 loss 0.1160 (0.1112) acc@1 0.9609 (0.9619) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-07-10 13:53:05] __main__ INFO: \u001b[0mEpoch 23 Step 351/351 lr 0.001000 loss 0.1803 (0.1120) acc@1 0.9453 (0.9615) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-07-10 13:53:05] __main__ INFO: \u001b[0mElapsed 33.71\n",
      "\u001b[32m[2020-07-10 13:53:05] __main__ INFO: \u001b[0mVal 23\n",
      "\u001b[32m[2020-07-10 13:53:06] __main__ INFO: \u001b[0mEpoch 23 loss 0.3219 acc@1 0.9052 acc@5 0.9964\n",
      "\u001b[32m[2020-07-10 13:53:06] __main__ INFO: \u001b[0mElapsed 1.08\n",
      "\u001b[32m[2020-07-10 13:53:06] __main__ INFO: \u001b[0mTrain 24 8073\n",
      "\u001b[32m[2020-07-10 13:53:16] __main__ INFO: \u001b[0mEpoch 24 Step 100/351 lr 0.001000 loss 0.0486 (0.1123) acc@1 0.9844 (0.9635) acc@5 1.0000 (0.9995)\n",
      "\u001b[32m[2020-07-10 13:53:25] __main__ INFO: \u001b[0mEpoch 24 Step 200/351 lr 0.001000 loss 0.1045 (0.1088) acc@1 0.9688 (0.9639) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-07-10 13:53:35] __main__ INFO: \u001b[0mEpoch 24 Step 300/351 lr 0.001000 loss 0.0880 (0.1093) acc@1 0.9609 (0.9633) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-07-10 13:53:40] __main__ INFO: \u001b[0mEpoch 24 Step 351/351 lr 0.001000 loss 0.0553 (0.1089) acc@1 0.9844 (0.9633) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-07-10 13:53:40] __main__ INFO: \u001b[0mElapsed 33.63\n",
      "\u001b[32m[2020-07-10 13:53:40] __main__ INFO: \u001b[0mVal 24\n",
      "\u001b[32m[2020-07-10 13:53:41] __main__ INFO: \u001b[0mEpoch 24 loss 0.3259 acc@1 0.9080 acc@5 0.9958\n",
      "\u001b[32m[2020-07-10 13:53:41] __main__ INFO: \u001b[0mElapsed 1.09\n",
      "\u001b[32m[2020-07-10 13:53:41] __main__ INFO: \u001b[0mTrain 25 8424\n",
      "\u001b[32m[2020-07-10 13:53:51] __main__ INFO: \u001b[0mEpoch 25 Step 100/351 lr 0.001000 loss 0.0799 (0.1031) acc@1 0.9688 (0.9652) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-07-10 13:54:00] __main__ INFO: \u001b[0mEpoch 25 Step 200/351 lr 0.001000 loss 0.1593 (0.1050) acc@1 0.9375 (0.9642) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-07-10 13:54:10] __main__ INFO: \u001b[0mEpoch 25 Step 300/351 lr 0.001000 loss 0.0812 (0.1065) acc@1 0.9688 (0.9635) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-10 13:54:15] __main__ INFO: \u001b[0mEpoch 25 Step 351/351 lr 0.001000 loss 0.0642 (0.1067) acc@1 0.9844 (0.9637) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-10 13:54:15] __main__ INFO: \u001b[0mElapsed 33.74\n",
      "\u001b[32m[2020-07-10 13:54:15] __main__ INFO: \u001b[0mVal 25\n",
      "\u001b[32m[2020-07-10 13:54:16] __main__ INFO: \u001b[0mEpoch 25 loss 0.3380 acc@1 0.9078 acc@5 0.9964\n",
      "\u001b[32m[2020-07-10 13:54:16] __main__ INFO: \u001b[0mElapsed 1.09\n",
      "\u001b[32m[2020-07-10 13:54:16] __main__ INFO: \u001b[0mTrain 26 8775\n",
      "\u001b[32m[2020-07-10 13:54:26] __main__ INFO: \u001b[0mEpoch 26 Step 100/351 lr 0.001000 loss 0.0825 (0.1026) acc@1 0.9766 (0.9635) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-10 13:54:35] __main__ INFO: \u001b[0mEpoch 26 Step 200/351 lr 0.001000 loss 0.1293 (0.1054) acc@1 0.9609 (0.9627) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-10 13:54:45] __main__ INFO: \u001b[0mEpoch 26 Step 300/351 lr 0.001000 loss 0.1689 (0.1064) acc@1 0.9531 (0.9624) acc@5 0.9922 (0.9998)\n",
      "\u001b[32m[2020-07-10 13:54:50] __main__ INFO: \u001b[0mEpoch 26 Step 351/351 lr 0.001000 loss 0.1295 (0.1068) acc@1 0.9531 (0.9625) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-10 13:54:50] __main__ INFO: \u001b[0mElapsed 33.75\n",
      "\u001b[32m[2020-07-10 13:54:50] __main__ INFO: \u001b[0mVal 26\n",
      "\u001b[32m[2020-07-10 13:54:51] __main__ INFO: \u001b[0mEpoch 26 loss 0.3396 acc@1 0.9052 acc@5 0.9964\n",
      "\u001b[32m[2020-07-10 13:54:51] __main__ INFO: \u001b[0mElapsed 1.08\n",
      "\u001b[32m[2020-07-10 13:54:51] __main__ INFO: \u001b[0mTrain 27 9126\n",
      "\u001b[32m[2020-07-10 13:55:00] __main__ INFO: \u001b[0mEpoch 27 Step 100/351 lr 0.001000 loss 0.0311 (0.0958) acc@1 1.0000 (0.9680) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-10 13:55:10] __main__ INFO: \u001b[0mEpoch 27 Step 200/351 lr 0.001000 loss 0.0527 (0.1007) acc@1 0.9766 (0.9661) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-07-10 13:55:19] __main__ INFO: \u001b[0mEpoch 27 Step 300/351 lr 0.001000 loss 0.1250 (0.1043) acc@1 0.9375 (0.9645) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-07-10 13:55:24] __main__ INFO: \u001b[0mEpoch 27 Step 351/351 lr 0.001000 loss 0.1527 (0.1050) acc@1 0.9375 (0.9644) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-07-10 13:55:24] __main__ INFO: \u001b[0mElapsed 33.70\n",
      "\u001b[32m[2020-07-10 13:55:24] __main__ INFO: \u001b[0mVal 27\n",
      "\u001b[32m[2020-07-10 13:55:25] __main__ INFO: \u001b[0mEpoch 27 loss 0.3352 acc@1 0.9050 acc@5 0.9958\n",
      "\u001b[32m[2020-07-10 13:55:25] __main__ INFO: \u001b[0mElapsed 1.08\n",
      "\u001b[32m[2020-07-10 13:55:25] __main__ INFO: \u001b[0mTrain 28 9477\n",
      "\u001b[32m[2020-07-10 13:55:35] __main__ INFO: \u001b[0mEpoch 28 Step 100/351 lr 0.001000 loss 0.0623 (0.0949) acc@1 0.9922 (0.9680) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-07-10 13:55:45] __main__ INFO: \u001b[0mEpoch 28 Step 200/351 lr 0.001000 loss 0.1333 (0.0943) acc@1 0.9453 (0.9670) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-10 13:55:54] __main__ INFO: \u001b[0mEpoch 28 Step 300/351 lr 0.001000 loss 0.0341 (0.0980) acc@1 0.9922 (0.9659) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-10 13:55:59] __main__ INFO: \u001b[0mEpoch 28 Step 351/351 lr 0.001000 loss 0.0621 (0.1000) acc@1 0.9766 (0.9654) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-10 13:55:59] __main__ INFO: \u001b[0mElapsed 33.69\n",
      "\u001b[32m[2020-07-10 13:55:59] __main__ INFO: \u001b[0mVal 28\n",
      "\u001b[32m[2020-07-10 13:56:00] __main__ INFO: \u001b[0mEpoch 28 loss 0.3376 acc@1 0.9036 acc@5 0.9968\n",
      "\u001b[32m[2020-07-10 13:56:00] __main__ INFO: \u001b[0mElapsed 1.11\n",
      "\u001b[32m[2020-07-10 13:56:00] __main__ INFO: \u001b[0mTrain 29 9828\n",
      "\u001b[32m[2020-07-10 13:56:10] __main__ INFO: \u001b[0mEpoch 29 Step 100/351 lr 0.001000 loss 0.0883 (0.0964) acc@1 0.9766 (0.9681) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-07-10 13:56:19] __main__ INFO: \u001b[0mEpoch 29 Step 200/351 lr 0.001000 loss 0.1060 (0.0965) acc@1 0.9531 (0.9673) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-07-10 13:56:29] __main__ INFO: \u001b[0mEpoch 29 Step 300/351 lr 0.001000 loss 0.0788 (0.0960) acc@1 0.9766 (0.9671) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-07-10 13:56:34] __main__ INFO: \u001b[0mEpoch 29 Step 351/351 lr 0.001000 loss 0.0875 (0.0974) acc@1 0.9609 (0.9666) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-07-10 13:56:34] __main__ INFO: \u001b[0mElapsed 33.67\n",
      "\u001b[32m[2020-07-10 13:56:34] __main__ INFO: \u001b[0mVal 29\n",
      "\u001b[32m[2020-07-10 13:56:35] __main__ INFO: \u001b[0mEpoch 29 loss 0.3377 acc@1 0.9062 acc@5 0.9960\n",
      "\u001b[32m[2020-07-10 13:56:35] __main__ INFO: \u001b[0mElapsed 1.09\n",
      "\u001b[32m[2020-07-10 13:56:35] __main__ INFO: \u001b[0mTrain 30 10179\n",
      "\u001b[32m[2020-07-10 13:56:45] __main__ INFO: \u001b[0mEpoch 30 Step 100/351 lr 0.001000 loss 0.0948 (0.0969) acc@1 0.9688 (0.9660) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-10 13:56:54] __main__ INFO: \u001b[0mEpoch 30 Step 200/351 lr 0.001000 loss 0.0548 (0.0988) acc@1 0.9766 (0.9657) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-10 13:57:04] __main__ INFO: \u001b[0mEpoch 30 Step 300/351 lr 0.001000 loss 0.0757 (0.0990) acc@1 0.9609 (0.9656) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-10 13:57:09] __main__ INFO: \u001b[0mEpoch 30 Step 351/351 lr 0.001000 loss 0.1422 (0.0980) acc@1 0.9297 (0.9661) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-10 13:57:09] __main__ INFO: \u001b[0mElapsed 33.68\n",
      "\u001b[32m[2020-07-10 13:57:09] __main__ INFO: \u001b[0mVal 30\n",
      "\u001b[32m[2020-07-10 13:57:10] __main__ INFO: \u001b[0mEpoch 30 loss 0.3408 acc@1 0.9080 acc@5 0.9966\n",
      "\u001b[32m[2020-07-10 13:57:10] __main__ INFO: \u001b[0mElapsed 1.09\n",
      "\u001b[32m[2020-07-10 13:57:10] __main__ INFO: \u001b[0mTrain 31 10530\n",
      "\u001b[32m[2020-07-10 13:57:20] __main__ INFO: \u001b[0mEpoch 31 Step 100/351 lr 0.001000 loss 0.1029 (0.0961) acc@1 0.9609 (0.9663) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-10 13:57:29] __main__ INFO: \u001b[0mEpoch 31 Step 200/351 lr 0.001000 loss 0.0791 (0.0958) acc@1 0.9688 (0.9665) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-07-10 13:57:39] __main__ INFO: \u001b[0mEpoch 31 Step 300/351 lr 0.001000 loss 0.0550 (0.0939) acc@1 0.9844 (0.9671) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-10 13:57:43] __main__ INFO: \u001b[0mEpoch 31 Step 351/351 lr 0.001000 loss 0.0800 (0.0950) acc@1 0.9688 (0.9668) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-10 13:57:44] __main__ INFO: \u001b[0mElapsed 33.72\n",
      "\u001b[32m[2020-07-10 13:57:44] __main__ INFO: \u001b[0mVal 31\n",
      "\u001b[32m[2020-07-10 13:57:45] __main__ INFO: \u001b[0mEpoch 31 loss 0.3374 acc@1 0.9076 acc@5 0.9964\n",
      "\u001b[32m[2020-07-10 13:57:45] __main__ INFO: \u001b[0mElapsed 1.09\n",
      "\u001b[32m[2020-07-10 13:57:45] __main__ INFO: \u001b[0mTrain 32 10881\n",
      "\u001b[32m[2020-07-10 13:57:54] __main__ INFO: \u001b[0mEpoch 32 Step 100/351 lr 0.001000 loss 0.0879 (0.0928) acc@1 0.9766 (0.9674) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-10 13:58:04] __main__ INFO: \u001b[0mEpoch 32 Step 200/351 lr 0.001000 loss 0.1093 (0.0936) acc@1 0.9531 (0.9685) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-10 13:58:13] __main__ INFO: \u001b[0mEpoch 32 Step 300/351 lr 0.001000 loss 0.0604 (0.0932) acc@1 0.9766 (0.9685) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-10 13:58:18] __main__ INFO: \u001b[0mEpoch 32 Step 351/351 lr 0.001000 loss 0.0447 (0.0931) acc@1 0.9766 (0.9686) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-10 13:58:18] __main__ INFO: \u001b[0mElapsed 33.63\n",
      "\u001b[32m[2020-07-10 13:58:18] __main__ INFO: \u001b[0mVal 32\n",
      "\u001b[32m[2020-07-10 13:58:19] __main__ INFO: \u001b[0mEpoch 32 loss 0.3371 acc@1 0.9072 acc@5 0.9968\n",
      "\u001b[32m[2020-07-10 13:58:19] __main__ INFO: \u001b[0mElapsed 1.10\n",
      "\u001b[32m[2020-07-10 13:58:19] __main__ INFO: \u001b[0mTrain 33 11232\n",
      "\u001b[32m[2020-07-10 13:58:29] __main__ INFO: \u001b[0mEpoch 33 Step 100/351 lr 0.001000 loss 0.0640 (0.0972) acc@1 0.9766 (0.9665) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-10 13:58:39] __main__ INFO: \u001b[0mEpoch 33 Step 200/351 lr 0.001000 loss 0.0704 (0.0950) acc@1 0.9766 (0.9680) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-07-10 13:58:48] __main__ INFO: \u001b[0mEpoch 33 Step 300/351 lr 0.001000 loss 0.1778 (0.0958) acc@1 0.9062 (0.9673) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-10 13:58:53] __main__ INFO: \u001b[0mEpoch 33 Step 351/351 lr 0.001000 loss 0.0656 (0.0950) acc@1 0.9844 (0.9673) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-07-10 13:58:53] __main__ INFO: \u001b[0mElapsed 33.79\n",
      "\u001b[32m[2020-07-10 13:58:53] __main__ INFO: \u001b[0mVal 33\n",
      "\u001b[32m[2020-07-10 13:58:54] __main__ INFO: \u001b[0mEpoch 33 loss 0.3410 acc@1 0.9080 acc@5 0.9968\n",
      "\u001b[32m[2020-07-10 13:58:54] __main__ INFO: \u001b[0mElapsed 1.09\n",
      "\u001b[32m[2020-07-10 13:58:54] __main__ INFO: \u001b[0mTrain 34 11583\n",
      "\u001b[32m[2020-07-10 13:59:04] __main__ INFO: \u001b[0mEpoch 34 Step 100/351 lr 0.001000 loss 0.0554 (0.0914) acc@1 0.9766 (0.9688) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-07-10 13:59:14] __main__ INFO: \u001b[0mEpoch 34 Step 200/351 lr 0.001000 loss 0.1264 (0.0910) acc@1 0.9531 (0.9693) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-10 13:59:23] __main__ INFO: \u001b[0mEpoch 34 Step 300/351 lr 0.001000 loss 0.0851 (0.0894) acc@1 0.9531 (0.9699) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-10 13:59:28] __main__ INFO: \u001b[0mEpoch 34 Step 351/351 lr 0.001000 loss 0.0623 (0.0889) acc@1 0.9922 (0.9701) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-10 13:59:28] __main__ INFO: \u001b[0mElapsed 33.80\n",
      "\u001b[32m[2020-07-10 13:59:28] __main__ INFO: \u001b[0mVal 34\n",
      "\u001b[32m[2020-07-10 13:59:29] __main__ INFO: \u001b[0mEpoch 34 loss 0.3439 acc@1 0.9096 acc@5 0.9968\n",
      "\u001b[32m[2020-07-10 13:59:29] __main__ INFO: \u001b[0mElapsed 1.09\n",
      "\u001b[32m[2020-07-10 13:59:29] __main__ INFO: \u001b[0mTrain 35 11934\n",
      "\u001b[32m[2020-07-10 13:59:39] __main__ INFO: \u001b[0mEpoch 35 Step 100/351 lr 0.001000 loss 0.0902 (0.0910) acc@1 0.9609 (0.9677) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-10 13:59:48] __main__ INFO: \u001b[0mEpoch 35 Step 200/351 lr 0.001000 loss 0.0736 (0.0898) acc@1 0.9844 (0.9688) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-10 13:59:58] __main__ INFO: \u001b[0mEpoch 35 Step 300/351 lr 0.001000 loss 0.0452 (0.0897) acc@1 0.9922 (0.9692) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-10 14:00:03] __main__ INFO: \u001b[0mEpoch 35 Step 351/351 lr 0.001000 loss 0.0958 (0.0894) acc@1 0.9453 (0.9691) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-10 14:00:03] __main__ INFO: \u001b[0mElapsed 33.88\n",
      "\u001b[32m[2020-07-10 14:00:03] __main__ INFO: \u001b[0mVal 35\n",
      "\u001b[32m[2020-07-10 14:00:04] __main__ INFO: \u001b[0mEpoch 35 loss 0.3452 acc@1 0.9090 acc@5 0.9960\n",
      "\u001b[32m[2020-07-10 14:00:04] __main__ INFO: \u001b[0mElapsed 1.10\n",
      "\u001b[32m[2020-07-10 14:00:04] __main__ INFO: \u001b[0mTrain 36 12285\n",
      "\u001b[32m[2020-07-10 14:00:14] __main__ INFO: \u001b[0mEpoch 36 Step 100/351 lr 0.001000 loss 0.0762 (0.0842) acc@1 0.9766 (0.9719) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-07-10 14:00:23] __main__ INFO: \u001b[0mEpoch 36 Step 200/351 lr 0.001000 loss 0.0822 (0.0850) acc@1 0.9766 (0.9715) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-07-10 14:00:33] __main__ INFO: \u001b[0mEpoch 36 Step 300/351 lr 0.001000 loss 0.0499 (0.0844) acc@1 0.9922 (0.9714) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-07-10 14:00:38] __main__ INFO: \u001b[0mEpoch 36 Step 351/351 lr 0.001000 loss 0.0766 (0.0846) acc@1 0.9688 (0.9711) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-10 14:00:38] __main__ INFO: \u001b[0mElapsed 33.77\n",
      "\u001b[32m[2020-07-10 14:00:38] __main__ INFO: \u001b[0mVal 36\n",
      "\u001b[32m[2020-07-10 14:00:39] __main__ INFO: \u001b[0mEpoch 36 loss 0.3504 acc@1 0.9078 acc@5 0.9956\n",
      "\u001b[32m[2020-07-10 14:00:39] __main__ INFO: \u001b[0mElapsed 1.09\n",
      "\u001b[32m[2020-07-10 14:00:39] __main__ INFO: \u001b[0mTrain 37 12636\n",
      "\u001b[32m[2020-07-10 14:00:49] __main__ INFO: \u001b[0mEpoch 37 Step 100/351 lr 0.001000 loss 0.1013 (0.0830) acc@1 0.9531 (0.9710) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-10 14:00:58] __main__ INFO: \u001b[0mEpoch 37 Step 200/351 lr 0.001000 loss 0.1076 (0.0855) acc@1 0.9609 (0.9704) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-10 14:01:08] __main__ INFO: \u001b[0mEpoch 37 Step 300/351 lr 0.001000 loss 0.0437 (0.0867) acc@1 0.9766 (0.9700) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-10 14:01:13] __main__ INFO: \u001b[0mEpoch 37 Step 351/351 lr 0.001000 loss 0.0878 (0.0869) acc@1 0.9766 (0.9699) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-10 14:01:13] __main__ INFO: \u001b[0mElapsed 33.67\n",
      "\u001b[32m[2020-07-10 14:01:13] __main__ INFO: \u001b[0mVal 37\n",
      "\u001b[32m[2020-07-10 14:01:14] __main__ INFO: \u001b[0mEpoch 37 loss 0.3490 acc@1 0.9080 acc@5 0.9958\n",
      "\u001b[32m[2020-07-10 14:01:14] __main__ INFO: \u001b[0mElapsed 1.10\n",
      "\u001b[32m[2020-07-10 14:01:14] __main__ INFO: \u001b[0mTrain 38 12987\n",
      "\u001b[32m[2020-07-10 14:01:23] __main__ INFO: \u001b[0mEpoch 38 Step 100/351 lr 0.001000 loss 0.0644 (0.0849) acc@1 0.9688 (0.9699) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-10 14:01:33] __main__ INFO: \u001b[0mEpoch 38 Step 200/351 lr 0.001000 loss 0.0742 (0.0876) acc@1 0.9844 (0.9698) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-10 14:01:43] __main__ INFO: \u001b[0mEpoch 38 Step 300/351 lr 0.001000 loss 0.0984 (0.0855) acc@1 0.9688 (0.9715) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-10 14:01:47] __main__ INFO: \u001b[0mEpoch 38 Step 351/351 lr 0.001000 loss 0.0811 (0.0865) acc@1 0.9688 (0.9710) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-10 14:01:47] __main__ INFO: \u001b[0mElapsed 33.71\n",
      "\u001b[32m[2020-07-10 14:01:47] __main__ INFO: \u001b[0mVal 38\n",
      "\u001b[32m[2020-07-10 14:01:49] __main__ INFO: \u001b[0mEpoch 38 loss 0.3472 acc@1 0.9080 acc@5 0.9968\n",
      "\u001b[32m[2020-07-10 14:01:49] __main__ INFO: \u001b[0mElapsed 1.12\n",
      "\u001b[32m[2020-07-10 14:01:49] __main__ INFO: \u001b[0mTrain 39 13338\n",
      "\u001b[32m[2020-07-10 14:01:58] __main__ INFO: \u001b[0mEpoch 39 Step 100/351 lr 0.001000 loss 0.0629 (0.0850) acc@1 0.9688 (0.9700) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-10 14:02:08] __main__ INFO: \u001b[0mEpoch 39 Step 200/351 lr 0.001000 loss 0.0309 (0.0839) acc@1 0.9922 (0.9696) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-10 14:02:17] __main__ INFO: \u001b[0mEpoch 39 Step 300/351 lr 0.001000 loss 0.1341 (0.0844) acc@1 0.9453 (0.9702) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-10 14:02:22] __main__ INFO: \u001b[0mEpoch 39 Step 351/351 lr 0.001000 loss 0.0952 (0.0853) acc@1 0.9688 (0.9700) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-10 14:02:22] __main__ INFO: \u001b[0mElapsed 33.68\n",
      "\u001b[32m[2020-07-10 14:02:22] __main__ INFO: \u001b[0mVal 39\n",
      "\u001b[32m[2020-07-10 14:02:23] __main__ INFO: \u001b[0mEpoch 39 loss 0.3553 acc@1 0.9078 acc@5 0.9958\n",
      "\u001b[32m[2020-07-10 14:02:23] __main__ INFO: \u001b[0mElapsed 1.09\n",
      "\u001b[32m[2020-07-10 14:02:23] __main__ INFO: \u001b[0mTrain 40 13689\n",
      "\u001b[32m[2020-07-10 14:02:33] __main__ INFO: \u001b[0mEpoch 40 Step 100/351 lr 0.001000 loss 0.0494 (0.0797) acc@1 0.9922 (0.9732) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-10 14:02:43] __main__ INFO: \u001b[0mEpoch 40 Step 200/351 lr 0.001000 loss 0.0630 (0.0812) acc@1 0.9844 (0.9721) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-10 14:02:52] __main__ INFO: \u001b[0mEpoch 40 Step 300/351 lr 0.001000 loss 0.1254 (0.0809) acc@1 0.9609 (0.9723) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-10 14:02:57] __main__ INFO: \u001b[0mEpoch 40 Step 351/351 lr 0.001000 loss 0.1779 (0.0817) acc@1 0.9609 (0.9719) acc@5 0.9922 (0.9998)\n",
      "\u001b[32m[2020-07-10 14:02:57] __main__ INFO: \u001b[0mElapsed 33.65\n",
      "\u001b[32m[2020-07-10 14:02:57] __main__ INFO: \u001b[0mVal 40\n",
      "\u001b[32m[2020-07-10 14:02:58] __main__ INFO: \u001b[0mEpoch 40 loss 0.3490 acc@1 0.9090 acc@5 0.9960\n",
      "\u001b[32m[2020-07-10 14:02:58] __main__ INFO: \u001b[0mElapsed 1.07\n",
      "\u001b[32m[2020-07-10 14:02:58] __main__ INFO: \u001b[0mTrain 41 14040\n",
      "\u001b[32m[2020-07-10 14:03:08] __main__ INFO: \u001b[0mEpoch 41 Step 100/351 lr 0.001000 loss 0.1074 (0.0756) acc@1 0.9688 (0.9739) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-10 14:03:17] __main__ INFO: \u001b[0mEpoch 41 Step 200/351 lr 0.001000 loss 0.0782 (0.0804) acc@1 0.9766 (0.9722) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-07-10 14:03:27] __main__ INFO: \u001b[0mEpoch 41 Step 300/351 lr 0.001000 loss 0.1021 (0.0804) acc@1 0.9453 (0.9721) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-07-10 14:03:32] __main__ INFO: \u001b[0mEpoch 41 Step 351/351 lr 0.001000 loss 0.0314 (0.0809) acc@1 0.9844 (0.9718) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-10 14:03:32] __main__ INFO: \u001b[0mElapsed 33.65\n",
      "\u001b[32m[2020-07-10 14:03:32] __main__ INFO: \u001b[0mVal 41\n",
      "\u001b[32m[2020-07-10 14:03:33] __main__ INFO: \u001b[0mEpoch 41 loss 0.3635 acc@1 0.9078 acc@5 0.9960\n",
      "\u001b[32m[2020-07-10 14:03:33] __main__ INFO: \u001b[0mElapsed 1.07\n",
      "\u001b[32m[2020-07-10 14:03:33] __main__ INFO: \u001b[0mTrain 42 14391\n",
      "\u001b[32m[2020-07-10 14:03:42] __main__ INFO: \u001b[0mEpoch 42 Step 100/351 lr 0.001000 loss 0.0611 (0.0814) acc@1 0.9766 (0.9721) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-10 14:03:52] __main__ INFO: \u001b[0mEpoch 42 Step 200/351 lr 0.001000 loss 0.1071 (0.0788) acc@1 0.9531 (0.9721) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-10 14:04:02] __main__ INFO: \u001b[0mEpoch 42 Step 300/351 lr 0.001000 loss 0.1021 (0.0785) acc@1 0.9531 (0.9721) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-10 14:04:06] __main__ INFO: \u001b[0mEpoch 42 Step 351/351 lr 0.001000 loss 0.0538 (0.0791) acc@1 0.9844 (0.9722) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-10 14:04:06] __main__ INFO: \u001b[0mElapsed 33.70\n",
      "\u001b[32m[2020-07-10 14:04:06] __main__ INFO: \u001b[0mVal 42\n",
      "\u001b[32m[2020-07-10 14:04:08] __main__ INFO: \u001b[0mEpoch 42 loss 0.3571 acc@1 0.9090 acc@5 0.9968\n",
      "\u001b[32m[2020-07-10 14:04:08] __main__ INFO: \u001b[0mElapsed 1.09\n",
      "\u001b[32m[2020-07-10 14:04:08] __main__ INFO: \u001b[0mTrain 43 14742\n",
      "\u001b[32m[2020-07-10 14:04:17] __main__ INFO: \u001b[0mEpoch 43 Step 100/351 lr 0.001000 loss 0.1324 (0.0818) acc@1 0.9531 (0.9709) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-10 14:04:27] __main__ INFO: \u001b[0mEpoch 43 Step 200/351 lr 0.001000 loss 0.0599 (0.0771) acc@1 0.9766 (0.9733) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-10 14:04:36] __main__ INFO: \u001b[0mEpoch 43 Step 300/351 lr 0.001000 loss 0.0780 (0.0784) acc@1 0.9766 (0.9729) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-10 14:04:41] __main__ INFO: \u001b[0mEpoch 43 Step 351/351 lr 0.001000 loss 0.0389 (0.0785) acc@1 0.9844 (0.9730) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-10 14:04:41] __main__ INFO: \u001b[0mElapsed 33.56\n",
      "\u001b[32m[2020-07-10 14:04:41] __main__ INFO: \u001b[0mVal 43\n",
      "\u001b[32m[2020-07-10 14:04:42] __main__ INFO: \u001b[0mEpoch 43 loss 0.3529 acc@1 0.9088 acc@5 0.9968\n",
      "\u001b[32m[2020-07-10 14:04:42] __main__ INFO: \u001b[0mElapsed 1.08\n",
      "\u001b[32m[2020-07-10 14:04:42] __main__ INFO: \u001b[0mTrain 44 15093\n",
      "\u001b[32m[2020-07-10 14:04:52] __main__ INFO: \u001b[0mEpoch 44 Step 100/351 lr 0.001000 loss 0.0337 (0.0759) acc@1 0.9922 (0.9751) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-10 14:05:01] __main__ INFO: \u001b[0mEpoch 44 Step 200/351 lr 0.001000 loss 0.0254 (0.0730) acc@1 0.9922 (0.9755) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-10 14:05:11] __main__ INFO: \u001b[0mEpoch 44 Step 300/351 lr 0.001000 loss 0.0818 (0.0735) acc@1 0.9766 (0.9753) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-10 14:05:16] __main__ INFO: \u001b[0mEpoch 44 Step 351/351 lr 0.001000 loss 0.0766 (0.0749) acc@1 0.9766 (0.9745) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-10 14:05:16] __main__ INFO: \u001b[0mElapsed 33.64\n",
      "\u001b[32m[2020-07-10 14:05:16] __main__ INFO: \u001b[0mVal 44\n",
      "\u001b[32m[2020-07-10 14:05:17] __main__ INFO: \u001b[0mEpoch 44 loss 0.3657 acc@1 0.9036 acc@5 0.9964\n",
      "\u001b[32m[2020-07-10 14:05:17] __main__ INFO: \u001b[0mElapsed 1.07\n",
      "\u001b[32m[2020-07-10 14:05:17] __main__ INFO: \u001b[0mTrain 45 15444\n",
      "\u001b[32m[2020-07-10 14:05:27] __main__ INFO: \u001b[0mEpoch 45 Step 100/351 lr 0.001000 loss 0.0658 (0.0727) acc@1 0.9688 (0.9754) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-10 14:05:36] __main__ INFO: \u001b[0mEpoch 45 Step 200/351 lr 0.001000 loss 0.0851 (0.0725) acc@1 0.9688 (0.9750) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-10 14:05:46] __main__ INFO: \u001b[0mEpoch 45 Step 300/351 lr 0.001000 loss 0.0598 (0.0743) acc@1 0.9844 (0.9746) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-10 14:05:51] __main__ INFO: \u001b[0mEpoch 45 Step 351/351 lr 0.001000 loss 0.1059 (0.0757) acc@1 0.9766 (0.9744) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-10 14:05:51] __main__ INFO: \u001b[0mElapsed 33.65\n",
      "\u001b[32m[2020-07-10 14:05:51] __main__ INFO: \u001b[0mVal 45\n",
      "\u001b[32m[2020-07-10 14:05:52] __main__ INFO: \u001b[0mEpoch 45 loss 0.3691 acc@1 0.9078 acc@5 0.9964\n",
      "\u001b[32m[2020-07-10 14:05:52] __main__ INFO: \u001b[0mElapsed 1.07\n",
      "\u001b[32m[2020-07-10 14:05:52] __main__ INFO: \u001b[0mTrain 46 15795\n",
      "\u001b[32m[2020-07-10 14:06:01] __main__ INFO: \u001b[0mEpoch 46 Step 100/351 lr 0.001000 loss 0.0457 (0.0703) acc@1 0.9844 (0.9763) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-10 14:06:11] __main__ INFO: \u001b[0mEpoch 46 Step 200/351 lr 0.001000 loss 0.0281 (0.0694) acc@1 1.0000 (0.9765) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-10 14:06:20] __main__ INFO: \u001b[0mEpoch 46 Step 300/351 lr 0.001000 loss 0.1295 (0.0721) acc@1 0.9609 (0.9758) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-10 14:06:25] __main__ INFO: \u001b[0mEpoch 46 Step 351/351 lr 0.001000 loss 0.0567 (0.0745) acc@1 0.9766 (0.9745) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-10 14:06:25] __main__ INFO: \u001b[0mElapsed 33.68\n",
      "\u001b[32m[2020-07-10 14:06:25] __main__ INFO: \u001b[0mVal 46\n",
      "\u001b[32m[2020-07-10 14:06:26] __main__ INFO: \u001b[0mEpoch 46 loss 0.3662 acc@1 0.9064 acc@5 0.9964\n",
      "\u001b[32m[2020-07-10 14:06:26] __main__ INFO: \u001b[0mElapsed 1.06\n",
      "\u001b[32m[2020-07-10 14:06:26] __main__ INFO: \u001b[0mTrain 47 16146\n",
      "\u001b[32m[2020-07-10 14:06:36] __main__ INFO: \u001b[0mEpoch 47 Step 100/351 lr 0.001000 loss 0.0535 (0.0723) acc@1 0.9844 (0.9763) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-10 14:06:46] __main__ INFO: \u001b[0mEpoch 47 Step 200/351 lr 0.001000 loss 0.0787 (0.0724) acc@1 0.9766 (0.9753) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-10 14:06:55] __main__ INFO: \u001b[0mEpoch 47 Step 300/351 lr 0.001000 loss 0.0977 (0.0709) acc@1 0.9688 (0.9754) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-10 14:07:00] __main__ INFO: \u001b[0mEpoch 47 Step 351/351 lr 0.001000 loss 0.0298 (0.0715) acc@1 1.0000 (0.9750) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-10 14:07:00] __main__ INFO: \u001b[0mElapsed 33.54\n",
      "\u001b[32m[2020-07-10 14:07:00] __main__ INFO: \u001b[0mVal 47\n",
      "\u001b[32m[2020-07-10 14:07:01] __main__ INFO: \u001b[0mEpoch 47 loss 0.3588 acc@1 0.9058 acc@5 0.9970\n",
      "\u001b[32m[2020-07-10 14:07:01] __main__ INFO: \u001b[0mElapsed 1.07\n",
      "\u001b[32m[2020-07-10 14:07:01] __main__ INFO: \u001b[0mTrain 48 16497\n",
      "\u001b[32m[2020-07-10 14:07:11] __main__ INFO: \u001b[0mEpoch 48 Step 100/351 lr 0.001000 loss 0.0729 (0.0681) acc@1 0.9766 (0.9765) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-10 14:07:20] __main__ INFO: \u001b[0mEpoch 48 Step 200/351 lr 0.001000 loss 0.0691 (0.0695) acc@1 0.9766 (0.9759) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-10 14:07:30] __main__ INFO: \u001b[0mEpoch 48 Step 300/351 lr 0.001000 loss 0.0703 (0.0686) acc@1 0.9766 (0.9763) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-10 14:07:34] __main__ INFO: \u001b[0mEpoch 48 Step 351/351 lr 0.001000 loss 0.1029 (0.0691) acc@1 0.9688 (0.9760) acc@5 0.9922 (0.9999)\n",
      "\u001b[32m[2020-07-10 14:07:34] __main__ INFO: \u001b[0mElapsed 33.50\n",
      "\u001b[32m[2020-07-10 14:07:34] __main__ INFO: \u001b[0mVal 48\n",
      "\u001b[32m[2020-07-10 14:07:36] __main__ INFO: \u001b[0mEpoch 48 loss 0.3703 acc@1 0.9080 acc@5 0.9968\n",
      "\u001b[32m[2020-07-10 14:07:36] __main__ INFO: \u001b[0mElapsed 1.07\n",
      "\u001b[32m[2020-07-10 14:07:36] __main__ INFO: \u001b[0mTrain 49 16848\n",
      "\u001b[32m[2020-07-10 14:07:45] __main__ INFO: \u001b[0mEpoch 49 Step 100/351 lr 0.001000 loss 0.0442 (0.0746) acc@1 0.9844 (0.9749) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-10 14:07:55] __main__ INFO: \u001b[0mEpoch 49 Step 200/351 lr 0.001000 loss 0.0548 (0.0700) acc@1 0.9844 (0.9759) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-10 14:08:04] __main__ INFO: \u001b[0mEpoch 49 Step 300/351 lr 0.001000 loss 0.0543 (0.0715) acc@1 0.9844 (0.9751) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-10 14:08:09] __main__ INFO: \u001b[0mEpoch 49 Step 351/351 lr 0.001000 loss 0.0718 (0.0720) acc@1 0.9688 (0.9750) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-10 14:08:09] __main__ INFO: \u001b[0mElapsed 33.67\n",
      "\u001b[32m[2020-07-10 14:08:09] __main__ INFO: \u001b[0mVal 49\n",
      "\u001b[32m[2020-07-10 14:08:10] __main__ INFO: \u001b[0mEpoch 49 loss 0.3666 acc@1 0.9070 acc@5 0.9968\n",
      "\u001b[32m[2020-07-10 14:08:10] __main__ INFO: \u001b[0mElapsed 1.11\n",
      "\u001b[32m[2020-07-10 14:08:10] __main__ INFO: \u001b[0mTrain 50 17199\n",
      "\u001b[32m[2020-07-10 14:08:20] __main__ INFO: \u001b[0mEpoch 50 Step 100/351 lr 0.001000 loss 0.1253 (0.0639) acc@1 0.9688 (0.9785) acc@5 0.9922 (0.9999)\n",
      "\u001b[32m[2020-07-10 14:08:30] __main__ INFO: \u001b[0mEpoch 50 Step 200/351 lr 0.001000 loss 0.1178 (0.0681) acc@1 0.9609 (0.9763) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-10 14:08:39] __main__ INFO: \u001b[0mEpoch 50 Step 300/351 lr 0.001000 loss 0.0269 (0.0676) acc@1 0.9922 (0.9762) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-10 14:08:44] __main__ INFO: \u001b[0mEpoch 50 Step 351/351 lr 0.001000 loss 0.0858 (0.0674) acc@1 0.9609 (0.9762) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-10 14:08:44] __main__ INFO: \u001b[0mElapsed 33.71\n",
      "\u001b[32m[2020-07-10 14:08:44] __main__ INFO: \u001b[0mVal 50\n",
      "\u001b[32m[2020-07-10 14:08:45] __main__ INFO: \u001b[0mEpoch 50 loss 0.3783 acc@1 0.9078 acc@5 0.9958\n",
      "\u001b[32m[2020-07-10 14:08:45] __main__ INFO: \u001b[0mElapsed 1.09\n",
      "\u001b[32m[2020-07-10 14:08:45] fvcore.common.checkpoint INFO: \u001b[0mSaving checkpoint to /home/ec2-user/SageMaker/experiments/resnet_basic_32_cm_1_1/exp00_resume400_50/checkpoint_00050.pth\n"
     ]
    }
   ],
   "source": [
    "# Resume training with the un-augmented data\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "!python train.py --config /home/ec2-user/SageMaker/experiments/resnet_basic_32_cm_1_.5/exp00/config.yaml \\\n",
    "    train.checkpoint /home/ec2-user/SageMaker/experiments/resnet_basic_32_cm_1_.5/exp00/checkpoint_00400.pth \\\n",
    "    dataset.name CIFAR10 \\\n",
    "    train.base_lr .001 \\\n",
    "    train.output_dir /home/ec2-user/SageMaker/experiments/resnet_basic_32_cm_1_.5/exp00_resume400_50 \\\n",
    "    scheduler.epochs 50\n",
    "\n",
    "#### Set LEARNING RATE based on ending LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-10 14:10:31] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/resnet_basic_32_cm_1_1/exp00_resume400_50/checkpoint_00050.pth\n",
      "Files already downloaded and verified\n",
      "100%|| 79/79 [00:02<00:00, 26.66it/s]\n",
      "\u001b[32m[2020-07-10 14:10:35] __main__ INFO: \u001b[0mElapsed 2.97\n",
      "\u001b[32m[2020-07-10 14:10:35] __main__ INFO: \u001b[0mLoss 0.4049 Accuracy 0.9010\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/resnet.yaml \\\n",
    "   model.resnet.depth 32 \\\n",
    "   test.batch_size 128 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/resnet_basic_32_cm_1_.5/exp00_resume400_50/checkpoint_00050.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/resnet_basic_32_cm_1_.5/exp00_resume400_50/test_results_0050_cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-10 14:10:51] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/resnet_basic_32_cm_1_1/exp00_resume400_50/checkpoint_00050.pth\n",
      "CIFAR 10.1\n",
      "100%|| 16/16 [00:00<00:00, 17.67it/s]\n",
      "\u001b[32m[2020-07-10 14:10:52] __main__ INFO: \u001b[0mElapsed 0.91\n",
      "\u001b[32m[2020-07-10 14:10:52] __main__ INFO: \u001b[0mLoss 0.8114 Accuracy 0.8110\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10.1 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/resnet.yaml \\\n",
    "   model.resnet.depth 32 \\\n",
    "   test.batch_size 128 \\\n",
    "   dataset.name CIFAR101 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/resnet_basic_32_cm_1_.5/exp00_resume400_50/checkpoint_00050.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/resnet_basic_32_cm_1_.5/exp00_resume400_50/test_results_0050_cifar101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-10 14:11:23] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/resnet_basic_32_cm_1_1/exp00/checkpoint_00400.pth\n",
      "Files already downloaded and verified\n",
      "100%|| 79/79 [00:02<00:00, 27.59it/s]\n",
      "\u001b[32m[2020-07-10 14:11:27] __main__ INFO: \u001b[0mElapsed 2.87\n",
      "\u001b[32m[2020-07-10 14:11:27] __main__ INFO: \u001b[0mLoss 0.5840 Accuracy 0.8608\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/resnet.yaml \\\n",
    "   model.resnet.depth 32 \\\n",
    "   test.batch_size 128 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/resnet_basic_32_cm_1_.5/exp00/checkpoint_00400.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/resnet_basic_32_cm_1_.5/exp00/test_results_0400_cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-10 14:11:36] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/resnet_basic_32_cm_1_1/exp00/checkpoint_00400.pth\n",
      "CIFAR 10.1\n",
      "100%|| 16/16 [00:00<00:00, 17.42it/s]\n",
      "\u001b[32m[2020-07-10 14:11:37] __main__ INFO: \u001b[0mElapsed 0.92\n",
      "\u001b[32m[2020-07-10 14:11:37] __main__ INFO: \u001b[0mLoss 1.0267 Accuracy 0.7240\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10.1 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "!python evaluate.py --config configs/cifar/resnet.yaml \\\n",
    "   model.resnet.depth 32 \\\n",
    "   test.batch_size 128 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/resnet_basic_32_cm_1_.5/exp00/checkpoint_00400.pth \\\n",
    "   dataset.name CIFAR101 \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/resnet_basic_32_cm_1_.5/exp00/test_results_0400_cifar101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Record the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Testset</th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Original_Accuracy</th>\n",
       "      <th>Original_CI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>resnet_basic_32</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>100</td>\n",
       "      <td>0.3604</td>\n",
       "      <td>0.9170</td>\n",
       "      <td>92.5</td>\n",
       "      <td>(92.0, 93.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>resnet_basic_32</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>160</td>\n",
       "      <td>0.4011</td>\n",
       "      <td>0.9232</td>\n",
       "      <td>92.5</td>\n",
       "      <td>(92.0, 93.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>resnet_basic_32</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>160</td>\n",
       "      <td>0.8051</td>\n",
       "      <td>0.8320</td>\n",
       "      <td>84.9</td>\n",
       "      <td>(83.2, 86.4)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Model    Testset  Epoch    Loss  Accuracy  Original_Accuracy  \\\n",
       "0  resnet_basic_32    cifar10    100  0.3604    0.9170               92.5   \n",
       "1  resnet_basic_32    cifar10    160  0.4011    0.9232               92.5   \n",
       "2  resnet_basic_32  cifar10.1    160  0.8051    0.8320               84.9   \n",
       "\n",
       "    Original_CI  \n",
       "0  (92.0, 93.0)  \n",
       "1  (92.0, 93.0)  \n",
       "2  (83.2, 86.4)  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write the results to a CSV file so that we can analyze later.\n",
    "import pandas as pd\n",
    "\n",
    "results = {'Model': ['resnet_basic_32', 'resnet_basic_32', 'resnet_basic_32']\n",
    "           'Testset': ['cifar10', 'cifar10', 'cifar10.1']\n",
    "           'Loss': [0.3604, 0.4011, 0.8051],\n",
    "           'Epoch': [100, 160, 160],\n",
    "           'Accuracy': [0.9170, 0.9232, 0.8320],\n",
    "           'Original_Accuracy': [92.5, 92.5, 84.9],\n",
    "           'Original_CI': [(92.0, 93.0), (92.0, 93.0), (83.2, 86.4)]\n",
    "           }\n",
    "\n",
    "df = pd.DataFrame(results, columns = ['Model', 'Testset', 'Epoch', 'Loss', 'Accuracy', \n",
    "                                      'Original_Accuracy', 'Original_CI'])\n",
    "\n",
    "\n",
    "df.to_csv('/home/ec2-user/SageMaker/experiments/resnet_basic_32/exp00/results.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Testset</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Original_Accuracy</th>\n",
       "      <th>Original_CI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>resnet_basic_32_cm_1_1</td>\n",
       "      <td>400</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.584</td>\n",
       "      <td>0.8608</td>\n",
       "      <td>92.5</td>\n",
       "      <td>(92.0, 93.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>resnet_basic_32_cm_1_1</td>\n",
       "      <td>400</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>1.0267</td>\n",
       "      <td>0.724</td>\n",
       "      <td>84.9</td>\n",
       "      <td>(83.2, 86.4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>resnet_basic_32_cm_1_1</td>\n",
       "      <td>50</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>0.8114</td>\n",
       "      <td>0.811</td>\n",
       "      <td>84.9</td>\n",
       "      <td>(83.2, 86.4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>resnet_basic_32_cm_1_1</td>\n",
       "      <td>50</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.4049</td>\n",
       "      <td>0.901</td>\n",
       "      <td>92.5</td>\n",
       "      <td>(92.0, 93.0)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Model Epoch    Testset    Loss Accuracy  \\\n",
       "0  resnet_basic_32_cm_1_1   400    cifar10   0.584   0.8608   \n",
       "1  resnet_basic_32_cm_1_1   400  cifar10.1  1.0267    0.724   \n",
       "2  resnet_basic_32_cm_1_1    50  cifar10.1  0.8114    0.811   \n",
       "3  resnet_basic_32_cm_1_1    50    cifar10  0.4049    0.901   \n",
       "\n",
       "   Original_Accuracy   Original_CI  \n",
       "0               92.5  (92.0, 93.0)  \n",
       "1               84.9  (83.2, 86.4)  \n",
       "2               84.9  (83.2, 86.4)  \n",
       "3               92.5  (92.0, 93.0)  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "model = 'resnet_basic_32_cm_1_.5'\n",
    "\n",
    "a = pd.Series([model, 400, 'cifar10' ])\n",
    "c = pd.Series([model, 400, 'cifar10.1' ])\n",
    "\n",
    "e = pd.Series([model, 50, 'cifar10.1' ])\n",
    "f = pd.Series([model, 50, 'cifar10' ])\n",
    "               \n",
    "df_results = pd.concat([a,c,e,f], axis=1).T\n",
    "df_results.columns = ['Model', 'Epoch', 'Testset', 'Loss', 'Accuracy']\n",
    "\n",
    "df_results['Original_Accuracy'] = df_results.apply((lambda row: 92.5 if row[2] == 'cifar10' else 84.9), axis=1)\n",
    "df_results['Original_CI'] = df_results.apply((lambda row: (92.0, 93.0) if row[2] == 'cifar10' else (83.2, 86.4)), axis=1)\n",
    "\n",
    "df_results.to_csv('/home/ec2-user/SageMaker/experiments/' + model + '/results.csv')\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the model checkpoints, configs, and results to S3 \n",
    "bucket='sagemaker-june29'\n",
    "prefix = 'sagemaker/results/original-models/resnet_basic_32_cm_1_.5'\n",
    "path = '/home/ec2-user/SageMaker/experiments/resnet_basic_32_cm_1_.5'\n",
    "\n",
    "s3_resource = boto3.resource(\"s3\", region_name=\"us-east-2\")\n",
    "\n",
    "def uploadDirectory(local_path,bucket_name,s3_prefix):\n",
    "\n",
    "    my_bucket = s3_resource.Bucket(bucket_name)\n",
    "    \n",
    "    for path, subdirs, files in os.walk(local_path):\n",
    "        path = path.replace(\"\\\\\",\"/\")\n",
    "        directory_name = path.replace(local_path,\"\")\n",
    "        for file in files:\n",
    "            #print(\"Local File:\", os.path.join(path, file))\n",
    "            #print(\"      Dest:\", s3_prefix+directory_name+'/'+file)\n",
    "            my_bucket.upload_file(os.path.join(path, file), s3_prefix+directory_name+'/'+file)\n",
    "    \n",
    "uploadDirectory(path,bucket,prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
