{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESNET 32\n",
    "\n",
    " - Training Dataset:  CutMix, beta=1, cutmix_prob=0.5\n",
    " - Sagemaker Notebook must be of type, conda_pytorch_p36\n",
    " \n",
    "#### Install Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import numpy \n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 1)) (1.18.1)\n",
      "Requirement already satisfied: torch>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 2)) (1.4.0)\n",
      "Requirement already satisfied: torchvision in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (0.5.0)\n",
      "Requirement already satisfied: fvcore in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (0.1.1.post20200711)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 5)) (4.44.1)\n",
      "Requirement already satisfied: yacs in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 6)) (0.1.7)\n",
      "Requirement already satisfied: apex from git+https://github.com/NVIDIA/apex.git#egg=apex in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 7)) (0.1)\n",
      "Requirement already satisfied: termcolor in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 8)) (1.1.0)\n",
      "Requirement already satisfied: thop<0.0.31.post2004070130 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 9)) (0.0.31.post2001170342)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (1.14.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (7.0.0)\n",
      "Requirement already satisfied: portalocker in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (1.7.0)\n",
      "Requirement already satisfied: tabulate in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (0.8.7)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (5.3.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: tensorboard in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (2.2.2)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.14.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.18.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (2.23.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (46.1.3.post20200330)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (0.9.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (0.4.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (3.2.2)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.18.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.7.0)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (0.34.2)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.30.0)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (3.12.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (2020.4.5.2)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (1.25.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (3.0.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard) (1.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (4.1.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (3.4.2)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard) (2.2.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard) (0.4.8)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r '/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt'\n",
    "\n",
    "# Need to add this to requirements.txt\n",
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-12 04:31:25] __main__ INFO: \u001b[0mdevice: cuda\n",
      "cudnn:\n",
      "  benchmark: True\n",
      "  deterministic: False\n",
      "dataset:\n",
      "  name: CIFAR10_CM_.5\n",
      "  dataset_dir: ''\n",
      "  image_size: 32\n",
      "  n_channels: 3\n",
      "  n_classes: 10\n",
      "model:\n",
      "  type: cifar\n",
      "  name: resnet\n",
      "  init_mode: kaiming_fan_out\n",
      "  vgg:\n",
      "    n_channels: [64, 128, 256, 512, 512]\n",
      "    n_layers: [2, 2, 3, 3, 3]\n",
      "    use_bn: True\n",
      "  resnet:\n",
      "    depth: 32\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "  resnet_preact:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "  wrn:\n",
      "    depth: 28\n",
      "    initial_channels: 16\n",
      "    widening_factor: 10\n",
      "    drop_rate: 0.0\n",
      "  densenet:\n",
      "    depth: 100\n",
      "    n_blocks: [6, 12, 24, 16]\n",
      "    block_type: bottleneck\n",
      "    growth_rate: 12\n",
      "    drop_rate: 0.0\n",
      "    compression_rate: 0.5\n",
      "  pyramidnet:\n",
      "    depth: 272\n",
      "    n_blocks: [3, 24, 36, 3]\n",
      "    initial_channels: 16\n",
      "    block_type: bottleneck\n",
      "    alpha: 200\n",
      "  resnext:\n",
      "    depth: 29\n",
      "    n_blocks: [3, 4, 6, 3]\n",
      "    initial_channels: 64\n",
      "    cardinality: 8\n",
      "    base_channels: 4\n",
      "  shake_shake:\n",
      "    depth: 26\n",
      "    initial_channels: 96\n",
      "    shake_forward: True\n",
      "    shake_backward: True\n",
      "    shake_image: True\n",
      "  se_resnet_preact:\n",
      "    depth: 110\n",
      "    initial_channels: 16\n",
      "    se_reduction: 16\n",
      "    block_type: basic\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "train:\n",
      "  checkpoint: ''\n",
      "  resume: False\n",
      "  precision: O0\n",
      "  batch_size: 128\n",
      "  subdivision: 1\n",
      "  optimizer: sgd\n",
      "  base_lr: 0.1\n",
      "  momentum: 0.9\n",
      "  nesterov: True\n",
      "  weight_decay: 0.0001\n",
      "  no_weight_decay_on_bn: False\n",
      "  gradient_clip: 0\n",
      "  start_epoch: 0\n",
      "  seed: 0\n",
      "  val_first: True\n",
      "  val_period: 1\n",
      "  val_ratio: 0.1\n",
      "  use_test_as_val: False\n",
      "  output_dir: /home/ec2-user/SageMaker/experiments/resnet_basic_32_cm_1_.5/exp00\n",
      "  log_period: 100\n",
      "  checkpoint_period: 100\n",
      "  use_tensorboard: True\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: True\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "  distributed: False\n",
      "  dist:\n",
      "    backend: nccl\n",
      "    init_method: env://\n",
      "    world_size: -1\n",
      "    node_rank: -1\n",
      "    local_rank: 0\n",
      "    use_sync_bn: False\n",
      "tensorboard:\n",
      "  train_images: False\n",
      "  val_images: False\n",
      "  model_params: False\n",
      "optim:\n",
      "  adam:\n",
      "    betas: (0.9, 0.999)\n",
      "  lars:\n",
      "    eps: 1e-09\n",
      "    threshold: 0.01\n",
      "  adabound:\n",
      "    betas: (0.9, 0.999)\n",
      "    final_lr: 0.1\n",
      "    gamma: 0.001\n",
      "scheduler:\n",
      "  epochs: 400\n",
      "  warmup:\n",
      "    type: none\n",
      "    epochs: 0\n",
      "    start_factor: 0.001\n",
      "    exponent: 4\n",
      "  type: multistep\n",
      "  milestones: [80, 120]\n",
      "  lr_decay: 0.1\n",
      "  lr_min_factor: 0.001\n",
      "  T0: 10\n",
      "  T_mul: 1.0\n",
      "validation:\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: False\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "augmentation:\n",
      "  use_random_crop: True\n",
      "  use_random_horizontal_flip: True\n",
      "  use_cutout: False\n",
      "  use_random_erasing: False\n",
      "  use_dual_cutout: False\n",
      "  use_mixup: False\n",
      "  use_ricap: False\n",
      "  use_cutmix: False\n",
      "  use_label_smoothing: False\n",
      "  random_crop:\n",
      "    padding: 4\n",
      "    fill: 0\n",
      "    padding_mode: constant\n",
      "  random_horizontal_flip:\n",
      "    prob: 0.5\n",
      "  cutout:\n",
      "    prob: 1.0\n",
      "    mask_size: 16\n",
      "    cut_inside: False\n",
      "    mask_color: 0\n",
      "    dual_cutout_alpha: 0.1\n",
      "  random_erasing:\n",
      "    prob: 0.5\n",
      "    area_ratio_range: [0.02, 0.4]\n",
      "    min_aspect_ratio: 0.3\n",
      "    max_attempt: 20\n",
      "  mixup:\n",
      "    alpha: 1.0\n",
      "  ricap:\n",
      "    beta: 0.3\n",
      "  cutmix:\n",
      "    alpha: 1.0\n",
      "  label_smoothing:\n",
      "    epsilon: 0.1\n",
      "tta:\n",
      "  use_resize: False\n",
      "  use_center_crop: False\n",
      "  resize: 256\n",
      "test:\n",
      "  checkpoint: ''\n",
      "  output_dir: ''\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    pin_memory: False\n",
      "\u001b[32m[2020-07-12 04:31:25] __main__ INFO: \u001b[0menv_info:\n",
      "  pytorch_version: 1.4.0\n",
      "  cuda_version: 10.1\n",
      "  cudnn_version: 7603\n",
      "  num_gpus: 1\n",
      "  gpu_name: Tesla K80\n",
      "  gpu_capability: 3.7\n",
      "Files already downloaded and verified\n",
      "\u001b[32m[2020-07-12 04:31:51] __main__ INFO: \u001b[0mMACs  : 69.76M\n",
      "\u001b[32m[2020-07-12 04:31:51] __main__ INFO: \u001b[0m#params: 466.91K\n",
      "Selected optimization level O0:  Pure FP32 training.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\",)\n",
      "\u001b[32m[2020-07-12 04:31:51] __main__ INFO: \u001b[0mVal 0\n",
      "\u001b[32m[2020-07-12 04:31:52] __main__ INFO: \u001b[0mEpoch 0 loss 459.5354 acc@1 0.1050 acc@5 0.5014\n",
      "\u001b[32m[2020-07-12 04:31:52] __main__ INFO: \u001b[0mElapsed 1.41\n",
      "\u001b[32m[2020-07-12 04:31:52] __main__ INFO: \u001b[0mTrain 1 0\n",
      "\u001b[32m[2020-07-12 04:32:02] __main__ INFO: \u001b[0mEpoch 1 Step 100/351 lr 0.100000 loss 2.2804 (2.8966) acc@1 0.1783 (0.1639) acc@5 0.5892 (0.5485)\n",
      "\u001b[32m[2020-07-12 04:32:11] __main__ INFO: \u001b[0mEpoch 1 Step 200/351 lr 0.100000 loss 2.2714 (2.5931) acc@1 0.1917 (0.1667) acc@5 0.6080 (0.5541)\n",
      "\u001b[32m[2020-07-12 04:32:20] __main__ INFO: \u001b[0mEpoch 1 Step 300/351 lr 0.100000 loss 2.2902 (2.4881) acc@1 0.1471 (0.1680) acc@5 0.5345 (0.5611)\n",
      "\u001b[32m[2020-07-12 04:32:25] __main__ INFO: \u001b[0mEpoch 1 Step 351/351 lr 0.100000 loss 2.2694 (2.4565) acc@1 0.1725 (0.1681) acc@5 0.5536 (0.5658)\n",
      "\u001b[32m[2020-07-12 04:32:25] __main__ INFO: \u001b[0mElapsed 32.49\n",
      "\u001b[32m[2020-07-12 04:32:25] __main__ INFO: \u001b[0mVal 1\n",
      "\u001b[32m[2020-07-12 04:32:26] __main__ INFO: \u001b[0mEpoch 1 loss 2.2901 acc@1 0.1256 acc@5 0.5760\n",
      "\u001b[32m[2020-07-12 04:32:26] __main__ INFO: \u001b[0mElapsed 1.05\n",
      "\u001b[32m[2020-07-12 04:32:26] __main__ INFO: \u001b[0mTrain 2 351\n",
      "\u001b[32m[2020-07-12 04:32:35] __main__ INFO: \u001b[0mEpoch 2 Step 100/351 lr 0.100000 loss 2.1102 (2.2076) acc@1 0.2681 (0.1821) acc@5 0.7085 (0.6443)\n",
      "\u001b[32m[2020-07-12 04:32:44] __main__ INFO: \u001b[0mEpoch 2 Step 200/351 lr 0.100000 loss 2.1144 (2.1681) acc@1 0.1452 (0.1850) acc@5 0.7566 (0.6746)\n",
      "\u001b[32m[2020-07-12 04:32:53] __main__ INFO: \u001b[0mEpoch 2 Step 300/351 lr 0.100000 loss 1.9925 (2.1430) acc@1 0.2392 (0.1905) acc@5 0.7529 (0.6918)\n",
      "\u001b[32m[2020-07-12 04:32:58] __main__ INFO: \u001b[0mEpoch 2 Step 351/351 lr 0.100000 loss 2.0170 (2.1299) acc@1 0.2097 (0.1953) acc@5 0.7572 (0.6990)\n",
      "\u001b[32m[2020-07-12 04:32:58] __main__ INFO: \u001b[0mElapsed 32.31\n",
      "\u001b[32m[2020-07-12 04:32:58] __main__ INFO: \u001b[0mVal 2\n",
      "\u001b[32m[2020-07-12 04:32:59] __main__ INFO: \u001b[0mEpoch 2 loss 1.9280 acc@1 0.2478 acc@5 0.8172\n",
      "\u001b[32m[2020-07-12 04:32:59] __main__ INFO: \u001b[0mElapsed 1.10\n",
      "\u001b[32m[2020-07-12 04:32:59] __main__ INFO: \u001b[0mTrain 3 702\n",
      "\u001b[32m[2020-07-12 04:33:08] __main__ INFO: \u001b[0mEpoch 3 Step 100/351 lr 0.100000 loss 2.0169 (2.0199) acc@1 0.2289 (0.2350) acc@5 0.7779 (0.7609)\n",
      "\u001b[32m[2020-07-12 04:33:18] __main__ INFO: \u001b[0mEpoch 3 Step 200/351 lr 0.100000 loss 1.9691 (2.0003) acc@1 0.2662 (0.2440) acc@5 0.8105 (0.7724)\n",
      "\u001b[32m[2020-07-12 04:33:27] __main__ INFO: \u001b[0mEpoch 3 Step 300/351 lr 0.100000 loss 1.8475 (1.9777) acc@1 0.2778 (0.2567) acc@5 0.8153 (0.7799)\n",
      "\u001b[32m[2020-07-12 04:33:31] __main__ INFO: \u001b[0mEpoch 3 Step 351/351 lr 0.100000 loss 1.8836 (1.9655) acc@1 0.3289 (0.2633) acc@5 0.8346 (0.7852)\n",
      "\u001b[32m[2020-07-12 04:33:31] __main__ INFO: \u001b[0mElapsed 32.42\n",
      "\u001b[32m[2020-07-12 04:33:31] __main__ INFO: \u001b[0mVal 3\n",
      "\u001b[32m[2020-07-12 04:33:32] __main__ INFO: \u001b[0mEpoch 3 loss 1.8382 acc@1 0.3022 acc@5 0.8460\n",
      "\u001b[32m[2020-07-12 04:33:32] __main__ INFO: \u001b[0mElapsed 1.05\n",
      "\u001b[32m[2020-07-12 04:33:32] __main__ INFO: \u001b[0mTrain 4 1053\n",
      "\u001b[32m[2020-07-12 04:33:42] __main__ INFO: \u001b[0mEpoch 4 Step 100/351 lr 0.100000 loss 1.8025 (1.8621) acc@1 0.3230 (0.3209) acc@5 0.8274 (0.8244)\n",
      "\u001b[32m[2020-07-12 04:33:51] __main__ INFO: \u001b[0mEpoch 4 Step 200/351 lr 0.100000 loss 1.7447 (1.8375) acc@1 0.3823 (0.3307) acc@5 0.8331 (0.8298)\n",
      "\u001b[32m[2020-07-12 04:34:00] __main__ INFO: \u001b[0mEpoch 4 Step 300/351 lr 0.100000 loss 1.7818 (1.8187) acc@1 0.3228 (0.3413) acc@5 0.8487 (0.8347)\n",
      "\u001b[32m[2020-07-12 04:34:05] __main__ INFO: \u001b[0mEpoch 4 Step 351/351 lr 0.100000 loss 1.8521 (1.8090) acc@1 0.3314 (0.3463) acc@5 0.8487 (0.8367)\n",
      "\u001b[32m[2020-07-12 04:34:05] __main__ INFO: \u001b[0mElapsed 32.39\n",
      "\u001b[32m[2020-07-12 04:34:05] __main__ INFO: \u001b[0mVal 4\n",
      "\u001b[32m[2020-07-12 04:34:06] __main__ INFO: \u001b[0mEpoch 4 loss 1.7878 acc@1 0.3732 acc@5 0.8312\n",
      "\u001b[32m[2020-07-12 04:34:06] __main__ INFO: \u001b[0mElapsed 1.05\n",
      "\u001b[32m[2020-07-12 04:34:06] __main__ INFO: \u001b[0mTrain 5 1404\n",
      "\u001b[32m[2020-07-12 04:34:15] __main__ INFO: \u001b[0mEpoch 5 Step 100/351 lr 0.100000 loss 1.7586 (1.7425) acc@1 0.4115 (0.3733) acc@5 0.8307 (0.8553)\n",
      "\u001b[32m[2020-07-12 04:34:24] __main__ INFO: \u001b[0mEpoch 5 Step 200/351 lr 0.100000 loss 1.6220 (1.7209) acc@1 0.3760 (0.3872) acc@5 0.9019 (0.8594)\n",
      "\u001b[32m[2020-07-12 04:34:34] __main__ INFO: \u001b[0mEpoch 5 Step 300/351 lr 0.100000 loss 1.5802 (1.7068) acc@1 0.4432 (0.3947) acc@5 0.8865 (0.8628)\n",
      "\u001b[32m[2020-07-12 04:34:38] __main__ INFO: \u001b[0mEpoch 5 Step 351/351 lr 0.100000 loss 1.7052 (1.7004) acc@1 0.4093 (0.3973) acc@5 0.8403 (0.8637)\n",
      "\u001b[32m[2020-07-12 04:34:38] __main__ INFO: \u001b[0mElapsed 32.38\n",
      "\u001b[32m[2020-07-12 04:34:38] __main__ INFO: \u001b[0mVal 5\n",
      "\u001b[32m[2020-07-12 04:34:39] __main__ INFO: \u001b[0mEpoch 5 loss 1.6356 acc@1 0.4222 acc@5 0.8786\n",
      "\u001b[32m[2020-07-12 04:34:39] __main__ INFO: \u001b[0mElapsed 1.03\n",
      "\u001b[32m[2020-07-12 04:34:39] __main__ INFO: \u001b[0mTrain 6 1755\n",
      "\u001b[32m[2020-07-12 04:34:49] __main__ INFO: \u001b[0mEpoch 6 Step 100/351 lr 0.100000 loss 1.6004 (1.6340) acc@1 0.4449 (0.4297) acc@5 0.8890 (0.8750)\n",
      "\u001b[32m[2020-07-12 04:34:58] __main__ INFO: \u001b[0mEpoch 6 Step 200/351 lr 0.100000 loss 1.6291 (1.6266) acc@1 0.4448 (0.4328) acc@5 0.8968 (0.8769)\n",
      "\u001b[32m[2020-07-12 04:35:07] __main__ INFO: \u001b[0mEpoch 6 Step 300/351 lr 0.100000 loss 1.5442 (1.6119) acc@1 0.4666 (0.4383) acc@5 0.8749 (0.8805)\n",
      "\u001b[32m[2020-07-12 04:35:12] __main__ INFO: \u001b[0mEpoch 6 Step 351/351 lr 0.100000 loss 1.3979 (1.6080) acc@1 0.5050 (0.4407) acc@5 0.9303 (0.8816)\n",
      "\u001b[32m[2020-07-12 04:35:12] __main__ INFO: \u001b[0mElapsed 32.43\n",
      "\u001b[32m[2020-07-12 04:35:12] __main__ INFO: \u001b[0mVal 6\n",
      "\u001b[32m[2020-07-12 04:35:13] __main__ INFO: \u001b[0mEpoch 6 loss 1.2668 acc@1 0.5490 acc@5 0.9388\n",
      "\u001b[32m[2020-07-12 04:35:13] __main__ INFO: \u001b[0mElapsed 1.03\n",
      "\u001b[32m[2020-07-12 04:35:13] __main__ INFO: \u001b[0mTrain 7 2106\n",
      "\u001b[32m[2020-07-12 04:35:22] __main__ INFO: \u001b[0mEpoch 7 Step 100/351 lr 0.100000 loss 1.3439 (1.5504) acc@1 0.5407 (0.4591) acc@5 0.9379 (0.8933)\n",
      "\u001b[32m[2020-07-12 04:35:31] __main__ INFO: \u001b[0mEpoch 7 Step 200/351 lr 0.100000 loss 1.4980 (1.5384) acc@1 0.4797 (0.4690) acc@5 0.8914 (0.8947)\n",
      "\u001b[32m[2020-07-12 04:35:41] __main__ INFO: \u001b[0mEpoch 7 Step 300/351 lr 0.100000 loss 1.5906 (1.5318) acc@1 0.5218 (0.4742) acc@5 0.8526 (0.8943)\n",
      "\u001b[32m[2020-07-12 04:35:45] __main__ INFO: \u001b[0mEpoch 7 Step 351/351 lr 0.100000 loss 1.3217 (1.5277) acc@1 0.5721 (0.4757) acc@5 0.8853 (0.8954)\n",
      "\u001b[32m[2020-07-12 04:35:45] __main__ INFO: \u001b[0mElapsed 32.42\n",
      "\u001b[32m[2020-07-12 04:35:45] __main__ INFO: \u001b[0mVal 7\n",
      "\u001b[32m[2020-07-12 04:35:46] __main__ INFO: \u001b[0mEpoch 7 loss 1.1359 acc@1 0.6070 acc@5 0.9494\n",
      "\u001b[32m[2020-07-12 04:35:46] __main__ INFO: \u001b[0mElapsed 1.07\n",
      "\u001b[32m[2020-07-12 04:35:46] __main__ INFO: \u001b[0mTrain 8 2457\n",
      "\u001b[32m[2020-07-12 04:35:56] __main__ INFO: \u001b[0mEpoch 8 Step 100/351 lr 0.100000 loss 1.5108 (1.4703) acc@1 0.4721 (0.4996) acc@5 0.8900 (0.9061)\n",
      "\u001b[32m[2020-07-12 04:36:05] __main__ INFO: \u001b[0mEpoch 8 Step 200/351 lr 0.100000 loss 1.4951 (1.4629) acc@1 0.4875 (0.5024) acc@5 0.9061 (0.9048)\n",
      "\u001b[32m[2020-07-12 04:36:14] __main__ INFO: \u001b[0mEpoch 8 Step 300/351 lr 0.100000 loss 1.5710 (1.4594) acc@1 0.4653 (0.5049) acc@5 0.9090 (0.9061)\n",
      "\u001b[32m[2020-07-12 04:36:19] __main__ INFO: \u001b[0mEpoch 8 Step 351/351 lr 0.100000 loss 1.3936 (1.4540) acc@1 0.5556 (0.5067) acc@5 0.8961 (0.9072)\n",
      "\u001b[32m[2020-07-12 04:36:19] __main__ INFO: \u001b[0mElapsed 32.55\n",
      "\u001b[32m[2020-07-12 04:36:19] __main__ INFO: \u001b[0mVal 8\n",
      "\u001b[32m[2020-07-12 04:36:20] __main__ INFO: \u001b[0mEpoch 8 loss 0.9866 acc@1 0.6702 acc@5 0.9628\n",
      "\u001b[32m[2020-07-12 04:36:20] __main__ INFO: \u001b[0mElapsed 1.06\n",
      "\u001b[32m[2020-07-12 04:36:20] __main__ INFO: \u001b[0mTrain 9 2808\n",
      "\u001b[32m[2020-07-12 04:36:29] __main__ INFO: \u001b[0mEpoch 9 Step 100/351 lr 0.100000 loss 1.3144 (1.4141) acc@1 0.5382 (0.5220) acc@5 0.9235 (0.9131)\n",
      "\u001b[32m[2020-07-12 04:36:39] __main__ INFO: \u001b[0mEpoch 9 Step 200/351 lr 0.100000 loss 1.5176 (1.4133) acc@1 0.4949 (0.5233) acc@5 0.8972 (0.9140)\n",
      "\u001b[32m[2020-07-12 04:36:48] __main__ INFO: \u001b[0mEpoch 9 Step 300/351 lr 0.100000 loss 1.3393 (1.4051) acc@1 0.5488 (0.5273) acc@5 0.9404 (0.9142)\n",
      "\u001b[32m[2020-07-12 04:36:52] __main__ INFO: \u001b[0mEpoch 9 Step 351/351 lr 0.100000 loss 1.4755 (1.4015) acc@1 0.5270 (0.5289) acc@5 0.9015 (0.9149)\n",
      "\u001b[32m[2020-07-12 04:36:52] __main__ INFO: \u001b[0mElapsed 32.56\n",
      "\u001b[32m[2020-07-12 04:36:52] __main__ INFO: \u001b[0mVal 9\n",
      "\u001b[32m[2020-07-12 04:36:54] __main__ INFO: \u001b[0mEpoch 9 loss 0.9724 acc@1 0.6778 acc@5 0.9602\n",
      "\u001b[32m[2020-07-12 04:36:54] __main__ INFO: \u001b[0mElapsed 1.05\n",
      "\u001b[32m[2020-07-12 04:36:54] __main__ INFO: \u001b[0mTrain 10 3159\n",
      "\u001b[32m[2020-07-12 04:37:03] __main__ INFO: \u001b[0mEpoch 10 Step 100/351 lr 0.100000 loss 1.3341 (1.3551) acc@1 0.5659 (0.5480) acc@5 0.9205 (0.9201)\n",
      "\u001b[32m[2020-07-12 04:37:12] __main__ INFO: \u001b[0mEpoch 10 Step 200/351 lr 0.100000 loss 1.5302 (1.3539) acc@1 0.4650 (0.5489) acc@5 0.8885 (0.9209)\n",
      "\u001b[32m[2020-07-12 04:37:21] __main__ INFO: \u001b[0mEpoch 10 Step 300/351 lr 0.100000 loss 1.2423 (1.3568) acc@1 0.5743 (0.5482) acc@5 0.9278 (0.9216)\n",
      "\u001b[32m[2020-07-12 04:37:26] __main__ INFO: \u001b[0mEpoch 10 Step 351/351 lr 0.100000 loss 1.3120 (1.3562) acc@1 0.5652 (0.5488) acc@5 0.9287 (0.9216)\n",
      "\u001b[32m[2020-07-12 04:37:26] __main__ INFO: \u001b[0mElapsed 32.44\n",
      "\u001b[32m[2020-07-12 04:37:26] __main__ INFO: \u001b[0mVal 10\n",
      "\u001b[32m[2020-07-12 04:37:27] __main__ INFO: \u001b[0mEpoch 10 loss 0.9575 acc@1 0.6828 acc@5 0.9646\n",
      "\u001b[32m[2020-07-12 04:37:27] __main__ INFO: \u001b[0mElapsed 1.03\n",
      "\u001b[32m[2020-07-12 04:37:27] __main__ INFO: \u001b[0mTrain 11 3510\n",
      "\u001b[32m[2020-07-12 04:37:36] __main__ INFO: \u001b[0mEpoch 11 Step 100/351 lr 0.100000 loss 1.2094 (1.3219) acc@1 0.5902 (0.5633) acc@5 0.9229 (0.9247)\n",
      "\u001b[32m[2020-07-12 04:37:46] __main__ INFO: \u001b[0mEpoch 11 Step 200/351 lr 0.100000 loss 1.4043 (1.3189) acc@1 0.5637 (0.5650) acc@5 0.9148 (0.9260)\n",
      "\u001b[32m[2020-07-12 04:37:55] __main__ INFO: \u001b[0mEpoch 11 Step 300/351 lr 0.100000 loss 1.3828 (1.3171) acc@1 0.5670 (0.5649) acc@5 0.9204 (0.9257)\n",
      "\u001b[32m[2020-07-12 04:37:59] __main__ INFO: \u001b[0mEpoch 11 Step 351/351 lr 0.100000 loss 1.3540 (1.3150) acc@1 0.5753 (0.5653) acc@5 0.9017 (0.9261)\n",
      "\u001b[32m[2020-07-12 04:37:59] __main__ INFO: \u001b[0mElapsed 32.41\n",
      "\u001b[32m[2020-07-12 04:37:59] __main__ INFO: \u001b[0mVal 11\n",
      "\u001b[32m[2020-07-12 04:38:00] __main__ INFO: \u001b[0mEpoch 11 loss 0.8339 acc@1 0.7454 acc@5 0.9722\n",
      "\u001b[32m[2020-07-12 04:38:00] __main__ INFO: \u001b[0mElapsed 1.05\n",
      "\u001b[32m[2020-07-12 04:38:00] __main__ INFO: \u001b[0mTrain 12 3861\n",
      "\u001b[32m[2020-07-12 04:38:10] __main__ INFO: \u001b[0mEpoch 12 Step 100/351 lr 0.100000 loss 1.2940 (1.2828) acc@1 0.5975 (0.5756) acc@5 0.9255 (0.9295)\n",
      "\u001b[32m[2020-07-12 04:38:19] __main__ INFO: \u001b[0mEpoch 12 Step 200/351 lr 0.100000 loss 1.1699 (1.2865) acc@1 0.6411 (0.5762) acc@5 0.9551 (0.9297)\n",
      "\u001b[32m[2020-07-12 04:38:28] __main__ INFO: \u001b[0mEpoch 12 Step 300/351 lr 0.100000 loss 1.4075 (1.2851) acc@1 0.5221 (0.5776) acc@5 0.9312 (0.9296)\n",
      "\u001b[32m[2020-07-12 04:38:33] __main__ INFO: \u001b[0mEpoch 12 Step 351/351 lr 0.100000 loss 1.3252 (1.2857) acc@1 0.5545 (0.5762) acc@5 0.9368 (0.9301)\n",
      "\u001b[32m[2020-07-12 04:38:33] __main__ INFO: \u001b[0mElapsed 32.41\n",
      "\u001b[32m[2020-07-12 04:38:33] __main__ INFO: \u001b[0mVal 12\n",
      "\u001b[32m[2020-07-12 04:38:34] __main__ INFO: \u001b[0mEpoch 12 loss 0.8689 acc@1 0.7344 acc@5 0.9754\n",
      "\u001b[32m[2020-07-12 04:38:34] __main__ INFO: \u001b[0mElapsed 1.06\n",
      "\u001b[32m[2020-07-12 04:38:34] __main__ INFO: \u001b[0mTrain 13 4212\n",
      "\u001b[32m[2020-07-12 04:38:43] __main__ INFO: \u001b[0mEpoch 13 Step 100/351 lr 0.100000 loss 1.2351 (1.2557) acc@1 0.6179 (0.5855) acc@5 0.9326 (0.9349)\n",
      "\u001b[32m[2020-07-12 04:38:52] __main__ INFO: \u001b[0mEpoch 13 Step 200/351 lr 0.100000 loss 1.3350 (1.2551) acc@1 0.5833 (0.5861) acc@5 0.9177 (0.9345)\n",
      "\u001b[32m[2020-07-12 04:39:02] __main__ INFO: \u001b[0mEpoch 13 Step 300/351 lr 0.100000 loss 1.1418 (1.2479) acc@1 0.6611 (0.5903) acc@5 0.9478 (0.9345)\n",
      "\u001b[32m[2020-07-12 04:39:06] __main__ INFO: \u001b[0mEpoch 13 Step 351/351 lr 0.100000 loss 1.3007 (1.2497) acc@1 0.5711 (0.5893) acc@5 0.9319 (0.9339)\n",
      "\u001b[32m[2020-07-12 04:39:06] __main__ INFO: \u001b[0mElapsed 32.46\n",
      "\u001b[32m[2020-07-12 04:39:06] __main__ INFO: \u001b[0mVal 13\n",
      "\u001b[32m[2020-07-12 04:39:07] __main__ INFO: \u001b[0mEpoch 13 loss 0.8178 acc@1 0.7422 acc@5 0.9742\n",
      "\u001b[32m[2020-07-12 04:39:07] __main__ INFO: \u001b[0mElapsed 1.04\n",
      "\u001b[32m[2020-07-12 04:39:07] __main__ INFO: \u001b[0mTrain 14 4563\n",
      "\u001b[32m[2020-07-12 04:39:17] __main__ INFO: \u001b[0mEpoch 14 Step 100/351 lr 0.100000 loss 1.2698 (1.2154) acc@1 0.5542 (0.6032) acc@5 0.9307 (0.9385)\n",
      "\u001b[32m[2020-07-12 04:39:26] __main__ INFO: \u001b[0mEpoch 14 Step 200/351 lr 0.100000 loss 1.3405 (1.2167) acc@1 0.5715 (0.6031) acc@5 0.9131 (0.9379)\n",
      "\u001b[32m[2020-07-12 04:39:35] __main__ INFO: \u001b[0mEpoch 14 Step 300/351 lr 0.100000 loss 1.2002 (1.2202) acc@1 0.5984 (0.6016) acc@5 0.9294 (0.9379)\n",
      "\u001b[32m[2020-07-12 04:39:40] __main__ INFO: \u001b[0mEpoch 14 Step 351/351 lr 0.100000 loss 1.1689 (1.2206) acc@1 0.6393 (0.6016) acc@5 0.9438 (0.9379)\n",
      "\u001b[32m[2020-07-12 04:39:40] __main__ INFO: \u001b[0mElapsed 32.48\n",
      "\u001b[32m[2020-07-12 04:39:40] __main__ INFO: \u001b[0mVal 14\n",
      "\u001b[32m[2020-07-12 04:39:41] __main__ INFO: \u001b[0mEpoch 14 loss 0.8229 acc@1 0.7426 acc@5 0.9782\n",
      "\u001b[32m[2020-07-12 04:39:41] __main__ INFO: \u001b[0mElapsed 1.04\n",
      "\u001b[32m[2020-07-12 04:39:41] __main__ INFO: \u001b[0mTrain 15 4914\n",
      "\u001b[32m[2020-07-12 04:39:50] __main__ INFO: \u001b[0mEpoch 15 Step 100/351 lr 0.100000 loss 1.2435 (1.1903) acc@1 0.5956 (0.6175) acc@5 0.9477 (0.9414)\n",
      "\u001b[32m[2020-07-12 04:39:59] __main__ INFO: \u001b[0mEpoch 15 Step 200/351 lr 0.100000 loss 1.2501 (1.2000) acc@1 0.5625 (0.6106) acc@5 0.9444 (0.9410)\n",
      "\u001b[32m[2020-07-12 04:40:09] __main__ INFO: \u001b[0mEpoch 15 Step 300/351 lr 0.100000 loss 1.2051 (1.1993) acc@1 0.6069 (0.6100) acc@5 0.9461 (0.9408)\n",
      "\u001b[32m[2020-07-12 04:40:13] __main__ INFO: \u001b[0mEpoch 15 Step 351/351 lr 0.100000 loss 1.3137 (1.1969) acc@1 0.5736 (0.6103) acc@5 0.9356 (0.9411)\n",
      "\u001b[32m[2020-07-12 04:40:13] __main__ INFO: \u001b[0mElapsed 32.36\n",
      "\u001b[32m[2020-07-12 04:40:13] __main__ INFO: \u001b[0mVal 15\n",
      "\u001b[32m[2020-07-12 04:40:14] __main__ INFO: \u001b[0mEpoch 15 loss 0.8008 acc@1 0.7436 acc@5 0.9726\n",
      "\u001b[32m[2020-07-12 04:40:14] __main__ INFO: \u001b[0mElapsed 1.03\n",
      "\u001b[32m[2020-07-12 04:40:14] __main__ INFO: \u001b[0mTrain 16 5265\n",
      "\u001b[32m[2020-07-12 04:40:24] __main__ INFO: \u001b[0mEpoch 16 Step 100/351 lr 0.100000 loss 1.1713 (1.1736) acc@1 0.6119 (0.6224) acc@5 0.9496 (0.9419)\n",
      "\u001b[32m[2020-07-12 04:40:33] __main__ INFO: \u001b[0mEpoch 16 Step 200/351 lr 0.100000 loss 1.2025 (1.1732) acc@1 0.6154 (0.6201) acc@5 0.9351 (0.9430)\n",
      "\u001b[32m[2020-07-12 04:40:42] __main__ INFO: \u001b[0mEpoch 16 Step 300/351 lr 0.100000 loss 1.1392 (1.1759) acc@1 0.6101 (0.6193) acc@5 0.9614 (0.9424)\n",
      "\u001b[32m[2020-07-12 04:40:47] __main__ INFO: \u001b[0mEpoch 16 Step 351/351 lr 0.100000 loss 1.0826 (1.1769) acc@1 0.6333 (0.6179) acc@5 0.9586 (0.9425)\n",
      "\u001b[32m[2020-07-12 04:40:47] __main__ INFO: \u001b[0mElapsed 32.42\n",
      "\u001b[32m[2020-07-12 04:40:47] __main__ INFO: \u001b[0mVal 16\n",
      "\u001b[32m[2020-07-12 04:40:48] __main__ INFO: \u001b[0mEpoch 16 loss 0.7234 acc@1 0.7752 acc@5 0.9846\n",
      "\u001b[32m[2020-07-12 04:40:48] __main__ INFO: \u001b[0mElapsed 1.05\n",
      "\u001b[32m[2020-07-12 04:40:48] __main__ INFO: \u001b[0mTrain 17 5616\n",
      "\u001b[32m[2020-07-12 04:40:57] __main__ INFO: \u001b[0mEpoch 17 Step 100/351 lr 0.100000 loss 1.0549 (1.1536) acc@1 0.6827 (0.6284) acc@5 0.9492 (0.9472)\n",
      "\u001b[32m[2020-07-12 04:41:06] __main__ INFO: \u001b[0mEpoch 17 Step 200/351 lr 0.100000 loss 1.3047 (1.1498) acc@1 0.6053 (0.6287) acc@5 0.9238 (0.9459)\n",
      "\u001b[32m[2020-07-12 04:41:16] __main__ INFO: \u001b[0mEpoch 17 Step 300/351 lr 0.100000 loss 1.2137 (1.1534) acc@1 0.5811 (0.6261) acc@5 0.9527 (0.9454)\n",
      "\u001b[32m[2020-07-12 04:41:20] __main__ INFO: \u001b[0mEpoch 17 Step 351/351 lr 0.100000 loss 1.2620 (1.1525) acc@1 0.5739 (0.6266) acc@5 0.9320 (0.9459)\n",
      "\u001b[32m[2020-07-12 04:41:20] __main__ INFO: \u001b[0mElapsed 32.60\n",
      "\u001b[32m[2020-07-12 04:41:20] __main__ INFO: \u001b[0mVal 17\n",
      "\u001b[32m[2020-07-12 04:41:21] __main__ INFO: \u001b[0mEpoch 17 loss 0.6966 acc@1 0.7824 acc@5 0.9858\n",
      "\u001b[32m[2020-07-12 04:41:21] __main__ INFO: \u001b[0mElapsed 1.06\n",
      "\u001b[32m[2020-07-12 04:41:21] __main__ INFO: \u001b[0mTrain 18 5967\n",
      "\u001b[32m[2020-07-12 04:41:31] __main__ INFO: \u001b[0mEpoch 18 Step 100/351 lr 0.100000 loss 1.2659 (1.1424) acc@1 0.5923 (0.6270) acc@5 0.9410 (0.9483)\n",
      "\u001b[32m[2020-07-12 04:41:40] __main__ INFO: \u001b[0mEpoch 18 Step 200/351 lr 0.100000 loss 1.0778 (1.1388) acc@1 0.6616 (0.6301) acc@5 0.9391 (0.9475)\n",
      "\u001b[32m[2020-07-12 04:41:49] __main__ INFO: \u001b[0mEpoch 18 Step 300/351 lr 0.100000 loss 1.1254 (1.1377) acc@1 0.6546 (0.6313) acc@5 0.9291 (0.9472)\n",
      "\u001b[32m[2020-07-12 04:41:54] __main__ INFO: \u001b[0mEpoch 18 Step 351/351 lr 0.100000 loss 1.2488 (1.1391) acc@1 0.6100 (0.6310) acc@5 0.9624 (0.9470)\n",
      "\u001b[32m[2020-07-12 04:41:54] __main__ INFO: \u001b[0mElapsed 32.54\n",
      "\u001b[32m[2020-07-12 04:41:54] __main__ INFO: \u001b[0mVal 18\n",
      "\u001b[32m[2020-07-12 04:41:55] __main__ INFO: \u001b[0mEpoch 18 loss 0.6891 acc@1 0.8002 acc@5 0.9850\n",
      "\u001b[32m[2020-07-12 04:41:55] __main__ INFO: \u001b[0mElapsed 1.04\n",
      "\u001b[32m[2020-07-12 04:41:55] __main__ INFO: \u001b[0mTrain 19 6318\n",
      "\u001b[32m[2020-07-12 04:42:04] __main__ INFO: \u001b[0mEpoch 19 Step 100/351 lr 0.100000 loss 0.9891 (1.1232) acc@1 0.7010 (0.6335) acc@5 0.9629 (0.9493)\n",
      "\u001b[32m[2020-07-12 04:42:14] __main__ INFO: \u001b[0mEpoch 19 Step 200/351 lr 0.100000 loss 1.0741 (1.1193) acc@1 0.6625 (0.6363) acc@5 0.9780 (0.9486)\n",
      "\u001b[32m[2020-07-12 04:42:23] __main__ INFO: \u001b[0mEpoch 19 Step 300/351 lr 0.100000 loss 1.1273 (1.1226) acc@1 0.6168 (0.6362) acc@5 0.9471 (0.9482)\n",
      "\u001b[32m[2020-07-12 04:42:27] __main__ INFO: \u001b[0mEpoch 19 Step 351/351 lr 0.100000 loss 1.0086 (1.1187) acc@1 0.6779 (0.6379) acc@5 0.9441 (0.9484)\n",
      "\u001b[32m[2020-07-12 04:42:27] __main__ INFO: \u001b[0mElapsed 32.39\n",
      "\u001b[32m[2020-07-12 04:42:27] __main__ INFO: \u001b[0mVal 19\n",
      "\u001b[32m[2020-07-12 04:42:28] __main__ INFO: \u001b[0mEpoch 19 loss 0.6438 acc@1 0.8068 acc@5 0.9870\n",
      "\u001b[32m[2020-07-12 04:42:28] __main__ INFO: \u001b[0mElapsed 1.04\n",
      "\u001b[32m[2020-07-12 04:42:28] __main__ INFO: \u001b[0mTrain 20 6669\n",
      "\u001b[32m[2020-07-12 04:42:38] __main__ INFO: \u001b[0mEpoch 20 Step 100/351 lr 0.100000 loss 1.1108 (1.1017) acc@1 0.6401 (0.6451) acc@5 0.9432 (0.9506)\n",
      "\u001b[32m[2020-07-12 04:42:47] __main__ INFO: \u001b[0mEpoch 20 Step 200/351 lr 0.100000 loss 1.1392 (1.1064) acc@1 0.6036 (0.6422) acc@5 0.9637 (0.9499)\n",
      "\u001b[32m[2020-07-12 04:42:56] __main__ INFO: \u001b[0mEpoch 20 Step 300/351 lr 0.100000 loss 1.2215 (1.1074) acc@1 0.6206 (0.6440) acc@5 0.9374 (0.9496)\n",
      "\u001b[32m[2020-07-12 04:43:01] __main__ INFO: \u001b[0mEpoch 20 Step 351/351 lr 0.100000 loss 0.9898 (1.1060) acc@1 0.6843 (0.6444) acc@5 0.9444 (0.9496)\n",
      "\u001b[32m[2020-07-12 04:43:01] __main__ INFO: \u001b[0mElapsed 32.40\n",
      "\u001b[32m[2020-07-12 04:43:01] __main__ INFO: \u001b[0mVal 20\n",
      "\u001b[32m[2020-07-12 04:43:02] __main__ INFO: \u001b[0mEpoch 20 loss 0.5930 acc@1 0.8238 acc@5 0.9868\n",
      "\u001b[32m[2020-07-12 04:43:02] __main__ INFO: \u001b[0mElapsed 1.05\n",
      "\u001b[32m[2020-07-12 04:43:02] __main__ INFO: \u001b[0mTrain 21 7020\n",
      "\u001b[32m[2020-07-12 04:43:11] __main__ INFO: \u001b[0mEpoch 21 Step 100/351 lr 0.100000 loss 0.9304 (1.0817) acc@1 0.7066 (0.6523) acc@5 0.9791 (0.9531)\n",
      "\u001b[32m[2020-07-12 04:43:20] __main__ INFO: \u001b[0mEpoch 21 Step 200/351 lr 0.100000 loss 1.0810 (1.0825) acc@1 0.6560 (0.6522) acc@5 0.9561 (0.9522)\n",
      "\u001b[32m[2020-07-12 04:43:30] __main__ INFO: \u001b[0mEpoch 21 Step 300/351 lr 0.100000 loss 1.0401 (1.0886) acc@1 0.6434 (0.6490) acc@5 0.9637 (0.9516)\n",
      "\u001b[32m[2020-07-12 04:43:34] __main__ INFO: \u001b[0mEpoch 21 Step 351/351 lr 0.100000 loss 1.0255 (1.0891) acc@1 0.6496 (0.6491) acc@5 0.9588 (0.9516)\n",
      "\u001b[32m[2020-07-12 04:43:34] __main__ INFO: \u001b[0mElapsed 32.35\n",
      "\u001b[32m[2020-07-12 04:43:34] __main__ INFO: \u001b[0mVal 21\n",
      "\u001b[32m[2020-07-12 04:43:35] __main__ INFO: \u001b[0mEpoch 21 loss 0.6708 acc@1 0.7954 acc@5 0.9870\n",
      "\u001b[32m[2020-07-12 04:43:35] __main__ INFO: \u001b[0mElapsed 1.05\n",
      "\u001b[32m[2020-07-12 04:43:35] __main__ INFO: \u001b[0mTrain 22 7371\n",
      "\u001b[32m[2020-07-12 04:43:45] __main__ INFO: \u001b[0mEpoch 22 Step 100/351 lr 0.100000 loss 1.0788 (1.0891) acc@1 0.6432 (0.6494) acc@5 0.9518 (0.9498)\n",
      "\u001b[32m[2020-07-12 04:43:54] __main__ INFO: \u001b[0mEpoch 22 Step 200/351 lr 0.100000 loss 0.9527 (1.0817) acc@1 0.6885 (0.6523) acc@5 0.9594 (0.9518)\n",
      "\u001b[32m[2020-07-12 04:44:03] __main__ INFO: \u001b[0mEpoch 22 Step 300/351 lr 0.100000 loss 1.1129 (1.0811) acc@1 0.6136 (0.6523) acc@5 0.9421 (0.9522)\n",
      "\u001b[32m[2020-07-12 04:44:08] __main__ INFO: \u001b[0mEpoch 22 Step 351/351 lr 0.100000 loss 0.9729 (1.0813) acc@1 0.6954 (0.6520) acc@5 0.9580 (0.9519)\n",
      "\u001b[32m[2020-07-12 04:44:08] __main__ INFO: \u001b[0mElapsed 32.44\n",
      "\u001b[32m[2020-07-12 04:44:08] __main__ INFO: \u001b[0mVal 22\n",
      "\u001b[32m[2020-07-12 04:44:09] __main__ INFO: \u001b[0mEpoch 22 loss 0.6141 acc@1 0.8250 acc@5 0.9892\n",
      "\u001b[32m[2020-07-12 04:44:09] __main__ INFO: \u001b[0mElapsed 1.04\n",
      "\u001b[32m[2020-07-12 04:44:09] __main__ INFO: \u001b[0mTrain 23 7722\n",
      "\u001b[32m[2020-07-12 04:44:18] __main__ INFO: \u001b[0mEpoch 23 Step 100/351 lr 0.100000 loss 1.2122 (1.0664) acc@1 0.6218 (0.6581) acc@5 0.9194 (0.9518)\n",
      "\u001b[32m[2020-07-12 04:44:27] __main__ INFO: \u001b[0mEpoch 23 Step 200/351 lr 0.100000 loss 0.9683 (1.0646) acc@1 0.7033 (0.6572) acc@5 0.9622 (0.9537)\n",
      "\u001b[32m[2020-07-12 04:44:36] __main__ INFO: \u001b[0mEpoch 23 Step 300/351 lr 0.100000 loss 1.0537 (1.0656) acc@1 0.6788 (0.6563) acc@5 0.9520 (0.9533)\n",
      "\u001b[32m[2020-07-12 04:44:41] __main__ INFO: \u001b[0mEpoch 23 Step 351/351 lr 0.100000 loss 0.9932 (1.0669) acc@1 0.6651 (0.6564) acc@5 0.9721 (0.9531)\n",
      "\u001b[32m[2020-07-12 04:44:41] __main__ INFO: \u001b[0mElapsed 32.39\n",
      "\u001b[32m[2020-07-12 04:44:41] __main__ INFO: \u001b[0mVal 23\n",
      "\u001b[32m[2020-07-12 04:44:42] __main__ INFO: \u001b[0mEpoch 23 loss 0.6247 acc@1 0.8202 acc@5 0.9866\n",
      "\u001b[32m[2020-07-12 04:44:42] __main__ INFO: \u001b[0mElapsed 1.04\n",
      "\u001b[32m[2020-07-12 04:44:42] __main__ INFO: \u001b[0mTrain 24 8073\n",
      "\u001b[32m[2020-07-12 04:44:52] __main__ INFO: \u001b[0mEpoch 24 Step 100/351 lr 0.100000 loss 1.1947 (1.0666) acc@1 0.6256 (0.6585) acc@5 0.9367 (0.9520)\n",
      "\u001b[32m[2020-07-12 04:45:01] __main__ INFO: \u001b[0mEpoch 24 Step 200/351 lr 0.100000 loss 1.0846 (1.0588) acc@1 0.6466 (0.6609) acc@5 0.9583 (0.9531)\n",
      "\u001b[32m[2020-07-12 04:45:10] __main__ INFO: \u001b[0mEpoch 24 Step 300/351 lr 0.100000 loss 1.0347 (1.0585) acc@1 0.6703 (0.6604) acc@5 0.9703 (0.9538)\n",
      "\u001b[32m[2020-07-12 04:45:15] __main__ INFO: \u001b[0mEpoch 24 Step 351/351 lr 0.100000 loss 1.1379 (1.0570) acc@1 0.6530 (0.6603) acc@5 0.9317 (0.9538)\n",
      "\u001b[32m[2020-07-12 04:45:15] __main__ INFO: \u001b[0mElapsed 32.43\n",
      "\u001b[32m[2020-07-12 04:45:15] __main__ INFO: \u001b[0mVal 24\n",
      "\u001b[32m[2020-07-12 04:45:16] __main__ INFO: \u001b[0mEpoch 24 loss 0.6668 acc@1 0.8074 acc@5 0.9870\n",
      "\u001b[32m[2020-07-12 04:45:16] __main__ INFO: \u001b[0mElapsed 1.05\n",
      "\u001b[32m[2020-07-12 04:45:16] __main__ INFO: \u001b[0mTrain 25 8424\n",
      "\u001b[32m[2020-07-12 04:45:25] __main__ INFO: \u001b[0mEpoch 25 Step 100/351 lr 0.100000 loss 1.1230 (1.0394) acc@1 0.6023 (0.6691) acc@5 0.9526 (0.9530)\n",
      "\u001b[32m[2020-07-12 04:45:34] __main__ INFO: \u001b[0mEpoch 25 Step 200/351 lr 0.100000 loss 1.0974 (1.0396) acc@1 0.6534 (0.6666) acc@5 0.9499 (0.9548)\n",
      "\u001b[32m[2020-07-12 04:45:44] __main__ INFO: \u001b[0mEpoch 25 Step 300/351 lr 0.100000 loss 1.1735 (1.0417) acc@1 0.6060 (0.6658) acc@5 0.9426 (0.9553)\n",
      "\u001b[32m[2020-07-12 04:45:48] __main__ INFO: \u001b[0mEpoch 25 Step 351/351 lr 0.100000 loss 1.0590 (1.0433) acc@1 0.6555 (0.6648) acc@5 0.9460 (0.9553)\n",
      "\u001b[32m[2020-07-12 04:45:48] __main__ INFO: \u001b[0mElapsed 32.55\n",
      "\u001b[32m[2020-07-12 04:45:48] __main__ INFO: \u001b[0mVal 25\n",
      "\u001b[32m[2020-07-12 04:45:49] __main__ INFO: \u001b[0mEpoch 25 loss 0.6220 acc@1 0.8246 acc@5 0.9880\n",
      "\u001b[32m[2020-07-12 04:45:49] __main__ INFO: \u001b[0mElapsed 1.05\n",
      "\u001b[32m[2020-07-12 04:45:49] __main__ INFO: \u001b[0mTrain 26 8775\n",
      "\u001b[32m[2020-07-12 04:45:59] __main__ INFO: \u001b[0mEpoch 26 Step 100/351 lr 0.100000 loss 1.1043 (1.0297) acc@1 0.6731 (0.6718) acc@5 0.9594 (0.9555)\n",
      "\u001b[32m[2020-07-12 04:46:08] __main__ INFO: \u001b[0mEpoch 26 Step 200/351 lr 0.100000 loss 1.0792 (1.0315) acc@1 0.6648 (0.6684) acc@5 0.9483 (0.9564)\n",
      "\u001b[32m[2020-07-12 04:46:17] __main__ INFO: \u001b[0mEpoch 26 Step 300/351 lr 0.100000 loss 1.0627 (1.0343) acc@1 0.6598 (0.6671) acc@5 0.9395 (0.9562)\n",
      "\u001b[32m[2020-07-12 04:46:22] __main__ INFO: \u001b[0mEpoch 26 Step 351/351 lr 0.100000 loss 1.0538 (1.0358) acc@1 0.6534 (0.6663) acc@5 0.9460 (0.9560)\n",
      "\u001b[32m[2020-07-12 04:46:22] __main__ INFO: \u001b[0mElapsed 32.58\n",
      "\u001b[32m[2020-07-12 04:46:22] __main__ INFO: \u001b[0mVal 26\n",
      "\u001b[32m[2020-07-12 04:46:23] __main__ INFO: \u001b[0mEpoch 26 loss 0.6778 acc@1 0.8112 acc@5 0.9812\n",
      "\u001b[32m[2020-07-12 04:46:23] __main__ INFO: \u001b[0mElapsed 1.07\n",
      "\u001b[32m[2020-07-12 04:46:23] __main__ INFO: \u001b[0mTrain 27 9126\n",
      "\u001b[32m[2020-07-12 04:46:32] __main__ INFO: \u001b[0mEpoch 27 Step 100/351 lr 0.100000 loss 0.9501 (1.0174) acc@1 0.6971 (0.6718) acc@5 0.9626 (0.9584)\n",
      "\u001b[32m[2020-07-12 04:46:42] __main__ INFO: \u001b[0mEpoch 27 Step 200/351 lr 0.100000 loss 0.9853 (1.0208) acc@1 0.7012 (0.6732) acc@5 0.9579 (0.9571)\n",
      "\u001b[32m[2020-07-12 04:46:51] __main__ INFO: \u001b[0mEpoch 27 Step 300/351 lr 0.100000 loss 0.9616 (1.0253) acc@1 0.6948 (0.6716) acc@5 0.9661 (0.9563)\n",
      "\u001b[32m[2020-07-12 04:46:55] __main__ INFO: \u001b[0mEpoch 27 Step 351/351 lr 0.100000 loss 1.0715 (1.0250) acc@1 0.6692 (0.6724) acc@5 0.9719 (0.9564)\n",
      "\u001b[32m[2020-07-12 04:46:55] __main__ INFO: \u001b[0mElapsed 32.45\n",
      "\u001b[32m[2020-07-12 04:46:55] __main__ INFO: \u001b[0mVal 27\n",
      "\u001b[32m[2020-07-12 04:46:56] __main__ INFO: \u001b[0mEpoch 27 loss 0.5858 acc@1 0.8270 acc@5 0.9890\n",
      "\u001b[32m[2020-07-12 04:46:56] __main__ INFO: \u001b[0mElapsed 1.05\n",
      "\u001b[32m[2020-07-12 04:46:56] __main__ INFO: \u001b[0mTrain 28 9477\n",
      "\u001b[32m[2020-07-12 04:47:06] __main__ INFO: \u001b[0mEpoch 28 Step 100/351 lr 0.100000 loss 1.0828 (1.0061) acc@1 0.6472 (0.6800) acc@5 0.9570 (0.9591)\n",
      "\u001b[32m[2020-07-12 04:47:15] __main__ INFO: \u001b[0mEpoch 28 Step 200/351 lr 0.100000 loss 1.0134 (1.0165) acc@1 0.6893 (0.6754) acc@5 0.9363 (0.9586)\n",
      "\u001b[32m[2020-07-12 04:47:24] __main__ INFO: \u001b[0mEpoch 28 Step 300/351 lr 0.100000 loss 1.0071 (1.0199) acc@1 0.6736 (0.6725) acc@5 0.9568 (0.9579)\n",
      "\u001b[32m[2020-07-12 04:47:29] __main__ INFO: \u001b[0mEpoch 28 Step 351/351 lr 0.100000 loss 0.9526 (1.0167) acc@1 0.7124 (0.6737) acc@5 0.9656 (0.9583)\n",
      "\u001b[32m[2020-07-12 04:47:29] __main__ INFO: \u001b[0mElapsed 32.38\n",
      "\u001b[32m[2020-07-12 04:47:29] __main__ INFO: \u001b[0mVal 28\n",
      "\u001b[32m[2020-07-12 04:47:30] __main__ INFO: \u001b[0mEpoch 28 loss 0.6115 acc@1 0.8198 acc@5 0.9900\n",
      "\u001b[32m[2020-07-12 04:47:30] __main__ INFO: \u001b[0mElapsed 1.04\n",
      "\u001b[32m[2020-07-12 04:47:30] __main__ INFO: \u001b[0mTrain 29 9828\n",
      "\u001b[32m[2020-07-12 04:47:39] __main__ INFO: \u001b[0mEpoch 29 Step 100/351 lr 0.100000 loss 1.0303 (0.9939) acc@1 0.6333 (0.6806) acc@5 0.9698 (0.9597)\n",
      "\u001b[32m[2020-07-12 04:47:48] __main__ INFO: \u001b[0mEpoch 29 Step 200/351 lr 0.100000 loss 1.0777 (0.9978) acc@1 0.6438 (0.6805) acc@5 0.9729 (0.9598)\n",
      "\u001b[32m[2020-07-12 04:47:58] __main__ INFO: \u001b[0mEpoch 29 Step 300/351 lr 0.100000 loss 1.0875 (1.0063) acc@1 0.6296 (0.6773) acc@5 0.9539 (0.9590)\n",
      "\u001b[32m[2020-07-12 04:48:02] __main__ INFO: \u001b[0mEpoch 29 Step 351/351 lr 0.100000 loss 0.9719 (1.0080) acc@1 0.7016 (0.6773) acc@5 0.9647 (0.9587)\n",
      "\u001b[32m[2020-07-12 04:48:02] __main__ INFO: \u001b[0mElapsed 32.41\n",
      "\u001b[32m[2020-07-12 04:48:02] __main__ INFO: \u001b[0mVal 29\n",
      "\u001b[32m[2020-07-12 04:48:03] __main__ INFO: \u001b[0mEpoch 29 loss 0.5798 acc@1 0.8422 acc@5 0.9910\n",
      "\u001b[32m[2020-07-12 04:48:03] __main__ INFO: \u001b[0mElapsed 1.06\n",
      "\u001b[32m[2020-07-12 04:48:03] __main__ INFO: \u001b[0mTrain 30 10179\n",
      "\u001b[32m[2020-07-12 04:48:13] __main__ INFO: \u001b[0mEpoch 30 Step 100/351 lr 0.100000 loss 1.0800 (0.9992) acc@1 0.6740 (0.6798) acc@5 0.9652 (0.9602)\n",
      "\u001b[32m[2020-07-12 04:48:22] __main__ INFO: \u001b[0mEpoch 30 Step 200/351 lr 0.100000 loss 0.9110 (0.9992) acc@1 0.6947 (0.6800) acc@5 0.9821 (0.9591)\n",
      "\u001b[32m[2020-07-12 04:48:31] __main__ INFO: \u001b[0mEpoch 30 Step 300/351 lr 0.100000 loss 0.9201 (0.9989) acc@1 0.6998 (0.6791) acc@5 0.9674 (0.9601)\n",
      "\u001b[32m[2020-07-12 04:48:36] __main__ INFO: \u001b[0mEpoch 30 Step 351/351 lr 0.100000 loss 1.0718 (1.0005) acc@1 0.6946 (0.6796) acc@5 0.9565 (0.9598)\n",
      "\u001b[32m[2020-07-12 04:48:36] __main__ INFO: \u001b[0mElapsed 32.41\n",
      "\u001b[32m[2020-07-12 04:48:36] __main__ INFO: \u001b[0mVal 30\n",
      "\u001b[32m[2020-07-12 04:48:37] __main__ INFO: \u001b[0mEpoch 30 loss 0.5868 acc@1 0.8362 acc@5 0.9896\n",
      "\u001b[32m[2020-07-12 04:48:37] __main__ INFO: \u001b[0mElapsed 1.11\n",
      "\u001b[32m[2020-07-12 04:48:37] __main__ INFO: \u001b[0mTrain 31 10530\n",
      "\u001b[32m[2020-07-12 04:48:46] __main__ INFO: \u001b[0mEpoch 31 Step 100/351 lr 0.100000 loss 1.0374 (0.9715) acc@1 0.6961 (0.6914) acc@5 0.9593 (0.9613)\n",
      "\u001b[32m[2020-07-12 04:48:55] __main__ INFO: \u001b[0mEpoch 31 Step 200/351 lr 0.100000 loss 0.8038 (0.9845) acc@1 0.7599 (0.6872) acc@5 0.9656 (0.9600)\n",
      "\u001b[32m[2020-07-12 04:49:05] __main__ INFO: \u001b[0mEpoch 31 Step 300/351 lr 0.100000 loss 0.8270 (0.9894) acc@1 0.7263 (0.6837) acc@5 0.9857 (0.9599)\n",
      "\u001b[32m[2020-07-12 04:49:09] __main__ INFO: \u001b[0mEpoch 31 Step 351/351 lr 0.100000 loss 0.9558 (0.9918) acc@1 0.6932 (0.6830) acc@5 0.9808 (0.9599)\n",
      "\u001b[32m[2020-07-12 04:49:09] __main__ INFO: \u001b[0mElapsed 32.50\n",
      "\u001b[32m[2020-07-12 04:49:09] __main__ INFO: \u001b[0mVal 31\n",
      "\u001b[32m[2020-07-12 04:49:10] __main__ INFO: \u001b[0mEpoch 31 loss 0.5907 acc@1 0.8322 acc@5 0.9886\n",
      "\u001b[32m[2020-07-12 04:49:10] __main__ INFO: \u001b[0mElapsed 1.06\n",
      "\u001b[32m[2020-07-12 04:49:10] __main__ INFO: \u001b[0mTrain 32 10881\n",
      "\u001b[32m[2020-07-12 04:49:20] __main__ INFO: \u001b[0mEpoch 32 Step 100/351 lr 0.100000 loss 1.1271 (0.9743) acc@1 0.6487 (0.6894) acc@5 0.9526 (0.9614)\n",
      "\u001b[32m[2020-07-12 04:49:29] __main__ INFO: \u001b[0mEpoch 32 Step 200/351 lr 0.100000 loss 1.1718 (0.9772) acc@1 0.6015 (0.6873) acc@5 0.9479 (0.9615)\n",
      "\u001b[32m[2020-07-12 04:49:38] __main__ INFO: \u001b[0mEpoch 32 Step 300/351 lr 0.100000 loss 1.0546 (0.9825) acc@1 0.6579 (0.6856) acc@5 0.9613 (0.9615)\n",
      "\u001b[32m[2020-07-12 04:49:43] __main__ INFO: \u001b[0mEpoch 32 Step 351/351 lr 0.100000 loss 0.9374 (0.9839) acc@1 0.7224 (0.6857) acc@5 0.9593 (0.9615)\n",
      "\u001b[32m[2020-07-12 04:49:43] __main__ INFO: \u001b[0mElapsed 32.34\n",
      "\u001b[32m[2020-07-12 04:49:43] __main__ INFO: \u001b[0mVal 32\n",
      "\u001b[32m[2020-07-12 04:49:44] __main__ INFO: \u001b[0mEpoch 32 loss 0.5643 acc@1 0.8416 acc@5 0.9882\n",
      "\u001b[32m[2020-07-12 04:49:44] __main__ INFO: \u001b[0mElapsed 1.02\n",
      "\u001b[32m[2020-07-12 04:49:44] __main__ INFO: \u001b[0mTrain 33 11232\n",
      "\u001b[32m[2020-07-12 04:49:53] __main__ INFO: \u001b[0mEpoch 33 Step 100/351 lr 0.100000 loss 1.1172 (0.9734) acc@1 0.6356 (0.6908) acc@5 0.9579 (0.9629)\n",
      "\u001b[32m[2020-07-12 04:50:02] __main__ INFO: \u001b[0mEpoch 33 Step 200/351 lr 0.100000 loss 1.0678 (0.9781) acc@1 0.6709 (0.6889) acc@5 0.9671 (0.9617)\n",
      "\u001b[32m[2020-07-12 04:50:12] __main__ INFO: \u001b[0mEpoch 33 Step 300/351 lr 0.100000 loss 1.0775 (0.9808) acc@1 0.6726 (0.6875) acc@5 0.9498 (0.9614)\n",
      "\u001b[32m[2020-07-12 04:50:16] __main__ INFO: \u001b[0mEpoch 33 Step 351/351 lr 0.100000 loss 0.9874 (0.9806) acc@1 0.6533 (0.6874) acc@5 0.9703 (0.9614)\n",
      "\u001b[32m[2020-07-12 04:50:16] __main__ INFO: \u001b[0mElapsed 32.40\n",
      "\u001b[32m[2020-07-12 04:50:16] __main__ INFO: \u001b[0mVal 33\n",
      "\u001b[32m[2020-07-12 04:50:17] __main__ INFO: \u001b[0mEpoch 33 loss 0.5118 acc@1 0.8608 acc@5 0.9916\n",
      "\u001b[32m[2020-07-12 04:50:17] __main__ INFO: \u001b[0mElapsed 1.04\n",
      "\u001b[32m[2020-07-12 04:50:17] __main__ INFO: \u001b[0mTrain 34 11583\n",
      "\u001b[32m[2020-07-12 04:50:27] __main__ INFO: \u001b[0mEpoch 34 Step 100/351 lr 0.100000 loss 0.9904 (0.9746) acc@1 0.6758 (0.6875) acc@5 0.9575 (0.9618)\n",
      "\u001b[32m[2020-07-12 04:50:36] __main__ INFO: \u001b[0mEpoch 34 Step 200/351 lr 0.100000 loss 0.9647 (0.9696) acc@1 0.7072 (0.6894) acc@5 0.9512 (0.9622)\n",
      "\u001b[32m[2020-07-12 04:50:45] __main__ INFO: \u001b[0mEpoch 34 Step 300/351 lr 0.100000 loss 0.9301 (0.9683) acc@1 0.6683 (0.6908) acc@5 0.9816 (0.9617)\n",
      "\u001b[32m[2020-07-12 04:50:50] __main__ INFO: \u001b[0mEpoch 34 Step 351/351 lr 0.100000 loss 0.9311 (0.9727) acc@1 0.7153 (0.6890) acc@5 0.9649 (0.9616)\n",
      "\u001b[32m[2020-07-12 04:50:50] __main__ INFO: \u001b[0mElapsed 32.48\n",
      "\u001b[32m[2020-07-12 04:50:50] __main__ INFO: \u001b[0mVal 34\n",
      "\u001b[32m[2020-07-12 04:50:51] __main__ INFO: \u001b[0mEpoch 34 loss 0.5204 acc@1 0.8602 acc@5 0.9908\n",
      "\u001b[32m[2020-07-12 04:50:51] __main__ INFO: \u001b[0mElapsed 1.05\n",
      "\u001b[32m[2020-07-12 04:50:51] __main__ INFO: \u001b[0mTrain 35 11934\n",
      "\u001b[32m[2020-07-12 04:51:00] __main__ INFO: \u001b[0mEpoch 35 Step 100/351 lr 0.100000 loss 0.8559 (0.9580) acc@1 0.7339 (0.6947) acc@5 0.9546 (0.9633)\n",
      "\u001b[32m[2020-07-12 04:51:09] __main__ INFO: \u001b[0mEpoch 35 Step 200/351 lr 0.100000 loss 1.0113 (0.9597) acc@1 0.6746 (0.6941) acc@5 0.9498 (0.9631)\n",
      "\u001b[32m[2020-07-12 04:51:19] __main__ INFO: \u001b[0mEpoch 35 Step 300/351 lr 0.100000 loss 0.9317 (0.9656) acc@1 0.7161 (0.6923) acc@5 0.9777 (0.9617)\n",
      "\u001b[32m[2020-07-12 04:51:23] __main__ INFO: \u001b[0mEpoch 35 Step 351/351 lr 0.100000 loss 0.9652 (0.9668) acc@1 0.6824 (0.6916) acc@5 0.9791 (0.9616)\n",
      "\u001b[32m[2020-07-12 04:51:23] __main__ INFO: \u001b[0mElapsed 32.57\n",
      "\u001b[32m[2020-07-12 04:51:23] __main__ INFO: \u001b[0mVal 35\n",
      "\u001b[32m[2020-07-12 04:51:24] __main__ INFO: \u001b[0mEpoch 35 loss 0.6078 acc@1 0.8288 acc@5 0.9882\n",
      "\u001b[32m[2020-07-12 04:51:24] __main__ INFO: \u001b[0mElapsed 1.05\n",
      "\u001b[32m[2020-07-12 04:51:24] __main__ INFO: \u001b[0mTrain 36 12285\n",
      "\u001b[32m[2020-07-12 04:51:34] __main__ INFO: \u001b[0mEpoch 36 Step 100/351 lr 0.100000 loss 1.0086 (0.9432) acc@1 0.6993 (0.6991) acc@5 0.9564 (0.9642)\n",
      "\u001b[32m[2020-07-12 04:51:43] __main__ INFO: \u001b[0mEpoch 36 Step 200/351 lr 0.100000 loss 0.9723 (0.9545) acc@1 0.6861 (0.6962) acc@5 0.9507 (0.9625)\n",
      "\u001b[32m[2020-07-12 04:51:52] __main__ INFO: \u001b[0mEpoch 36 Step 300/351 lr 0.100000 loss 0.9307 (0.9575) acc@1 0.7149 (0.6952) acc@5 0.9609 (0.9624)\n",
      "\u001b[32m[2020-07-12 04:51:57] __main__ INFO: \u001b[0mEpoch 36 Step 351/351 lr 0.100000 loss 0.9681 (0.9597) acc@1 0.7306 (0.6942) acc@5 0.9536 (0.9628)\n",
      "\u001b[32m[2020-07-12 04:51:57] __main__ INFO: \u001b[0mElapsed 32.56\n",
      "\u001b[32m[2020-07-12 04:51:57] __main__ INFO: \u001b[0mVal 36\n",
      "\u001b[32m[2020-07-12 05:24:07] __main__ INFO: \u001b[0mEpoch 94 Step 200/351 lr 0.010000 loss 0.6340 (0.6366) acc@1 0.7912 (0.8072) acc@5 0.9883 (0.9868)\n"
     ]
    }
   ],
   "source": [
    "# Train the model per the settings specified in the original paper\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "!python train.py --config configs/cifar/resnet.yaml \\\n",
    "    model.resnet.depth 32 \\\n",
    "    train.batch_size 128 \\\n",
    "    dataset.name CIFAR10_CM_.5 \\\n",
    "    train.base_lr 0.1 \\\n",
    "    train.output_dir /home/ec2-user/SageMaker/experiments/resnet_basic_32_cm_1_.5/exp00 \\\n",
    "    scheduler.epochs 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Refine the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-12 15:55:18] __main__ INFO: \u001b[0mdevice: cuda\n",
      "cudnn:\n",
      "  benchmark: True\n",
      "  deterministic: False\n",
      "dataset:\n",
      "  name: CIFAR10\n",
      "  dataset_dir: ~/.torch/datasets/CIFAR10\n",
      "  image_size: 32\n",
      "  n_channels: 3\n",
      "  n_classes: 10\n",
      "model:\n",
      "  type: cifar\n",
      "  name: resnet\n",
      "  init_mode: kaiming_fan_out\n",
      "  vgg:\n",
      "    n_channels: [64, 128, 256, 512, 512]\n",
      "    n_layers: [2, 2, 3, 3, 3]\n",
      "    use_bn: True\n",
      "  resnet:\n",
      "    depth: 32\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "  resnet_preact:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "  wrn:\n",
      "    depth: 28\n",
      "    initial_channels: 16\n",
      "    widening_factor: 10\n",
      "    drop_rate: 0.0\n",
      "  densenet:\n",
      "    depth: 100\n",
      "    n_blocks: [6, 12, 24, 16]\n",
      "    block_type: bottleneck\n",
      "    growth_rate: 12\n",
      "    drop_rate: 0.0\n",
      "    compression_rate: 0.5\n",
      "  pyramidnet:\n",
      "    depth: 272\n",
      "    n_blocks: [3, 24, 36, 3]\n",
      "    initial_channels: 16\n",
      "    block_type: bottleneck\n",
      "    alpha: 200\n",
      "  resnext:\n",
      "    depth: 29\n",
      "    n_blocks: [3, 4, 6, 3]\n",
      "    initial_channels: 64\n",
      "    cardinality: 8\n",
      "    base_channels: 4\n",
      "  shake_shake:\n",
      "    depth: 26\n",
      "    initial_channels: 96\n",
      "    shake_forward: True\n",
      "    shake_backward: True\n",
      "    shake_image: True\n",
      "  se_resnet_preact:\n",
      "    depth: 110\n",
      "    initial_channels: 16\n",
      "    se_reduction: 16\n",
      "    block_type: basic\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "train:\n",
      "  checkpoint: /home/ec2-user/SageMaker/experiments/resnet_basic_32_cm_1_.5/exp00/checkpoint_00400.pth\n",
      "  resume: False\n",
      "  precision: O0\n",
      "  batch_size: 128\n",
      "  subdivision: 1\n",
      "  optimizer: sgd\n",
      "  base_lr: 0.001\n",
      "  momentum: 0.9\n",
      "  nesterov: True\n",
      "  weight_decay: 0.0001\n",
      "  no_weight_decay_on_bn: False\n",
      "  gradient_clip: 0\n",
      "  start_epoch: 0\n",
      "  seed: 0\n",
      "  val_first: True\n",
      "  val_period: 1\n",
      "  val_ratio: 0.1\n",
      "  use_test_as_val: False\n",
      "  output_dir: /home/ec2-user/SageMaker/experiments/resnet_basic_32_cm_1_.5/exp00_resume400_50\n",
      "  log_period: 100\n",
      "  checkpoint_period: 100\n",
      "  use_tensorboard: True\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: True\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "  distributed: False\n",
      "  dist:\n",
      "    backend: nccl\n",
      "    init_method: env://\n",
      "    world_size: -1\n",
      "    node_rank: -1\n",
      "    local_rank: 0\n",
      "    use_sync_bn: False\n",
      "tensorboard:\n",
      "  train_images: False\n",
      "  val_images: False\n",
      "  model_params: False\n",
      "optim:\n",
      "  adam:\n",
      "    betas: (0.9, 0.999)\n",
      "  lars:\n",
      "    eps: 1e-09\n",
      "    threshold: 0.01\n",
      "  adabound:\n",
      "    betas: (0.9, 0.999)\n",
      "    final_lr: 0.1\n",
      "    gamma: 0.001\n",
      "scheduler:\n",
      "  epochs: 50\n",
      "  warmup:\n",
      "    type: none\n",
      "    epochs: 0\n",
      "    start_factor: 0.001\n",
      "    exponent: 4\n",
      "  type: multistep\n",
      "  milestones: [80, 120]\n",
      "  lr_decay: 0.1\n",
      "  lr_min_factor: 0.001\n",
      "  T0: 10\n",
      "  T_mul: 1.0\n",
      "validation:\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: False\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "augmentation:\n",
      "  use_random_crop: True\n",
      "  use_random_horizontal_flip: True\n",
      "  use_cutout: False\n",
      "  use_random_erasing: False\n",
      "  use_dual_cutout: False\n",
      "  use_mixup: False\n",
      "  use_ricap: False\n",
      "  use_cutmix: False\n",
      "  use_label_smoothing: False\n",
      "  random_crop:\n",
      "    padding: 4\n",
      "    fill: 0\n",
      "    padding_mode: constant\n",
      "  random_horizontal_flip:\n",
      "    prob: 0.5\n",
      "  cutout:\n",
      "    prob: 1.0\n",
      "    mask_size: 16\n",
      "    cut_inside: False\n",
      "    mask_color: 0\n",
      "    dual_cutout_alpha: 0.1\n",
      "  random_erasing:\n",
      "    prob: 0.5\n",
      "    area_ratio_range: [0.02, 0.4]\n",
      "    min_aspect_ratio: 0.3\n",
      "    max_attempt: 20\n",
      "  mixup:\n",
      "    alpha: 1.0\n",
      "  ricap:\n",
      "    beta: 0.3\n",
      "  cutmix:\n",
      "    alpha: 1.0\n",
      "  label_smoothing:\n",
      "    epsilon: 0.1\n",
      "tta:\n",
      "  use_resize: False\n",
      "  use_center_crop: False\n",
      "  resize: 256\n",
      "test:\n",
      "  checkpoint: ''\n",
      "  output_dir: ''\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    pin_memory: False\n",
      "\u001b[32m[2020-07-12 15:55:18] __main__ INFO: \u001b[0menv_info:\n",
      "  pytorch_version: 1.4.0\n",
      "  cuda_version: 10.1\n",
      "  cudnn_version: 7603\n",
      "  num_gpus: 1\n",
      "  gpu_name: Tesla K80\n",
      "  gpu_capability: 3.7\n",
      "Files already downloaded and verified\n",
      "\u001b[32m[2020-07-12 15:55:20] __main__ INFO: \u001b[0mMACs  : 69.76M\n",
      "\u001b[32m[2020-07-12 15:55:20] __main__ INFO: \u001b[0m#params: 466.91K\n",
      "Selected optimization level O0:  Pure FP32 training.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\",)\n",
      "\u001b[32m[2020-07-12 15:55:21] __main__ INFO: \u001b[0mVal 0\n",
      "\u001b[32m[2020-07-12 15:55:22] __main__ INFO: \u001b[0mEpoch 0 loss 2.2797 acc@1 0.3992 acc@5 0.7858\n",
      "\u001b[32m[2020-07-12 15:55:22] __main__ INFO: \u001b[0mElapsed 1.39\n",
      "\u001b[32m[2020-07-12 15:55:22] __main__ INFO: \u001b[0mTrain 1 0\n",
      "\u001b[32m[2020-07-12 15:55:31] __main__ INFO: \u001b[0mEpoch 1 Step 100/351 lr 0.001000 loss 0.0878 (0.1636) acc@1 0.9688 (0.9614) acc@5 1.0000 (0.9982)\n",
      "\u001b[32m[2020-07-12 15:55:40] __main__ INFO: \u001b[0mEpoch 1 Step 200/351 lr 0.001000 loss 0.0315 (0.1418) acc@1 1.0000 (0.9645) acc@5 1.0000 (0.9986)\n",
      "\u001b[32m[2020-07-12 15:55:49] __main__ INFO: \u001b[0mEpoch 1 Step 300/351 lr 0.001000 loss 0.0717 (0.1327) acc@1 0.9844 (0.9660) acc@5 1.0000 (0.9988)\n",
      "\u001b[32m[2020-07-12 15:55:54] __main__ INFO: \u001b[0mEpoch 1 Step 351/351 lr 0.001000 loss 0.1467 (0.1312) acc@1 0.9531 (0.9659) acc@5 1.0000 (0.9988)\n",
      "\u001b[32m[2020-07-12 15:55:54] __main__ INFO: \u001b[0mElapsed 31.98\n",
      "\u001b[32m[2020-07-12 15:55:54] __main__ INFO: \u001b[0mVal 1\n",
      "\u001b[32m[2020-07-12 15:55:55] __main__ INFO: \u001b[0mEpoch 1 loss 0.3777 acc@1 0.9030 acc@5 0.9924\n",
      "\u001b[32m[2020-07-12 15:55:55] __main__ INFO: \u001b[0mElapsed 1.04\n",
      "\u001b[32m[2020-07-12 15:55:55] __main__ INFO: \u001b[0mTrain 2 351\n",
      "\u001b[32m[2020-07-12 15:56:04] __main__ INFO: \u001b[0mEpoch 2 Step 100/351 lr 0.001000 loss 0.1035 (0.1117) acc@1 0.9609 (0.9680) acc@5 1.0000 (0.9993)\n",
      "\u001b[32m[2020-07-12 15:56:13] __main__ INFO: \u001b[0mEpoch 2 Step 200/351 lr 0.001000 loss 0.2426 (0.1131) acc@1 0.9375 (0.9671) acc@5 0.9922 (0.9993)\n",
      "\u001b[32m[2020-07-12 15:56:22] __main__ INFO: \u001b[0mEpoch 2 Step 300/351 lr 0.001000 loss 0.1255 (0.1100) acc@1 0.9766 (0.9679) acc@5 1.0000 (0.9991)\n",
      "\u001b[32m[2020-07-12 15:56:27] __main__ INFO: \u001b[0mEpoch 2 Step 351/351 lr 0.001000 loss 0.0956 (0.1100) acc@1 0.9688 (0.9682) acc@5 1.0000 (0.9991)\n",
      "\u001b[32m[2020-07-12 15:56:27] __main__ INFO: \u001b[0mElapsed 31.84\n",
      "\u001b[32m[2020-07-12 15:56:27] __main__ INFO: \u001b[0mVal 2\n",
      "\u001b[32m[2020-07-12 15:56:28] __main__ INFO: \u001b[0mEpoch 2 loss 0.3567 acc@1 0.9048 acc@5 0.9934\n",
      "\u001b[32m[2020-07-12 15:56:28] __main__ INFO: \u001b[0mElapsed 1.05\n",
      "\u001b[32m[2020-07-12 15:56:28] __main__ INFO: \u001b[0mTrain 3 702\n",
      "\u001b[32m[2020-07-12 15:56:37] __main__ INFO: \u001b[0mEpoch 3 Step 100/351 lr 0.001000 loss 0.0433 (0.0945) acc@1 0.9844 (0.9720) acc@5 1.0000 (0.9992)\n",
      "\u001b[32m[2020-07-12 15:56:46] __main__ INFO: \u001b[0mEpoch 3 Step 200/351 lr 0.001000 loss 0.0804 (0.1011) acc@1 0.9844 (0.9705) acc@5 1.0000 (0.9992)\n",
      "\u001b[32m[2020-07-12 15:56:55] __main__ INFO: \u001b[0mEpoch 3 Step 300/351 lr 0.001000 loss 0.0515 (0.1011) acc@1 0.9844 (0.9697) acc@5 1.0000 (0.9993)\n",
      "\u001b[32m[2020-07-12 15:57:00] __main__ INFO: \u001b[0mEpoch 3 Step 351/351 lr 0.001000 loss 0.1183 (0.1002) acc@1 0.9766 (0.9701) acc@5 1.0000 (0.9993)\n",
      "\u001b[32m[2020-07-12 15:57:00] __main__ INFO: \u001b[0mElapsed 31.82\n",
      "\u001b[32m[2020-07-12 15:57:00] __main__ INFO: \u001b[0mVal 3\n",
      "\u001b[32m[2020-07-12 15:57:01] __main__ INFO: \u001b[0mEpoch 3 loss 0.3525 acc@1 0.9046 acc@5 0.9936\n",
      "\u001b[32m[2020-07-12 15:57:01] __main__ INFO: \u001b[0mElapsed 1.03\n",
      "\u001b[32m[2020-07-12 15:57:01] __main__ INFO: \u001b[0mTrain 4 1053\n",
      "\u001b[32m[2020-07-12 15:57:10] __main__ INFO: \u001b[0mEpoch 4 Step 100/351 lr 0.001000 loss 0.0940 (0.1027) acc@1 0.9609 (0.9688) acc@5 1.0000 (0.9995)\n",
      "\u001b[32m[2020-07-12 15:57:19] __main__ INFO: \u001b[0mEpoch 4 Step 200/351 lr 0.001000 loss 0.0789 (0.0999) acc@1 0.9688 (0.9704) acc@5 1.0000 (0.9993)\n",
      "\u001b[32m[2020-07-12 15:57:28] __main__ INFO: \u001b[0mEpoch 4 Step 300/351 lr 0.001000 loss 0.1558 (0.0968) acc@1 0.9688 (0.9710) acc@5 1.0000 (0.9995)\n",
      "\u001b[32m[2020-07-12 15:57:32] __main__ INFO: \u001b[0mEpoch 4 Step 351/351 lr 0.001000 loss 0.0604 (0.0958) acc@1 0.9922 (0.9713) acc@5 1.0000 (0.9995)\n",
      "\u001b[32m[2020-07-12 15:57:32] __main__ INFO: \u001b[0mElapsed 31.71\n",
      "\u001b[32m[2020-07-12 15:57:32] __main__ INFO: \u001b[0mVal 4\n",
      "\u001b[32m[2020-07-12 15:57:33] __main__ INFO: \u001b[0mEpoch 4 loss 0.3548 acc@1 0.9030 acc@5 0.9940\n",
      "\u001b[32m[2020-07-12 15:57:33] __main__ INFO: \u001b[0mElapsed 1.03\n",
      "\u001b[32m[2020-07-12 15:57:33] __main__ INFO: \u001b[0mTrain 5 1404\n",
      "\u001b[32m[2020-07-12 15:57:43] __main__ INFO: \u001b[0mEpoch 5 Step 100/351 lr 0.001000 loss 0.1334 (0.0916) acc@1 0.9531 (0.9719) acc@5 0.9922 (0.9997)\n",
      "\u001b[32m[2020-07-12 15:57:52] __main__ INFO: \u001b[0mEpoch 5 Step 200/351 lr 0.001000 loss 0.0826 (0.0920) acc@1 0.9766 (0.9715) acc@5 1.0000 (0.9995)\n",
      "\u001b[32m[2020-07-12 15:58:01] __main__ INFO: \u001b[0mEpoch 5 Step 300/351 lr 0.001000 loss 0.0595 (0.0900) acc@1 0.9844 (0.9724) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-07-12 15:58:05] __main__ INFO: \u001b[0mEpoch 5 Step 351/351 lr 0.001000 loss 0.0652 (0.0904) acc@1 0.9766 (0.9724) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-07-12 15:58:05] __main__ INFO: \u001b[0mElapsed 31.74\n",
      "\u001b[32m[2020-07-12 15:58:05] __main__ INFO: \u001b[0mVal 5\n",
      "\u001b[32m[2020-07-12 15:58:06] __main__ INFO: \u001b[0mEpoch 5 loss 0.3572 acc@1 0.9030 acc@5 0.9948\n",
      "\u001b[32m[2020-07-12 15:58:06] __main__ INFO: \u001b[0mElapsed 1.06\n",
      "\u001b[32m[2020-07-12 15:58:06] __main__ INFO: \u001b[0mTrain 6 1755\n",
      "\u001b[32m[2020-07-12 15:58:15] __main__ INFO: \u001b[0mEpoch 6 Step 100/351 lr 0.001000 loss 0.0822 (0.0866) acc@1 0.9688 (0.9730) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-07-12 15:58:24] __main__ INFO: \u001b[0mEpoch 6 Step 200/351 lr 0.001000 loss 0.1046 (0.0918) acc@1 0.9688 (0.9711) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-07-12 15:58:33] __main__ INFO: \u001b[0mEpoch 6 Step 300/351 lr 0.001000 loss 0.0727 (0.0895) acc@1 0.9766 (0.9723) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-07-12 15:58:38] __main__ INFO: \u001b[0mEpoch 6 Step 351/351 lr 0.001000 loss 0.1105 (0.0907) acc@1 0.9453 (0.9717) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-07-12 15:58:38] __main__ INFO: \u001b[0mElapsed 31.74\n",
      "\u001b[32m[2020-07-12 15:58:38] __main__ INFO: \u001b[0mVal 6\n",
      "\u001b[32m[2020-07-12 15:58:39] __main__ INFO: \u001b[0mEpoch 6 loss 0.3518 acc@1 0.9038 acc@5 0.9932\n",
      "\u001b[32m[2020-07-12 15:58:39] __main__ INFO: \u001b[0mElapsed 1.02\n",
      "\u001b[32m[2020-07-12 15:58:39] __main__ INFO: \u001b[0mTrain 7 2106\n",
      "\u001b[32m[2020-07-12 15:58:48] __main__ INFO: \u001b[0mEpoch 7 Step 100/351 lr 0.001000 loss 0.0819 (0.0789) acc@1 0.9766 (0.9760) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-12 15:58:57] __main__ INFO: \u001b[0mEpoch 7 Step 200/351 lr 0.001000 loss 0.0461 (0.0859) acc@1 0.9922 (0.9740) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-12 15:59:06] __main__ INFO: \u001b[0mEpoch 7 Step 300/351 lr 0.001000 loss 0.1010 (0.0872) acc@1 0.9766 (0.9736) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-07-12 15:59:11] __main__ INFO: \u001b[0mEpoch 7 Step 351/351 lr 0.001000 loss 0.0963 (0.0888) acc@1 0.9688 (0.9734) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-07-12 15:59:11] __main__ INFO: \u001b[0mElapsed 31.75\n",
      "\u001b[32m[2020-07-12 15:59:11] __main__ INFO: \u001b[0mVal 7\n",
      "\u001b[32m[2020-07-12 15:59:12] __main__ INFO: \u001b[0mEpoch 7 loss 0.3498 acc@1 0.9058 acc@5 0.9940\n",
      "\u001b[32m[2020-07-12 15:59:12] __main__ INFO: \u001b[0mElapsed 1.04\n",
      "\u001b[32m[2020-07-12 15:59:12] __main__ INFO: \u001b[0mTrain 8 2457\n",
      "\u001b[32m[2020-07-12 15:59:21] __main__ INFO: \u001b[0mEpoch 8 Step 100/351 lr 0.001000 loss 0.0455 (0.0862) acc@1 0.9922 (0.9730) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-12 15:59:30] __main__ INFO: \u001b[0mEpoch 8 Step 200/351 lr 0.001000 loss 0.0209 (0.0825) acc@1 1.0000 (0.9741) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-07-12 15:59:39] __main__ INFO: \u001b[0mEpoch 8 Step 300/351 lr 0.001000 loss 0.1551 (0.0828) acc@1 0.9609 (0.9742) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-07-12 15:59:44] __main__ INFO: \u001b[0mEpoch 8 Step 351/351 lr 0.001000 loss 0.1110 (0.0843) acc@1 0.9766 (0.9744) acc@5 1.0000 (0.9995)\n",
      "\u001b[32m[2020-07-12 15:59:44] __main__ INFO: \u001b[0mElapsed 31.79\n",
      "\u001b[32m[2020-07-12 15:59:44] __main__ INFO: \u001b[0mVal 8\n",
      "\u001b[32m[2020-07-12 15:59:45] __main__ INFO: \u001b[0mEpoch 8 loss 0.3436 acc@1 0.9046 acc@5 0.9942\n",
      "\u001b[32m[2020-07-12 15:59:45] __main__ INFO: \u001b[0mElapsed 1.05\n",
      "\u001b[32m[2020-07-12 15:59:45] __main__ INFO: \u001b[0mTrain 9 2808\n",
      "\u001b[32m[2020-07-12 15:59:54] __main__ INFO: \u001b[0mEpoch 9 Step 100/351 lr 0.001000 loss 0.0440 (0.0816) acc@1 0.9766 (0.9746) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-07-12 16:00:03] __main__ INFO: \u001b[0mEpoch 9 Step 200/351 lr 0.001000 loss 0.1311 (0.0817) acc@1 0.9688 (0.9744) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-07-12 16:00:12] __main__ INFO: \u001b[0mEpoch 9 Step 300/351 lr 0.001000 loss 0.0207 (0.0808) acc@1 1.0000 (0.9748) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-07-12 16:00:16] __main__ INFO: \u001b[0mEpoch 9 Step 351/351 lr 0.001000 loss 0.0170 (0.0811) acc@1 1.0000 (0.9747) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-07-12 16:00:16] __main__ INFO: \u001b[0mElapsed 31.75\n",
      "\u001b[32m[2020-07-12 16:00:16] __main__ INFO: \u001b[0mVal 9\n",
      "\u001b[32m[2020-07-12 16:00:17] __main__ INFO: \u001b[0mEpoch 9 loss 0.3469 acc@1 0.9048 acc@5 0.9938\n",
      "\u001b[32m[2020-07-12 16:00:17] __main__ INFO: \u001b[0mElapsed 1.04\n",
      "\u001b[32m[2020-07-12 16:00:17] __main__ INFO: \u001b[0mTrain 10 3159\n",
      "\u001b[32m[2020-07-12 16:00:27] __main__ INFO: \u001b[0mEpoch 10 Step 100/351 lr 0.001000 loss 0.0094 (0.0695) acc@1 1.0000 (0.9779) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-07-12 16:00:36] __main__ INFO: \u001b[0mEpoch 10 Step 200/351 lr 0.001000 loss 0.2099 (0.0748) acc@1 0.9219 (0.9766) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-07-12 16:00:45] __main__ INFO: \u001b[0mEpoch 10 Step 300/351 lr 0.001000 loss 0.0573 (0.0764) acc@1 0.9844 (0.9762) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-07-12 16:00:49] __main__ INFO: \u001b[0mEpoch 10 Step 351/351 lr 0.001000 loss 0.1361 (0.0780) acc@1 0.9531 (0.9760) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-07-12 16:00:49] __main__ INFO: \u001b[0mElapsed 31.83\n",
      "\u001b[32m[2020-07-12 16:00:49] __main__ INFO: \u001b[0mVal 10\n",
      "\u001b[32m[2020-07-12 16:00:50] __main__ INFO: \u001b[0mEpoch 10 loss 0.3502 acc@1 0.9052 acc@5 0.9946\n",
      "\u001b[32m[2020-07-12 16:00:50] __main__ INFO: \u001b[0mElapsed 1.03\n",
      "\u001b[32m[2020-07-12 16:00:50] __main__ INFO: \u001b[0mTrain 11 3510\n",
      "\u001b[32m[2020-07-12 16:00:59] __main__ INFO: \u001b[0mEpoch 11 Step 100/351 lr 0.001000 loss 0.0626 (0.0771) acc@1 0.9844 (0.9755) acc@5 1.0000 (0.9995)\n",
      "\u001b[32m[2020-07-12 16:01:08] __main__ INFO: \u001b[0mEpoch 11 Step 200/351 lr 0.001000 loss 0.2007 (0.0798) acc@1 0.9531 (0.9741) acc@5 0.9922 (0.9996)\n",
      "\u001b[32m[2020-07-12 16:01:17] __main__ INFO: \u001b[0mEpoch 11 Step 300/351 lr 0.001000 loss 0.0359 (0.0768) acc@1 0.9922 (0.9756) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-07-12 16:01:22] __main__ INFO: \u001b[0mEpoch 11 Step 351/351 lr 0.001000 loss 0.0863 (0.0773) acc@1 0.9844 (0.9756) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-07-12 16:01:22] __main__ INFO: \u001b[0mElapsed 31.87\n",
      "\u001b[32m[2020-07-12 16:01:22] __main__ INFO: \u001b[0mVal 11\n",
      "\u001b[32m[2020-07-12 16:01:23] __main__ INFO: \u001b[0mEpoch 11 loss 0.3506 acc@1 0.9044 acc@5 0.9932\n",
      "\u001b[32m[2020-07-12 16:01:23] __main__ INFO: \u001b[0mElapsed 1.04\n",
      "\u001b[32m[2020-07-12 16:01:23] __main__ INFO: \u001b[0mTrain 12 3861\n",
      "\u001b[32m[2020-07-12 16:01:32] __main__ INFO: \u001b[0mEpoch 12 Step 100/351 lr 0.001000 loss 0.0776 (0.0741) acc@1 0.9766 (0.9767) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-12 16:01:41] __main__ INFO: \u001b[0mEpoch 12 Step 200/351 lr 0.001000 loss 0.0621 (0.0766) acc@1 0.9922 (0.9753) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-12 16:01:50] __main__ INFO: \u001b[0mEpoch 12 Step 300/351 lr 0.001000 loss 0.1446 (0.0760) acc@1 0.9609 (0.9756) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-12 16:01:55] __main__ INFO: \u001b[0mEpoch 12 Step 351/351 lr 0.001000 loss 0.0892 (0.0763) acc@1 0.9766 (0.9754) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-12 16:01:55] __main__ INFO: \u001b[0mElapsed 31.83\n",
      "\u001b[32m[2020-07-12 16:01:55] __main__ INFO: \u001b[0mVal 12\n",
      "\u001b[32m[2020-07-12 16:01:56] __main__ INFO: \u001b[0mEpoch 12 loss 0.3508 acc@1 0.9060 acc@5 0.9938\n",
      "\u001b[32m[2020-07-12 16:01:56] __main__ INFO: \u001b[0mElapsed 1.05\n",
      "\u001b[32m[2020-07-12 16:01:56] __main__ INFO: \u001b[0mTrain 13 4212\n",
      "\u001b[32m[2020-07-12 16:02:05] __main__ INFO: \u001b[0mEpoch 13 Step 100/351 lr 0.001000 loss 0.1087 (0.0734) acc@1 0.9688 (0.9774) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-12 16:02:14] __main__ INFO: \u001b[0mEpoch 13 Step 200/351 lr 0.001000 loss 0.0183 (0.0716) acc@1 1.0000 (0.9777) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-12 16:02:23] __main__ INFO: \u001b[0mEpoch 13 Step 300/351 lr 0.001000 loss 0.0577 (0.0733) acc@1 0.9844 (0.9772) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-12 16:02:28] __main__ INFO: \u001b[0mEpoch 13 Step 351/351 lr 0.001000 loss 0.0528 (0.0744) acc@1 0.9922 (0.9767) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-12 16:02:28] __main__ INFO: \u001b[0mElapsed 31.69\n",
      "\u001b[32m[2020-07-12 16:02:28] __main__ INFO: \u001b[0mVal 13\n",
      "\u001b[32m[2020-07-12 16:02:29] __main__ INFO: \u001b[0mEpoch 13 loss 0.3567 acc@1 0.9020 acc@5 0.9944\n",
      "\u001b[32m[2020-07-12 16:02:29] __main__ INFO: \u001b[0mElapsed 1.03\n",
      "\u001b[32m[2020-07-12 16:02:29] __main__ INFO: \u001b[0mTrain 14 4563\n",
      "\u001b[32m[2020-07-12 16:02:38] __main__ INFO: \u001b[0mEpoch 14 Step 100/351 lr 0.001000 loss 0.0712 (0.0708) acc@1 0.9688 (0.9766) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-12 16:02:47] __main__ INFO: \u001b[0mEpoch 14 Step 200/351 lr 0.001000 loss 0.0110 (0.0724) acc@1 1.0000 (0.9771) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-12 16:02:56] __main__ INFO: \u001b[0mEpoch 14 Step 300/351 lr 0.001000 loss 0.0786 (0.0707) acc@1 0.9766 (0.9778) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-12 16:03:01] __main__ INFO: \u001b[0mEpoch 14 Step 351/351 lr 0.001000 loss 0.0562 (0.0714) acc@1 0.9844 (0.9777) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-12 16:03:01] __main__ INFO: \u001b[0mElapsed 31.75\n",
      "\u001b[32m[2020-07-12 16:03:01] __main__ INFO: \u001b[0mVal 14\n",
      "\u001b[32m[2020-07-12 16:03:02] __main__ INFO: \u001b[0mEpoch 14 loss 0.3515 acc@1 0.9056 acc@5 0.9942\n",
      "\u001b[32m[2020-07-12 16:03:02] __main__ INFO: \u001b[0mElapsed 1.05\n",
      "\u001b[32m[2020-07-12 16:03:02] __main__ INFO: \u001b[0mTrain 15 4914\n",
      "\u001b[32m[2020-07-12 16:03:11] __main__ INFO: \u001b[0mEpoch 15 Step 100/351 lr 0.001000 loss 0.0156 (0.0719) acc@1 0.9922 (0.9771) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-12 16:03:20] __main__ INFO: \u001b[0mEpoch 15 Step 200/351 lr 0.001000 loss 0.0604 (0.0695) acc@1 0.9844 (0.9776) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-12 16:03:29] __main__ INFO: \u001b[0mEpoch 15 Step 300/351 lr 0.001000 loss 0.0748 (0.0700) acc@1 0.9844 (0.9778) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-07-12 16:03:33] __main__ INFO: \u001b[0mEpoch 15 Step 351/351 lr 0.001000 loss 0.0446 (0.0703) acc@1 0.9844 (0.9780) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-07-12 16:03:33] __main__ INFO: \u001b[0mElapsed 31.67\n",
      "\u001b[32m[2020-07-12 16:03:33] __main__ INFO: \u001b[0mVal 15\n",
      "\u001b[32m[2020-07-12 16:03:34] __main__ INFO: \u001b[0mEpoch 15 loss 0.3586 acc@1 0.9036 acc@5 0.9950\n",
      "\u001b[32m[2020-07-12 16:03:34] __main__ INFO: \u001b[0mElapsed 1.06\n",
      "\u001b[32m[2020-07-12 16:03:34] __main__ INFO: \u001b[0mTrain 16 5265\n",
      "\u001b[32m[2020-07-12 16:03:43] __main__ INFO: \u001b[0mEpoch 16 Step 100/351 lr 0.001000 loss 0.1175 (0.0678) acc@1 0.9688 (0.9792) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-07-12 16:03:52] __main__ INFO: \u001b[0mEpoch 16 Step 200/351 lr 0.001000 loss 0.0997 (0.0687) acc@1 0.9609 (0.9782) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-07-12 16:04:01] __main__ INFO: \u001b[0mEpoch 16 Step 300/351 lr 0.001000 loss 0.0337 (0.0683) acc@1 1.0000 (0.9782) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-07-12 16:04:06] __main__ INFO: \u001b[0mEpoch 16 Step 351/351 lr 0.001000 loss 0.0971 (0.0694) acc@1 0.9688 (0.9777) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-07-12 16:04:06] __main__ INFO: \u001b[0mElapsed 31.75\n",
      "\u001b[32m[2020-07-12 16:04:06] __main__ INFO: \u001b[0mVal 16\n",
      "\u001b[32m[2020-07-12 16:04:07] __main__ INFO: \u001b[0mEpoch 16 loss 0.3565 acc@1 0.9070 acc@5 0.9944\n",
      "\u001b[32m[2020-07-12 16:04:07] __main__ INFO: \u001b[0mElapsed 1.04\n",
      "\u001b[32m[2020-07-12 16:04:07] __main__ INFO: \u001b[0mTrain 17 5616\n",
      "\u001b[32m[2020-07-12 16:04:16] __main__ INFO: \u001b[0mEpoch 17 Step 100/351 lr 0.001000 loss 0.0981 (0.0654) acc@1 0.9609 (0.9787) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-07-12 16:04:25] __main__ INFO: \u001b[0mEpoch 17 Step 200/351 lr 0.001000 loss 0.0695 (0.0641) acc@1 0.9609 (0.9793) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-12 16:04:34] __main__ INFO: \u001b[0mEpoch 17 Step 300/351 lr 0.001000 loss 0.0510 (0.0665) acc@1 0.9844 (0.9787) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-12 16:04:39] __main__ INFO: \u001b[0mEpoch 17 Step 351/351 lr 0.001000 loss 0.0558 (0.0664) acc@1 0.9766 (0.9788) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-12 16:04:39] __main__ INFO: \u001b[0mElapsed 31.73\n",
      "\u001b[32m[2020-07-12 16:04:39] __main__ INFO: \u001b[0mVal 17\n",
      "\u001b[32m[2020-07-12 16:04:40] __main__ INFO: \u001b[0mEpoch 17 loss 0.3644 acc@1 0.9050 acc@5 0.9948\n",
      "\u001b[32m[2020-07-12 16:04:40] __main__ INFO: \u001b[0mElapsed 1.05\n",
      "\u001b[32m[2020-07-12 16:04:40] __main__ INFO: \u001b[0mTrain 18 5967\n",
      "\u001b[32m[2020-07-12 16:04:49] __main__ INFO: \u001b[0mEpoch 18 Step 100/351 lr 0.001000 loss 0.0462 (0.0685) acc@1 0.9844 (0.9776) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-12 16:04:58] __main__ INFO: \u001b[0mEpoch 18 Step 200/351 lr 0.001000 loss 0.0721 (0.0650) acc@1 0.9688 (0.9794) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-12 16:05:07] __main__ INFO: \u001b[0mEpoch 18 Step 300/351 lr 0.001000 loss 0.1143 (0.0653) acc@1 0.9609 (0.9789) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-12 16:05:12] __main__ INFO: \u001b[0mEpoch 18 Step 351/351 lr 0.001000 loss 0.0289 (0.0650) acc@1 0.9922 (0.9789) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-12 16:05:12] __main__ INFO: \u001b[0mElapsed 31.68\n",
      "\u001b[32m[2020-07-12 16:05:12] __main__ INFO: \u001b[0mVal 18\n",
      "\u001b[32m[2020-07-12 16:05:13] __main__ INFO: \u001b[0mEpoch 18 loss 0.3662 acc@1 0.9062 acc@5 0.9954\n",
      "\u001b[32m[2020-07-12 16:05:13] __main__ INFO: \u001b[0mElapsed 1.04\n",
      "\u001b[32m[2020-07-12 16:05:13] __main__ INFO: \u001b[0mTrain 19 6318\n",
      "\u001b[32m[2020-07-12 16:05:22] __main__ INFO: \u001b[0mEpoch 19 Step 100/351 lr 0.001000 loss 0.0518 (0.0647) acc@1 0.9922 (0.9784) acc@5 1.0000 (0.9995)\n",
      "\u001b[32m[2020-07-12 16:05:31] __main__ INFO: \u001b[0mEpoch 19 Step 200/351 lr 0.001000 loss 0.0281 (0.0633) acc@1 0.9844 (0.9792) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-07-12 16:05:40] __main__ INFO: \u001b[0mEpoch 19 Step 300/351 lr 0.001000 loss 0.0973 (0.0655) acc@1 0.9844 (0.9786) acc@5 0.9922 (0.9997)\n",
      "\u001b[32m[2020-07-12 16:05:44] __main__ INFO: \u001b[0mEpoch 19 Step 351/351 lr 0.001000 loss 0.0154 (0.0649) acc@1 1.0000 (0.9788) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-07-12 16:05:44] __main__ INFO: \u001b[0mElapsed 31.78\n",
      "\u001b[32m[2020-07-12 16:05:44] __main__ INFO: \u001b[0mVal 19\n",
      "\u001b[32m[2020-07-12 16:05:45] __main__ INFO: \u001b[0mEpoch 19 loss 0.3726 acc@1 0.9018 acc@5 0.9952\n",
      "\u001b[32m[2020-07-12 16:05:45] __main__ INFO: \u001b[0mElapsed 1.04\n",
      "\u001b[32m[2020-07-12 16:05:45] __main__ INFO: \u001b[0mTrain 20 6669\n",
      "\u001b[32m[2020-07-12 16:05:55] __main__ INFO: \u001b[0mEpoch 20 Step 100/351 lr 0.001000 loss 0.0572 (0.0606) acc@1 0.9766 (0.9810) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-12 16:06:04] __main__ INFO: \u001b[0mEpoch 20 Step 200/351 lr 0.001000 loss 0.1177 (0.0602) acc@1 0.9688 (0.9812) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-12 16:06:13] __main__ INFO: \u001b[0mEpoch 20 Step 300/351 lr 0.001000 loss 0.0514 (0.0608) acc@1 0.9844 (0.9807) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-12 16:06:17] __main__ INFO: \u001b[0mEpoch 20 Step 351/351 lr 0.001000 loss 0.0405 (0.0622) acc@1 0.9766 (0.9801) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-12 16:06:17] __main__ INFO: \u001b[0mElapsed 31.86\n",
      "\u001b[32m[2020-07-12 16:06:17] __main__ INFO: \u001b[0mVal 20\n",
      "\u001b[32m[2020-07-12 16:06:18] __main__ INFO: \u001b[0mEpoch 20 loss 0.3722 acc@1 0.9034 acc@5 0.9950\n",
      "\u001b[32m[2020-07-12 16:06:18] __main__ INFO: \u001b[0mElapsed 1.04\n",
      "\u001b[32m[2020-07-12 16:06:18] __main__ INFO: \u001b[0mTrain 21 7020\n",
      "\u001b[32m[2020-07-12 16:06:27] __main__ INFO: \u001b[0mEpoch 21 Step 100/351 lr 0.001000 loss 0.1011 (0.0622) acc@1 0.9609 (0.9806) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-12 16:06:37] __main__ INFO: \u001b[0mEpoch 21 Step 200/351 lr 0.001000 loss 0.0344 (0.0614) acc@1 0.9844 (0.9805) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-12 16:06:46] __main__ INFO: \u001b[0mEpoch 21 Step 300/351 lr 0.001000 loss 0.0574 (0.0612) acc@1 0.9766 (0.9808) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-12 16:06:50] __main__ INFO: \u001b[0mEpoch 21 Step 351/351 lr 0.001000 loss 0.0392 (0.0612) acc@1 0.9844 (0.9806) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-12 16:06:50] __main__ INFO: \u001b[0mElapsed 31.86\n",
      "\u001b[32m[2020-07-12 16:06:50] __main__ INFO: \u001b[0mVal 21\n",
      "\u001b[32m[2020-07-12 16:06:51] __main__ INFO: \u001b[0mEpoch 21 loss 0.3732 acc@1 0.9046 acc@5 0.9950\n",
      "\u001b[32m[2020-07-12 16:06:51] __main__ INFO: \u001b[0mElapsed 1.04\n",
      "\u001b[32m[2020-07-12 16:06:51] __main__ INFO: \u001b[0mTrain 22 7371\n",
      "\u001b[32m[2020-07-12 16:07:00] __main__ INFO: \u001b[0mEpoch 22 Step 100/351 lr 0.001000 loss 0.0750 (0.0597) acc@1 0.9922 (0.9813) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-12 16:07:09] __main__ INFO: \u001b[0mEpoch 22 Step 200/351 lr 0.001000 loss 0.0707 (0.0591) acc@1 0.9688 (0.9814) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-12 16:07:18] __main__ INFO: \u001b[0mEpoch 22 Step 300/351 lr 0.001000 loss 0.0605 (0.0612) acc@1 0.9844 (0.9803) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-12 16:07:23] __main__ INFO: \u001b[0mEpoch 22 Step 351/351 lr 0.001000 loss 0.0536 (0.0600) acc@1 0.9844 (0.9808) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-12 16:07:23] __main__ INFO: \u001b[0mElapsed 31.79\n",
      "\u001b[32m[2020-07-12 16:07:23] __main__ INFO: \u001b[0mVal 22\n",
      "\u001b[32m[2020-07-12 16:07:24] __main__ INFO: \u001b[0mEpoch 22 loss 0.3682 acc@1 0.9054 acc@5 0.9952\n",
      "\u001b[32m[2020-07-12 16:07:24] __main__ INFO: \u001b[0mElapsed 1.04\n",
      "\u001b[32m[2020-07-12 16:07:24] __main__ INFO: \u001b[0mTrain 23 7722\n",
      "\u001b[32m[2020-07-12 16:07:33] __main__ INFO: \u001b[0mEpoch 23 Step 100/351 lr 0.001000 loss 0.0291 (0.0588) acc@1 1.0000 (0.9811) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-12 16:07:42] __main__ INFO: \u001b[0mEpoch 23 Step 200/351 lr 0.001000 loss 0.0453 (0.0608) acc@1 0.9844 (0.9800) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-12 16:07:51] __main__ INFO: \u001b[0mEpoch 23 Step 300/351 lr 0.001000 loss 0.0460 (0.0605) acc@1 0.9922 (0.9803) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-12 16:07:56] __main__ INFO: \u001b[0mEpoch 23 Step 351/351 lr 0.001000 loss 0.1251 (0.0601) acc@1 0.9688 (0.9808) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-12 16:07:56] __main__ INFO: \u001b[0mElapsed 31.74\n",
      "\u001b[32m[2020-07-12 16:07:56] __main__ INFO: \u001b[0mVal 23\n",
      "\u001b[32m[2020-07-12 16:07:57] __main__ INFO: \u001b[0mEpoch 23 loss 0.3713 acc@1 0.9074 acc@5 0.9956\n",
      "\u001b[32m[2020-07-12 16:07:57] __main__ INFO: \u001b[0mElapsed 1.03\n",
      "\u001b[32m[2020-07-12 16:07:57] __main__ INFO: \u001b[0mTrain 24 8073\n",
      "\u001b[32m[2020-07-12 16:08:06] __main__ INFO: \u001b[0mEpoch 24 Step 100/351 lr 0.001000 loss 0.0389 (0.0562) acc@1 0.9766 (0.9815) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-07-12 16:08:15] __main__ INFO: \u001b[0mEpoch 24 Step 200/351 lr 0.001000 loss 0.0241 (0.0562) acc@1 1.0000 (0.9818) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-12 16:08:24] __main__ INFO: \u001b[0mEpoch 24 Step 300/351 lr 0.001000 loss 0.1004 (0.0589) acc@1 0.9688 (0.9809) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-12 16:08:29] __main__ INFO: \u001b[0mEpoch 24 Step 351/351 lr 0.001000 loss 0.0201 (0.0592) acc@1 0.9922 (0.9809) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-12 16:08:29] __main__ INFO: \u001b[0mElapsed 31.74\n",
      "\u001b[32m[2020-07-12 16:08:29] __main__ INFO: \u001b[0mVal 24\n",
      "\u001b[32m[2020-07-12 16:08:30] __main__ INFO: \u001b[0mEpoch 24 loss 0.3758 acc@1 0.9060 acc@5 0.9948\n",
      "\u001b[32m[2020-07-12 16:08:30] __main__ INFO: \u001b[0mElapsed 1.03\n",
      "\u001b[32m[2020-07-12 16:08:30] __main__ INFO: \u001b[0mTrain 25 8424\n",
      "\u001b[32m[2020-07-12 16:08:39] __main__ INFO: \u001b[0mEpoch 25 Step 100/351 lr 0.001000 loss 0.0202 (0.0557) acc@1 0.9922 (0.9818) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-12 16:08:48] __main__ INFO: \u001b[0mEpoch 25 Step 200/351 lr 0.001000 loss 0.1061 (0.0561) acc@1 0.9609 (0.9822) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-12 16:08:57] __main__ INFO: \u001b[0mEpoch 25 Step 300/351 lr 0.001000 loss 0.0616 (0.0572) acc@1 0.9766 (0.9818) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-12 16:09:01] __main__ INFO: \u001b[0mEpoch 25 Step 351/351 lr 0.001000 loss 0.0583 (0.0569) acc@1 0.9844 (0.9819) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-12 16:09:01] __main__ INFO: \u001b[0mElapsed 31.74\n",
      "\u001b[32m[2020-07-12 16:09:01] __main__ INFO: \u001b[0mVal 25\n",
      "\u001b[32m[2020-07-12 16:09:02] __main__ INFO: \u001b[0mEpoch 25 loss 0.3735 acc@1 0.9036 acc@5 0.9956\n",
      "\u001b[32m[2020-07-12 16:09:02] __main__ INFO: \u001b[0mElapsed 1.05\n",
      "\u001b[32m[2020-07-12 16:09:02] __main__ INFO: \u001b[0mTrain 26 8775\n",
      "\u001b[32m[2020-07-12 16:09:12] __main__ INFO: \u001b[0mEpoch 26 Step 100/351 lr 0.001000 loss 0.0127 (0.0554) acc@1 1.0000 (0.9817) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-12 16:09:20] __main__ INFO: \u001b[0mEpoch 26 Step 200/351 lr 0.001000 loss 0.0977 (0.0536) acc@1 0.9609 (0.9828) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-12 16:09:29] __main__ INFO: \u001b[0mEpoch 26 Step 300/351 lr 0.001000 loss 0.1180 (0.0544) acc@1 0.9766 (0.9823) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-12 16:09:34] __main__ INFO: \u001b[0mEpoch 26 Step 351/351 lr 0.001000 loss 0.0287 (0.0544) acc@1 0.9922 (0.9824) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-12 16:09:34] __main__ INFO: \u001b[0mElapsed 31.69\n",
      "\u001b[32m[2020-07-12 16:09:34] __main__ INFO: \u001b[0mVal 26\n",
      "\u001b[32m[2020-07-12 16:09:35] __main__ INFO: \u001b[0mEpoch 26 loss 0.3802 acc@1 0.9038 acc@5 0.9950\n",
      "\u001b[32m[2020-07-12 16:09:35] __main__ INFO: \u001b[0mElapsed 1.05\n",
      "\u001b[32m[2020-07-12 16:09:35] __main__ INFO: \u001b[0mTrain 27 9126\n",
      "\u001b[32m[2020-07-12 16:09:44] __main__ INFO: \u001b[0mEpoch 27 Step 100/351 lr 0.001000 loss 0.0436 (0.0536) acc@1 0.9922 (0.9837) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-12 16:09:53] __main__ INFO: \u001b[0mEpoch 27 Step 200/351 lr 0.001000 loss 0.0577 (0.0579) acc@1 0.9766 (0.9821) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-12 16:10:02] __main__ INFO: \u001b[0mEpoch 27 Step 300/351 lr 0.001000 loss 0.0188 (0.0564) acc@1 0.9922 (0.9823) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-12 16:10:07] __main__ INFO: \u001b[0mEpoch 27 Step 351/351 lr 0.001000 loss 0.0430 (0.0568) acc@1 0.9844 (0.9820) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-12 16:10:07] __main__ INFO: \u001b[0mElapsed 31.78\n",
      "\u001b[32m[2020-07-12 16:10:07] __main__ INFO: \u001b[0mVal 27\n",
      "\u001b[32m[2020-07-12 16:10:08] __main__ INFO: \u001b[0mEpoch 27 loss 0.3726 acc@1 0.9058 acc@5 0.9950\n",
      "\u001b[32m[2020-07-12 16:10:08] __main__ INFO: \u001b[0mElapsed 1.05\n",
      "\u001b[32m[2020-07-12 16:10:08] __main__ INFO: \u001b[0mTrain 28 9477\n",
      "\u001b[32m[2020-07-12 16:10:17] __main__ INFO: \u001b[0mEpoch 28 Step 100/351 lr 0.001000 loss 0.0474 (0.0491) acc@1 0.9766 (0.9845) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-12 16:10:26] __main__ INFO: \u001b[0mEpoch 28 Step 200/351 lr 0.001000 loss 0.0268 (0.0523) acc@1 1.0000 (0.9836) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-12 16:10:35] __main__ INFO: \u001b[0mEpoch 28 Step 300/351 lr 0.001000 loss 0.0090 (0.0528) acc@1 1.0000 (0.9838) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-12 16:10:40] __main__ INFO: \u001b[0mEpoch 28 Step 351/351 lr 0.001000 loss 0.0573 (0.0540) acc@1 0.9766 (0.9833) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-12 16:10:40] __main__ INFO: \u001b[0mElapsed 31.79\n",
      "\u001b[32m[2020-07-12 16:10:40] __main__ INFO: \u001b[0mVal 28\n",
      "\u001b[32m[2020-07-12 16:10:41] __main__ INFO: \u001b[0mEpoch 28 loss 0.3772 acc@1 0.9032 acc@5 0.9956\n",
      "\u001b[32m[2020-07-12 16:10:41] __main__ INFO: \u001b[0mElapsed 1.06\n",
      "\u001b[32m[2020-07-12 16:10:41] __main__ INFO: \u001b[0mTrain 29 9828\n",
      "\u001b[32m[2020-07-12 16:10:50] __main__ INFO: \u001b[0mEpoch 29 Step 100/351 lr 0.001000 loss 0.0171 (0.0550) acc@1 1.0000 (0.9821) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-12 16:10:59] __main__ INFO: \u001b[0mEpoch 29 Step 200/351 lr 0.001000 loss 0.0188 (0.0527) acc@1 1.0000 (0.9832) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-12 16:11:08] __main__ INFO: \u001b[0mEpoch 29 Step 300/351 lr 0.001000 loss 0.0173 (0.0528) acc@1 1.0000 (0.9830) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-12 16:11:13] __main__ INFO: \u001b[0mEpoch 29 Step 351/351 lr 0.001000 loss 0.1048 (0.0534) acc@1 0.9688 (0.9829) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-12 16:11:13] __main__ INFO: \u001b[0mElapsed 31.86\n",
      "\u001b[32m[2020-07-12 16:11:13] __main__ INFO: \u001b[0mVal 29\n",
      "\u001b[32m[2020-07-12 16:11:14] __main__ INFO: \u001b[0mEpoch 29 loss 0.3805 acc@1 0.9040 acc@5 0.9946\n",
      "\u001b[32m[2020-07-12 16:11:14] __main__ INFO: \u001b[0mElapsed 1.07\n",
      "\u001b[32m[2020-07-12 16:11:14] __main__ INFO: \u001b[0mTrain 30 10179\n",
      "\u001b[32m[2020-07-12 16:11:23] __main__ INFO: \u001b[0mEpoch 30 Step 100/351 lr 0.001000 loss 0.0538 (0.0526) acc@1 0.9844 (0.9827) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-12 16:11:32] __main__ INFO: \u001b[0mEpoch 30 Step 200/351 lr 0.001000 loss 0.0909 (0.0537) acc@1 0.9766 (0.9831) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-12 16:11:41] __main__ INFO: \u001b[0mEpoch 30 Step 300/351 lr 0.001000 loss 0.0160 (0.0523) acc@1 1.0000 (0.9836) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-12 16:11:46] __main__ INFO: \u001b[0mEpoch 30 Step 351/351 lr 0.001000 loss 0.0472 (0.0525) acc@1 0.9922 (0.9836) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-12 16:11:46] __main__ INFO: \u001b[0mElapsed 31.90\n",
      "\u001b[32m[2020-07-12 16:11:46] __main__ INFO: \u001b[0mVal 30\n",
      "\u001b[32m[2020-07-12 16:11:47] __main__ INFO: \u001b[0mEpoch 30 loss 0.3809 acc@1 0.9078 acc@5 0.9954\n",
      "\u001b[32m[2020-07-12 16:11:47] __main__ INFO: \u001b[0mElapsed 1.05\n",
      "\u001b[32m[2020-07-12 16:11:47] __main__ INFO: \u001b[0mTrain 31 10530\n",
      "\u001b[32m[2020-07-12 16:11:56] __main__ INFO: \u001b[0mEpoch 31 Step 100/351 lr 0.001000 loss 0.0363 (0.0530) acc@1 0.9922 (0.9835) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-12 16:12:05] __main__ INFO: \u001b[0mEpoch 31 Step 200/351 lr 0.001000 loss 0.0892 (0.0516) acc@1 0.9609 (0.9834) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-12 16:12:14] __main__ INFO: \u001b[0mEpoch 31 Step 300/351 lr 0.001000 loss 0.0538 (0.0496) acc@1 0.9766 (0.9836) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-12 16:12:18] __main__ INFO: \u001b[0mEpoch 31 Step 351/351 lr 0.001000 loss 0.0545 (0.0496) acc@1 0.9766 (0.9836) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-12 16:12:19] __main__ INFO: \u001b[0mElapsed 31.79\n",
      "\u001b[32m[2020-07-12 16:12:19] __main__ INFO: \u001b[0mVal 31\n",
      "\u001b[32m[2020-07-12 16:12:20] __main__ INFO: \u001b[0mEpoch 31 loss 0.3854 acc@1 0.9068 acc@5 0.9952\n",
      "\u001b[32m[2020-07-12 16:12:20] __main__ INFO: \u001b[0mElapsed 1.04\n",
      "\u001b[32m[2020-07-12 16:12:20] __main__ INFO: \u001b[0mTrain 32 10881\n",
      "\u001b[32m[2020-07-12 16:12:29] __main__ INFO: \u001b[0mEpoch 32 Step 100/351 lr 0.001000 loss 0.0146 (0.0500) acc@1 1.0000 (0.9828) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-12 16:12:38] __main__ INFO: \u001b[0mEpoch 32 Step 200/351 lr 0.001000 loss 0.0781 (0.0525) acc@1 0.9766 (0.9829) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-12 16:12:47] __main__ INFO: \u001b[0mEpoch 32 Step 300/351 lr 0.001000 loss 0.0780 (0.0536) acc@1 0.9844 (0.9828) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-12 16:12:51] __main__ INFO: \u001b[0mEpoch 32 Step 351/351 lr 0.001000 loss 0.0662 (0.0528) acc@1 0.9609 (0.9830) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-12 16:12:51] __main__ INFO: \u001b[0mElapsed 31.82\n",
      "\u001b[32m[2020-07-12 16:12:51] __main__ INFO: \u001b[0mVal 32\n",
      "\u001b[32m[2020-07-12 16:12:52] __main__ INFO: \u001b[0mEpoch 32 loss 0.3854 acc@1 0.9040 acc@5 0.9952\n",
      "\u001b[32m[2020-07-12 16:12:52] __main__ INFO: \u001b[0mElapsed 1.02\n",
      "\u001b[32m[2020-07-12 16:12:52] __main__ INFO: \u001b[0mTrain 33 11232\n",
      "\u001b[32m[2020-07-12 16:13:01] __main__ INFO: \u001b[0mEpoch 33 Step 100/351 lr 0.001000 loss 0.0740 (0.0531) acc@1 0.9766 (0.9834) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-12 16:13:10] __main__ INFO: \u001b[0mEpoch 33 Step 200/351 lr 0.001000 loss 0.0485 (0.0525) acc@1 0.9844 (0.9839) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-12 16:13:19] __main__ INFO: \u001b[0mEpoch 33 Step 300/351 lr 0.001000 loss 0.0176 (0.0509) acc@1 0.9922 (0.9840) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-12 16:13:24] __main__ INFO: \u001b[0mEpoch 33 Step 351/351 lr 0.001000 loss 0.0549 (0.0506) acc@1 0.9844 (0.9838) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-12 16:13:24] __main__ INFO: \u001b[0mElapsed 31.67\n",
      "\u001b[32m[2020-07-12 16:13:24] __main__ INFO: \u001b[0mVal 33\n",
      "\u001b[32m[2020-07-12 16:13:25] __main__ INFO: \u001b[0mEpoch 33 loss 0.3865 acc@1 0.9070 acc@5 0.9950\n",
      "\u001b[32m[2020-07-12 16:13:25] __main__ INFO: \u001b[0mElapsed 1.03\n",
      "\u001b[32m[2020-07-12 16:13:25] __main__ INFO: \u001b[0mTrain 34 11583\n",
      "\u001b[32m[2020-07-12 16:13:34] __main__ INFO: \u001b[0mEpoch 34 Step 100/351 lr 0.001000 loss 0.0164 (0.0495) acc@1 1.0000 (0.9846) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-12 16:13:43] __main__ INFO: \u001b[0mEpoch 34 Step 200/351 lr 0.001000 loss 0.0749 (0.0490) acc@1 0.9766 (0.9846) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-12 16:13:52] __main__ INFO: \u001b[0mEpoch 34 Step 300/351 lr 0.001000 loss 0.0082 (0.0494) acc@1 1.0000 (0.9844) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-12 16:13:57] __main__ INFO: \u001b[0mEpoch 34 Step 351/351 lr 0.001000 loss 0.0571 (0.0497) acc@1 0.9688 (0.9842) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-12 16:13:57] __main__ INFO: \u001b[0mElapsed 31.68\n",
      "\u001b[32m[2020-07-12 16:13:57] __main__ INFO: \u001b[0mVal 34\n",
      "\u001b[32m[2020-07-12 16:13:58] __main__ INFO: \u001b[0mEpoch 34 loss 0.3859 acc@1 0.9066 acc@5 0.9946\n",
      "\u001b[32m[2020-07-12 16:13:58] __main__ INFO: \u001b[0mElapsed 1.03\n",
      "\u001b[32m[2020-07-12 16:13:58] __main__ INFO: \u001b[0mTrain 35 11934\n",
      "\u001b[32m[2020-07-12 16:14:07] __main__ INFO: \u001b[0mEpoch 35 Step 100/351 lr 0.001000 loss 0.0761 (0.0470) acc@1 0.9609 (0.9840) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-12 16:14:16] __main__ INFO: \u001b[0mEpoch 35 Step 200/351 lr 0.001000 loss 0.0562 (0.0485) acc@1 0.9766 (0.9837) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-12 16:14:25] __main__ INFO: \u001b[0mEpoch 35 Step 300/351 lr 0.001000 loss 0.0172 (0.0498) acc@1 0.9922 (0.9834) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-12 16:14:30] __main__ INFO: \u001b[0mEpoch 35 Step 351/351 lr 0.001000 loss 0.0722 (0.0501) acc@1 0.9688 (0.9832) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-12 16:14:30] __main__ INFO: \u001b[0mElapsed 31.77\n",
      "\u001b[32m[2020-07-12 16:14:30] __main__ INFO: \u001b[0mVal 35\n",
      "\u001b[32m[2020-07-12 16:14:31] __main__ INFO: \u001b[0mEpoch 35 loss 0.3824 acc@1 0.9046 acc@5 0.9948\n",
      "\u001b[32m[2020-07-12 16:14:31] __main__ INFO: \u001b[0mElapsed 1.06\n",
      "\u001b[32m[2020-07-12 16:14:31] __main__ INFO: \u001b[0mTrain 36 12285\n",
      "\u001b[32m[2020-07-12 16:14:40] __main__ INFO: \u001b[0mEpoch 36 Step 100/351 lr 0.001000 loss 0.0169 (0.0504) acc@1 1.0000 (0.9828) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-12 16:14:49] __main__ INFO: \u001b[0mEpoch 36 Step 200/351 lr 0.001000 loss 0.0366 (0.0501) acc@1 0.9844 (0.9833) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-12 16:14:58] __main__ INFO: \u001b[0mEpoch 36 Step 300/351 lr 0.001000 loss 0.0490 (0.0497) acc@1 0.9844 (0.9837) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-12 16:15:02] __main__ INFO: \u001b[0mEpoch 36 Step 351/351 lr 0.001000 loss 0.0298 (0.0488) acc@1 0.9922 (0.9841) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-12 16:15:02] __main__ INFO: \u001b[0mElapsed 31.77\n",
      "\u001b[32m[2020-07-12 16:15:02] __main__ INFO: \u001b[0mVal 36\n",
      "\u001b[32m[2020-07-12 16:15:03] __main__ INFO: \u001b[0mEpoch 36 loss 0.3948 acc@1 0.9050 acc@5 0.9956\n",
      "\u001b[32m[2020-07-12 16:15:03] __main__ INFO: \u001b[0mElapsed 1.04\n",
      "\u001b[32m[2020-07-12 16:15:03] __main__ INFO: \u001b[0mTrain 37 12636\n",
      "\u001b[32m[2020-07-12 16:15:13] __main__ INFO: \u001b[0mEpoch 37 Step 100/351 lr 0.001000 loss 0.0453 (0.0443) acc@1 0.9844 (0.9853) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-12 16:15:22] __main__ INFO: \u001b[0mEpoch 37 Step 200/351 lr 0.001000 loss 0.0981 (0.0469) acc@1 0.9609 (0.9848) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-12 16:15:31] __main__ INFO: \u001b[0mEpoch 37 Step 300/351 lr 0.001000 loss 0.0431 (0.0471) acc@1 0.9766 (0.9844) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-12 16:15:35] __main__ INFO: \u001b[0mEpoch 37 Step 351/351 lr 0.001000 loss 0.0394 (0.0479) acc@1 0.9922 (0.9842) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-12 16:15:35] __main__ INFO: \u001b[0mElapsed 31.76\n",
      "\u001b[32m[2020-07-12 16:15:35] __main__ INFO: \u001b[0mVal 37\n",
      "\u001b[32m[2020-07-12 16:15:36] __main__ INFO: \u001b[0mEpoch 37 loss 0.3899 acc@1 0.9066 acc@5 0.9940\n",
      "\u001b[32m[2020-07-12 16:15:36] __main__ INFO: \u001b[0mElapsed 1.06\n",
      "\u001b[32m[2020-07-12 16:15:36] __main__ INFO: \u001b[0mTrain 38 12987\n",
      "\u001b[32m[2020-07-12 16:15:46] __main__ INFO: \u001b[0mEpoch 38 Step 100/351 lr 0.001000 loss 0.0844 (0.0453) acc@1 0.9688 (0.9846) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-12 16:15:55] __main__ INFO: \u001b[0mEpoch 38 Step 200/351 lr 0.001000 loss 0.0427 (0.0485) acc@1 0.9922 (0.9839) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-12 16:16:04] __main__ INFO: \u001b[0mEpoch 38 Step 300/351 lr 0.001000 loss 0.0593 (0.0476) acc@1 0.9844 (0.9843) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-12 16:16:08] __main__ INFO: \u001b[0mEpoch 38 Step 351/351 lr 0.001000 loss 0.0568 (0.0477) acc@1 0.9766 (0.9842) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-12 16:16:08] __main__ INFO: \u001b[0mElapsed 31.93\n",
      "\u001b[32m[2020-07-12 16:16:08] __main__ INFO: \u001b[0mVal 38\n",
      "\u001b[32m[2020-07-12 16:16:09] __main__ INFO: \u001b[0mEpoch 38 loss 0.3855 acc@1 0.9050 acc@5 0.9950\n",
      "\u001b[32m[2020-07-12 16:16:09] __main__ INFO: \u001b[0mElapsed 1.05\n",
      "\u001b[32m[2020-07-12 16:16:09] __main__ INFO: \u001b[0mTrain 39 13338\n",
      "\u001b[32m[2020-07-12 16:16:18] __main__ INFO: \u001b[0mEpoch 39 Step 100/351 lr 0.001000 loss 0.0375 (0.0474) acc@1 0.9922 (0.9832) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-12 16:16:28] __main__ INFO: \u001b[0mEpoch 39 Step 200/351 lr 0.001000 loss 0.0814 (0.0485) acc@1 0.9766 (0.9834) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-12 16:16:37] __main__ INFO: \u001b[0mEpoch 39 Step 300/351 lr 0.001000 loss 0.0409 (0.0473) acc@1 0.9766 (0.9839) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-12 16:16:41] __main__ INFO: \u001b[0mEpoch 39 Step 351/351 lr 0.001000 loss 0.0595 (0.0476) acc@1 0.9688 (0.9838) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-12 16:16:41] __main__ INFO: \u001b[0mElapsed 31.90\n",
      "\u001b[32m[2020-07-12 16:16:41] __main__ INFO: \u001b[0mVal 39\n",
      "\u001b[32m[2020-07-12 16:16:42] __main__ INFO: \u001b[0mEpoch 39 loss 0.3893 acc@1 0.9052 acc@5 0.9946\n",
      "\u001b[32m[2020-07-12 16:16:42] __main__ INFO: \u001b[0mElapsed 1.04\n",
      "\u001b[32m[2020-07-12 16:16:42] __main__ INFO: \u001b[0mTrain 40 13689\n",
      "\u001b[32m[2020-07-12 16:16:51] __main__ INFO: \u001b[0mEpoch 40 Step 100/351 lr 0.001000 loss 0.0210 (0.0446) acc@1 0.9922 (0.9859) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-12 16:17:00] __main__ INFO: \u001b[0mEpoch 40 Step 200/351 lr 0.001000 loss 0.0315 (0.0451) acc@1 0.9844 (0.9853) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-12 16:17:09] __main__ INFO: \u001b[0mEpoch 40 Step 300/351 lr 0.001000 loss 0.0278 (0.0450) acc@1 0.9922 (0.9855) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-12 16:17:14] __main__ INFO: \u001b[0mEpoch 40 Step 351/351 lr 0.001000 loss 0.1929 (0.0458) acc@1 0.9453 (0.9852) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-12 16:17:14] __main__ INFO: \u001b[0mElapsed 31.76\n",
      "\u001b[32m[2020-07-12 16:17:14] __main__ INFO: \u001b[0mVal 40\n",
      "\u001b[32m[2020-07-12 16:17:15] __main__ INFO: \u001b[0mEpoch 40 loss 0.3909 acc@1 0.9070 acc@5 0.9948\n",
      "\u001b[32m[2020-07-12 16:17:15] __main__ INFO: \u001b[0mElapsed 1.05\n",
      "\u001b[32m[2020-07-12 16:17:15] __main__ INFO: \u001b[0mTrain 41 14040\n",
      "\u001b[32m[2020-07-12 16:17:24] __main__ INFO: \u001b[0mEpoch 41 Step 100/351 lr 0.001000 loss 0.0539 (0.0471) acc@1 0.9844 (0.9853) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-12 16:17:33] __main__ INFO: \u001b[0mEpoch 41 Step 200/351 lr 0.001000 loss 0.0457 (0.0447) acc@1 0.9922 (0.9862) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-12 16:17:42] __main__ INFO: \u001b[0mEpoch 41 Step 300/351 lr 0.001000 loss 0.0629 (0.0451) acc@1 0.9844 (0.9861) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-12 16:17:47] __main__ INFO: \u001b[0mEpoch 41 Step 351/351 lr 0.001000 loss 0.0256 (0.0465) acc@1 0.9922 (0.9856) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-12 16:17:47] __main__ INFO: \u001b[0mElapsed 31.68\n",
      "\u001b[32m[2020-07-12 16:17:47] __main__ INFO: \u001b[0mVal 41\n",
      "\u001b[32m[2020-07-12 16:17:48] __main__ INFO: \u001b[0mEpoch 41 loss 0.3878 acc@1 0.9048 acc@5 0.9948\n",
      "\u001b[32m[2020-07-12 16:17:48] __main__ INFO: \u001b[0mElapsed 1.03\n",
      "\u001b[32m[2020-07-12 16:17:48] __main__ INFO: \u001b[0mTrain 42 14391\n",
      "\u001b[32m[2020-07-12 16:17:57] __main__ INFO: \u001b[0mEpoch 42 Step 100/351 lr 0.001000 loss 0.0123 (0.0441) acc@1 1.0000 (0.9861) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-12 16:18:06] __main__ INFO: \u001b[0mEpoch 42 Step 200/351 lr 0.001000 loss 0.0762 (0.0427) acc@1 0.9922 (0.9869) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-12 16:18:15] __main__ INFO: \u001b[0mEpoch 42 Step 300/351 lr 0.001000 loss 0.0370 (0.0437) acc@1 0.9922 (0.9865) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-12 16:18:19] __main__ INFO: \u001b[0mEpoch 42 Step 351/351 lr 0.001000 loss 0.0578 (0.0443) acc@1 0.9844 (0.9863) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-12 16:18:19] __main__ INFO: \u001b[0mElapsed 31.70\n",
      "\u001b[32m[2020-07-12 16:18:19] __main__ INFO: \u001b[0mVal 42\n",
      "\u001b[32m[2020-07-12 16:18:21] __main__ INFO: \u001b[0mEpoch 42 loss 0.3957 acc@1 0.9080 acc@5 0.9940\n",
      "\u001b[32m[2020-07-12 16:18:21] __main__ INFO: \u001b[0mElapsed 1.06\n",
      "\u001b[32m[2020-07-12 16:18:21] __main__ INFO: \u001b[0mTrain 43 14742\n",
      "\u001b[32m[2020-07-12 16:18:30] __main__ INFO: \u001b[0mEpoch 43 Step 100/351 lr 0.001000 loss 0.0311 (0.0414) acc@1 0.9922 (0.9869) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-12 16:18:39] __main__ INFO: \u001b[0mEpoch 43 Step 200/351 lr 0.001000 loss 0.0232 (0.0411) acc@1 0.9922 (0.9870) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-12 16:18:48] __main__ INFO: \u001b[0mEpoch 43 Step 300/351 lr 0.001000 loss 0.0524 (0.0431) acc@1 0.9766 (0.9865) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-12 16:18:52] __main__ INFO: \u001b[0mEpoch 43 Step 351/351 lr 0.001000 loss 0.0174 (0.0431) acc@1 1.0000 (0.9865) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-12 16:18:52] __main__ INFO: \u001b[0mElapsed 31.76\n",
      "\u001b[32m[2020-07-12 16:18:52] __main__ INFO: \u001b[0mVal 43\n",
      "\u001b[32m[2020-07-12 16:18:53] __main__ INFO: \u001b[0mEpoch 43 loss 0.3981 acc@1 0.9028 acc@5 0.9944\n",
      "\u001b[32m[2020-07-12 16:18:53] __main__ INFO: \u001b[0mElapsed 1.07\n",
      "\u001b[32m[2020-07-12 16:18:53] __main__ INFO: \u001b[0mTrain 44 15093\n",
      "\u001b[32m[2020-07-12 16:19:02] __main__ INFO: \u001b[0mEpoch 44 Step 100/351 lr 0.001000 loss 0.0385 (0.0387) acc@1 0.9844 (0.9888) acc@5 1.0000 (0.9998)\n",
      "\u001b[32m[2020-07-12 16:19:11] __main__ INFO: \u001b[0mEpoch 44 Step 200/351 lr 0.001000 loss 0.0187 (0.0413) acc@1 0.9922 (0.9871) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-12 16:19:20] __main__ INFO: \u001b[0mEpoch 44 Step 300/351 lr 0.001000 loss 0.0171 (0.0409) acc@1 1.0000 (0.9871) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-12 16:19:25] __main__ INFO: \u001b[0mEpoch 44 Step 351/351 lr 0.001000 loss 0.0115 (0.0415) acc@1 0.9922 (0.9867) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-12 16:19:25] __main__ INFO: \u001b[0mElapsed 31.69\n",
      "\u001b[32m[2020-07-12 16:19:25] __main__ INFO: \u001b[0mVal 44\n",
      "\u001b[32m[2020-07-12 16:19:26] __main__ INFO: \u001b[0mEpoch 44 loss 0.3983 acc@1 0.9034 acc@5 0.9946\n",
      "\u001b[32m[2020-07-12 16:19:26] __main__ INFO: \u001b[0mElapsed 1.03\n",
      "\u001b[32m[2020-07-12 16:19:26] __main__ INFO: \u001b[0mTrain 45 15444\n",
      "\u001b[32m[2020-07-12 16:19:35] __main__ INFO: \u001b[0mEpoch 45 Step 100/351 lr 0.001000 loss 0.0729 (0.0424) acc@1 0.9688 (0.9858) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-12 16:19:44] __main__ INFO: \u001b[0mEpoch 45 Step 200/351 lr 0.001000 loss 0.0131 (0.0439) acc@1 1.0000 (0.9857) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-12 16:19:53] __main__ INFO: \u001b[0mEpoch 45 Step 300/351 lr 0.001000 loss 0.0294 (0.0430) acc@1 0.9922 (0.9858) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-12 16:19:58] __main__ INFO: \u001b[0mEpoch 45 Step 351/351 lr 0.001000 loss 0.0464 (0.0434) acc@1 0.9766 (0.9857) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-12 16:19:58] __main__ INFO: \u001b[0mElapsed 31.73\n",
      "\u001b[32m[2020-07-12 16:19:58] __main__ INFO: \u001b[0mVal 45\n",
      "\u001b[32m[2020-07-12 16:19:59] __main__ INFO: \u001b[0mEpoch 45 loss 0.4080 acc@1 0.9042 acc@5 0.9944\n",
      "\u001b[32m[2020-07-12 16:19:59] __main__ INFO: \u001b[0mElapsed 1.03\n",
      "\u001b[32m[2020-07-12 16:19:59] __main__ INFO: \u001b[0mTrain 46 15795\n",
      "\u001b[32m[2020-07-12 16:20:08] __main__ INFO: \u001b[0mEpoch 46 Step 100/351 lr 0.001000 loss 0.0119 (0.0378) acc@1 1.0000 (0.9869) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-12 16:20:17] __main__ INFO: \u001b[0mEpoch 46 Step 200/351 lr 0.001000 loss 0.0261 (0.0391) acc@1 0.9922 (0.9870) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-12 16:20:26] __main__ INFO: \u001b[0mEpoch 46 Step 300/351 lr 0.001000 loss 0.0284 (0.0404) acc@1 0.9922 (0.9869) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-12 16:20:31] __main__ INFO: \u001b[0mEpoch 46 Step 351/351 lr 0.001000 loss 0.0144 (0.0405) acc@1 1.0000 (0.9869) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-12 16:20:31] __main__ INFO: \u001b[0mElapsed 31.69\n",
      "\u001b[32m[2020-07-12 16:20:31] __main__ INFO: \u001b[0mVal 46\n",
      "\u001b[32m[2020-07-12 16:20:32] __main__ INFO: \u001b[0mEpoch 46 loss 0.3990 acc@1 0.9086 acc@5 0.9950\n",
      "\u001b[32m[2020-07-12 16:20:32] __main__ INFO: \u001b[0mElapsed 1.04\n",
      "\u001b[32m[2020-07-12 16:20:32] __main__ INFO: \u001b[0mTrain 47 16146\n",
      "\u001b[32m[2020-07-12 16:20:41] __main__ INFO: \u001b[0mEpoch 47 Step 100/351 lr 0.001000 loss 0.0237 (0.0376) acc@1 0.9922 (0.9883) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-12 16:20:50] __main__ INFO: \u001b[0mEpoch 47 Step 200/351 lr 0.001000 loss 0.0317 (0.0385) acc@1 0.9844 (0.9882) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-12 16:20:59] __main__ INFO: \u001b[0mEpoch 47 Step 300/351 lr 0.001000 loss 0.0751 (0.0394) acc@1 0.9766 (0.9877) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-12 16:21:03] __main__ INFO: \u001b[0mEpoch 47 Step 351/351 lr 0.001000 loss 0.0817 (0.0393) acc@1 0.9688 (0.9877) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-12 16:21:03] __main__ INFO: \u001b[0mElapsed 31.91\n",
      "\u001b[32m[2020-07-12 16:21:03] __main__ INFO: \u001b[0mVal 47\n",
      "\u001b[32m[2020-07-12 16:21:05] __main__ INFO: \u001b[0mEpoch 47 loss 0.4066 acc@1 0.9032 acc@5 0.9946\n",
      "\u001b[32m[2020-07-12 16:21:05] __main__ INFO: \u001b[0mElapsed 1.03\n",
      "\u001b[32m[2020-07-12 16:21:05] __main__ INFO: \u001b[0mTrain 48 16497\n",
      "\u001b[32m[2020-07-12 16:21:14] __main__ INFO: \u001b[0mEpoch 48 Step 100/351 lr 0.001000 loss 0.0659 (0.0392) acc@1 0.9766 (0.9861) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-12 16:21:23] __main__ INFO: \u001b[0mEpoch 48 Step 200/351 lr 0.001000 loss 0.0801 (0.0399) acc@1 0.9844 (0.9865) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-12 16:21:32] __main__ INFO: \u001b[0mEpoch 48 Step 300/351 lr 0.001000 loss 0.0169 (0.0397) acc@1 0.9922 (0.9868) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-12 16:21:36] __main__ INFO: \u001b[0mEpoch 48 Step 351/351 lr 0.001000 loss 0.0749 (0.0396) acc@1 0.9688 (0.9869) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-12 16:21:36] __main__ INFO: \u001b[0mElapsed 31.91\n",
      "\u001b[32m[2020-07-12 16:21:36] __main__ INFO: \u001b[0mVal 48\n",
      "\u001b[32m[2020-07-12 16:21:38] __main__ INFO: \u001b[0mEpoch 48 loss 0.4034 acc@1 0.9076 acc@5 0.9952\n",
      "\u001b[32m[2020-07-12 16:21:38] __main__ INFO: \u001b[0mElapsed 1.08\n",
      "\u001b[32m[2020-07-12 16:21:38] __main__ INFO: \u001b[0mTrain 49 16848\n",
      "\u001b[32m[2020-07-12 16:21:47] __main__ INFO: \u001b[0mEpoch 49 Step 100/351 lr 0.001000 loss 0.0124 (0.0389) acc@1 1.0000 (0.9877) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-12 16:21:56] __main__ INFO: \u001b[0mEpoch 49 Step 200/351 lr 0.001000 loss 0.0752 (0.0380) acc@1 0.9844 (0.9876) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-12 16:22:05] __main__ INFO: \u001b[0mEpoch 49 Step 300/351 lr 0.001000 loss 0.0210 (0.0393) acc@1 0.9922 (0.9873) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-12 16:22:09] __main__ INFO: \u001b[0mEpoch 49 Step 351/351 lr 0.001000 loss 0.0416 (0.0391) acc@1 0.9922 (0.9873) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-12 16:22:09] __main__ INFO: \u001b[0mElapsed 31.86\n",
      "\u001b[32m[2020-07-12 16:22:09] __main__ INFO: \u001b[0mVal 49\n",
      "\u001b[32m[2020-07-12 16:22:10] __main__ INFO: \u001b[0mEpoch 49 loss 0.4042 acc@1 0.9044 acc@5 0.9948\n",
      "\u001b[32m[2020-07-12 16:22:10] __main__ INFO: \u001b[0mElapsed 1.05\n",
      "\u001b[32m[2020-07-12 16:22:10] __main__ INFO: \u001b[0mTrain 50 17199\n",
      "\u001b[32m[2020-07-12 16:22:20] __main__ INFO: \u001b[0mEpoch 50 Step 100/351 lr 0.001000 loss 0.0505 (0.0412) acc@1 0.9766 (0.9860) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-12 16:22:29] __main__ INFO: \u001b[0mEpoch 50 Step 200/351 lr 0.001000 loss 0.0656 (0.0424) acc@1 0.9844 (0.9863) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-12 16:22:38] __main__ INFO: \u001b[0mEpoch 50 Step 300/351 lr 0.001000 loss 0.0146 (0.0419) acc@1 1.0000 (0.9866) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-12 16:22:42] __main__ INFO: \u001b[0mEpoch 50 Step 351/351 lr 0.001000 loss 0.0106 (0.0408) acc@1 1.0000 (0.9869) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-12 16:22:42] __main__ INFO: \u001b[0mElapsed 31.71\n",
      "\u001b[32m[2020-07-12 16:22:42] __main__ INFO: \u001b[0mVal 50\n",
      "\u001b[32m[2020-07-12 16:22:43] __main__ INFO: \u001b[0mEpoch 50 loss 0.4015 acc@1 0.9078 acc@5 0.9948\n",
      "\u001b[32m[2020-07-12 16:22:43] __main__ INFO: \u001b[0mElapsed 1.02\n",
      "\u001b[32m[2020-07-12 16:22:43] fvcore.common.checkpoint INFO: \u001b[0mSaving checkpoint to /home/ec2-user/SageMaker/experiments/resnet_basic_32_cm_1_.5/exp00_resume400_50/checkpoint_00050.pth\n"
     ]
    }
   ],
   "source": [
    "# Resume training with the un-augmented data\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "!python train.py --config /home/ec2-user/SageMaker/experiments/resnet_basic_32_cm_1_.5/exp00/config.yaml \\\n",
    "    train.checkpoint /home/ec2-user/SageMaker/experiments/resnet_basic_32_cm_1_.5/exp00/checkpoint_00400.pth \\\n",
    "    dataset.name CIFAR10 \\\n",
    "    train.base_lr .001 \\\n",
    "    train.output_dir /home/ec2-user/SageMaker/experiments/resnet_basic_32_cm_1_.5/exp00_resume400_50 \\\n",
    "    scheduler.epochs 50\n",
    "\n",
    "#### Set LEARNING RATE based on ending LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-12 16:23:14] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/resnet_basic_32_cm_1_.5/exp00_resume400_50/checkpoint_00050.pth\n",
      "Files already downloaded and verified\n",
      "100%|| 79/79 [00:02<00:00, 27.13it/s]\n",
      "\u001b[32m[2020-07-12 16:23:17] __main__ INFO: \u001b[0mElapsed 2.91\n",
      "\u001b[32m[2020-07-12 16:23:17] __main__ INFO: \u001b[0mLoss 0.4136 Accuracy 0.9054\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/resnet.yaml \\\n",
    "   model.resnet.depth 32 \\\n",
    "   test.batch_size 128 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/resnet_basic_32_cm_1_.5/exp00_resume400_50/checkpoint_00050.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/resnet_basic_32_cm_1_.5/exp00_resume400_50/test_results_0050_cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-12 16:24:03] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/resnet_basic_32_cm_1_.5/exp00_resume400_50/checkpoint_00050.pth\n",
      "CIFAR 10.1\n",
      "100%|| 16/16 [00:00<00:00, 18.03it/s]\n",
      "\u001b[32m[2020-07-12 16:24:04] __main__ INFO: \u001b[0mElapsed 0.89\n",
      "\u001b[32m[2020-07-12 16:24:04] __main__ INFO: \u001b[0mLoss 0.8687 Accuracy 0.8255\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10.1 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/resnet.yaml \\\n",
    "   model.resnet.depth 32 \\\n",
    "   test.batch_size 128 \\\n",
    "   dataset.name CIFAR101 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/resnet_basic_32_cm_1_.5/exp00_resume400_50/checkpoint_00050.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/resnet_basic_32_cm_1_.5/exp00_resume400_50/test_results_0050_cifar101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-12 16:24:38] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/resnet_basic_32_cm_1_.5/exp00/checkpoint_00400.pth\n",
      "Files already downloaded and verified\n",
      "100%|| 79/79 [00:02<00:00, 26.91it/s]\n",
      "\u001b[32m[2020-07-12 16:24:41] __main__ INFO: \u001b[0mElapsed 2.94\n",
      "\u001b[32m[2020-07-12 16:24:41] __main__ INFO: \u001b[0mLoss 2.2737 Accuracy 0.4043\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/resnet.yaml \\\n",
    "   model.resnet.depth 32 \\\n",
    "   test.batch_size 128 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/resnet_basic_32_cm_1_.5/exp00/checkpoint_00400.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/resnet_basic_32_cm_1_.5/exp00/test_results_0400_cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-12 16:24:59] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/resnet_basic_32_cm_1_.5/exp00/checkpoint_00400.pth\n",
      "CIFAR 10.1\n",
      "100%|| 16/16 [00:00<00:00, 20.97it/s]\n",
      "\u001b[32m[2020-07-12 16:25:00] __main__ INFO: \u001b[0mElapsed 0.76\n",
      "\u001b[32m[2020-07-12 16:25:00] __main__ INFO: \u001b[0mLoss 2.4917 Accuracy 0.3520\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10.1 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "!python evaluate.py --config configs/cifar/resnet.yaml \\\n",
    "   model.resnet.depth 32 \\\n",
    "   test.batch_size 128 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/resnet_basic_32_cm_1_.5/exp00/checkpoint_00400.pth \\\n",
    "   dataset.name CIFAR101 \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/resnet_basic_32_cm_1_.5/exp00/test_results_0400_cifar101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Record the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Testset</th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Original_Accuracy</th>\n",
       "      <th>Original_CI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>resnet_basic_32</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>100</td>\n",
       "      <td>0.3604</td>\n",
       "      <td>0.9170</td>\n",
       "      <td>92.5</td>\n",
       "      <td>(92.0, 93.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>resnet_basic_32</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>160</td>\n",
       "      <td>0.4011</td>\n",
       "      <td>0.9232</td>\n",
       "      <td>92.5</td>\n",
       "      <td>(92.0, 93.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>resnet_basic_32</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>160</td>\n",
       "      <td>0.8051</td>\n",
       "      <td>0.8320</td>\n",
       "      <td>84.9</td>\n",
       "      <td>(83.2, 86.4)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Model    Testset  Epoch    Loss  Accuracy  Original_Accuracy  \\\n",
       "0  resnet_basic_32    cifar10    100  0.3604    0.9170               92.5   \n",
       "1  resnet_basic_32    cifar10    160  0.4011    0.9232               92.5   \n",
       "2  resnet_basic_32  cifar10.1    160  0.8051    0.8320               84.9   \n",
       "\n",
       "    Original_CI  \n",
       "0  (92.0, 93.0)  \n",
       "1  (92.0, 93.0)  \n",
       "2  (83.2, 86.4)  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write the results to a CSV file so that we can analyze later.\n",
    "import pandas as pd\n",
    "\n",
    "results = {'Model': ['resnet_basic_32', 'resnet_basic_32', 'resnet_basic_32']\n",
    "           'Testset': ['cifar10', 'cifar10', 'cifar10.1']\n",
    "           'Loss': [0.3604, 0.4011, 0.8051],\n",
    "           'Epoch': [100, 160, 160],\n",
    "           'Accuracy': [0.9170, 0.9232, 0.8320],\n",
    "           'Original_Accuracy': [92.5, 92.5, 84.9],\n",
    "           'Original_CI': [(92.0, 93.0), (92.0, 93.0), (83.2, 86.4)]\n",
    "           }\n",
    "\n",
    "df = pd.DataFrame(results, columns = ['Model', 'Testset', 'Epoch', 'Loss', 'Accuracy', \n",
    "                                      'Original_Accuracy', 'Original_CI'])\n",
    "\n",
    "\n",
    "df.to_csv('/home/ec2-user/SageMaker/experiments/resnet_basic_32/exp00/results.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Testset</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Original_Accuracy</th>\n",
       "      <th>Original_CI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>resnet_basic_32_cm_1_.5</td>\n",
       "      <td>400</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>2.2737</td>\n",
       "      <td>0.4043</td>\n",
       "      <td>92.5</td>\n",
       "      <td>(92.0, 93.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>resnet_basic_32_cm_1_.5</td>\n",
       "      <td>400</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>2.4917</td>\n",
       "      <td>0.352</td>\n",
       "      <td>84.9</td>\n",
       "      <td>(83.2, 86.4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>resnet_basic_32_cm_1_.5_refined400</td>\n",
       "      <td>50</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>0.8687</td>\n",
       "      <td>0.8255</td>\n",
       "      <td>84.9</td>\n",
       "      <td>(83.2, 86.4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>resnet_basic_32_cm_1_.5_refined400</td>\n",
       "      <td>50</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.4136</td>\n",
       "      <td>0.9054</td>\n",
       "      <td>92.5</td>\n",
       "      <td>(92.0, 93.0)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Model Epoch    Testset    Loss Accuracy  \\\n",
       "0             resnet_basic_32_cm_1_.5   400    cifar10  2.2737   0.4043   \n",
       "1             resnet_basic_32_cm_1_.5   400  cifar10.1  2.4917    0.352   \n",
       "2  resnet_basic_32_cm_1_.5_refined400    50  cifar10.1  0.8687   0.8255   \n",
       "3  resnet_basic_32_cm_1_.5_refined400    50    cifar10  0.4136   0.9054   \n",
       "\n",
       "   Original_Accuracy   Original_CI  \n",
       "0               92.5  (92.0, 93.0)  \n",
       "1               84.9  (83.2, 86.4)  \n",
       "2               84.9  (83.2, 86.4)  \n",
       "3               92.5  (92.0, 93.0)  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "model = 'resnet_basic_32_cm_1_.5'\n",
    "model_refined = model + '_refined400'\n",
    "\n",
    "a = pd.Series([model, 400, 'cifar10', 2.2737, 0.4043])\n",
    "c = pd.Series([model, 400, 'cifar10.1', 2.4917, 0.3520 ])\n",
    "\n",
    "e = pd.Series([model_refined, 50, 'cifar10.1', 0.8687, 0.8255])\n",
    "f = pd.Series([model_refined, 50, 'cifar10', 0.4136, 0.9054])\n",
    "               \n",
    "df_results = pd.concat([a,c,e,f], axis=1).T\n",
    "df_results.columns = ['Model', 'Epoch', 'Testset', 'Loss', 'Accuracy']\n",
    "\n",
    "df_results['Original_Accuracy'] = df_results.apply((lambda row: 92.5 if row[2] == 'cifar10' else 84.9), axis=1)\n",
    "df_results['Original_CI'] = df_results.apply((lambda row: (92.0, 93.0) if row[2] == 'cifar10' else (83.2, 86.4)), axis=1)\n",
    "\n",
    "df_results.to_csv('/home/ec2-user/SageMaker/experiments/' + model + '/results.csv')\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the model checkpoints, configs, and results to S3 \n",
    "bucket='sagemaker-june29'\n",
    "prefix = 'sagemaker/results/original-models/resnet_basic_32_cm_1_.5'\n",
    "path = '/home/ec2-user/SageMaker/experiments/resnet_basic_32_cm_1_.5'\n",
    "\n",
    "s3_resource = boto3.resource(\"s3\", region_name=\"us-east-2\")\n",
    "\n",
    "def uploadDirectory(local_path,bucket_name,s3_prefix):\n",
    "\n",
    "    my_bucket = s3_resource.Bucket(bucket_name)\n",
    "    \n",
    "    for path, subdirs, files in os.walk(local_path):\n",
    "        path = path.replace(\"\\\\\",\"/\")\n",
    "        directory_name = path.replace(local_path,\"\")\n",
    "        for file in files:\n",
    "            #print(\"Local File:\", os.path.join(path, file))\n",
    "            #print(\"      Dest:\", s3_prefix+directory_name+'/'+file)\n",
    "            my_bucket.upload_file(os.path.join(path, file), s3_prefix+directory_name+'/'+file)\n",
    "    \n",
    "uploadDirectory(path,bucket,prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
