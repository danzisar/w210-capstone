{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wide Residual Net\n",
    "\n",
    " - Training Dataset:  CutMix, beta=1, cutmix_prob=1\n",
    " - Sagemaker Notebook must be of type, conda_pytorch_p36\n",
    " \n",
    "#### Install Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import numpy \n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 1)) (1.18.1)\n",
      "Requirement already satisfied: torch>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 2)) (1.4.0)\n",
      "Requirement already satisfied: torchvision in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (0.5.0)\n",
      "Requirement already satisfied: fvcore in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (0.1.1.post20200711)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 5)) (4.44.1)\n",
      "Requirement already satisfied: yacs in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 6)) (0.1.7)\n",
      "Requirement already satisfied: apex from git+https://github.com/NVIDIA/apex.git#egg=apex in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 7)) (0.1)\n",
      "Requirement already satisfied: termcolor in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 8)) (1.1.0)\n",
      "Requirement already satisfied: thop<0.0.31.post2004070130 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 9)) (0.0.31.post2001170342)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (1.14.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (7.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (5.3.1)\n",
      "Requirement already satisfied: portalocker in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (1.7.0)\n",
      "Requirement already satisfied: tabulate in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (0.8.7)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: tensorboard in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (2.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (3.2.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.7.0)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (3.12.2)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.30.0)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (0.34.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (0.9.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.18.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.18.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.14.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (46.1.3.post20200330)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (2.23.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (0.4.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard) (1.5.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (4.1.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (3.4.2)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (2020.4.5.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (3.0.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard) (2.2.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r '/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt'\n",
    "\n",
    "# Need to add this to requirements.txt\n",
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-10 23:30:35] __main__ INFO: \u001b[0mdevice: cuda\n",
      "cudnn:\n",
      "  benchmark: True\n",
      "  deterministic: False\n",
      "dataset:\n",
      "  name: CIFAR10_CM_1\n",
      "  dataset_dir: ''\n",
      "  image_size: 32\n",
      "  n_channels: 3\n",
      "  n_classes: 10\n",
      "model:\n",
      "  type: cifar\n",
      "  name: wrn\n",
      "  init_mode: kaiming_fan_in\n",
      "  vgg:\n",
      "    n_channels: [64, 128, 256, 512, 512]\n",
      "    n_layers: [2, 2, 3, 3, 3]\n",
      "    use_bn: True\n",
      "  resnet:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "  resnet_preact:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "  wrn:\n",
      "    depth: 28\n",
      "    initial_channels: 16\n",
      "    widening_factor: 10\n",
      "    drop_rate: 0.0\n",
      "  densenet:\n",
      "    depth: 100\n",
      "    n_blocks: [6, 12, 24, 16]\n",
      "    block_type: bottleneck\n",
      "    growth_rate: 12\n",
      "    drop_rate: 0.0\n",
      "    compression_rate: 0.5\n",
      "  pyramidnet:\n",
      "    depth: 272\n",
      "    n_blocks: [3, 24, 36, 3]\n",
      "    initial_channels: 16\n",
      "    block_type: bottleneck\n",
      "    alpha: 200\n",
      "  resnext:\n",
      "    depth: 29\n",
      "    n_blocks: [3, 4, 6, 3]\n",
      "    initial_channels: 64\n",
      "    cardinality: 8\n",
      "    base_channels: 4\n",
      "  shake_shake:\n",
      "    depth: 26\n",
      "    initial_channels: 96\n",
      "    shake_forward: True\n",
      "    shake_backward: True\n",
      "    shake_image: True\n",
      "  se_resnet_preact:\n",
      "    depth: 110\n",
      "    initial_channels: 16\n",
      "    se_reduction: 16\n",
      "    block_type: basic\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "train:\n",
      "  checkpoint: ''\n",
      "  resume: False\n",
      "  precision: O0\n",
      "  batch_size: 128\n",
      "  subdivision: 1\n",
      "  optimizer: sgd\n",
      "  base_lr: 0.1\n",
      "  momentum: 0.9\n",
      "  nesterov: True\n",
      "  weight_decay: 0.0005\n",
      "  no_weight_decay_on_bn: False\n",
      "  gradient_clip: 0\n",
      "  start_epoch: 0\n",
      "  seed: 0\n",
      "  val_first: True\n",
      "  val_period: 1\n",
      "  val_ratio: 0.1\n",
      "  use_test_as_val: False\n",
      "  output_dir: /home/ec2-user/SageMaker/experiments/wrn_28_10_cm_1_1/exp00\n",
      "  log_period: 100\n",
      "  checkpoint_period: 100\n",
      "  use_tensorboard: True\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: True\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "  distributed: False\n",
      "  dist:\n",
      "    backend: nccl\n",
      "    init_method: env://\n",
      "    world_size: -1\n",
      "    node_rank: -1\n",
      "    local_rank: 0\n",
      "    use_sync_bn: False\n",
      "tensorboard:\n",
      "  train_images: False\n",
      "  val_images: False\n",
      "  model_params: False\n",
      "optim:\n",
      "  adam:\n",
      "    betas: (0.9, 0.999)\n",
      "  lars:\n",
      "    eps: 1e-09\n",
      "    threshold: 0.01\n",
      "  adabound:\n",
      "    betas: (0.9, 0.999)\n",
      "    final_lr: 0.1\n",
      "    gamma: 0.001\n",
      "scheduler:\n",
      "  epochs: 400\n",
      "  warmup:\n",
      "    type: none\n",
      "    epochs: 0\n",
      "    start_factor: 0.001\n",
      "    exponent: 4\n",
      "  type: multistep\n",
      "  milestones: [60, 120, 160]\n",
      "  lr_decay: 0.2\n",
      "  lr_min_factor: 0.001\n",
      "  T0: 10\n",
      "  T_mul: 1.0\n",
      "validation:\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: False\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "augmentation:\n",
      "  use_random_crop: True\n",
      "  use_random_horizontal_flip: True\n",
      "  use_cutout: False\n",
      "  use_random_erasing: False\n",
      "  use_dual_cutout: False\n",
      "  use_mixup: False\n",
      "  use_ricap: False\n",
      "  use_cutmix: False\n",
      "  use_label_smoothing: False\n",
      "  random_crop:\n",
      "    padding: 4\n",
      "    fill: 0\n",
      "    padding_mode: constant\n",
      "  random_horizontal_flip:\n",
      "    prob: 0.5\n",
      "  cutout:\n",
      "    prob: 1.0\n",
      "    mask_size: 16\n",
      "    cut_inside: False\n",
      "    mask_color: 0\n",
      "    dual_cutout_alpha: 0.1\n",
      "  random_erasing:\n",
      "    prob: 0.5\n",
      "    area_ratio_range: [0.02, 0.4]\n",
      "    min_aspect_ratio: 0.3\n",
      "    max_attempt: 20\n",
      "  mixup:\n",
      "    alpha: 1.0\n",
      "  ricap:\n",
      "    beta: 0.3\n",
      "  cutmix:\n",
      "    alpha: 1.0\n",
      "  label_smoothing:\n",
      "    epsilon: 0.1\n",
      "tta:\n",
      "  use_resize: False\n",
      "  use_center_crop: False\n",
      "  resize: 256\n",
      "test:\n",
      "  checkpoint: ''\n",
      "  output_dir: ''\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    pin_memory: False\n",
      "\u001b[32m[2020-07-10 23:30:35] __main__ INFO: \u001b[0menv_info:\n",
      "  pytorch_version: 1.4.0\n",
      "  cuda_version: 10.1\n",
      "  cudnn_version: 7603\n",
      "  num_gpus: 1\n",
      "  gpu_name: Tesla K80\n",
      "  gpu_capability: 3.7\n",
      "Files already downloaded and verified\n",
      "\u001b[32m[2020-07-10 23:31:00] __main__ INFO: \u001b[0mMACs  : 5.25G\n",
      "\u001b[32m[2020-07-10 23:31:00] __main__ INFO: \u001b[0m#params: 36.48M\n",
      "Selected optimization level O0:  Pure FP32 training.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\",)\n",
      "\u001b[32m[2020-07-10 23:31:00] __main__ INFO: \u001b[0mVal 0\n",
      "\u001b[32m[2020-07-10 23:31:20] __main__ INFO: \u001b[0mEpoch 0 loss 117.1037 acc@1 0.0968 acc@5 0.5502\n",
      "\u001b[32m[2020-07-10 23:31:20] __main__ INFO: \u001b[0mElapsed 20.56\n",
      "\u001b[32m[2020-07-10 23:31:20] __main__ INFO: \u001b[0mTrain 1 0\n",
      "\u001b[32m[2020-07-10 23:33:23] __main__ INFO: \u001b[0mEpoch 1 Step 100/351 lr 0.100000 loss 1.9789 (2.1742) acc@1 0.3265 (0.2242) acc@5 0.7932 (0.7149)\n",
      "\u001b[32m[2020-07-10 23:35:21] __main__ INFO: \u001b[0mEpoch 1 Step 200/351 lr 0.100000 loss 1.9484 (2.0839) acc@1 0.3111 (0.2590) acc@5 0.7829 (0.7492)\n",
      "\u001b[32m[2020-07-10 23:37:18] __main__ INFO: \u001b[0mEpoch 1 Step 300/351 lr 0.100000 loss 1.8705 (2.0240) acc@1 0.3515 (0.2850) acc@5 0.8347 (0.7695)\n",
      "\u001b[32m[2020-07-10 23:38:19] __main__ INFO: \u001b[0mEpoch 1 Step 351/351 lr 0.100000 loss 1.8985 (1.9990) acc@1 0.3266 (0.2963) acc@5 0.8032 (0.7774)\n",
      "\u001b[32m[2020-07-10 23:38:19] __main__ INFO: \u001b[0mElapsed 418.33\n",
      "\u001b[32m[2020-07-10 23:38:19] __main__ INFO: \u001b[0mVal 1\n",
      "\u001b[32m[2020-07-10 23:38:32] __main__ INFO: \u001b[0mEpoch 1 loss 1.2925 acc@1 0.5534 acc@5 0.9518\n",
      "\u001b[32m[2020-07-10 23:38:32] __main__ INFO: \u001b[0mElapsed 13.90\n",
      "\u001b[32m[2020-07-10 23:38:32] __main__ INFO: \u001b[0mTrain 2 351\n",
      "\u001b[32m[2020-07-10 23:40:31] __main__ INFO: \u001b[0mEpoch 2 Step 100/351 lr 0.100000 loss 1.8242 (1.8218) acc@1 0.3467 (0.3752) acc@5 0.8457 (0.8326)\n",
      "\u001b[32m[2020-07-10 23:42:29] __main__ INFO: \u001b[0mEpoch 2 Step 200/351 lr 0.100000 loss 1.8161 (1.7989) acc@1 0.3443 (0.3862) acc@5 0.8661 (0.8394)\n",
      "\u001b[32m[2020-07-10 23:44:26] __main__ INFO: \u001b[0mEpoch 2 Step 300/351 lr 0.100000 loss 1.7492 (1.7828) acc@1 0.3913 (0.3938) acc@5 0.8420 (0.8437)\n",
      "\u001b[32m[2020-07-10 23:45:27] __main__ INFO: \u001b[0mEpoch 2 Step 351/351 lr 0.100000 loss 1.6616 (1.7724) acc@1 0.4669 (0.3981) acc@5 0.8725 (0.8468)\n",
      "\u001b[32m[2020-07-10 23:45:27] __main__ INFO: \u001b[0mElapsed 414.22\n",
      "\u001b[32m[2020-07-10 23:45:27] __main__ INFO: \u001b[0mVal 2\n",
      "\u001b[32m[2020-07-10 23:45:41] __main__ INFO: \u001b[0mEpoch 2 loss 1.0836 acc@1 0.6402 acc@5 0.9620\n",
      "\u001b[32m[2020-07-10 23:45:41] __main__ INFO: \u001b[0mElapsed 13.96\n",
      "\u001b[32m[2020-07-10 23:45:41] __main__ INFO: \u001b[0mTrain 3 702\n",
      "\u001b[32m[2020-07-10 23:47:39] __main__ INFO: \u001b[0mEpoch 3 Step 100/351 lr 0.100000 loss 1.6758 (1.6937) acc@1 0.4259 (0.4332) acc@5 0.8702 (0.8662)\n",
      "\u001b[32m[2020-07-10 23:49:37] __main__ INFO: \u001b[0mEpoch 3 Step 200/351 lr 0.100000 loss 1.6645 (1.6923) acc@1 0.4184 (0.4339) acc@5 0.8797 (0.8659)\n",
      "\u001b[32m[2020-07-10 23:51:35] __main__ INFO: \u001b[0mEpoch 3 Step 300/351 lr 0.100000 loss 1.6996 (1.6815) acc@1 0.4049 (0.4391) acc@5 0.8844 (0.8693)\n",
      "\u001b[32m[2020-07-10 23:52:36] __main__ INFO: \u001b[0mEpoch 3 Step 351/351 lr 0.100000 loss 1.6582 (1.6787) acc@1 0.4600 (0.4401) acc@5 0.8708 (0.8704)\n",
      "\u001b[32m[2020-07-10 23:52:36] __main__ INFO: \u001b[0mElapsed 414.88\n",
      "\u001b[32m[2020-07-10 23:52:36] __main__ INFO: \u001b[0mVal 3\n",
      "\u001b[32m[2020-07-10 23:52:49] __main__ INFO: \u001b[0mEpoch 3 loss 1.0272 acc@1 0.6760 acc@5 0.9734\n",
      "\u001b[32m[2020-07-10 23:52:49] __main__ INFO: \u001b[0mElapsed 13.89\n",
      "\u001b[32m[2020-07-10 23:52:49] __main__ INFO: \u001b[0mTrain 4 1053\n",
      "\u001b[32m[2020-07-10 23:54:48] __main__ INFO: \u001b[0mEpoch 4 Step 100/351 lr 0.100000 loss 1.7044 (1.6261) acc@1 0.4558 (0.4618) acc@5 0.8586 (0.8827)\n",
      "\u001b[32m[2020-07-10 23:56:46] __main__ INFO: \u001b[0mEpoch 4 Step 200/351 lr 0.100000 loss 1.6587 (1.6243) acc@1 0.4316 (0.4625) acc@5 0.8703 (0.8821)\n",
      "\u001b[32m[2020-07-10 23:58:44] __main__ INFO: \u001b[0mEpoch 4 Step 300/351 lr 0.100000 loss 1.6363 (1.6168) acc@1 0.4735 (0.4653) acc@5 0.8914 (0.8839)\n",
      "\u001b[32m[2020-07-10 23:59:44] __main__ INFO: \u001b[0mEpoch 4 Step 351/351 lr 0.100000 loss 1.5955 (1.6129) acc@1 0.4953 (0.4673) acc@5 0.8760 (0.8845)\n",
      "\u001b[32m[2020-07-10 23:59:44] __main__ INFO: \u001b[0mElapsed 414.55\n",
      "\u001b[32m[2020-07-10 23:59:44] __main__ INFO: \u001b[0mVal 4\n",
      "\u001b[32m[2020-07-10 23:59:58] __main__ INFO: \u001b[0mEpoch 4 loss 1.0078 acc@1 0.6678 acc@5 0.9624\n",
      "\u001b[32m[2020-07-10 23:59:58] __main__ INFO: \u001b[0mElapsed 13.92\n",
      "\u001b[32m[2020-07-10 23:59:58] __main__ INFO: \u001b[0mTrain 5 1404\n",
      "\u001b[32m[2020-07-11 00:01:56] __main__ INFO: \u001b[0mEpoch 5 Step 100/351 lr 0.100000 loss 1.5927 (1.5711) acc@1 0.4509 (0.4802) acc@5 0.8795 (0.8972)\n",
      "\u001b[32m[2020-07-11 00:03:54] __main__ INFO: \u001b[0mEpoch 5 Step 200/351 lr 0.100000 loss 1.4791 (1.5668) acc@1 0.5151 (0.4859) acc@5 0.9216 (0.8958)\n",
      "\u001b[32m[2020-07-11 00:05:52] __main__ INFO: \u001b[0mEpoch 5 Step 300/351 lr 0.100000 loss 1.6368 (1.5661) acc@1 0.4355 (0.4856) acc@5 0.8904 (0.8954)\n",
      "\u001b[32m[2020-07-11 00:06:52] __main__ INFO: \u001b[0mEpoch 5 Step 351/351 lr 0.100000 loss 1.4812 (1.5647) acc@1 0.5393 (0.4853) acc@5 0.9120 (0.8956)\n",
      "\u001b[32m[2020-07-11 00:06:52] __main__ INFO: \u001b[0mElapsed 414.41\n",
      "\u001b[32m[2020-07-11 00:06:52] __main__ INFO: \u001b[0mVal 5\n",
      "\u001b[32m[2020-07-11 00:07:06] __main__ INFO: \u001b[0mEpoch 5 loss 0.9720 acc@1 0.6780 acc@5 0.9710\n",
      "\u001b[32m[2020-07-11 00:07:06] __main__ INFO: \u001b[0mElapsed 13.95\n",
      "\u001b[32m[2020-07-11 00:07:06] __main__ INFO: \u001b[0mTrain 6 1755\n",
      "\u001b[32m[2020-07-11 00:09:05] __main__ INFO: \u001b[0mEpoch 6 Step 100/351 lr 0.100000 loss 1.4452 (1.5306) acc@1 0.5066 (0.4975) acc@5 0.9221 (0.9020)\n",
      "\u001b[32m[2020-07-11 00:11:03] __main__ INFO: \u001b[0mEpoch 6 Step 200/351 lr 0.100000 loss 1.5394 (1.5253) acc@1 0.4949 (0.4986) acc@5 0.9376 (0.9026)\n",
      "\u001b[32m[2020-07-11 00:13:00] __main__ INFO: \u001b[0mEpoch 6 Step 300/351 lr 0.100000 loss 1.5255 (1.5223) acc@1 0.4751 (0.4994) acc@5 0.9132 (0.9032)\n",
      "\u001b[32m[2020-07-11 00:14:00] __main__ INFO: \u001b[0mEpoch 6 Step 351/351 lr 0.100000 loss 1.5585 (1.5237) acc@1 0.5102 (0.4985) acc@5 0.9012 (0.9034)\n",
      "\u001b[32m[2020-07-11 00:14:00] __main__ INFO: \u001b[0mElapsed 414.24\n",
      "\u001b[32m[2020-07-11 00:14:00] __main__ INFO: \u001b[0mVal 6\n",
      "\u001b[32m[2020-07-11 00:14:14] __main__ INFO: \u001b[0mEpoch 6 loss 0.8154 acc@1 0.7560 acc@5 0.9834\n",
      "\u001b[32m[2020-07-11 00:14:14] __main__ INFO: \u001b[0mElapsed 13.91\n",
      "\u001b[32m[2020-07-11 00:14:14] __main__ INFO: \u001b[0mTrain 7 2106\n",
      "\u001b[32m[2020-07-11 00:16:13] __main__ INFO: \u001b[0mEpoch 7 Step 100/351 lr 0.100000 loss 1.6097 (1.4917) acc@1 0.4763 (0.5080) acc@5 0.8837 (0.9106)\n",
      "\u001b[32m[2020-07-11 00:18:11] __main__ INFO: \u001b[0mEpoch 7 Step 200/351 lr 0.100000 loss 1.4005 (1.4908) acc@1 0.5829 (0.5094) acc@5 0.9065 (0.9110)\n",
      "\u001b[32m[2020-07-11 00:20:09] __main__ INFO: \u001b[0mEpoch 7 Step 300/351 lr 0.100000 loss 1.4434 (1.4902) acc@1 0.5182 (0.5084) acc@5 0.9124 (0.9106)\n",
      "\u001b[32m[2020-07-11 00:21:09] __main__ INFO: \u001b[0mEpoch 7 Step 351/351 lr 0.100000 loss 1.5489 (1.4877) acc@1 0.5022 (0.5090) acc@5 0.8888 (0.9110)\n",
      "\u001b[32m[2020-07-11 00:21:09] __main__ INFO: \u001b[0mElapsed 414.35\n",
      "\u001b[32m[2020-07-11 00:21:09] __main__ INFO: \u001b[0mVal 7\n",
      "\u001b[32m[2020-07-11 00:21:23] __main__ INFO: \u001b[0mEpoch 7 loss 0.7501 acc@1 0.7824 acc@5 0.9896\n",
      "\u001b[32m[2020-07-11 00:21:23] __main__ INFO: \u001b[0mElapsed 13.91\n",
      "\u001b[32m[2020-07-11 00:21:23] __main__ INFO: \u001b[0mTrain 8 2457\n",
      "\u001b[32m[2020-07-11 00:23:21] __main__ INFO: \u001b[0mEpoch 8 Step 100/351 lr 0.100000 loss 1.4694 (1.4648) acc@1 0.4934 (0.5137) acc@5 0.9274 (0.9156)\n",
      "\u001b[32m[2020-07-11 00:25:19] __main__ INFO: \u001b[0mEpoch 8 Step 200/351 lr 0.100000 loss 1.3482 (1.4521) acc@1 0.5969 (0.5193) acc@5 0.9044 (0.9170)\n",
      "\u001b[32m[2020-07-11 00:27:17] __main__ INFO: \u001b[0mEpoch 8 Step 300/351 lr 0.100000 loss 1.4286 (1.4539) acc@1 0.5274 (0.5189) acc@5 0.9177 (0.9166)\n",
      "\u001b[32m[2020-07-11 00:28:17] __main__ INFO: \u001b[0mEpoch 8 Step 351/351 lr 0.100000 loss 1.4736 (1.4529) acc@1 0.5278 (0.5190) acc@5 0.8934 (0.9163)\n",
      "\u001b[32m[2020-07-11 00:28:17] __main__ INFO: \u001b[0mElapsed 413.94\n",
      "\u001b[32m[2020-07-11 00:28:17] __main__ INFO: \u001b[0mVal 8\n",
      "\u001b[32m[2020-07-11 00:28:31] __main__ INFO: \u001b[0mEpoch 8 loss 0.6946 acc@1 0.7940 acc@5 0.9848\n",
      "\u001b[32m[2020-07-11 00:28:31] __main__ INFO: \u001b[0mElapsed 13.94\n",
      "\u001b[32m[2020-07-11 00:28:31] __main__ INFO: \u001b[0mTrain 9 2808\n",
      "\u001b[32m[2020-07-11 00:30:29] __main__ INFO: \u001b[0mEpoch 9 Step 100/351 lr 0.100000 loss 1.4305 (1.4185) acc@1 0.5095 (0.5307) acc@5 0.9253 (0.9220)\n",
      "\u001b[32m[2020-07-11 00:32:26] __main__ INFO: \u001b[0mEpoch 9 Step 200/351 lr 0.100000 loss 1.3455 (1.4206) acc@1 0.5663 (0.5300) acc@5 0.9269 (0.9225)\n",
      "\u001b[32m[2020-07-11 00:34:24] __main__ INFO: \u001b[0mEpoch 9 Step 300/351 lr 0.100000 loss 1.3679 (1.4205) acc@1 0.5637 (0.5299) acc@5 0.9124 (0.9221)\n",
      "\u001b[32m[2020-07-11 00:35:24] __main__ INFO: \u001b[0mEpoch 9 Step 351/351 lr 0.100000 loss 1.3839 (1.4205) acc@1 0.5674 (0.5299) acc@5 0.9113 (0.9219)\n",
      "\u001b[32m[2020-07-11 00:35:24] __main__ INFO: \u001b[0mElapsed 413.69\n",
      "\u001b[32m[2020-07-11 00:35:24] __main__ INFO: \u001b[0mVal 9\n",
      "\u001b[32m[2020-07-11 00:35:38] __main__ INFO: \u001b[0mEpoch 9 loss 0.7549 acc@1 0.7988 acc@5 0.9864\n",
      "\u001b[32m[2020-07-11 00:35:38] __main__ INFO: \u001b[0mElapsed 13.88\n",
      "\u001b[32m[2020-07-11 00:35:38] __main__ INFO: \u001b[0mTrain 10 3159\n",
      "\u001b[32m[2020-07-11 00:37:36] __main__ INFO: \u001b[0mEpoch 10 Step 100/351 lr 0.100000 loss 1.3673 (1.3963) acc@1 0.5280 (0.5364) acc@5 0.9219 (0.9251)\n",
      "\u001b[32m[2020-07-11 00:39:34] __main__ INFO: \u001b[0mEpoch 10 Step 200/351 lr 0.100000 loss 1.4435 (1.3967) acc@1 0.4811 (0.5360) acc@5 0.9451 (0.9255)\n",
      "\u001b[32m[2020-07-11 00:41:32] __main__ INFO: \u001b[0mEpoch 10 Step 300/351 lr 0.100000 loss 1.3985 (1.3967) acc@1 0.5250 (0.5367) acc@5 0.9142 (0.9260)\n",
      "\u001b[32m[2020-07-11 00:42:32] __main__ INFO: \u001b[0mEpoch 10 Step 351/351 lr 0.100000 loss 1.3110 (1.3968) acc@1 0.5712 (0.5364) acc@5 0.9233 (0.9259)\n",
      "\u001b[32m[2020-07-11 00:42:32] __main__ INFO: \u001b[0mElapsed 413.87\n",
      "\u001b[32m[2020-07-11 00:42:32] __main__ INFO: \u001b[0mVal 10\n",
      "\u001b[32m[2020-07-11 00:42:46] __main__ INFO: \u001b[0mEpoch 10 loss 0.5748 acc@1 0.8508 acc@5 0.9900\n",
      "\u001b[32m[2020-07-11 00:42:46] __main__ INFO: \u001b[0mElapsed 13.89\n",
      "\u001b[32m[2020-07-11 00:42:46] __main__ INFO: \u001b[0mTrain 11 3510\n",
      "\u001b[32m[2020-07-11 00:44:44] __main__ INFO: \u001b[0mEpoch 11 Step 100/351 lr 0.100000 loss 1.3457 (1.3704) acc@1 0.5287 (0.5428) acc@5 0.9558 (0.9313)\n",
      "\u001b[32m[2020-07-11 00:46:42] __main__ INFO: \u001b[0mEpoch 11 Step 200/351 lr 0.100000 loss 1.4524 (1.3736) acc@1 0.5108 (0.5425) acc@5 0.9102 (0.9294)\n",
      "\u001b[32m[2020-07-11 00:48:40] __main__ INFO: \u001b[0mEpoch 11 Step 300/351 lr 0.100000 loss 1.4087 (1.3721) acc@1 0.5645 (0.5424) acc@5 0.8980 (0.9303)\n",
      "\u001b[32m[2020-07-11 00:49:40] __main__ INFO: \u001b[0mEpoch 11 Step 351/351 lr 0.100000 loss 1.4876 (1.3728) acc@1 0.4834 (0.5420) acc@5 0.9055 (0.9300)\n",
      "\u001b[32m[2020-07-11 00:49:40] __main__ INFO: \u001b[0mElapsed 413.69\n",
      "\u001b[32m[2020-07-11 00:49:40] __main__ INFO: \u001b[0mVal 11\n",
      "\u001b[32m[2020-07-11 00:49:53] __main__ INFO: \u001b[0mEpoch 11 loss 0.6258 acc@1 0.8212 acc@5 0.9924\n",
      "\u001b[32m[2020-07-11 00:49:53] __main__ INFO: \u001b[0mElapsed 13.89\n",
      "\u001b[32m[2020-07-11 00:49:53] __main__ INFO: \u001b[0mTrain 12 3861\n",
      "\u001b[32m[2020-07-11 00:51:51] __main__ INFO: \u001b[0mEpoch 12 Step 100/351 lr 0.100000 loss 1.4051 (1.3439) acc@1 0.5106 (0.5480) acc@5 0.9123 (0.9357)\n",
      "\u001b[32m[2020-07-11 00:53:49] __main__ INFO: \u001b[0mEpoch 12 Step 200/351 lr 0.100000 loss 1.4386 (1.3471) acc@1 0.5037 (0.5497) acc@5 0.9124 (0.9347)\n",
      "\u001b[32m[2020-07-11 00:55:47] __main__ INFO: \u001b[0mEpoch 12 Step 300/351 lr 0.100000 loss 1.3442 (1.3501) acc@1 0.5499 (0.5491) acc@5 0.9382 (0.9338)\n",
      "\u001b[32m[2020-07-11 00:56:47] __main__ INFO: \u001b[0mEpoch 12 Step 351/351 lr 0.100000 loss 1.3054 (1.3513) acc@1 0.5729 (0.5492) acc@5 0.9443 (0.9334)\n",
      "\u001b[32m[2020-07-11 00:56:47] __main__ INFO: \u001b[0mElapsed 413.62\n",
      "\u001b[32m[2020-07-11 00:56:47] __main__ INFO: \u001b[0mVal 12\n",
      "\u001b[32m[2020-07-11 00:57:01] __main__ INFO: \u001b[0mEpoch 12 loss 0.6171 acc@1 0.8426 acc@5 0.9886\n",
      "\u001b[32m[2020-07-11 00:57:01] __main__ INFO: \u001b[0mElapsed 13.86\n",
      "\u001b[32m[2020-07-11 00:57:01] __main__ INFO: \u001b[0mTrain 13 4212\n",
      "\u001b[32m[2020-07-11 00:58:59] __main__ INFO: \u001b[0mEpoch 13 Step 100/351 lr 0.100000 loss 1.3511 (1.3261) acc@1 0.5562 (0.5546) acc@5 0.9324 (0.9389)\n",
      "\u001b[32m[2020-07-11 01:00:57] __main__ INFO: \u001b[0mEpoch 13 Step 200/351 lr 0.100000 loss 1.3080 (1.3287) acc@1 0.5448 (0.5545) acc@5 0.9371 (0.9380)\n",
      "\u001b[32m[2020-07-11 01:02:54] __main__ INFO: \u001b[0mEpoch 13 Step 300/351 lr 0.100000 loss 1.3333 (1.3332) acc@1 0.5289 (0.5541) acc@5 0.9297 (0.9368)\n",
      "\u001b[32m[2020-07-11 01:03:54] __main__ INFO: \u001b[0mEpoch 13 Step 351/351 lr 0.100000 loss 1.3192 (1.3356) acc@1 0.5499 (0.5538) acc@5 0.9228 (0.9362)\n",
      "\u001b[32m[2020-07-11 01:03:54] __main__ INFO: \u001b[0mElapsed 413.45\n",
      "\u001b[32m[2020-07-11 01:03:54] __main__ INFO: \u001b[0mVal 13\n",
      "\u001b[32m[2020-07-11 01:04:08] __main__ INFO: \u001b[0mEpoch 13 loss 0.6545 acc@1 0.8266 acc@5 0.9902\n",
      "\u001b[32m[2020-07-11 01:04:08] __main__ INFO: \u001b[0mElapsed 13.89\n",
      "\u001b[32m[2020-07-11 01:04:08] __main__ INFO: \u001b[0mTrain 14 4563\n",
      "\u001b[32m[2020-07-11 01:06:06] __main__ INFO: \u001b[0mEpoch 14 Step 100/351 lr 0.100000 loss 1.3156 (1.3112) acc@1 0.5664 (0.5595) acc@5 0.9255 (0.9396)\n",
      "\u001b[32m[2020-07-11 01:08:04] __main__ INFO: \u001b[0mEpoch 14 Step 200/351 lr 0.100000 loss 1.2700 (1.3180) acc@1 0.6056 (0.5583) acc@5 0.9137 (0.9374)\n",
      "\u001b[32m[2020-07-11 01:10:02] __main__ INFO: \u001b[0mEpoch 14 Step 300/351 lr 0.100000 loss 1.3243 (1.3210) acc@1 0.5533 (0.5572) acc@5 0.9288 (0.9368)\n",
      "\u001b[32m[2020-07-11 01:11:02] __main__ INFO: \u001b[0mEpoch 14 Step 351/351 lr 0.100000 loss 1.4078 (1.3206) acc@1 0.5301 (0.5568) acc@5 0.9196 (0.9371)\n",
      "\u001b[32m[2020-07-11 01:11:02] __main__ INFO: \u001b[0mElapsed 414.00\n",
      "\u001b[32m[2020-07-11 01:11:02] __main__ INFO: \u001b[0mVal 14\n",
      "\u001b[32m[2020-07-11 01:11:16] __main__ INFO: \u001b[0mEpoch 14 loss 0.7749 acc@1 0.7890 acc@5 0.9820\n",
      "\u001b[32m[2020-07-11 01:11:16] __main__ INFO: \u001b[0mElapsed 13.89\n",
      "\u001b[32m[2020-07-11 01:11:16] __main__ INFO: \u001b[0mTrain 15 4914\n",
      "\u001b[32m[2020-07-11 01:13:14] __main__ INFO: \u001b[0mEpoch 15 Step 100/351 lr 0.100000 loss 1.2774 (1.2907) acc@1 0.5508 (0.5698) acc@5 0.9468 (0.9403)\n",
      "\u001b[32m[2020-07-11 01:15:12] __main__ INFO: \u001b[0mEpoch 15 Step 200/351 lr 0.100000 loss 1.3491 (1.2988) acc@1 0.5546 (0.5655) acc@5 0.9466 (0.9401)\n",
      "\u001b[32m[2020-07-11 01:17:09] __main__ INFO: \u001b[0mEpoch 15 Step 300/351 lr 0.100000 loss 1.3376 (1.3021) acc@1 0.5719 (0.5637) acc@5 0.9321 (0.9404)\n",
      "\u001b[32m[2020-07-11 01:18:09] __main__ INFO: \u001b[0mEpoch 15 Step 351/351 lr 0.100000 loss 1.2506 (1.3033) acc@1 0.5482 (0.5633) acc@5 0.9504 (0.9401)\n",
      "\u001b[32m[2020-07-11 01:18:09] __main__ INFO: \u001b[0mElapsed 413.27\n",
      "\u001b[32m[2020-07-11 01:18:09] __main__ INFO: \u001b[0mVal 15\n",
      "\u001b[32m[2020-07-11 01:18:23] __main__ INFO: \u001b[0mEpoch 15 loss 0.9350 acc@1 0.7282 acc@5 0.9692\n",
      "\u001b[32m[2020-07-11 01:18:23] __main__ INFO: \u001b[0mElapsed 13.88\n",
      "\u001b[32m[2020-07-11 01:18:23] __main__ INFO: \u001b[0mTrain 16 5265\n",
      "\u001b[32m[2020-07-11 01:20:21] __main__ INFO: \u001b[0mEpoch 16 Step 100/351 lr 0.100000 loss 1.2769 (1.2773) acc@1 0.5652 (0.5697) acc@5 0.9478 (0.9455)\n",
      "\u001b[32m[2020-07-11 01:22:19] __main__ INFO: \u001b[0mEpoch 16 Step 200/351 lr 0.100000 loss 1.1931 (1.2875) acc@1 0.6103 (0.5662) acc@5 0.9615 (0.9437)\n",
      "\u001b[32m[2020-07-11 01:24:17] __main__ INFO: \u001b[0mEpoch 16 Step 300/351 lr 0.100000 loss 1.2745 (1.2889) acc@1 0.5722 (0.5659) acc@5 0.9460 (0.9436)\n",
      "\u001b[32m[2020-07-11 01:25:17] __main__ INFO: \u001b[0mEpoch 16 Step 351/351 lr 0.100000 loss 1.2451 (1.2892) acc@1 0.5653 (0.5661) acc@5 0.9565 (0.9428)\n",
      "\u001b[32m[2020-07-11 01:25:17] __main__ INFO: \u001b[0mElapsed 413.29\n",
      "\u001b[32m[2020-07-11 01:25:17] __main__ INFO: \u001b[0mVal 16\n",
      "\u001b[32m[2020-07-11 01:25:30] __main__ INFO: \u001b[0mEpoch 16 loss 0.6649 acc@1 0.8150 acc@5 0.9862\n",
      "\u001b[32m[2020-07-11 01:25:30] __main__ INFO: \u001b[0mElapsed 13.87\n",
      "\u001b[32m[2020-07-11 01:25:30] __main__ INFO: \u001b[0mTrain 17 5616\n",
      "\u001b[32m[2020-07-11 01:27:28] __main__ INFO: \u001b[0mEpoch 17 Step 100/351 lr 0.100000 loss 1.3291 (1.2625) acc@1 0.5278 (0.5735) acc@5 0.9453 (0.9472)\n",
      "\u001b[32m[2020-07-11 01:29:26] __main__ INFO: \u001b[0mEpoch 17 Step 200/351 lr 0.100000 loss 1.3543 (1.2714) acc@1 0.5243 (0.5699) acc@5 0.9482 (0.9467)\n",
      "\u001b[32m[2020-07-11 01:31:24] __main__ INFO: \u001b[0mEpoch 17 Step 300/351 lr 0.100000 loss 1.2672 (1.2757) acc@1 0.5733 (0.5691) acc@5 0.9442 (0.9451)\n"
     ]
    }
   ],
   "source": [
    "# Train the model per the settings specified in the original paper\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "!python train.py --config configs/cifar/wrn.yaml \\\n",
    "    model.wrn.depth 28 \\\n",
    "    model.wrn.widening_factor 10 \\\n",
    "    train.batch_size 128 \\\n",
    "    train.base_lr 0.1 \\\n",
    "    dataset.name CIFAR10_CM_1 \\\n",
    "    train.base_lr 0.1 \\\n",
    "    train.output_dir /home/ec2-user/SageMaker/experiments/wrn_28_10_cm_1_1/exp00 \\\n",
    "    scheduler.epochs 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Refine the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-13 00:23:12] __main__ INFO: \u001b[0mdevice: cuda\n",
      "cudnn:\n",
      "  benchmark: True\n",
      "  deterministic: False\n",
      "dataset:\n",
      "  name: CIFAR10_CM_1\n",
      "  dataset_dir: ''\n",
      "  image_size: 32\n",
      "  n_channels: 3\n",
      "  n_classes: 10\n",
      "model:\n",
      "  type: cifar\n",
      "  name: wrn\n",
      "  init_mode: kaiming_fan_in\n",
      "  vgg:\n",
      "    n_channels: [64, 128, 256, 512, 512]\n",
      "    n_layers: [2, 2, 3, 3, 3]\n",
      "    use_bn: True\n",
      "  resnet:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "  resnet_preact:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "  wrn:\n",
      "    depth: 28\n",
      "    initial_channels: 16\n",
      "    widening_factor: 10\n",
      "    drop_rate: 0.0\n",
      "  densenet:\n",
      "    depth: 100\n",
      "    n_blocks: [6, 12, 24, 16]\n",
      "    block_type: bottleneck\n",
      "    growth_rate: 12\n",
      "    drop_rate: 0.0\n",
      "    compression_rate: 0.5\n",
      "  pyramidnet:\n",
      "    depth: 272\n",
      "    n_blocks: [3, 24, 36, 3]\n",
      "    initial_channels: 16\n",
      "    block_type: bottleneck\n",
      "    alpha: 200\n",
      "  resnext:\n",
      "    depth: 29\n",
      "    n_blocks: [3, 4, 6, 3]\n",
      "    initial_channels: 64\n",
      "    cardinality: 8\n",
      "    base_channels: 4\n",
      "  shake_shake:\n",
      "    depth: 26\n",
      "    initial_channels: 96\n",
      "    shake_forward: True\n",
      "    shake_backward: True\n",
      "    shake_image: True\n",
      "  se_resnet_preact:\n",
      "    depth: 110\n",
      "    initial_channels: 16\n",
      "    se_reduction: 16\n",
      "    block_type: basic\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "train:\n",
      "  checkpoint: /home/ec2-user/SageMaker/experiments/wrn_28_10_cm_1_1/exp00/checkpoint_00300.pth\n",
      "  resume: False\n",
      "  precision: O0\n",
      "  batch_size: 128\n",
      "  subdivision: 1\n",
      "  optimizer: sgd\n",
      "  base_lr: 0.0008\n",
      "  momentum: 0.9\n",
      "  nesterov: True\n",
      "  weight_decay: 0.0005\n",
      "  no_weight_decay_on_bn: False\n",
      "  gradient_clip: 0\n",
      "  start_epoch: 0\n",
      "  seed: 0\n",
      "  val_first: True\n",
      "  val_period: 1\n",
      "  val_ratio: 0.1\n",
      "  use_test_as_val: False\n",
      "  output_dir: /home/ec2-user/SageMaker/experiments/wrn_28_10_cm_1_1/exp00_resume300\n",
      "  log_period: 100\n",
      "  checkpoint_period: 100\n",
      "  use_tensorboard: True\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: True\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "  distributed: False\n",
      "  dist:\n",
      "    backend: nccl\n",
      "    init_method: env://\n",
      "    world_size: -1\n",
      "    node_rank: -1\n",
      "    local_rank: 0\n",
      "    use_sync_bn: False\n",
      "tensorboard:\n",
      "  train_images: False\n",
      "  val_images: False\n",
      "  model_params: False\n",
      "optim:\n",
      "  adam:\n",
      "    betas: (0.9, 0.999)\n",
      "  lars:\n",
      "    eps: 1e-09\n",
      "    threshold: 0.01\n",
      "  adabound:\n",
      "    betas: (0.9, 0.999)\n",
      "    final_lr: 0.1\n",
      "    gamma: 0.001\n",
      "scheduler:\n",
      "  epochs: 100\n",
      "  warmup:\n",
      "    type: none\n",
      "    epochs: 0\n",
      "    start_factor: 0.001\n",
      "    exponent: 4\n",
      "  type: multistep\n",
      "  milestones: [60, 120, 160]\n",
      "  lr_decay: 0.2\n",
      "  lr_min_factor: 0.001\n",
      "  T0: 10\n",
      "  T_mul: 1.0\n",
      "validation:\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: False\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "augmentation:\n",
      "  use_random_crop: True\n",
      "  use_random_horizontal_flip: True\n",
      "  use_cutout: False\n",
      "  use_random_erasing: False\n",
      "  use_dual_cutout: False\n",
      "  use_mixup: False\n",
      "  use_ricap: False\n",
      "  use_cutmix: False\n",
      "  use_label_smoothing: False\n",
      "  random_crop:\n",
      "    padding: 4\n",
      "    fill: 0\n",
      "    padding_mode: constant\n",
      "  random_horizontal_flip:\n",
      "    prob: 0.5\n",
      "  cutout:\n",
      "    prob: 1.0\n",
      "    mask_size: 16\n",
      "    cut_inside: False\n",
      "    mask_color: 0\n",
      "    dual_cutout_alpha: 0.1\n",
      "  random_erasing:\n",
      "    prob: 0.5\n",
      "    area_ratio_range: [0.02, 0.4]\n",
      "    min_aspect_ratio: 0.3\n",
      "    max_attempt: 20\n",
      "  mixup:\n",
      "    alpha: 1.0\n",
      "  ricap:\n",
      "    beta: 0.3\n",
      "  cutmix:\n",
      "    alpha: 1.0\n",
      "  label_smoothing:\n",
      "    epsilon: 0.1\n",
      "tta:\n",
      "  use_resize: False\n",
      "  use_center_crop: False\n",
      "  resize: 256\n",
      "test:\n",
      "  checkpoint: ''\n",
      "  output_dir: ''\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    pin_memory: False\n",
      "\u001b[32m[2020-07-13 00:23:12] __main__ INFO: \u001b[0menv_info:\n",
      "  pytorch_version: 1.4.0\n",
      "  cuda_version: 10.1\n",
      "  cudnn_version: 7603\n",
      "  num_gpus: 1\n",
      "  gpu_name: Tesla K80\n",
      "  gpu_capability: 3.7\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /home/ec2-user/.torch/datasets/CIFAR10/cifar-10-python.tar.gz\n",
      "170500096it [00:03, 53004330.65it/s]                                            \n",
      "Extracting /home/ec2-user/.torch/datasets/CIFAR10/cifar-10-python.tar.gz to /home/ec2-user/.torch/datasets/CIFAR10\n",
      "\u001b[32m[2020-07-13 00:24:20] __main__ INFO: \u001b[0mMACs  : 5.25G\n",
      "\u001b[32m[2020-07-13 00:24:20] __main__ INFO: \u001b[0m#params: 36.48M\n",
      "Selected optimization level O0:  Pure FP32 training.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\",)\n",
      "\u001b[32m[2020-07-13 00:24:22] __main__ INFO: \u001b[0mVal 0\n",
      "\u001b[32m[2020-07-13 00:24:43] __main__ INFO: \u001b[0mEpoch 0 loss 0.0979 acc@1 0.9872 acc@5 0.9998\n",
      "\u001b[32m[2020-07-13 00:24:43] __main__ INFO: \u001b[0mElapsed 20.02\n",
      "\u001b[32m[2020-07-13 00:24:43] __main__ INFO: \u001b[0mTrain 1 0\n",
      "\u001b[32m[2020-07-13 00:26:42] __main__ INFO: \u001b[0mEpoch 1 Step 100/351 lr 0.000800 loss 0.5614 (0.5512) acc@1 0.7457 (0.7299) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 00:28:36] __main__ INFO: \u001b[0mEpoch 1 Step 200/351 lr 0.000800 loss 0.5437 (0.5536) acc@1 0.7193 (0.7274) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 00:30:30] __main__ INFO: \u001b[0mEpoch 1 Step 300/351 lr 0.000800 loss 0.5454 (0.5532) acc@1 0.7303 (0.7278) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 00:31:28] __main__ INFO: \u001b[0mEpoch 1 Step 351/351 lr 0.000800 loss 0.5379 (0.5527) acc@1 0.7350 (0.7285) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 00:31:28] __main__ INFO: \u001b[0mElapsed 405.71\n",
      "\u001b[32m[2020-07-13 00:31:28] __main__ INFO: \u001b[0mVal 1\n",
      "\u001b[32m[2020-07-13 00:31:42] __main__ INFO: \u001b[0mEpoch 1 loss 0.0954 acc@1 0.9888 acc@5 0.9998\n",
      "\u001b[32m[2020-07-13 00:31:42] __main__ INFO: \u001b[0mElapsed 13.53\n",
      "\u001b[32m[2020-07-13 00:31:42] __main__ INFO: \u001b[0mTrain 2 351\n",
      "\u001b[32m[2020-07-13 00:33:36] __main__ INFO: \u001b[0mEpoch 2 Step 100/351 lr 0.000800 loss 0.5564 (0.5547) acc@1 0.7333 (0.7272) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 00:35:30] __main__ INFO: \u001b[0mEpoch 2 Step 200/351 lr 0.000800 loss 0.5407 (0.5534) acc@1 0.7355 (0.7287) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 00:37:25] __main__ INFO: \u001b[0mEpoch 2 Step 300/351 lr 0.000800 loss 0.5495 (0.5532) acc@1 0.7253 (0.7282) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 00:38:23] __main__ INFO: \u001b[0mEpoch 2 Step 351/351 lr 0.000800 loss 0.5634 (0.5527) acc@1 0.7249 (0.7285) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 00:38:23] __main__ INFO: \u001b[0mElapsed 401.20\n",
      "\u001b[32m[2020-07-13 00:38:23] __main__ INFO: \u001b[0mVal 2\n",
      "\u001b[32m[2020-07-13 00:38:36] __main__ INFO: \u001b[0mEpoch 2 loss 0.0967 acc@1 0.9886 acc@5 0.9994\n",
      "\u001b[32m[2020-07-13 00:38:36] __main__ INFO: \u001b[0mElapsed 13.43\n",
      "\u001b[32m[2020-07-13 00:38:36] __main__ INFO: \u001b[0mTrain 3 702\n",
      "\u001b[32m[2020-07-13 00:40:31] __main__ INFO: \u001b[0mEpoch 3 Step 100/351 lr 0.000800 loss 0.5772 (0.5500) acc@1 0.7260 (0.7312) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 00:42:25] __main__ INFO: \u001b[0mEpoch 3 Step 200/351 lr 0.000800 loss 0.5530 (0.5507) acc@1 0.7288 (0.7304) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 00:44:19] __main__ INFO: \u001b[0mEpoch 3 Step 300/351 lr 0.000800 loss 0.5761 (0.5520) acc@1 0.6978 (0.7289) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 00:45:18] __main__ INFO: \u001b[0mEpoch 3 Step 351/351 lr 0.000800 loss 0.5331 (0.5527) acc@1 0.7450 (0.7285) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 00:45:18] __main__ INFO: \u001b[0mElapsed 401.25\n",
      "\u001b[32m[2020-07-13 00:45:18] __main__ INFO: \u001b[0mVal 3\n",
      "\u001b[32m[2020-07-13 00:45:31] __main__ INFO: \u001b[0mEpoch 3 loss 0.0987 acc@1 0.9886 acc@5 0.9996\n",
      "\u001b[32m[2020-07-13 00:45:31] __main__ INFO: \u001b[0mElapsed 13.44\n",
      "\u001b[32m[2020-07-13 00:45:31] __main__ INFO: \u001b[0mTrain 4 1053\n",
      "\u001b[32m[2020-07-13 00:47:25] __main__ INFO: \u001b[0mEpoch 4 Step 100/351 lr 0.000800 loss 0.5429 (0.5532) acc@1 0.7446 (0.7281) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 00:49:20] __main__ INFO: \u001b[0mEpoch 4 Step 200/351 lr 0.000800 loss 0.5659 (0.5525) acc@1 0.7145 (0.7287) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 00:51:14] __main__ INFO: \u001b[0mEpoch 4 Step 300/351 lr 0.000800 loss 0.5941 (0.5527) acc@1 0.6975 (0.7286) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 00:52:12] __main__ INFO: \u001b[0mEpoch 4 Step 351/351 lr 0.000800 loss 0.5271 (0.5527) acc@1 0.7444 (0.7286) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 00:52:12] __main__ INFO: \u001b[0mElapsed 401.16\n",
      "\u001b[32m[2020-07-13 00:52:12] __main__ INFO: \u001b[0mVal 4\n",
      "\u001b[32m[2020-07-13 00:52:26] __main__ INFO: \u001b[0mEpoch 4 loss 0.1020 acc@1 0.9880 acc@5 0.9996\n",
      "\u001b[32m[2020-07-13 00:52:26] __main__ INFO: \u001b[0mElapsed 13.43\n",
      "\u001b[32m[2020-07-13 00:52:26] __main__ INFO: \u001b[0mTrain 5 1404\n",
      "\u001b[32m[2020-07-13 00:54:20] __main__ INFO: \u001b[0mEpoch 5 Step 100/351 lr 0.000800 loss 0.5798 (0.5508) acc@1 0.7035 (0.7305) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 00:56:14] __main__ INFO: \u001b[0mEpoch 5 Step 200/351 lr 0.000800 loss 0.5377 (0.5511) acc@1 0.7364 (0.7299) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 00:58:09] __main__ INFO: \u001b[0mEpoch 5 Step 300/351 lr 0.000800 loss 0.5624 (0.5523) acc@1 0.7229 (0.7289) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 00:59:07] __main__ INFO: \u001b[0mEpoch 5 Step 351/351 lr 0.000800 loss 0.5525 (0.5526) acc@1 0.7322 (0.7286) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 00:59:07] __main__ INFO: \u001b[0mElapsed 401.21\n",
      "\u001b[32m[2020-07-13 00:59:07] __main__ INFO: \u001b[0mVal 5\n",
      "\u001b[32m[2020-07-13 00:59:20] __main__ INFO: \u001b[0mEpoch 5 loss 0.0999 acc@1 0.9880 acc@5 0.9996\n",
      "\u001b[32m[2020-07-13 00:59:20] __main__ INFO: \u001b[0mElapsed 13.45\n",
      "\u001b[32m[2020-07-13 00:59:20] __main__ INFO: \u001b[0mTrain 6 1755\n",
      "\u001b[32m[2020-07-13 01:01:15] __main__ INFO: \u001b[0mEpoch 6 Step 100/351 lr 0.000800 loss 0.5390 (0.5526) acc@1 0.7347 (0.7300) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 01:03:10] __main__ INFO: \u001b[0mEpoch 6 Step 200/351 lr 0.000800 loss 0.5601 (0.5520) acc@1 0.7242 (0.7295) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 01:05:04] __main__ INFO: \u001b[0mEpoch 6 Step 300/351 lr 0.000800 loss 0.5693 (0.5523) acc@1 0.7110 (0.7290) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 01:06:02] __main__ INFO: \u001b[0mEpoch 6 Step 351/351 lr 0.000800 loss 0.5592 (0.5526) acc@1 0.7264 (0.7286) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 01:06:03] __main__ INFO: \u001b[0mElapsed 402.17\n",
      "\u001b[32m[2020-07-13 01:06:03] __main__ INFO: \u001b[0mVal 6\n",
      "\u001b[32m[2020-07-13 01:06:16] __main__ INFO: \u001b[0mEpoch 6 loss 0.0982 acc@1 0.9886 acc@5 1.0000\n",
      "\u001b[32m[2020-07-13 01:06:16] __main__ INFO: \u001b[0mElapsed 13.44\n",
      "\u001b[32m[2020-07-13 01:06:16] __main__ INFO: \u001b[0mTrain 7 2106\n",
      "\u001b[32m[2020-07-13 01:08:11] __main__ INFO: \u001b[0mEpoch 7 Step 100/351 lr 0.000800 loss 0.5939 (0.5536) acc@1 0.7048 (0.7275) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 01:10:05] __main__ INFO: \u001b[0mEpoch 7 Step 200/351 lr 0.000800 loss 0.5189 (0.5529) acc@1 0.7582 (0.7281) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 01:12:00] __main__ INFO: \u001b[0mEpoch 7 Step 300/351 lr 0.000800 loss 0.5774 (0.5534) acc@1 0.7044 (0.7280) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 01:12:58] __main__ INFO: \u001b[0mEpoch 7 Step 351/351 lr 0.000800 loss 0.5688 (0.5526) acc@1 0.7085 (0.7286) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 01:12:58] __main__ INFO: \u001b[0mElapsed 402.31\n",
      "\u001b[32m[2020-07-13 01:12:58] __main__ INFO: \u001b[0mVal 7\n",
      "\u001b[32m[2020-07-13 01:13:12] __main__ INFO: \u001b[0mEpoch 7 loss 0.0939 acc@1 0.9882 acc@5 0.9998\n",
      "\u001b[32m[2020-07-13 01:13:12] __main__ INFO: \u001b[0mElapsed 13.48\n",
      "\u001b[32m[2020-07-13 01:13:12] __main__ INFO: \u001b[0mTrain 8 2457\n",
      "\u001b[32m[2020-07-13 01:15:06] __main__ INFO: \u001b[0mEpoch 8 Step 100/351 lr 0.000800 loss 0.5672 (0.5557) acc@1 0.7245 (0.7263) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 01:17:01] __main__ INFO: \u001b[0mEpoch 8 Step 200/351 lr 0.000800 loss 0.5316 (0.5527) acc@1 0.7457 (0.7281) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 01:18:55] __main__ INFO: \u001b[0mEpoch 8 Step 300/351 lr 0.000800 loss 0.5697 (0.5527) acc@1 0.7134 (0.7283) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 01:19:54] __main__ INFO: \u001b[0mEpoch 8 Step 351/351 lr 0.000800 loss 0.5243 (0.5525) acc@1 0.7574 (0.7287) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 01:19:54] __main__ INFO: \u001b[0mElapsed 402.11\n",
      "\u001b[32m[2020-07-13 01:19:54] __main__ INFO: \u001b[0mVal 8\n",
      "\u001b[32m[2020-07-13 01:20:07] __main__ INFO: \u001b[0mEpoch 8 loss 0.0941 acc@1 0.9880 acc@5 0.9998\n",
      "\u001b[32m[2020-07-13 01:20:07] __main__ INFO: \u001b[0mElapsed 13.43\n",
      "\u001b[32m[2020-07-13 01:20:07] __main__ INFO: \u001b[0mTrain 9 2808\n",
      "\u001b[32m[2020-07-13 01:22:01] __main__ INFO: \u001b[0mEpoch 9 Step 100/351 lr 0.000800 loss 0.5636 (0.5513) acc@1 0.7146 (0.7291) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 01:23:54] __main__ INFO: \u001b[0mEpoch 9 Step 200/351 lr 0.000800 loss 0.5459 (0.5516) acc@1 0.7490 (0.7291) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 01:25:48] __main__ INFO: \u001b[0mEpoch 9 Step 300/351 lr 0.000800 loss 0.5492 (0.5523) acc@1 0.7218 (0.7287) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 01:26:46] __main__ INFO: \u001b[0mEpoch 9 Step 351/351 lr 0.000800 loss 0.5165 (0.5525) acc@1 0.7579 (0.7286) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 01:26:46] __main__ INFO: \u001b[0mElapsed 398.54\n",
      "\u001b[32m[2020-07-13 01:26:46] __main__ INFO: \u001b[0mVal 9\n",
      "\u001b[32m[2020-07-13 01:26:59] __main__ INFO: \u001b[0mEpoch 9 loss 0.0992 acc@1 0.9882 acc@5 0.9998\n",
      "\u001b[32m[2020-07-13 01:26:59] __main__ INFO: \u001b[0mElapsed 13.35\n",
      "\u001b[32m[2020-07-13 01:26:59] __main__ INFO: \u001b[0mTrain 10 3159\n",
      "\u001b[32m[2020-07-13 01:28:53] __main__ INFO: \u001b[0mEpoch 10 Step 100/351 lr 0.000800 loss 0.5142 (0.5541) acc@1 0.7439 (0.7269) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 01:30:46] __main__ INFO: \u001b[0mEpoch 10 Step 200/351 lr 0.000800 loss 0.5943 (0.5529) acc@1 0.6908 (0.7283) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 01:32:40] __main__ INFO: \u001b[0mEpoch 10 Step 300/351 lr 0.000800 loss 0.5606 (0.5531) acc@1 0.7352 (0.7282) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 01:33:39] __main__ INFO: \u001b[0mEpoch 10 Step 351/351 lr 0.000800 loss 0.5464 (0.5526) acc@1 0.7245 (0.7285) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 01:33:39] __main__ INFO: \u001b[0mElapsed 399.41\n",
      "\u001b[32m[2020-07-13 01:33:39] __main__ INFO: \u001b[0mVal 10\n",
      "\u001b[32m[2020-07-13 01:33:52] __main__ INFO: \u001b[0mEpoch 10 loss 0.0996 acc@1 0.9880 acc@5 0.9998\n",
      "\u001b[32m[2020-07-13 01:33:52] __main__ INFO: \u001b[0mElapsed 13.39\n",
      "\u001b[32m[2020-07-13 01:33:52] __main__ INFO: \u001b[0mTrain 11 3510\n",
      "\u001b[32m[2020-07-13 01:35:46] __main__ INFO: \u001b[0mEpoch 11 Step 100/351 lr 0.000800 loss 0.5308 (0.5517) acc@1 0.7444 (0.7286) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 01:37:40] __main__ INFO: \u001b[0mEpoch 11 Step 200/351 lr 0.000800 loss 0.5819 (0.5526) acc@1 0.7097 (0.7284) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 01:39:35] __main__ INFO: \u001b[0mEpoch 11 Step 300/351 lr 0.000800 loss 0.5636 (0.5527) acc@1 0.7319 (0.7285) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 01:40:33] __main__ INFO: \u001b[0mEpoch 11 Step 351/351 lr 0.000800 loss 0.5793 (0.5526) acc@1 0.7132 (0.7286) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 01:40:33] __main__ INFO: \u001b[0mElapsed 400.95\n",
      "\u001b[32m[2020-07-13 01:40:33] __main__ INFO: \u001b[0mVal 11\n",
      "\u001b[32m[2020-07-13 01:40:46] __main__ INFO: \u001b[0mEpoch 11 loss 0.0929 acc@1 0.9892 acc@5 0.9998\n",
      "\u001b[32m[2020-07-13 01:40:46] __main__ INFO: \u001b[0mElapsed 13.44\n",
      "\u001b[32m[2020-07-13 01:40:46] __main__ INFO: \u001b[0mTrain 12 3861\n",
      "\u001b[32m[2020-07-13 01:42:41] __main__ INFO: \u001b[0mEpoch 12 Step 100/351 lr 0.000800 loss 0.5609 (0.5531) acc@1 0.7152 (0.7284) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 01:44:35] __main__ INFO: \u001b[0mEpoch 12 Step 200/351 lr 0.000800 loss 0.5684 (0.5529) acc@1 0.7229 (0.7284) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 01:46:29] __main__ INFO: \u001b[0mEpoch 12 Step 300/351 lr 0.000800 loss 0.5514 (0.5528) acc@1 0.7326 (0.7285) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 01:47:27] __main__ INFO: \u001b[0mEpoch 12 Step 351/351 lr 0.000800 loss 0.5471 (0.5527) acc@1 0.7311 (0.7286) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 01:47:27] __main__ INFO: \u001b[0mElapsed 400.43\n",
      "\u001b[32m[2020-07-13 01:47:27] __main__ INFO: \u001b[0mVal 12\n",
      "\u001b[32m[2020-07-13 01:47:40] __main__ INFO: \u001b[0mEpoch 12 loss 0.1023 acc@1 0.9878 acc@5 1.0000\n",
      "\u001b[32m[2020-07-13 01:47:40] __main__ INFO: \u001b[0mElapsed 13.42\n",
      "\u001b[32m[2020-07-13 01:47:40] __main__ INFO: \u001b[0mTrain 13 4212\n",
      "\u001b[32m[2020-07-13 01:49:35] __main__ INFO: \u001b[0mEpoch 13 Step 100/351 lr 0.000800 loss 0.5536 (0.5537) acc@1 0.7230 (0.7275) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 01:51:29] __main__ INFO: \u001b[0mEpoch 13 Step 200/351 lr 0.000800 loss 0.5532 (0.5531) acc@1 0.7339 (0.7283) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 01:53:23] __main__ INFO: \u001b[0mEpoch 13 Step 300/351 lr 0.000800 loss 0.5353 (0.5528) acc@1 0.7244 (0.7287) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 01:54:21] __main__ INFO: \u001b[0mEpoch 13 Step 351/351 lr 0.000800 loss 0.5509 (0.5526) acc@1 0.7219 (0.7285) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 01:54:21] __main__ INFO: \u001b[0mElapsed 400.55\n",
      "\u001b[32m[2020-07-13 01:54:21] __main__ INFO: \u001b[0mVal 13\n",
      "\u001b[32m[2020-07-13 01:54:34] __main__ INFO: \u001b[0mEpoch 13 loss 0.0980 acc@1 0.9882 acc@5 0.9998\n",
      "\u001b[32m[2020-07-13 01:54:34] __main__ INFO: \u001b[0mElapsed 13.41\n",
      "\u001b[32m[2020-07-13 01:54:34] __main__ INFO: \u001b[0mTrain 14 4563\n",
      "\u001b[32m[2020-07-13 01:56:28] __main__ INFO: \u001b[0mEpoch 14 Step 100/351 lr 0.000800 loss 0.5338 (0.5526) acc@1 0.7489 (0.7283) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 01:58:22] __main__ INFO: \u001b[0mEpoch 14 Step 200/351 lr 0.000800 loss 0.5620 (0.5543) acc@1 0.7257 (0.7278) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 02:00:16] __main__ INFO: \u001b[0mEpoch 14 Step 300/351 lr 0.000800 loss 0.5669 (0.5531) acc@1 0.7337 (0.7285) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 02:01:14] __main__ INFO: \u001b[0mEpoch 14 Step 351/351 lr 0.000800 loss 0.5689 (0.5527) acc@1 0.7202 (0.7285) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 02:01:14] __main__ INFO: \u001b[0mElapsed 400.26\n",
      "\u001b[32m[2020-07-13 02:01:14] __main__ INFO: \u001b[0mVal 14\n",
      "\u001b[32m[2020-07-13 02:01:28] __main__ INFO: \u001b[0mEpoch 14 loss 0.0990 acc@1 0.9878 acc@5 0.9998\n",
      "\u001b[32m[2020-07-13 02:01:28] __main__ INFO: \u001b[0mElapsed 13.42\n",
      "\u001b[32m[2020-07-13 02:01:28] __main__ INFO: \u001b[0mTrain 15 4914\n",
      "\u001b[32m[2020-07-13 02:03:22] __main__ INFO: \u001b[0mEpoch 15 Step 100/351 lr 0.000800 loss 0.5258 (0.5490) acc@1 0.7545 (0.7305) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 02:05:16] __main__ INFO: \u001b[0mEpoch 15 Step 200/351 lr 0.000800 loss 0.5718 (0.5506) acc@1 0.7106 (0.7301) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 02:07:10] __main__ INFO: \u001b[0mEpoch 15 Step 300/351 lr 0.000800 loss 0.5317 (0.5524) acc@1 0.7566 (0.7284) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 02:08:08] __main__ INFO: \u001b[0mEpoch 15 Step 351/351 lr 0.000800 loss 0.5093 (0.5525) acc@1 0.7625 (0.7285) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 02:08:08] __main__ INFO: \u001b[0mElapsed 400.41\n",
      "\u001b[32m[2020-07-13 02:08:08] __main__ INFO: \u001b[0mVal 15\n",
      "\u001b[32m[2020-07-13 02:08:22] __main__ INFO: \u001b[0mEpoch 15 loss 0.1005 acc@1 0.9884 acc@5 1.0000\n",
      "\u001b[32m[2020-07-13 02:08:22] __main__ INFO: \u001b[0mElapsed 13.40\n",
      "\u001b[32m[2020-07-13 02:08:22] __main__ INFO: \u001b[0mTrain 16 5265\n",
      "\u001b[32m[2020-07-13 02:10:16] __main__ INFO: \u001b[0mEpoch 16 Step 100/351 lr 0.000800 loss 0.5623 (0.5522) acc@1 0.7197 (0.7289) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 02:12:10] __main__ INFO: \u001b[0mEpoch 16 Step 200/351 lr 0.000800 loss 0.5110 (0.5535) acc@1 0.7549 (0.7279) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 02:14:04] __main__ INFO: \u001b[0mEpoch 16 Step 300/351 lr 0.000800 loss 0.5283 (0.5524) acc@1 0.7475 (0.7287) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 02:15:02] __main__ INFO: \u001b[0mEpoch 16 Step 351/351 lr 0.000800 loss 0.5514 (0.5524) acc@1 0.7327 (0.7287) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 02:15:02] __main__ INFO: \u001b[0mElapsed 400.66\n",
      "\u001b[32m[2020-07-13 02:15:02] __main__ INFO: \u001b[0mVal 16\n",
      "\u001b[32m[2020-07-13 02:15:16] __main__ INFO: \u001b[0mEpoch 16 loss 0.0986 acc@1 0.9876 acc@5 0.9998\n",
      "\u001b[32m[2020-07-13 02:15:16] __main__ INFO: \u001b[0mElapsed 13.41\n",
      "\u001b[32m[2020-07-13 02:15:16] __main__ INFO: \u001b[0mTrain 17 5616\n",
      "\u001b[32m[2020-07-13 02:17:10] __main__ INFO: \u001b[0mEpoch 17 Step 100/351 lr 0.000800 loss 0.5731 (0.5494) acc@1 0.7204 (0.7318) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 02:19:04] __main__ INFO: \u001b[0mEpoch 17 Step 200/351 lr 0.000800 loss 0.5908 (0.5525) acc@1 0.6947 (0.7289) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 02:20:57] __main__ INFO: \u001b[0mEpoch 17 Step 300/351 lr 0.000800 loss 0.5652 (0.5517) acc@1 0.7190 (0.7291) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 02:21:55] __main__ INFO: \u001b[0mEpoch 17 Step 351/351 lr 0.000800 loss 0.5565 (0.5525) acc@1 0.7342 (0.7285) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 02:21:55] __main__ INFO: \u001b[0mElapsed 399.26\n",
      "\u001b[32m[2020-07-13 02:21:55] __main__ INFO: \u001b[0mVal 17\n",
      "\u001b[32m[2020-07-13 02:22:08] __main__ INFO: \u001b[0mEpoch 17 loss 0.0978 acc@1 0.9880 acc@5 0.9998\n",
      "\u001b[32m[2020-07-13 02:22:08] __main__ INFO: \u001b[0mElapsed 13.35\n",
      "\u001b[32m[2020-07-13 02:22:08] __main__ INFO: \u001b[0mTrain 18 5967\n",
      "\u001b[32m[2020-07-13 02:24:02] __main__ INFO: \u001b[0mEpoch 18 Step 100/351 lr 0.000800 loss 0.5667 (0.5525) acc@1 0.7109 (0.7294) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 02:25:56] __main__ INFO: \u001b[0mEpoch 18 Step 200/351 lr 0.000800 loss 0.5723 (0.5543) acc@1 0.7072 (0.7276) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 02:27:50] __main__ INFO: \u001b[0mEpoch 18 Step 300/351 lr 0.000800 loss 0.5271 (0.5529) acc@1 0.7483 (0.7286) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 02:28:48] __main__ INFO: \u001b[0mEpoch 18 Step 351/351 lr 0.000800 loss 0.5491 (0.5526) acc@1 0.7281 (0.7286) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 02:28:48] __main__ INFO: \u001b[0mElapsed 399.53\n",
      "\u001b[32m[2020-07-13 02:28:48] __main__ INFO: \u001b[0mVal 18\n",
      "\u001b[32m[2020-07-13 02:29:01] __main__ INFO: \u001b[0mEpoch 18 loss 0.1006 acc@1 0.9882 acc@5 0.9998\n",
      "\u001b[32m[2020-07-13 02:29:01] __main__ INFO: \u001b[0mElapsed 13.46\n",
      "\u001b[32m[2020-07-13 02:29:01] __main__ INFO: \u001b[0mTrain 19 6318\n",
      "\u001b[32m[2020-07-13 02:30:55] __main__ INFO: \u001b[0mEpoch 19 Step 100/351 lr 0.000800 loss 0.5835 (0.5547) acc@1 0.7000 (0.7285) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 02:32:48] __main__ INFO: \u001b[0mEpoch 19 Step 200/351 lr 0.000800 loss 0.5793 (0.5540) acc@1 0.7046 (0.7281) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 02:34:42] __main__ INFO: \u001b[0mEpoch 19 Step 300/351 lr 0.000800 loss 0.5462 (0.5532) acc@1 0.7218 (0.7282) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 02:35:40] __main__ INFO: \u001b[0mEpoch 19 Step 351/351 lr 0.000800 loss 0.5294 (0.5525) acc@1 0.7462 (0.7286) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 02:35:40] __main__ INFO: \u001b[0mElapsed 398.23\n",
      "\u001b[32m[2020-07-13 02:35:40] __main__ INFO: \u001b[0mVal 19\n",
      "\u001b[32m[2020-07-13 02:35:53] __main__ INFO: \u001b[0mEpoch 19 loss 0.0965 acc@1 0.9880 acc@5 0.9998\n",
      "\u001b[32m[2020-07-13 02:35:53] __main__ INFO: \u001b[0mElapsed 13.34\n",
      "\u001b[32m[2020-07-13 02:35:53] __main__ INFO: \u001b[0mTrain 20 6669\n",
      "\u001b[32m[2020-07-13 02:37:46] __main__ INFO: \u001b[0mEpoch 20 Step 100/351 lr 0.000800 loss 0.5537 (0.5523) acc@1 0.7295 (0.7279) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 02:39:40] __main__ INFO: \u001b[0mEpoch 20 Step 200/351 lr 0.000800 loss 0.5588 (0.5530) acc@1 0.7244 (0.7280) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 02:41:33] __main__ INFO: \u001b[0mEpoch 20 Step 300/351 lr 0.000800 loss 0.5583 (0.5524) acc@1 0.7284 (0.7289) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 02:42:31] __main__ INFO: \u001b[0mEpoch 20 Step 351/351 lr 0.000800 loss 0.5634 (0.5525) acc@1 0.7261 (0.7286) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 02:42:31] __main__ INFO: \u001b[0mElapsed 397.91\n",
      "\u001b[32m[2020-07-13 02:42:31] __main__ INFO: \u001b[0mVal 20\n",
      "\u001b[32m[2020-07-13 02:42:44] __main__ INFO: \u001b[0mEpoch 20 loss 0.0991 acc@1 0.9882 acc@5 1.0000\n",
      "\u001b[32m[2020-07-13 02:42:44] __main__ INFO: \u001b[0mElapsed 13.35\n",
      "\u001b[32m[2020-07-13 02:42:44] __main__ INFO: \u001b[0mTrain 21 7020\n",
      "\u001b[32m[2020-07-13 02:44:38] __main__ INFO: \u001b[0mEpoch 21 Step 100/351 lr 0.000800 loss 0.5526 (0.5537) acc@1 0.7179 (0.7270) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 02:46:32] __main__ INFO: \u001b[0mEpoch 21 Step 200/351 lr 0.000800 loss 0.5666 (0.5526) acc@1 0.7242 (0.7279) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 02:48:26] __main__ INFO: \u001b[0mEpoch 21 Step 300/351 lr 0.000800 loss 0.5794 (0.5524) acc@1 0.6984 (0.7282) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 02:49:24] __main__ INFO: \u001b[0mEpoch 21 Step 351/351 lr 0.000800 loss 0.5018 (0.5526) acc@1 0.7759 (0.7285) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 02:49:24] __main__ INFO: \u001b[0mElapsed 399.89\n",
      "\u001b[32m[2020-07-13 02:49:24] __main__ INFO: \u001b[0mVal 21\n",
      "\u001b[32m[2020-07-13 02:49:37] __main__ INFO: \u001b[0mEpoch 21 loss 0.0991 acc@1 0.9878 acc@5 0.9998\n",
      "\u001b[32m[2020-07-13 02:49:37] __main__ INFO: \u001b[0mElapsed 13.39\n",
      "\u001b[32m[2020-07-13 02:49:37] __main__ INFO: \u001b[0mTrain 22 7371\n",
      "\u001b[32m[2020-07-13 02:51:32] __main__ INFO: \u001b[0mEpoch 22 Step 100/351 lr 0.000800 loss 0.5516 (0.5519) acc@1 0.7327 (0.7290) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 02:53:26] __main__ INFO: \u001b[0mEpoch 22 Step 200/351 lr 0.000800 loss 0.5485 (0.5515) acc@1 0.7150 (0.7290) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 02:55:20] __main__ INFO: \u001b[0mEpoch 22 Step 300/351 lr 0.000800 loss 0.5558 (0.5527) acc@1 0.7311 (0.7284) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 02:56:18] __main__ INFO: \u001b[0mEpoch 22 Step 351/351 lr 0.000800 loss 0.5250 (0.5524) acc@1 0.7471 (0.7286) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 02:56:18] __main__ INFO: \u001b[0mElapsed 400.42\n",
      "\u001b[32m[2020-07-13 02:56:18] __main__ INFO: \u001b[0mVal 22\n",
      "\u001b[32m[2020-07-13 02:56:31] __main__ INFO: \u001b[0mEpoch 22 loss 0.0968 acc@1 0.9878 acc@5 0.9998\n",
      "\u001b[32m[2020-07-13 02:56:31] __main__ INFO: \u001b[0mElapsed 13.41\n",
      "\u001b[32m[2020-07-13 02:56:31] __main__ INFO: \u001b[0mTrain 23 7722\n",
      "\u001b[32m[2020-07-13 02:58:25] __main__ INFO: \u001b[0mEpoch 23 Step 100/351 lr 0.000800 loss 0.5695 (0.5532) acc@1 0.7172 (0.7281) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 03:00:20] __main__ INFO: \u001b[0mEpoch 23 Step 200/351 lr 0.000800 loss 0.5743 (0.5520) acc@1 0.7184 (0.7288) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 03:02:14] __main__ INFO: \u001b[0mEpoch 23 Step 300/351 lr 0.000800 loss 0.5342 (0.5525) acc@1 0.7396 (0.7286) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 03:03:12] __main__ INFO: \u001b[0mEpoch 23 Step 351/351 lr 0.000800 loss 0.5710 (0.5525) acc@1 0.7190 (0.7285) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 03:03:12] __main__ INFO: \u001b[0mElapsed 400.33\n",
      "\u001b[32m[2020-07-13 03:03:12] __main__ INFO: \u001b[0mVal 23\n",
      "\u001b[32m[2020-07-13 03:03:25] __main__ INFO: \u001b[0mEpoch 23 loss 0.0977 acc@1 0.9880 acc@5 0.9998\n",
      "\u001b[32m[2020-07-13 03:03:25] __main__ INFO: \u001b[0mElapsed 13.36\n",
      "\u001b[32m[2020-07-13 03:03:25] __main__ INFO: \u001b[0mTrain 24 8073\n",
      "\u001b[32m[2020-07-13 03:05:19] __main__ INFO: \u001b[0mEpoch 24 Step 100/351 lr 0.000800 loss 0.5385 (0.5528) acc@1 0.7473 (0.7272) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 03:07:13] __main__ INFO: \u001b[0mEpoch 24 Step 200/351 lr 0.000800 loss 0.5534 (0.5521) acc@1 0.7246 (0.7283) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 03:09:07] __main__ INFO: \u001b[0mEpoch 24 Step 300/351 lr 0.000800 loss 0.5072 (0.5518) acc@1 0.7707 (0.7290) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 03:10:05] __main__ INFO: \u001b[0mEpoch 24 Step 351/351 lr 0.000800 loss 0.5664 (0.5526) acc@1 0.7186 (0.7285) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 03:10:05] __main__ INFO: \u001b[0mElapsed 400.25\n",
      "\u001b[32m[2020-07-13 03:10:05] __main__ INFO: \u001b[0mVal 24\n",
      "\u001b[32m[2020-07-13 03:10:19] __main__ INFO: \u001b[0mEpoch 24 loss 0.1002 acc@1 0.9872 acc@5 0.9998\n",
      "\u001b[32m[2020-07-13 03:10:19] __main__ INFO: \u001b[0mElapsed 13.45\n",
      "\u001b[32m[2020-07-13 03:10:19] __main__ INFO: \u001b[0mTrain 25 8424\n",
      "\u001b[32m[2020-07-13 03:12:13] __main__ INFO: \u001b[0mEpoch 25 Step 100/351 lr 0.000800 loss 0.5142 (0.5514) acc@1 0.7564 (0.7290) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 03:14:07] __main__ INFO: \u001b[0mEpoch 25 Step 200/351 lr 0.000800 loss 0.5645 (0.5527) acc@1 0.7096 (0.7279) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 03:16:01] __main__ INFO: \u001b[0mEpoch 25 Step 300/351 lr 0.000800 loss 0.5304 (0.5519) acc@1 0.7478 (0.7289) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 03:16:59] __main__ INFO: \u001b[0mEpoch 25 Step 351/351 lr 0.000800 loss 0.5937 (0.5525) acc@1 0.7093 (0.7285) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 03:16:59] __main__ INFO: \u001b[0mElapsed 400.36\n",
      "\u001b[32m[2020-07-13 03:16:59] __main__ INFO: \u001b[0mVal 25\n",
      "\u001b[32m[2020-07-13 03:17:13] __main__ INFO: \u001b[0mEpoch 25 loss 0.0992 acc@1 0.9880 acc@5 1.0000\n",
      "\u001b[32m[2020-07-13 03:17:13] __main__ INFO: \u001b[0mElapsed 13.45\n",
      "\u001b[32m[2020-07-13 03:17:13] __main__ INFO: \u001b[0mTrain 26 8775\n",
      "\u001b[32m[2020-07-13 03:19:07] __main__ INFO: \u001b[0mEpoch 26 Step 100/351 lr 0.000800 loss 0.5578 (0.5530) acc@1 0.7178 (0.7287) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 03:21:01] __main__ INFO: \u001b[0mEpoch 26 Step 200/351 lr 0.000800 loss 0.5439 (0.5536) acc@1 0.7290 (0.7279) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 03:22:55] __main__ INFO: \u001b[0mEpoch 26 Step 300/351 lr 0.000800 loss 0.5703 (0.5531) acc@1 0.7270 (0.7282) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 03:23:53] __main__ INFO: \u001b[0mEpoch 26 Step 351/351 lr 0.000800 loss 0.5349 (0.5525) acc@1 0.7488 (0.7284) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 03:23:53] __main__ INFO: \u001b[0mElapsed 400.40\n",
      "\u001b[32m[2020-07-13 03:23:53] __main__ INFO: \u001b[0mVal 26\n",
      "\u001b[32m[2020-07-13 03:24:06] __main__ INFO: \u001b[0mEpoch 26 loss 0.0959 acc@1 0.9890 acc@5 1.0000\n",
      "\u001b[32m[2020-07-13 03:24:06] __main__ INFO: \u001b[0mElapsed 13.44\n",
      "\u001b[32m[2020-07-13 03:24:06] __main__ INFO: \u001b[0mTrain 27 9126\n",
      "\u001b[32m[2020-07-13 03:26:01] __main__ INFO: \u001b[0mEpoch 27 Step 100/351 lr 0.000800 loss 0.5801 (0.5530) acc@1 0.6976 (0.7292) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 03:27:55] __main__ INFO: \u001b[0mEpoch 27 Step 200/351 lr 0.000800 loss 0.5695 (0.5522) acc@1 0.7213 (0.7294) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 03:29:49] __main__ INFO: \u001b[0mEpoch 27 Step 300/351 lr 0.000800 loss 0.5587 (0.5530) acc@1 0.7209 (0.7283) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 03:30:48] __main__ INFO: \u001b[0mEpoch 27 Step 351/351 lr 0.000800 loss 0.5318 (0.5526) acc@1 0.7445 (0.7286) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 03:30:48] __main__ INFO: \u001b[0mElapsed 401.18\n",
      "\u001b[32m[2020-07-13 03:30:48] __main__ INFO: \u001b[0mVal 27\n",
      "\u001b[32m[2020-07-13 03:31:01] __main__ INFO: \u001b[0mEpoch 27 loss 0.0986 acc@1 0.9884 acc@5 1.0000\n",
      "\u001b[32m[2020-07-13 03:31:01] __main__ INFO: \u001b[0mElapsed 13.44\n",
      "\u001b[32m[2020-07-13 03:31:01] __main__ INFO: \u001b[0mTrain 28 9477\n",
      "\u001b[32m[2020-07-13 03:32:55] __main__ INFO: \u001b[0mEpoch 28 Step 100/351 lr 0.000800 loss 0.5601 (0.5492) acc@1 0.7320 (0.7314) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 03:34:50] __main__ INFO: \u001b[0mEpoch 28 Step 200/351 lr 0.000800 loss 0.5616 (0.5527) acc@1 0.7198 (0.7287) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 03:36:44] __main__ INFO: \u001b[0mEpoch 28 Step 300/351 lr 0.000800 loss 0.5565 (0.5527) acc@1 0.7260 (0.7283) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 03:37:42] __main__ INFO: \u001b[0mEpoch 28 Step 351/351 lr 0.000800 loss 0.5742 (0.5525) acc@1 0.7202 (0.7286) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 03:37:42] __main__ INFO: \u001b[0mElapsed 400.90\n",
      "\u001b[32m[2020-07-13 03:37:42] __main__ INFO: \u001b[0mVal 28\n",
      "\u001b[32m[2020-07-13 03:37:55] __main__ INFO: \u001b[0mEpoch 28 loss 0.0968 acc@1 0.9894 acc@5 0.9998\n",
      "\u001b[32m[2020-07-13 03:37:55] __main__ INFO: \u001b[0mElapsed 13.42\n",
      "\u001b[32m[2020-07-13 03:37:55] __main__ INFO: \u001b[0mTrain 29 9828\n",
      "\u001b[32m[2020-07-13 03:39:49] __main__ INFO: \u001b[0mEpoch 29 Step 100/351 lr 0.000800 loss 0.5077 (0.5509) acc@1 0.7574 (0.7305) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 03:41:43] __main__ INFO: \u001b[0mEpoch 29 Step 200/351 lr 0.000800 loss 0.5208 (0.5518) acc@1 0.7562 (0.7294) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 03:43:38] __main__ INFO: \u001b[0mEpoch 29 Step 300/351 lr 0.000800 loss 0.5641 (0.5526) acc@1 0.7247 (0.7284) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 03:44:36] __main__ INFO: \u001b[0mEpoch 29 Step 351/351 lr 0.000800 loss 0.5645 (0.5525) acc@1 0.7138 (0.7286) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 03:44:36] __main__ INFO: \u001b[0mElapsed 400.45\n",
      "\u001b[32m[2020-07-13 03:44:36] __main__ INFO: \u001b[0mVal 29\n",
      "\u001b[32m[2020-07-13 03:44:49] __main__ INFO: \u001b[0mEpoch 29 loss 0.0944 acc@1 0.9888 acc@5 1.0000\n",
      "\u001b[32m[2020-07-13 03:44:49] __main__ INFO: \u001b[0mElapsed 13.45\n",
      "\u001b[32m[2020-07-13 03:44:49] __main__ INFO: \u001b[0mTrain 30 10179\n"
     ]
    }
   ],
   "source": [
    "## Model hung during last 100 epochs -> Finish training from last checkpoint with augmented data\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "!python train.py --config /home/ec2-user/SageMaker/experiments/wrn_28_10_cm_1_1/exp00/config.yaml \\\n",
    "    train.checkpoint /home/ec2-user/SageMaker/experiments/wrn_28_10_cm_1_1/exp00/checkpoint_00300.pth \\\n",
    "    dataset.name CIFAR10_CM_1 \\\n",
    "    model.wrn.depth 28 \\\n",
    "    model.wrn.widening_factor 10 \\\n",
    "    train.batch_size 128 \\\n",
    "    train.base_lr 0.0008 \\\n",
    "    train.output_dir /home/ec2-user/SageMaker/experiments/wrn_28_10_cm_1_1/exp00_resume300 \\\n",
    "    scheduler.epochs 100\n",
    "\n",
    "#### Set LEARNING RATE based on ending LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-13 12:25:23] __main__ INFO: \u001b[0mdevice: cuda\n",
      "cudnn:\n",
      "  benchmark: True\n",
      "  deterministic: False\n",
      "dataset:\n",
      "  name: CIFAR10\n",
      "  dataset_dir: ~/.torch/datasets/CIFAR10\n",
      "  image_size: 32\n",
      "  n_channels: 3\n",
      "  n_classes: 10\n",
      "model:\n",
      "  type: cifar\n",
      "  name: wrn\n",
      "  init_mode: kaiming_fan_in\n",
      "  vgg:\n",
      "    n_channels: [64, 128, 256, 512, 512]\n",
      "    n_layers: [2, 2, 3, 3, 3]\n",
      "    use_bn: True\n",
      "  resnet:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "  resnet_preact:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "  wrn:\n",
      "    depth: 28\n",
      "    initial_channels: 16\n",
      "    widening_factor: 10\n",
      "    drop_rate: 0.0\n",
      "  densenet:\n",
      "    depth: 100\n",
      "    n_blocks: [6, 12, 24, 16]\n",
      "    block_type: bottleneck\n",
      "    growth_rate: 12\n",
      "    drop_rate: 0.0\n",
      "    compression_rate: 0.5\n",
      "  pyramidnet:\n",
      "    depth: 272\n",
      "    n_blocks: [3, 24, 36, 3]\n",
      "    initial_channels: 16\n",
      "    block_type: bottleneck\n",
      "    alpha: 200\n",
      "  resnext:\n",
      "    depth: 29\n",
      "    n_blocks: [3, 4, 6, 3]\n",
      "    initial_channels: 64\n",
      "    cardinality: 8\n",
      "    base_channels: 4\n",
      "  shake_shake:\n",
      "    depth: 26\n",
      "    initial_channels: 96\n",
      "    shake_forward: True\n",
      "    shake_backward: True\n",
      "    shake_image: True\n",
      "  se_resnet_preact:\n",
      "    depth: 110\n",
      "    initial_channels: 16\n",
      "    se_reduction: 16\n",
      "    block_type: basic\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "train:\n",
      "  checkpoint: /home/ec2-user/SageMaker/experiments/wrn_28_10_cm_1_1/exp00_resume300/checkpoint_00400.pth\n",
      "  resume: False\n",
      "  precision: O0\n",
      "  batch_size: 128\n",
      "  subdivision: 1\n",
      "  optimizer: sgd\n",
      "  base_lr: 0.00016\n",
      "  momentum: 0.9\n",
      "  nesterov: True\n",
      "  weight_decay: 0.0005\n",
      "  no_weight_decay_on_bn: False\n",
      "  gradient_clip: 0\n",
      "  start_epoch: 0\n",
      "  seed: 0\n",
      "  val_first: True\n",
      "  val_period: 1\n",
      "  val_ratio: 0.1\n",
      "  use_test_as_val: False\n",
      "  output_dir: /home/ec2-user/SageMaker/experiments/wrn_28_10_cm_1_1/exp00_resume400_50\n",
      "  log_period: 100\n",
      "  checkpoint_period: 100\n",
      "  use_tensorboard: True\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: True\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "  distributed: False\n",
      "  dist:\n",
      "    backend: nccl\n",
      "    init_method: env://\n",
      "    world_size: -1\n",
      "    node_rank: -1\n",
      "    local_rank: 0\n",
      "    use_sync_bn: False\n",
      "tensorboard:\n",
      "  train_images: False\n",
      "  val_images: False\n",
      "  model_params: False\n",
      "optim:\n",
      "  adam:\n",
      "    betas: (0.9, 0.999)\n",
      "  lars:\n",
      "    eps: 1e-09\n",
      "    threshold: 0.01\n",
      "  adabound:\n",
      "    betas: (0.9, 0.999)\n",
      "    final_lr: 0.1\n",
      "    gamma: 0.001\n",
      "scheduler:\n",
      "  epochs: 50\n",
      "  warmup:\n",
      "    type: none\n",
      "    epochs: 0\n",
      "    start_factor: 0.001\n",
      "    exponent: 4\n",
      "  type: multistep\n",
      "  milestones: [60, 120, 160]\n",
      "  lr_decay: 0.2\n",
      "  lr_min_factor: 0.001\n",
      "  T0: 10\n",
      "  T_mul: 1.0\n",
      "validation:\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: False\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "augmentation:\n",
      "  use_random_crop: True\n",
      "  use_random_horizontal_flip: True\n",
      "  use_cutout: False\n",
      "  use_random_erasing: False\n",
      "  use_dual_cutout: False\n",
      "  use_mixup: False\n",
      "  use_ricap: False\n",
      "  use_cutmix: False\n",
      "  use_label_smoothing: False\n",
      "  random_crop:\n",
      "    padding: 4\n",
      "    fill: 0\n",
      "    padding_mode: constant\n",
      "  random_horizontal_flip:\n",
      "    prob: 0.5\n",
      "  cutout:\n",
      "    prob: 1.0\n",
      "    mask_size: 16\n",
      "    cut_inside: False\n",
      "    mask_color: 0\n",
      "    dual_cutout_alpha: 0.1\n",
      "  random_erasing:\n",
      "    prob: 0.5\n",
      "    area_ratio_range: [0.02, 0.4]\n",
      "    min_aspect_ratio: 0.3\n",
      "    max_attempt: 20\n",
      "  mixup:\n",
      "    alpha: 1.0\n",
      "  ricap:\n",
      "    beta: 0.3\n",
      "  cutmix:\n",
      "    alpha: 1.0\n",
      "  label_smoothing:\n",
      "    epsilon: 0.1\n",
      "tta:\n",
      "  use_resize: False\n",
      "  use_center_crop: False\n",
      "  resize: 256\n",
      "test:\n",
      "  checkpoint: ''\n",
      "  output_dir: ''\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    pin_memory: False\n",
      "\u001b[32m[2020-07-13 12:25:23] __main__ INFO: \u001b[0menv_info:\n",
      "  pytorch_version: 1.4.0\n",
      "  cuda_version: 10.1\n",
      "  cudnn_version: 7603\n",
      "  num_gpus: 1\n",
      "  gpu_name: Tesla K80\n",
      "  gpu_capability: 3.7\n",
      "Files already downloaded and verified\n",
      "\u001b[32m[2020-07-13 12:25:27] __main__ INFO: \u001b[0mMACs  : 5.25G\n",
      "\u001b[32m[2020-07-13 12:25:27] __main__ INFO: \u001b[0m#params: 36.48M\n",
      "Selected optimization level O0:  Pure FP32 training.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\",)\n",
      "\u001b[32m[2020-07-13 12:25:28] __main__ INFO: \u001b[0mVal 0\n",
      "\u001b[32m[2020-07-13 12:25:47] __main__ INFO: \u001b[0mEpoch 0 loss 0.2298 acc@1 0.9470 acc@5 0.9950\n",
      "\u001b[32m[2020-07-13 12:25:47] __main__ INFO: \u001b[0mElapsed 19.85\n",
      "\u001b[32m[2020-07-13 12:25:47] __main__ INFO: \u001b[0mTrain 1 0\n",
      "\u001b[32m[2020-07-13 12:27:46] __main__ INFO: \u001b[0mEpoch 1 Step 100/351 lr 0.000160 loss 0.0207 (0.0524) acc@1 0.9922 (0.9905) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-07-13 12:29:39] __main__ INFO: \u001b[0mEpoch 1 Step 200/351 lr 0.000160 loss 0.0130 (0.0432) acc@1 1.0000 (0.9911) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-07-13 12:31:33] __main__ INFO: \u001b[0mEpoch 1 Step 300/351 lr 0.000160 loss 0.0368 (0.0395) acc@1 0.9844 (0.9915) acc@5 1.0000 (0.9996)\n",
      "\u001b[32m[2020-07-13 12:32:31] __main__ INFO: \u001b[0mEpoch 1 Step 351/351 lr 0.000160 loss 0.0206 (0.0374) acc@1 1.0000 (0.9918) acc@5 1.0000 (0.9997)\n",
      "\u001b[32m[2020-07-13 12:32:31] __main__ INFO: \u001b[0mElapsed 403.68\n",
      "\u001b[32m[2020-07-13 12:32:31] __main__ INFO: \u001b[0mVal 1\n",
      "\u001b[32m[2020-07-13 12:32:44] __main__ INFO: \u001b[0mEpoch 1 loss 0.1394 acc@1 0.9626 acc@5 0.9980\n",
      "\u001b[32m[2020-07-13 12:32:44] __main__ INFO: \u001b[0mElapsed 13.39\n",
      "\u001b[32m[2020-07-13 12:32:44] __main__ INFO: \u001b[0mTrain 2 351\n",
      "\u001b[32m[2020-07-13 12:34:38] __main__ INFO: \u001b[0mEpoch 2 Step 100/351 lr 0.000160 loss 0.0259 (0.0264) acc@1 0.9844 (0.9934) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-13 12:36:32] __main__ INFO: \u001b[0mEpoch 2 Step 200/351 lr 0.000160 loss 0.0068 (0.0225) acc@1 1.0000 (0.9943) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 12:38:26] __main__ INFO: \u001b[0mEpoch 2 Step 300/351 lr 0.000160 loss 0.0093 (0.0213) acc@1 1.0000 (0.9947) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-13 12:39:24] __main__ INFO: \u001b[0mEpoch 2 Step 351/351 lr 0.000160 loss 0.0135 (0.0211) acc@1 1.0000 (0.9948) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-13 12:39:24] __main__ INFO: \u001b[0mElapsed 399.69\n",
      "\u001b[32m[2020-07-13 12:39:24] __main__ INFO: \u001b[0mVal 2\n",
      "\u001b[32m[2020-07-13 12:39:37] __main__ INFO: \u001b[0mEpoch 2 loss 0.1381 acc@1 0.9656 acc@5 0.9972\n",
      "\u001b[32m[2020-07-13 12:39:37] __main__ INFO: \u001b[0mElapsed 13.37\n",
      "\u001b[32m[2020-07-13 12:39:37] __main__ INFO: \u001b[0mTrain 3 702\n",
      "\u001b[32m[2020-07-13 12:41:31] __main__ INFO: \u001b[0mEpoch 3 Step 100/351 lr 0.000160 loss 0.0431 (0.0134) acc@1 0.9844 (0.9973) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 12:43:25] __main__ INFO: \u001b[0mEpoch 3 Step 200/351 lr 0.000160 loss 0.0348 (0.0154) acc@1 0.9922 (0.9964) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-13 12:45:19] __main__ INFO: \u001b[0mEpoch 3 Step 300/351 lr 0.000160 loss 0.0192 (0.0150) acc@1 1.0000 (0.9964) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-13 12:46:17] __main__ INFO: \u001b[0mEpoch 3 Step 351/351 lr 0.000160 loss 0.0585 (0.0155) acc@1 0.9922 (0.9963) acc@5 1.0000 (0.9999)\n",
      "\u001b[32m[2020-07-13 12:46:17] __main__ INFO: \u001b[0mElapsed 399.69\n",
      "\u001b[32m[2020-07-13 12:46:17] __main__ INFO: \u001b[0mVal 3\n",
      "\u001b[32m[2020-07-13 12:46:31] __main__ INFO: \u001b[0mEpoch 3 loss 0.1467 acc@1 0.9612 acc@5 0.9970\n",
      "\u001b[32m[2020-07-13 12:46:31] __main__ INFO: \u001b[0mElapsed 13.42\n",
      "\u001b[32m[2020-07-13 12:46:31] __main__ INFO: \u001b[0mTrain 4 1053\n",
      "\u001b[32m[2020-07-13 12:48:25] __main__ INFO: \u001b[0mEpoch 4 Step 100/351 lr 0.000160 loss 0.0077 (0.0138) acc@1 1.0000 (0.9966) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 12:50:19] __main__ INFO: \u001b[0mEpoch 4 Step 200/351 lr 0.000160 loss 0.0049 (0.0132) acc@1 1.0000 (0.9967) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 12:52:13] __main__ INFO: \u001b[0mEpoch 4 Step 300/351 lr 0.000160 loss 0.0058 (0.0133) acc@1 1.0000 (0.9965) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 12:53:11] __main__ INFO: \u001b[0mEpoch 4 Step 351/351 lr 0.000160 loss 0.0045 (0.0130) acc@1 1.0000 (0.9966) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 12:53:11] __main__ INFO: \u001b[0mElapsed 399.97\n",
      "\u001b[32m[2020-07-13 12:53:11] __main__ INFO: \u001b[0mVal 4\n",
      "\u001b[32m[2020-07-13 12:53:24] __main__ INFO: \u001b[0mEpoch 4 loss 0.1483 acc@1 0.9628 acc@5 0.9974\n",
      "\u001b[32m[2020-07-13 12:53:24] __main__ INFO: \u001b[0mElapsed 13.45\n",
      "\u001b[32m[2020-07-13 12:53:24] __main__ INFO: \u001b[0mTrain 5 1404\n",
      "\u001b[32m[2020-07-13 12:55:18] __main__ INFO: \u001b[0mEpoch 5 Step 100/351 lr 0.000160 loss 0.0034 (0.0100) acc@1 1.0000 (0.9977) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 12:57:12] __main__ INFO: \u001b[0mEpoch 5 Step 200/351 lr 0.000160 loss 0.0484 (0.0109) acc@1 0.9844 (0.9972) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 12:59:05] __main__ INFO: \u001b[0mEpoch 5 Step 300/351 lr 0.000160 loss 0.0081 (0.0113) acc@1 0.9922 (0.9972) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 13:00:03] __main__ INFO: \u001b[0mEpoch 5 Step 351/351 lr 0.000160 loss 0.0050 (0.0110) acc@1 1.0000 (0.9972) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 13:00:03] __main__ INFO: \u001b[0mElapsed 399.37\n",
      "\u001b[32m[2020-07-13 13:00:03] __main__ INFO: \u001b[0mVal 5\n",
      "\u001b[32m[2020-07-13 13:00:17] __main__ INFO: \u001b[0mEpoch 5 loss 0.1523 acc@1 0.9622 acc@5 0.9976\n",
      "\u001b[32m[2020-07-13 13:00:17] __main__ INFO: \u001b[0mElapsed 13.38\n",
      "\u001b[32m[2020-07-13 13:00:17] __main__ INFO: \u001b[0mTrain 6 1755\n",
      "\u001b[32m[2020-07-13 13:02:11] __main__ INFO: \u001b[0mEpoch 6 Step 100/351 lr 0.000160 loss 0.0479 (0.0099) acc@1 0.9922 (0.9975) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 13:04:05] __main__ INFO: \u001b[0mEpoch 6 Step 200/351 lr 0.000160 loss 0.0036 (0.0093) acc@1 1.0000 (0.9977) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 13:05:59] __main__ INFO: \u001b[0mEpoch 6 Step 300/351 lr 0.000160 loss 0.0034 (0.0105) acc@1 1.0000 (0.9975) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 13:06:57] __main__ INFO: \u001b[0mEpoch 6 Step 351/351 lr 0.000160 loss 0.0065 (0.0103) acc@1 1.0000 (0.9976) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 13:06:57] __main__ INFO: \u001b[0mElapsed 400.17\n",
      "\u001b[32m[2020-07-13 13:06:57] __main__ INFO: \u001b[0mVal 6\n",
      "\u001b[32m[2020-07-13 13:07:10] __main__ INFO: \u001b[0mEpoch 6 loss 0.1499 acc@1 0.9646 acc@5 0.9978\n",
      "\u001b[32m[2020-07-13 13:07:10] __main__ INFO: \u001b[0mElapsed 13.45\n",
      "\u001b[32m[2020-07-13 13:07:10] __main__ INFO: \u001b[0mTrain 7 2106\n",
      "\u001b[32m[2020-07-13 13:09:04] __main__ INFO: \u001b[0mEpoch 7 Step 100/351 lr 0.000160 loss 0.0090 (0.0095) acc@1 1.0000 (0.9979) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 13:10:58] __main__ INFO: \u001b[0mEpoch 7 Step 200/351 lr 0.000160 loss 0.0027 (0.0094) acc@1 1.0000 (0.9979) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 13:12:52] __main__ INFO: \u001b[0mEpoch 7 Step 300/351 lr 0.000160 loss 0.0021 (0.0087) acc@1 1.0000 (0.9980) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 13:13:50] __main__ INFO: \u001b[0mEpoch 7 Step 351/351 lr 0.000160 loss 0.0038 (0.0090) acc@1 1.0000 (0.9979) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 13:13:50] __main__ INFO: \u001b[0mElapsed 399.21\n",
      "\u001b[32m[2020-07-13 13:13:50] __main__ INFO: \u001b[0mVal 7\n",
      "\u001b[32m[2020-07-13 13:14:03] __main__ INFO: \u001b[0mEpoch 7 loss 0.1470 acc@1 0.9648 acc@5 0.9972\n",
      "\u001b[32m[2020-07-13 13:14:03] __main__ INFO: \u001b[0mElapsed 13.39\n",
      "\u001b[32m[2020-07-13 13:14:03] __main__ INFO: \u001b[0mTrain 8 2457\n",
      "\u001b[32m[2020-07-13 13:15:57] __main__ INFO: \u001b[0mEpoch 8 Step 100/351 lr 0.000160 loss 0.0030 (0.0066) acc@1 1.0000 (0.9984) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 13:17:50] __main__ INFO: \u001b[0mEpoch 8 Step 200/351 lr 0.000160 loss 0.0034 (0.0068) acc@1 1.0000 (0.9985) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 13:19:44] __main__ INFO: \u001b[0mEpoch 8 Step 300/351 lr 0.000160 loss 0.0021 (0.0070) acc@1 1.0000 (0.9985) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 13:20:42] __main__ INFO: \u001b[0mEpoch 8 Step 351/351 lr 0.000160 loss 0.0193 (0.0074) acc@1 0.9922 (0.9983) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 13:20:42] __main__ INFO: \u001b[0mElapsed 399.24\n",
      "\u001b[32m[2020-07-13 13:20:42] __main__ INFO: \u001b[0mVal 8\n",
      "\u001b[32m[2020-07-13 13:20:56] __main__ INFO: \u001b[0mEpoch 8 loss 0.1521 acc@1 0.9648 acc@5 0.9970\n",
      "\u001b[32m[2020-07-13 13:20:56] __main__ INFO: \u001b[0mElapsed 13.43\n",
      "\u001b[32m[2020-07-13 13:20:56] __main__ INFO: \u001b[0mTrain 9 2808\n",
      "\u001b[32m[2020-07-13 13:22:50] __main__ INFO: \u001b[0mEpoch 9 Step 100/351 lr 0.000160 loss 0.0134 (0.0074) acc@1 0.9922 (0.9984) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 13:24:43] __main__ INFO: \u001b[0mEpoch 9 Step 200/351 lr 0.000160 loss 0.0025 (0.0067) acc@1 1.0000 (0.9986) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 13:26:37] __main__ INFO: \u001b[0mEpoch 9 Step 300/351 lr 0.000160 loss 0.0127 (0.0064) acc@1 0.9922 (0.9987) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 13:27:35] __main__ INFO: \u001b[0mEpoch 9 Step 351/351 lr 0.000160 loss 0.0022 (0.0062) acc@1 1.0000 (0.9988) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 13:27:35] __main__ INFO: \u001b[0mElapsed 399.37\n",
      "\u001b[32m[2020-07-13 13:27:35] __main__ INFO: \u001b[0mVal 9\n",
      "\u001b[32m[2020-07-13 13:27:48] __main__ INFO: \u001b[0mEpoch 9 loss 0.1575 acc@1 0.9648 acc@5 0.9978\n",
      "\u001b[32m[2020-07-13 13:27:48] __main__ INFO: \u001b[0mElapsed 13.41\n",
      "\u001b[32m[2020-07-13 13:27:48] __main__ INFO: \u001b[0mTrain 10 3159\n",
      "\u001b[32m[2020-07-13 13:29:42] __main__ INFO: \u001b[0mEpoch 10 Step 100/351 lr 0.000160 loss 0.0027 (0.0049) acc@1 1.0000 (0.9988) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 13:31:36] __main__ INFO: \u001b[0mEpoch 10 Step 200/351 lr 0.000160 loss 0.0019 (0.0049) acc@1 1.0000 (0.9990) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 13:33:30] __main__ INFO: \u001b[0mEpoch 10 Step 300/351 lr 0.000160 loss 0.0020 (0.0055) acc@1 1.0000 (0.9988) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 13:34:28] __main__ INFO: \u001b[0mEpoch 10 Step 351/351 lr 0.000160 loss 0.0025 (0.0053) acc@1 1.0000 (0.9988) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 13:34:28] __main__ INFO: \u001b[0mElapsed 399.22\n",
      "\u001b[32m[2020-07-13 13:34:28] __main__ INFO: \u001b[0mVal 10\n",
      "\u001b[32m[2020-07-13 13:34:41] __main__ INFO: \u001b[0mEpoch 10 loss 0.1584 acc@1 0.9660 acc@5 0.9976\n",
      "\u001b[32m[2020-07-13 13:34:41] __main__ INFO: \u001b[0mElapsed 13.42\n",
      "\u001b[32m[2020-07-13 13:34:41] __main__ INFO: \u001b[0mTrain 11 3510\n",
      "\u001b[32m[2020-07-13 13:36:35] __main__ INFO: \u001b[0mEpoch 11 Step 100/351 lr 0.000160 loss 0.0021 (0.0053) acc@1 1.0000 (0.9991) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 13:38:29] __main__ INFO: \u001b[0mEpoch 11 Step 200/351 lr 0.000160 loss 0.0020 (0.0054) acc@1 1.0000 (0.9991) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 13:40:22] __main__ INFO: \u001b[0mEpoch 11 Step 300/351 lr 0.000160 loss 0.0049 (0.0050) acc@1 1.0000 (0.9992) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 13:41:20] __main__ INFO: \u001b[0mEpoch 11 Step 351/351 lr 0.000160 loss 0.0020 (0.0050) acc@1 1.0000 (0.9992) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 13:41:20] __main__ INFO: \u001b[0mElapsed 399.34\n",
      "\u001b[32m[2020-07-13 13:41:20] __main__ INFO: \u001b[0mVal 11\n",
      "\u001b[32m[2020-07-13 13:41:34] __main__ INFO: \u001b[0mEpoch 11 loss 0.1565 acc@1 0.9654 acc@5 0.9976\n",
      "\u001b[32m[2020-07-13 13:41:34] __main__ INFO: \u001b[0mElapsed 13.45\n",
      "\u001b[32m[2020-07-13 13:41:34] __main__ INFO: \u001b[0mTrain 12 3861\n",
      "\u001b[32m[2020-07-13 13:43:28] __main__ INFO: \u001b[0mEpoch 12 Step 100/351 lr 0.000160 loss 0.0029 (0.0052) acc@1 1.0000 (0.9989) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 13:45:22] __main__ INFO: \u001b[0mEpoch 12 Step 200/351 lr 0.000160 loss 0.0041 (0.0049) acc@1 1.0000 (0.9991) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 13:47:16] __main__ INFO: \u001b[0mEpoch 12 Step 300/351 lr 0.000160 loss 0.0024 (0.0043) acc@1 1.0000 (0.9992) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 13:48:14] __main__ INFO: \u001b[0mEpoch 12 Step 351/351 lr 0.000160 loss 0.0021 (0.0046) acc@1 1.0000 (0.9991) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 13:48:14] __main__ INFO: \u001b[0mElapsed 399.80\n",
      "\u001b[32m[2020-07-13 13:48:14] __main__ INFO: \u001b[0mVal 12\n",
      "\u001b[32m[2020-07-13 13:48:27] __main__ INFO: \u001b[0mEpoch 12 loss 0.1623 acc@1 0.9638 acc@5 0.9970\n",
      "\u001b[32m[2020-07-13 13:48:27] __main__ INFO: \u001b[0mElapsed 13.44\n",
      "\u001b[32m[2020-07-13 13:48:27] __main__ INFO: \u001b[0mTrain 13 4212\n",
      "\u001b[32m[2020-07-13 13:50:21] __main__ INFO: \u001b[0mEpoch 13 Step 100/351 lr 0.000160 loss 0.0024 (0.0043) acc@1 1.0000 (0.9991) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 13:52:15] __main__ INFO: \u001b[0mEpoch 13 Step 200/351 lr 0.000160 loss 0.0031 (0.0045) acc@1 1.0000 (0.9992) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 13:54:09] __main__ INFO: \u001b[0mEpoch 13 Step 300/351 lr 0.000160 loss 0.0029 (0.0050) acc@1 1.0000 (0.9991) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 13:55:07] __main__ INFO: \u001b[0mEpoch 13 Step 351/351 lr 0.000160 loss 0.0022 (0.0050) acc@1 1.0000 (0.9991) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 13:55:07] __main__ INFO: \u001b[0mElapsed 399.64\n",
      "\u001b[32m[2020-07-13 13:55:07] __main__ INFO: \u001b[0mVal 13\n",
      "\u001b[32m[2020-07-13 13:55:20] __main__ INFO: \u001b[0mEpoch 13 loss 0.1639 acc@1 0.9640 acc@5 0.9972\n",
      "\u001b[32m[2020-07-13 13:55:20] __main__ INFO: \u001b[0mElapsed 13.41\n",
      "\u001b[32m[2020-07-13 13:55:20] __main__ INFO: \u001b[0mTrain 14 4563\n",
      "\u001b[32m[2020-07-13 13:57:14] __main__ INFO: \u001b[0mEpoch 14 Step 100/351 lr 0.000160 loss 0.0019 (0.0043) acc@1 1.0000 (0.9993) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 13:59:08] __main__ INFO: \u001b[0mEpoch 14 Step 200/351 lr 0.000160 loss 0.0031 (0.0041) acc@1 1.0000 (0.9993) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 14:01:02] __main__ INFO: \u001b[0mEpoch 14 Step 300/351 lr 0.000160 loss 0.0020 (0.0038) acc@1 1.0000 (0.9994) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 14:02:00] __main__ INFO: \u001b[0mEpoch 14 Step 351/351 lr 0.000160 loss 0.0028 (0.0039) acc@1 1.0000 (0.9994) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 14:02:00] __main__ INFO: \u001b[0mElapsed 399.97\n",
      "\u001b[32m[2020-07-13 14:02:00] __main__ INFO: \u001b[0mVal 14\n",
      "\u001b[32m[2020-07-13 14:02:14] __main__ INFO: \u001b[0mEpoch 14 loss 0.1665 acc@1 0.9628 acc@5 0.9970\n",
      "\u001b[32m[2020-07-13 14:02:14] __main__ INFO: \u001b[0mElapsed 13.42\n",
      "\u001b[32m[2020-07-13 14:02:14] __main__ INFO: \u001b[0mTrain 15 4914\n",
      "\u001b[32m[2020-07-13 14:04:08] __main__ INFO: \u001b[0mEpoch 15 Step 100/351 lr 0.000160 loss 0.0027 (0.0036) acc@1 1.0000 (0.9995) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 14:06:01] __main__ INFO: \u001b[0mEpoch 15 Step 200/351 lr 0.000160 loss 0.0023 (0.0038) acc@1 1.0000 (0.9994) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 14:07:55] __main__ INFO: \u001b[0mEpoch 15 Step 300/351 lr 0.000160 loss 0.0022 (0.0041) acc@1 1.0000 (0.9994) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 14:08:54] __main__ INFO: \u001b[0mEpoch 15 Step 351/351 lr 0.000160 loss 0.0020 (0.0042) acc@1 1.0000 (0.9994) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 14:08:54] __main__ INFO: \u001b[0mElapsed 400.01\n",
      "\u001b[32m[2020-07-13 14:08:54] __main__ INFO: \u001b[0mVal 15\n",
      "\u001b[32m[2020-07-13 14:09:07] __main__ INFO: \u001b[0mEpoch 15 loss 0.1666 acc@1 0.9650 acc@5 0.9960\n",
      "\u001b[32m[2020-07-13 14:09:07] __main__ INFO: \u001b[0mElapsed 13.44\n",
      "\u001b[32m[2020-07-13 14:09:07] __main__ INFO: \u001b[0mTrain 16 5265\n",
      "\u001b[32m[2020-07-13 14:11:01] __main__ INFO: \u001b[0mEpoch 16 Step 100/351 lr 0.000160 loss 0.0035 (0.0039) acc@1 1.0000 (0.9994) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 14:12:55] __main__ INFO: \u001b[0mEpoch 16 Step 200/351 lr 0.000160 loss 0.0026 (0.0039) acc@1 1.0000 (0.9994) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 14:14:49] __main__ INFO: \u001b[0mEpoch 16 Step 300/351 lr 0.000160 loss 0.0017 (0.0038) acc@1 1.0000 (0.9994) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 14:15:47] __main__ INFO: \u001b[0mEpoch 16 Step 351/351 lr 0.000160 loss 0.0032 (0.0039) acc@1 1.0000 (0.9994) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 14:15:47] __main__ INFO: \u001b[0mElapsed 400.19\n",
      "\u001b[32m[2020-07-13 14:15:47] __main__ INFO: \u001b[0mVal 16\n",
      "\u001b[32m[2020-07-13 14:16:01] __main__ INFO: \u001b[0mEpoch 16 loss 0.1691 acc@1 0.9630 acc@5 0.9970\n",
      "\u001b[32m[2020-07-13 14:16:01] __main__ INFO: \u001b[0mElapsed 13.42\n",
      "\u001b[32m[2020-07-13 14:16:01] __main__ INFO: \u001b[0mTrain 17 5616\n",
      "\u001b[32m[2020-07-13 14:17:55] __main__ INFO: \u001b[0mEpoch 17 Step 100/351 lr 0.000160 loss 0.0101 (0.0044) acc@1 0.9922 (0.9991) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 14:19:49] __main__ INFO: \u001b[0mEpoch 17 Step 200/351 lr 0.000160 loss 0.0033 (0.0039) acc@1 1.0000 (0.9993) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 14:21:43] __main__ INFO: \u001b[0mEpoch 17 Step 300/351 lr 0.000160 loss 0.0044 (0.0042) acc@1 1.0000 (0.9992) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 14:22:41] __main__ INFO: \u001b[0mEpoch 17 Step 351/351 lr 0.000160 loss 0.0028 (0.0042) acc@1 1.0000 (0.9992) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 14:22:41] __main__ INFO: \u001b[0mElapsed 400.19\n",
      "\u001b[32m[2020-07-13 14:22:41] __main__ INFO: \u001b[0mVal 17\n",
      "\u001b[32m[2020-07-13 14:22:54] __main__ INFO: \u001b[0mEpoch 17 loss 0.1688 acc@1 0.9622 acc@5 0.9972\n",
      "\u001b[32m[2020-07-13 14:22:54] __main__ INFO: \u001b[0mElapsed 13.43\n",
      "\u001b[32m[2020-07-13 14:22:54] __main__ INFO: \u001b[0mTrain 18 5967\n",
      "\u001b[32m[2020-07-13 14:24:48] __main__ INFO: \u001b[0mEpoch 18 Step 100/351 lr 0.000160 loss 0.0019 (0.0036) acc@1 1.0000 (0.9993) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 14:26:42] __main__ INFO: \u001b[0mEpoch 18 Step 200/351 lr 0.000160 loss 0.0022 (0.0034) acc@1 1.0000 (0.9995) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 14:28:36] __main__ INFO: \u001b[0mEpoch 18 Step 300/351 lr 0.000160 loss 0.0023 (0.0034) acc@1 1.0000 (0.9995) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 14:29:34] __main__ INFO: \u001b[0mEpoch 18 Step 351/351 lr 0.000160 loss 0.0072 (0.0033) acc@1 1.0000 (0.9995) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 14:29:34] __main__ INFO: \u001b[0mElapsed 400.19\n",
      "\u001b[32m[2020-07-13 14:29:34] __main__ INFO: \u001b[0mVal 18\n",
      "\u001b[32m[2020-07-13 14:29:48] __main__ INFO: \u001b[0mEpoch 18 loss 0.1685 acc@1 0.9634 acc@5 0.9966\n",
      "\u001b[32m[2020-07-13 14:29:48] __main__ INFO: \u001b[0mElapsed 13.43\n",
      "\u001b[32m[2020-07-13 14:29:48] __main__ INFO: \u001b[0mTrain 19 6318\n",
      "\u001b[32m[2020-07-13 14:31:42] __main__ INFO: \u001b[0mEpoch 19 Step 100/351 lr 0.000160 loss 0.0030 (0.0029) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 14:33:36] __main__ INFO: \u001b[0mEpoch 19 Step 200/351 lr 0.000160 loss 0.0046 (0.0031) acc@1 1.0000 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 14:35:30] __main__ INFO: \u001b[0mEpoch 19 Step 300/351 lr 0.000160 loss 0.0033 (0.0033) acc@1 1.0000 (0.9995) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 14:36:28] __main__ INFO: \u001b[0mEpoch 19 Step 351/351 lr 0.000160 loss 0.0075 (0.0033) acc@1 1.0000 (0.9995) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 14:36:28] __main__ INFO: \u001b[0mElapsed 399.86\n",
      "\u001b[32m[2020-07-13 14:36:28] __main__ INFO: \u001b[0mVal 19\n",
      "\u001b[32m[2020-07-13 14:36:41] __main__ INFO: \u001b[0mEpoch 19 loss 0.1758 acc@1 0.9644 acc@5 0.9968\n",
      "\u001b[32m[2020-07-13 14:36:41] __main__ INFO: \u001b[0mElapsed 13.47\n",
      "\u001b[32m[2020-07-13 14:36:41] __main__ INFO: \u001b[0mTrain 20 6669\n",
      "\u001b[32m[2020-07-13 14:38:35] __main__ INFO: \u001b[0mEpoch 20 Step 100/351 lr 0.000160 loss 0.0015 (0.0033) acc@1 1.0000 (0.9994) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 14:40:29] __main__ INFO: \u001b[0mEpoch 20 Step 200/351 lr 0.000160 loss 0.0017 (0.0032) acc@1 1.0000 (0.9994) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 14:42:23] __main__ INFO: \u001b[0mEpoch 20 Step 300/351 lr 0.000160 loss 0.0016 (0.0035) acc@1 1.0000 (0.9994) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 14:43:21] __main__ INFO: \u001b[0mEpoch 20 Step 351/351 lr 0.000160 loss 0.0022 (0.0034) acc@1 1.0000 (0.9994) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 14:43:21] __main__ INFO: \u001b[0mElapsed 400.26\n",
      "\u001b[32m[2020-07-13 14:43:21] __main__ INFO: \u001b[0mVal 20\n",
      "\u001b[32m[2020-07-13 14:43:35] __main__ INFO: \u001b[0mEpoch 20 loss 0.1736 acc@1 0.9652 acc@5 0.9966\n",
      "\u001b[32m[2020-07-13 14:43:35] __main__ INFO: \u001b[0mElapsed 13.45\n",
      "\u001b[32m[2020-07-13 14:43:35] __main__ INFO: \u001b[0mTrain 21 7020\n",
      "\u001b[32m[2020-07-13 14:45:29] __main__ INFO: \u001b[0mEpoch 21 Step 100/351 lr 0.000160 loss 0.0018 (0.0031) acc@1 1.0000 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 14:47:23] __main__ INFO: \u001b[0mEpoch 21 Step 200/351 lr 0.000160 loss 0.0017 (0.0031) acc@1 1.0000 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 14:49:17] __main__ INFO: \u001b[0mEpoch 21 Step 300/351 lr 0.000160 loss 0.0014 (0.0032) acc@1 1.0000 (0.9996) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 14:50:15] __main__ INFO: \u001b[0mEpoch 21 Step 351/351 lr 0.000160 loss 0.0027 (0.0032) acc@1 1.0000 (0.9996) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 14:50:15] __main__ INFO: \u001b[0mElapsed 399.97\n",
      "\u001b[32m[2020-07-13 14:50:15] __main__ INFO: \u001b[0mVal 21\n",
      "\u001b[32m[2020-07-13 14:50:28] __main__ INFO: \u001b[0mEpoch 21 loss 0.1765 acc@1 0.9620 acc@5 0.9960\n",
      "\u001b[32m[2020-07-13 14:50:28] __main__ INFO: \u001b[0mElapsed 13.45\n",
      "\u001b[32m[2020-07-13 14:50:28] __main__ INFO: \u001b[0mTrain 22 7371\n",
      "\u001b[32m[2020-07-13 14:52:23] __main__ INFO: \u001b[0mEpoch 22 Step 100/351 lr 0.000160 loss 0.0016 (0.0027) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 14:54:16] __main__ INFO: \u001b[0mEpoch 22 Step 200/351 lr 0.000160 loss 0.0019 (0.0027) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 14:56:10] __main__ INFO: \u001b[0mEpoch 22 Step 300/351 lr 0.000160 loss 0.0040 (0.0029) acc@1 1.0000 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 14:57:08] __main__ INFO: \u001b[0mEpoch 22 Step 351/351 lr 0.000160 loss 0.0016 (0.0032) acc@1 1.0000 (0.9996) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 14:57:08] __main__ INFO: \u001b[0mElapsed 399.91\n",
      "\u001b[32m[2020-07-13 14:57:08] __main__ INFO: \u001b[0mVal 22\n",
      "\u001b[32m[2020-07-13 14:57:22] __main__ INFO: \u001b[0mEpoch 22 loss 0.1787 acc@1 0.9648 acc@5 0.9962\n",
      "\u001b[32m[2020-07-13 14:57:22] __main__ INFO: \u001b[0mElapsed 13.43\n",
      "\u001b[32m[2020-07-13 14:57:22] __main__ INFO: \u001b[0mTrain 23 7722\n",
      "\u001b[32m[2020-07-13 14:59:16] __main__ INFO: \u001b[0mEpoch 23 Step 100/351 lr 0.000160 loss 0.0026 (0.0034) acc@1 1.0000 (0.9993) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 15:01:10] __main__ INFO: \u001b[0mEpoch 23 Step 200/351 lr 0.000160 loss 0.0016 (0.0032) acc@1 1.0000 (0.9995) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 15:03:04] __main__ INFO: \u001b[0mEpoch 23 Step 300/351 lr 0.000160 loss 0.0018 (0.0037) acc@1 1.0000 (0.9995) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 15:04:02] __main__ INFO: \u001b[0mEpoch 23 Step 351/351 lr 0.000160 loss 0.0029 (0.0038) acc@1 1.0000 (0.9994) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 15:04:02] __main__ INFO: \u001b[0mElapsed 399.95\n",
      "\u001b[32m[2020-07-13 15:04:02] __main__ INFO: \u001b[0mVal 23\n",
      "\u001b[32m[2020-07-13 15:04:15] __main__ INFO: \u001b[0mEpoch 23 loss 0.1754 acc@1 0.9626 acc@5 0.9966\n",
      "\u001b[32m[2020-07-13 15:04:15] __main__ INFO: \u001b[0mElapsed 13.45\n",
      "\u001b[32m[2020-07-13 15:04:15] __main__ INFO: \u001b[0mTrain 24 8073\n",
      "\u001b[32m[2020-07-13 15:06:09] __main__ INFO: \u001b[0mEpoch 24 Step 100/351 lr 0.000160 loss 0.0049 (0.0025) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 15:08:03] __main__ INFO: \u001b[0mEpoch 24 Step 200/351 lr 0.000160 loss 0.0016 (0.0024) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 15:09:57] __main__ INFO: \u001b[0mEpoch 24 Step 300/351 lr 0.000160 loss 0.0055 (0.0027) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 15:10:55] __main__ INFO: \u001b[0mEpoch 24 Step 351/351 lr 0.000160 loss 0.0019 (0.0028) acc@1 1.0000 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 15:10:55] __main__ INFO: \u001b[0mElapsed 399.70\n",
      "\u001b[32m[2020-07-13 15:10:55] __main__ INFO: \u001b[0mVal 24\n",
      "\u001b[32m[2020-07-13 15:11:08] __main__ INFO: \u001b[0mEpoch 24 loss 0.1774 acc@1 0.9630 acc@5 0.9966\n",
      "\u001b[32m[2020-07-13 15:11:08] __main__ INFO: \u001b[0mElapsed 13.42\n",
      "\u001b[32m[2020-07-13 15:11:08] __main__ INFO: \u001b[0mTrain 25 8424\n",
      "\u001b[32m[2020-07-13 15:13:02] __main__ INFO: \u001b[0mEpoch 25 Step 100/351 lr 0.000160 loss 0.0017 (0.0025) acc@1 1.0000 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 15:14:56] __main__ INFO: \u001b[0mEpoch 25 Step 200/351 lr 0.000160 loss 0.0022 (0.0024) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 15:16:50] __main__ INFO: \u001b[0mEpoch 25 Step 300/351 lr 0.000160 loss 0.0023 (0.0025) acc@1 1.0000 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 15:17:48] __main__ INFO: \u001b[0mEpoch 25 Step 351/351 lr 0.000160 loss 0.0015 (0.0027) acc@1 1.0000 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 15:17:48] __main__ INFO: \u001b[0mElapsed 399.66\n",
      "\u001b[32m[2020-07-13 15:17:48] __main__ INFO: \u001b[0mVal 25\n",
      "\u001b[32m[2020-07-13 15:18:01] __main__ INFO: \u001b[0mEpoch 25 loss 0.1754 acc@1 0.9626 acc@5 0.9966\n",
      "\u001b[32m[2020-07-13 15:18:01] __main__ INFO: \u001b[0mElapsed 13.43\n",
      "\u001b[32m[2020-07-13 15:18:01] __main__ INFO: \u001b[0mTrain 26 8775\n",
      "\u001b[32m[2020-07-13 15:19:56] __main__ INFO: \u001b[0mEpoch 26 Step 100/351 lr 0.000160 loss 0.0036 (0.0026) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 15:21:50] __main__ INFO: \u001b[0mEpoch 26 Step 200/351 lr 0.000160 loss 0.0024 (0.0027) acc@1 1.0000 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 15:23:44] __main__ INFO: \u001b[0mEpoch 26 Step 300/351 lr 0.000160 loss 0.0046 (0.0028) acc@1 1.0000 (0.9996) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 15:24:42] __main__ INFO: \u001b[0mEpoch 26 Step 351/351 lr 0.000160 loss 0.0021 (0.0027) acc@1 1.0000 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 15:24:42] __main__ INFO: \u001b[0mElapsed 400.35\n",
      "\u001b[32m[2020-07-13 15:24:42] __main__ INFO: \u001b[0mVal 26\n",
      "\u001b[32m[2020-07-13 15:24:55] __main__ INFO: \u001b[0mEpoch 26 loss 0.1755 acc@1 0.9618 acc@5 0.9964\n",
      "\u001b[32m[2020-07-13 15:24:55] __main__ INFO: \u001b[0mElapsed 13.44\n",
      "\u001b[32m[2020-07-13 15:24:55] __main__ INFO: \u001b[0mTrain 27 9126\n",
      "\u001b[32m[2020-07-13 15:26:49] __main__ INFO: \u001b[0mEpoch 27 Step 100/351 lr 0.000160 loss 0.0017 (0.0028) acc@1 1.0000 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 15:28:43] __main__ INFO: \u001b[0mEpoch 27 Step 200/351 lr 0.000160 loss 0.0015 (0.0027) acc@1 1.0000 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 15:30:37] __main__ INFO: \u001b[0mEpoch 27 Step 300/351 lr 0.000160 loss 0.0027 (0.0026) acc@1 1.0000 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 15:31:35] __main__ INFO: \u001b[0mEpoch 27 Step 351/351 lr 0.000160 loss 0.0030 (0.0027) acc@1 1.0000 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 15:31:35] __main__ INFO: \u001b[0mElapsed 399.51\n",
      "\u001b[32m[2020-07-13 15:31:35] __main__ INFO: \u001b[0mVal 27\n",
      "\u001b[32m[2020-07-13 15:31:48] __main__ INFO: \u001b[0mEpoch 27 loss 0.1798 acc@1 0.9624 acc@5 0.9964\n",
      "\u001b[32m[2020-07-13 15:31:48] __main__ INFO: \u001b[0mElapsed 13.43\n",
      "\u001b[32m[2020-07-13 15:31:48] __main__ INFO: \u001b[0mTrain 28 9477\n",
      "\u001b[32m[2020-07-13 15:33:42] __main__ INFO: \u001b[0mEpoch 28 Step 100/351 lr 0.000160 loss 0.0021 (0.0031) acc@1 1.0000 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 15:35:36] __main__ INFO: \u001b[0mEpoch 28 Step 200/351 lr 0.000160 loss 0.0015 (0.0027) acc@1 1.0000 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 15:37:30] __main__ INFO: \u001b[0mEpoch 28 Step 300/351 lr 0.000160 loss 0.0014 (0.0026) acc@1 1.0000 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 15:38:28] __main__ INFO: \u001b[0mEpoch 28 Step 351/351 lr 0.000160 loss 0.0022 (0.0025) acc@1 1.0000 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 15:38:28] __main__ INFO: \u001b[0mElapsed 399.88\n",
      "\u001b[32m[2020-07-13 15:38:28] __main__ INFO: \u001b[0mVal 28\n",
      "\u001b[32m[2020-07-13 15:38:41] __main__ INFO: \u001b[0mEpoch 28 loss 0.1788 acc@1 0.9616 acc@5 0.9962\n",
      "\u001b[32m[2020-07-13 15:38:41] __main__ INFO: \u001b[0mElapsed 13.43\n",
      "\u001b[32m[2020-07-13 15:38:41] __main__ INFO: \u001b[0mTrain 29 9828\n",
      "\u001b[32m[2020-07-13 15:40:35] __main__ INFO: \u001b[0mEpoch 29 Step 100/351 lr 0.000160 loss 0.0016 (0.0023) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 15:42:29] __main__ INFO: \u001b[0mEpoch 29 Step 200/351 lr 0.000160 loss 0.0058 (0.0023) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 15:44:23] __main__ INFO: \u001b[0mEpoch 29 Step 300/351 lr 0.000160 loss 0.0014 (0.0023) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 15:45:21] __main__ INFO: \u001b[0mEpoch 29 Step 351/351 lr 0.000160 loss 0.0020 (0.0023) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 15:45:21] __main__ INFO: \u001b[0mElapsed 399.84\n",
      "\u001b[32m[2020-07-13 15:45:21] __main__ INFO: \u001b[0mVal 29\n",
      "\u001b[32m[2020-07-13 15:45:35] __main__ INFO: \u001b[0mEpoch 29 loss 0.1806 acc@1 0.9630 acc@5 0.9960\n",
      "\u001b[32m[2020-07-13 15:45:35] __main__ INFO: \u001b[0mElapsed 13.43\n",
      "\u001b[32m[2020-07-13 15:45:35] __main__ INFO: \u001b[0mTrain 30 10179\n",
      "\u001b[32m[2020-07-13 15:47:29] __main__ INFO: \u001b[0mEpoch 30 Step 100/351 lr 0.000160 loss 0.0018 (0.0028) acc@1 1.0000 (0.9996) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 15:49:22] __main__ INFO: \u001b[0mEpoch 30 Step 200/351 lr 0.000160 loss 0.0020 (0.0029) acc@1 1.0000 (0.9996) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 15:51:16] __main__ INFO: \u001b[0mEpoch 30 Step 300/351 lr 0.000160 loss 0.0024 (0.0027) acc@1 1.0000 (0.9996) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 15:52:14] __main__ INFO: \u001b[0mEpoch 30 Step 351/351 lr 0.000160 loss 0.0024 (0.0028) acc@1 1.0000 (0.9996) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 15:52:14] __main__ INFO: \u001b[0mElapsed 399.63\n",
      "\u001b[32m[2020-07-13 15:52:14] __main__ INFO: \u001b[0mVal 30\n",
      "\u001b[32m[2020-07-13 15:52:28] __main__ INFO: \u001b[0mEpoch 30 loss 0.1856 acc@1 0.9606 acc@5 0.9966\n",
      "\u001b[32m[2020-07-13 15:52:28] __main__ INFO: \u001b[0mElapsed 13.46\n",
      "\u001b[32m[2020-07-13 15:52:28] __main__ INFO: \u001b[0mTrain 31 10530\n",
      "\u001b[32m[2020-07-13 15:54:22] __main__ INFO: \u001b[0mEpoch 31 Step 100/351 lr 0.000160 loss 0.0016 (0.0025) acc@1 1.0000 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 15:56:16] __main__ INFO: \u001b[0mEpoch 31 Step 200/351 lr 0.000160 loss 0.0016 (0.0026) acc@1 1.0000 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 15:58:10] __main__ INFO: \u001b[0mEpoch 31 Step 300/351 lr 0.000160 loss 0.0038 (0.0027) acc@1 1.0000 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 15:59:08] __main__ INFO: \u001b[0mEpoch 31 Step 351/351 lr 0.000160 loss 0.0014 (0.0027) acc@1 1.0000 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 15:59:08] __main__ INFO: \u001b[0mElapsed 400.23\n",
      "\u001b[32m[2020-07-13 15:59:08] __main__ INFO: \u001b[0mVal 31\n",
      "\u001b[32m[2020-07-13 15:59:21] __main__ INFO: \u001b[0mEpoch 31 loss 0.1813 acc@1 0.9626 acc@5 0.9964\n",
      "\u001b[32m[2020-07-13 15:59:21] __main__ INFO: \u001b[0mElapsed 13.40\n",
      "\u001b[32m[2020-07-13 15:59:21] __main__ INFO: \u001b[0mTrain 32 10881\n",
      "\u001b[32m[2020-07-13 16:01:15] __main__ INFO: \u001b[0mEpoch 32 Step 100/351 lr 0.000160 loss 0.0024 (0.0028) acc@1 1.0000 (0.9995) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 16:03:09] __main__ INFO: \u001b[0mEpoch 32 Step 200/351 lr 0.000160 loss 0.0014 (0.0032) acc@1 1.0000 (0.9995) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 16:05:03] __main__ INFO: \u001b[0mEpoch 32 Step 300/351 lr 0.000160 loss 0.0015 (0.0031) acc@1 1.0000 (0.9994) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 16:06:01] __main__ INFO: \u001b[0mEpoch 32 Step 351/351 lr 0.000160 loss 0.0023 (0.0031) acc@1 1.0000 (0.9995) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 16:06:01] __main__ INFO: \u001b[0mElapsed 400.05\n",
      "\u001b[32m[2020-07-13 16:06:01] __main__ INFO: \u001b[0mVal 32\n",
      "\u001b[32m[2020-07-13 16:06:15] __main__ INFO: \u001b[0mEpoch 32 loss 0.1805 acc@1 0.9614 acc@5 0.9966\n",
      "\u001b[32m[2020-07-13 16:06:15] __main__ INFO: \u001b[0mElapsed 13.46\n",
      "\u001b[32m[2020-07-13 16:06:15] __main__ INFO: \u001b[0mTrain 33 11232\n",
      "\u001b[32m[2020-07-13 16:08:09] __main__ INFO: \u001b[0mEpoch 33 Step 100/351 lr 0.000160 loss 0.0024 (0.0023) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 16:10:03] __main__ INFO: \u001b[0mEpoch 33 Step 200/351 lr 0.000160 loss 0.0075 (0.0028) acc@1 0.9922 (0.9996) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 16:11:57] __main__ INFO: \u001b[0mEpoch 33 Step 300/351 lr 0.000160 loss 0.0018 (0.0026) acc@1 1.0000 (0.9996) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 16:12:55] __main__ INFO: \u001b[0mEpoch 33 Step 351/351 lr 0.000160 loss 0.0013 (0.0026) acc@1 1.0000 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 16:12:55] __main__ INFO: \u001b[0mElapsed 400.15\n",
      "\u001b[32m[2020-07-13 16:12:55] __main__ INFO: \u001b[0mVal 33\n",
      "\u001b[32m[2020-07-13 16:13:08] __main__ INFO: \u001b[0mEpoch 33 loss 0.1797 acc@1 0.9616 acc@5 0.9962\n",
      "\u001b[32m[2020-07-13 16:13:08] __main__ INFO: \u001b[0mElapsed 13.39\n",
      "\u001b[32m[2020-07-13 16:13:08] __main__ INFO: \u001b[0mTrain 34 11583\n",
      "\u001b[32m[2020-07-13 16:15:02] __main__ INFO: \u001b[0mEpoch 34 Step 100/351 lr 0.000160 loss 0.0048 (0.0021) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 16:16:56] __main__ INFO: \u001b[0mEpoch 34 Step 200/351 lr 0.000160 loss 0.0014 (0.0021) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 16:18:50] __main__ INFO: \u001b[0mEpoch 34 Step 300/351 lr 0.000160 loss 0.0013 (0.0024) acc@1 1.0000 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 16:19:48] __main__ INFO: \u001b[0mEpoch 34 Step 351/351 lr 0.000160 loss 0.0016 (0.0023) acc@1 1.0000 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 16:19:48] __main__ INFO: \u001b[0mElapsed 399.84\n",
      "\u001b[32m[2020-07-13 16:19:48] __main__ INFO: \u001b[0mVal 34\n",
      "\u001b[32m[2020-07-13 16:20:02] __main__ INFO: \u001b[0mEpoch 34 loss 0.1772 acc@1 0.9630 acc@5 0.9962\n",
      "\u001b[32m[2020-07-13 16:20:02] __main__ INFO: \u001b[0mElapsed 13.41\n",
      "\u001b[32m[2020-07-13 16:20:02] __main__ INFO: \u001b[0mTrain 35 11934\n",
      "\u001b[32m[2020-07-13 16:21:56] __main__ INFO: \u001b[0mEpoch 35 Step 100/351 lr 0.000160 loss 0.0014 (0.0021) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 16:23:50] __main__ INFO: \u001b[0mEpoch 35 Step 200/351 lr 0.000160 loss 0.0017 (0.0022) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 16:25:43] __main__ INFO: \u001b[0mEpoch 35 Step 300/351 lr 0.000160 loss 0.0015 (0.0025) acc@1 1.0000 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 16:26:41] __main__ INFO: \u001b[0mEpoch 35 Step 351/351 lr 0.000160 loss 0.0027 (0.0025) acc@1 1.0000 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 16:26:41] __main__ INFO: \u001b[0mElapsed 399.63\n",
      "\u001b[32m[2020-07-13 16:26:41] __main__ INFO: \u001b[0mVal 35\n",
      "\u001b[32m[2020-07-13 16:26:55] __main__ INFO: \u001b[0mEpoch 35 loss 0.1787 acc@1 0.9618 acc@5 0.9960\n",
      "\u001b[32m[2020-07-13 16:26:55] __main__ INFO: \u001b[0mElapsed 13.41\n",
      "\u001b[32m[2020-07-13 16:26:55] __main__ INFO: \u001b[0mTrain 36 12285\n",
      "\u001b[32m[2020-07-13 16:28:49] __main__ INFO: \u001b[0mEpoch 36 Step 100/351 lr 0.000160 loss 0.0022 (0.0025) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 16:30:42] __main__ INFO: \u001b[0mEpoch 36 Step 200/351 lr 0.000160 loss 0.0014 (0.0023) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 16:32:36] __main__ INFO: \u001b[0mEpoch 36 Step 300/351 lr 0.000160 loss 0.0014 (0.0022) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 16:33:34] __main__ INFO: \u001b[0mEpoch 36 Step 351/351 lr 0.000160 loss 0.0016 (0.0022) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 16:33:34] __main__ INFO: \u001b[0mElapsed 399.42\n",
      "\u001b[32m[2020-07-13 16:33:34] __main__ INFO: \u001b[0mVal 36\n",
      "\u001b[32m[2020-07-13 16:33:48] __main__ INFO: \u001b[0mEpoch 36 loss 0.1793 acc@1 0.9622 acc@5 0.9960\n",
      "\u001b[32m[2020-07-13 16:33:48] __main__ INFO: \u001b[0mElapsed 13.40\n",
      "\u001b[32m[2020-07-13 16:33:48] __main__ INFO: \u001b[0mTrain 37 12636\n",
      "\u001b[32m[2020-07-13 16:35:42] __main__ INFO: \u001b[0mEpoch 37 Step 100/351 lr 0.000160 loss 0.0013 (0.0039) acc@1 1.0000 (0.9996) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 16:37:35] __main__ INFO: \u001b[0mEpoch 37 Step 200/351 lr 0.000160 loss 0.0016 (0.0030) acc@1 1.0000 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 16:39:30] __main__ INFO: \u001b[0mEpoch 37 Step 300/351 lr 0.000160 loss 0.0016 (0.0027) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 16:40:28] __main__ INFO: \u001b[0mEpoch 37 Step 351/351 lr 0.000160 loss 0.0014 (0.0026) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 16:40:28] __main__ INFO: \u001b[0mElapsed 400.23\n",
      "\u001b[32m[2020-07-13 16:40:28] __main__ INFO: \u001b[0mVal 37\n",
      "\u001b[32m[2020-07-13 16:40:41] __main__ INFO: \u001b[0mEpoch 37 loss 0.1810 acc@1 0.9630 acc@5 0.9962\n",
      "\u001b[32m[2020-07-13 16:40:41] __main__ INFO: \u001b[0mElapsed 13.45\n",
      "\u001b[32m[2020-07-13 16:40:41] __main__ INFO: \u001b[0mTrain 38 12987\n",
      "\u001b[32m[2020-07-13 16:42:35] __main__ INFO: \u001b[0mEpoch 38 Step 100/351 lr 0.000160 loss 0.0015 (0.0023) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 16:44:29] __main__ INFO: \u001b[0mEpoch 38 Step 200/351 lr 0.000160 loss 0.0017 (0.0023) acc@1 1.0000 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 16:46:23] __main__ INFO: \u001b[0mEpoch 38 Step 300/351 lr 0.000160 loss 0.0021 (0.0024) acc@1 1.0000 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 16:47:21] __main__ INFO: \u001b[0mEpoch 38 Step 351/351 lr 0.000160 loss 0.0015 (0.0023) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 16:47:21] __main__ INFO: \u001b[0mElapsed 399.57\n",
      "\u001b[32m[2020-07-13 16:47:21] __main__ INFO: \u001b[0mVal 38\n",
      "\u001b[32m[2020-07-13 16:47:34] __main__ INFO: \u001b[0mEpoch 38 loss 0.1833 acc@1 0.9626 acc@5 0.9964\n",
      "\u001b[32m[2020-07-13 16:47:34] __main__ INFO: \u001b[0mElapsed 13.44\n",
      "\u001b[32m[2020-07-13 16:47:34] __main__ INFO: \u001b[0mTrain 39 13338\n",
      "\u001b[32m[2020-07-13 16:49:28] __main__ INFO: \u001b[0mEpoch 39 Step 100/351 lr 0.000160 loss 0.0051 (0.0020) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 16:51:22] __main__ INFO: \u001b[0mEpoch 39 Step 200/351 lr 0.000160 loss 0.0015 (0.0019) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 16:53:16] __main__ INFO: \u001b[0mEpoch 39 Step 300/351 lr 0.000160 loss 0.0016 (0.0020) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 16:54:14] __main__ INFO: \u001b[0mEpoch 39 Step 351/351 lr 0.000160 loss 0.0016 (0.0021) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 16:54:14] __main__ INFO: \u001b[0mElapsed 399.69\n",
      "\u001b[32m[2020-07-13 16:54:14] __main__ INFO: \u001b[0mVal 39\n",
      "\u001b[32m[2020-07-13 16:54:27] __main__ INFO: \u001b[0mEpoch 39 loss 0.1854 acc@1 0.9622 acc@5 0.9960\n",
      "\u001b[32m[2020-07-13 16:54:27] __main__ INFO: \u001b[0mElapsed 13.40\n",
      "\u001b[32m[2020-07-13 16:54:27] __main__ INFO: \u001b[0mTrain 40 13689\n",
      "\u001b[32m[2020-07-13 16:56:21] __main__ INFO: \u001b[0mEpoch 40 Step 100/351 lr 0.000160 loss 0.0068 (0.0033) acc@1 1.0000 (0.9996) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 16:58:15] __main__ INFO: \u001b[0mEpoch 40 Step 200/351 lr 0.000160 loss 0.0013 (0.0026) acc@1 1.0000 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 17:00:09] __main__ INFO: \u001b[0mEpoch 40 Step 300/351 lr 0.000160 loss 0.0014 (0.0025) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 17:01:07] __main__ INFO: \u001b[0mEpoch 40 Step 351/351 lr 0.000160 loss 0.0062 (0.0026) acc@1 1.0000 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 17:01:07] __main__ INFO: \u001b[0mElapsed 399.75\n",
      "\u001b[32m[2020-07-13 17:01:07] __main__ INFO: \u001b[0mVal 40\n",
      "\u001b[32m[2020-07-13 17:01:21] __main__ INFO: \u001b[0mEpoch 40 loss 0.1806 acc@1 0.9624 acc@5 0.9962\n",
      "\u001b[32m[2020-07-13 17:01:21] __main__ INFO: \u001b[0mElapsed 13.43\n",
      "\u001b[32m[2020-07-13 17:01:21] __main__ INFO: \u001b[0mTrain 41 14040\n",
      "\u001b[32m[2020-07-13 17:03:14] __main__ INFO: \u001b[0mEpoch 41 Step 100/351 lr 0.000160 loss 0.0013 (0.0020) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 17:05:08] __main__ INFO: \u001b[0mEpoch 41 Step 200/351 lr 0.000160 loss 0.0015 (0.0019) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 17:07:02] __main__ INFO: \u001b[0mEpoch 41 Step 300/351 lr 0.000160 loss 0.0016 (0.0018) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 17:08:00] __main__ INFO: \u001b[0mEpoch 41 Step 351/351 lr 0.000160 loss 0.0023 (0.0019) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 17:08:00] __main__ INFO: \u001b[0mElapsed 399.71\n",
      "\u001b[32m[2020-07-13 17:08:00] __main__ INFO: \u001b[0mVal 41\n",
      "\u001b[32m[2020-07-13 17:08:14] __main__ INFO: \u001b[0mEpoch 41 loss 0.1887 acc@1 0.9614 acc@5 0.9962\n",
      "\u001b[32m[2020-07-13 17:08:14] __main__ INFO: \u001b[0mElapsed 13.43\n",
      "\u001b[32m[2020-07-13 17:08:14] __main__ INFO: \u001b[0mTrain 42 14391\n",
      "\u001b[32m[2020-07-13 17:10:08] __main__ INFO: \u001b[0mEpoch 42 Step 100/351 lr 0.000160 loss 0.0018 (0.0030) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 17:12:02] __main__ INFO: \u001b[0mEpoch 42 Step 200/351 lr 0.000160 loss 0.0015 (0.0026) acc@1 1.0000 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 17:13:56] __main__ INFO: \u001b[0mEpoch 42 Step 300/351 lr 0.000160 loss 0.0021 (0.0025) acc@1 1.0000 (0.9997) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 17:14:54] __main__ INFO: \u001b[0mEpoch 42 Step 351/351 lr 0.000160 loss 0.0014 (0.0024) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 17:14:54] __main__ INFO: \u001b[0mElapsed 400.10\n",
      "\u001b[32m[2020-07-13 17:14:54] __main__ INFO: \u001b[0mVal 42\n",
      "\u001b[32m[2020-07-13 17:15:07] __main__ INFO: \u001b[0mEpoch 42 loss 0.1841 acc@1 0.9618 acc@5 0.9962\n",
      "\u001b[32m[2020-07-13 17:15:07] __main__ INFO: \u001b[0mElapsed 13.42\n",
      "\u001b[32m[2020-07-13 17:15:07] __main__ INFO: \u001b[0mTrain 43 14742\n",
      "\u001b[32m[2020-07-13 17:17:01] __main__ INFO: \u001b[0mEpoch 43 Step 100/351 lr 0.000160 loss 0.0015 (0.0017) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 17:18:55] __main__ INFO: \u001b[0mEpoch 43 Step 200/351 lr 0.000160 loss 0.0012 (0.0022) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 17:20:49] __main__ INFO: \u001b[0mEpoch 43 Step 300/351 lr 0.000160 loss 0.0014 (0.0021) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 17:21:47] __main__ INFO: \u001b[0mEpoch 43 Step 351/351 lr 0.000160 loss 0.0014 (0.0023) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 17:21:47] __main__ INFO: \u001b[0mElapsed 399.69\n",
      "\u001b[32m[2020-07-13 17:21:47] __main__ INFO: \u001b[0mVal 43\n",
      "\u001b[32m[2020-07-13 17:22:00] __main__ INFO: \u001b[0mEpoch 43 loss 0.1858 acc@1 0.9620 acc@5 0.9962\n",
      "\u001b[32m[2020-07-13 17:22:00] __main__ INFO: \u001b[0mElapsed 13.44\n",
      "\u001b[32m[2020-07-13 17:22:00] __main__ INFO: \u001b[0mTrain 44 15093\n",
      "\u001b[32m[2020-07-13 17:23:54] __main__ INFO: \u001b[0mEpoch 44 Step 100/351 lr 0.000160 loss 0.0013 (0.0021) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 17:25:48] __main__ INFO: \u001b[0mEpoch 44 Step 200/351 lr 0.000160 loss 0.0014 (0.0023) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 17:27:43] __main__ INFO: \u001b[0mEpoch 44 Step 300/351 lr 0.000160 loss 0.0021 (0.0022) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 17:28:41] __main__ INFO: \u001b[0mEpoch 44 Step 351/351 lr 0.000160 loss 0.0013 (0.0022) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 17:28:41] __main__ INFO: \u001b[0mElapsed 400.42\n",
      "\u001b[32m[2020-07-13 17:28:41] __main__ INFO: \u001b[0mVal 44\n",
      "\u001b[32m[2020-07-13 17:28:54] __main__ INFO: \u001b[0mEpoch 44 loss 0.1830 acc@1 0.9628 acc@5 0.9958\n",
      "\u001b[32m[2020-07-13 17:28:54] __main__ INFO: \u001b[0mElapsed 13.43\n",
      "\u001b[32m[2020-07-13 17:28:54] __main__ INFO: \u001b[0mTrain 45 15444\n",
      "\u001b[32m[2020-07-13 17:30:48] __main__ INFO: \u001b[0mEpoch 45 Step 100/351 lr 0.000160 loss 0.0018 (0.0022) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 17:32:42] __main__ INFO: \u001b[0mEpoch 45 Step 200/351 lr 0.000160 loss 0.0018 (0.0020) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 17:34:36] __main__ INFO: \u001b[0mEpoch 45 Step 300/351 lr 0.000160 loss 0.0015 (0.0019) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 17:35:34] __main__ INFO: \u001b[0mEpoch 45 Step 351/351 lr 0.000160 loss 0.0015 (0.0019) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 17:35:34] __main__ INFO: \u001b[0mElapsed 399.79\n",
      "\u001b[32m[2020-07-13 17:35:34] __main__ INFO: \u001b[0mVal 45\n",
      "\u001b[32m[2020-07-13 17:35:47] __main__ INFO: \u001b[0mEpoch 45 loss 0.1922 acc@1 0.9632 acc@5 0.9954\n",
      "\u001b[32m[2020-07-13 17:35:47] __main__ INFO: \u001b[0mElapsed 13.43\n",
      "\u001b[32m[2020-07-13 17:35:47] __main__ INFO: \u001b[0mTrain 46 15795\n",
      "\u001b[32m[2020-07-13 17:37:41] __main__ INFO: \u001b[0mEpoch 46 Step 100/351 lr 0.000160 loss 0.0013 (0.0019) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 17:39:35] __main__ INFO: \u001b[0mEpoch 46 Step 200/351 lr 0.000160 loss 0.0015 (0.0019) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 17:41:29] __main__ INFO: \u001b[0mEpoch 46 Step 300/351 lr 0.000160 loss 0.0014 (0.0019) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 17:42:27] __main__ INFO: \u001b[0mEpoch 46 Step 351/351 lr 0.000160 loss 0.0018 (0.0019) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 17:42:27] __main__ INFO: \u001b[0mElapsed 399.79\n",
      "\u001b[32m[2020-07-13 17:42:27] __main__ INFO: \u001b[0mVal 46\n",
      "\u001b[32m[2020-07-13 17:42:41] __main__ INFO: \u001b[0mEpoch 46 loss 0.1846 acc@1 0.9636 acc@5 0.9958\n",
      "\u001b[32m[2020-07-13 17:42:41] __main__ INFO: \u001b[0mElapsed 13.40\n",
      "\u001b[32m[2020-07-13 17:42:41] __main__ INFO: \u001b[0mTrain 47 16146\n",
      "\u001b[32m[2020-07-13 17:44:34] __main__ INFO: \u001b[0mEpoch 47 Step 100/351 lr 0.000160 loss 0.0016 (0.0017) acc@1 1.0000 (1.0000) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 17:46:28] __main__ INFO: \u001b[0mEpoch 47 Step 200/351 lr 0.000160 loss 0.0016 (0.0016) acc@1 1.0000 (1.0000) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 17:48:22] __main__ INFO: \u001b[0mEpoch 47 Step 300/351 lr 0.000160 loss 0.0013 (0.0017) acc@1 1.0000 (1.0000) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 17:49:20] __main__ INFO: \u001b[0mEpoch 47 Step 351/351 lr 0.000160 loss 0.0014 (0.0017) acc@1 1.0000 (1.0000) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 17:49:20] __main__ INFO: \u001b[0mElapsed 399.45\n",
      "\u001b[32m[2020-07-13 17:49:20] __main__ INFO: \u001b[0mVal 47\n",
      "\u001b[32m[2020-07-13 17:49:33] __main__ INFO: \u001b[0mEpoch 47 loss 0.1816 acc@1 0.9638 acc@5 0.9954\n",
      "\u001b[32m[2020-07-13 17:49:33] __main__ INFO: \u001b[0mElapsed 13.44\n",
      "\u001b[32m[2020-07-13 17:49:33] __main__ INFO: \u001b[0mTrain 48 16497\n",
      "\u001b[32m[2020-07-13 17:51:28] __main__ INFO: \u001b[0mEpoch 48 Step 100/351 lr 0.000160 loss 0.0013 (0.0016) acc@1 1.0000 (1.0000) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 17:53:21] __main__ INFO: \u001b[0mEpoch 48 Step 200/351 lr 0.000160 loss 0.0013 (0.0018) acc@1 1.0000 (1.0000) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 17:55:15] __main__ INFO: \u001b[0mEpoch 48 Step 300/351 lr 0.000160 loss 0.0017 (0.0017) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 17:56:13] __main__ INFO: \u001b[0mEpoch 48 Step 351/351 lr 0.000160 loss 0.0015 (0.0019) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 17:56:14] __main__ INFO: \u001b[0mElapsed 400.03\n",
      "\u001b[32m[2020-07-13 17:56:14] __main__ INFO: \u001b[0mVal 48\n",
      "\u001b[32m[2020-07-13 17:56:27] __main__ INFO: \u001b[0mEpoch 48 loss 0.1888 acc@1 0.9620 acc@5 0.9950\n",
      "\u001b[32m[2020-07-13 17:56:27] __main__ INFO: \u001b[0mElapsed 13.42\n",
      "\u001b[32m[2020-07-13 17:56:27] __main__ INFO: \u001b[0mTrain 49 16848\n",
      "\u001b[32m[2020-07-13 17:58:21] __main__ INFO: \u001b[0mEpoch 49 Step 100/351 lr 0.000160 loss 0.0013 (0.0020) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 18:00:15] __main__ INFO: \u001b[0mEpoch 49 Step 200/351 lr 0.000160 loss 0.0018 (0.0018) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 18:02:09] __main__ INFO: \u001b[0mEpoch 49 Step 300/351 lr 0.000160 loss 0.0014 (0.0018) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 18:03:07] __main__ INFO: \u001b[0mEpoch 49 Step 351/351 lr 0.000160 loss 0.0015 (0.0018) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 18:03:07] __main__ INFO: \u001b[0mElapsed 399.84\n",
      "\u001b[32m[2020-07-13 18:03:07] __main__ INFO: \u001b[0mVal 49\n",
      "\u001b[32m[2020-07-13 18:03:20] __main__ INFO: \u001b[0mEpoch 49 loss 0.1826 acc@1 0.9626 acc@5 0.9962\n",
      "\u001b[32m[2020-07-13 18:03:20] __main__ INFO: \u001b[0mElapsed 13.44\n",
      "\u001b[32m[2020-07-13 18:03:20] __main__ INFO: \u001b[0mTrain 50 17199\n",
      "\u001b[32m[2020-07-13 18:05:14] __main__ INFO: \u001b[0mEpoch 50 Step 100/351 lr 0.000160 loss 0.0016 (0.0022) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 18:07:08] __main__ INFO: \u001b[0mEpoch 50 Step 200/351 lr 0.000160 loss 0.0020 (0.0025) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 18:09:02] __main__ INFO: \u001b[0mEpoch 50 Step 300/351 lr 0.000160 loss 0.0013 (0.0022) acc@1 1.0000 (0.9998) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 18:10:00] __main__ INFO: \u001b[0mEpoch 50 Step 351/351 lr 0.000160 loss 0.0014 (0.0021) acc@1 1.0000 (0.9999) acc@5 1.0000 (1.0000)\n",
      "\u001b[32m[2020-07-13 18:10:00] __main__ INFO: \u001b[0mElapsed 399.98\n",
      "\u001b[32m[2020-07-13 18:10:00] __main__ INFO: \u001b[0mVal 50\n",
      "\u001b[32m[2020-07-13 18:10:14] __main__ INFO: \u001b[0mEpoch 50 loss 0.1867 acc@1 0.9628 acc@5 0.9958\n",
      "\u001b[32m[2020-07-13 18:10:14] __main__ INFO: \u001b[0mElapsed 13.45\n",
      "\u001b[32m[2020-07-13 18:10:14] fvcore.common.checkpoint INFO: \u001b[0mSaving checkpoint to /home/ec2-user/SageMaker/experiments/wrn_28_10_cm_1_1/exp00_resume400_50/checkpoint_00050.pth\n"
     ]
    }
   ],
   "source": [
    "# Resume training with the un-augmented data\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "!python train.py --config /home/ec2-user/SageMaker/experiments/wrn_28_10_cm_1_1/exp00_resume300/config.yaml \\\n",
    "    train.checkpoint /home/ec2-user/SageMaker/experiments/wrn_28_10_cm_1_1/exp00_resume300/checkpoint_00400.pth \\\n",
    "    dataset.name CIFAR10 \\\n",
    "    model.wrn.depth 28 \\\n",
    "    model.wrn.widening_factor 10 \\\n",
    "    train.batch_size 128 \\\n",
    "    train.base_lr 0.00016 \\\n",
    "    train.output_dir /home/ec2-user/SageMaker/experiments/wrn_28_10_cm_1_1/exp00_resume400_50 \\\n",
    "    scheduler.epochs 50\n",
    "\n",
    "#### Set LEARNING RATE based on ending LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-13 18:49:36] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/wrn_28_10_cm_1_1/exp00_resume400_50/checkpoint_00050.pth\n",
      "Files already downloaded and verified\n",
      "100%|███████████████████████████████████████████| 79/79 [00:27<00:00,  2.92it/s]\n",
      "\u001b[32m[2020-07-13 18:50:04] __main__ INFO: \u001b[0mElapsed 27.03\n",
      "\u001b[32m[2020-07-13 18:50:04] __main__ INFO: \u001b[0mLoss 0.2518 Accuracy 0.9520\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/wrn.yaml \\\n",
    "   model.resnet.depth 32 \\\n",
    "   test.batch_size 128 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/wrn_28_10_cm_1_1/exp00_resume400_50/checkpoint_00050.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/wrn_28_10_cm_1_1/exp00_resume400_50/test_results_0050_cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-13 18:50:15] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/wrn_28_10_cm_1_1/exp00_resume400_50/checkpoint_00050.pth\n",
      "CIFAR 10.1\n",
      "100%|███████████████████████████████████████████| 16/16 [00:05<00:00,  2.82it/s]\n",
      "\u001b[32m[2020-07-13 18:50:21] __main__ INFO: \u001b[0mElapsed 5.69\n",
      "\u001b[32m[2020-07-13 18:50:21] __main__ INFO: \u001b[0mLoss 0.6063 Accuracy 0.8830\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10.1 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/wrn.yaml \\\n",
    "   model.resnet.depth 32 \\\n",
    "   test.batch_size 128 \\\n",
    "   dataset.name CIFAR101 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/wrn_28_10_cm_1_1/exp00_resume400_50/checkpoint_00050.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/wrn_28_10_cm_1_1/exp00_resume400_50/test_results_0050_cifar101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-13 18:51:40] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/wrn_28_10_cm_1_1/exp00_resume300/checkpoint_00400.pth\n",
      "Files already downloaded and verified\n",
      "100%|███████████████████████████████████████████| 79/79 [00:27<00:00,  2.92it/s]\n",
      "\u001b[32m[2020-07-13 18:52:09] __main__ INFO: \u001b[0mElapsed 27.07\n",
      "\u001b[32m[2020-07-13 18:52:09] __main__ INFO: \u001b[0mLoss 0.2854 Accuracy 0.9323\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/wrn.yaml \\\n",
    "   model.resnet.depth 32 \\\n",
    "   test.batch_size 128 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/wrn_28_10_cm_1_1/exp00_resume300/checkpoint_00400.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/wrn_28_10_cm_1_1/exp00/test_results_0400_cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-07-13 18:52:28] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/wrn_28_10_cm_1_1/exp00_resume300/checkpoint_00400.pth\n",
      "CIFAR 10.1\n",
      "100%|███████████████████████████████████████████| 16/16 [00:05<00:00,  2.80it/s]\n",
      "\u001b[32m[2020-07-13 18:52:34] __main__ INFO: \u001b[0mElapsed 5.71\n",
      "\u001b[32m[2020-07-13 18:52:34] __main__ INFO: \u001b[0mLoss 0.5985 Accuracy 0.8560\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10.1 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "!python evaluate.py --config configs/cifar/wrn.yaml \\\n",
    "   model.resnet.depth 32 \\\n",
    "   test.batch_size 128 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/wrn_28_10_cm_1_1/exp00_resume300/checkpoint_00400.pth \\\n",
    "   dataset.name CIFAR101 \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/wrn_28_10_cm_1_1/exp00/test_results_0400_cifar101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Record the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Testset</th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Original_Accuracy</th>\n",
       "      <th>Original_CI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wrn_28_10</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>100</td>\n",
       "      <td>0.2299</td>\n",
       "      <td>0.9311</td>\n",
       "      <td>95.9</td>\n",
       "      <td>(95.5, 96.3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wrn_28_10</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>200</td>\n",
       "      <td>0.1760</td>\n",
       "      <td>0.9578</td>\n",
       "      <td>95.9</td>\n",
       "      <td>(95.5, 96.3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wrn_28_10</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.3896</td>\n",
       "      <td>0.8975</td>\n",
       "      <td>89.7</td>\n",
       "      <td>(88.3, 91.0)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Model    Testset  Epoch    Loss  Accuracy  Original_Accuracy  \\\n",
       "0  wrn_28_10    cifar10    100  0.2299    0.9311               95.9   \n",
       "1  wrn_28_10    cifar10    200  0.1760    0.9578               95.9   \n",
       "2  wrn_28_10  cifar10.1    200  0.3896    0.8975               89.7   \n",
       "\n",
       "    Original_CI  \n",
       "0  (95.5, 96.3)  \n",
       "1  (95.5, 96.3)  \n",
       "2  (88.3, 91.0)  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write the results to a CSV file so that we can analyze later.\n",
    "import pandas as pd\n",
    "\n",
    "results = {'Model': ['wrn_28_10', 'wrn_28_10', 'wrn_28_10'],\n",
    "           'Testset': ['cifar10', 'cifar10', 'cifar10.1'],\n",
    "           'Epoch': [100, 200, 200],\n",
    "           'Loss': [0.2299, 0.1760, 0.3896],\n",
    "           'Accuracy': [0.9311, 0.9578, 0.8975],\n",
    "           'Original_Accuracy': [95.9, 95.9, 89.7],\n",
    "           'Original_CI': [(95.5, 96.3), (95.5, 96.3), (88.3, 91.0)]\n",
    "           }\n",
    "\n",
    "df = pd.DataFrame(results, columns = ['Model', 'Testset', 'Epoch', 'Loss', 'Accuracy', \n",
    "                                      'Original_Accuracy', 'Original_CI'])\n",
    "\n",
    "#df.to_csv('/home/ec2-user/SageMaker/experiments/wrn_28_10/exp00/results.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Testset</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Original_Accuracy</th>\n",
       "      <th>Original_CI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wrn_28_10_cm_1_1</td>\n",
       "      <td>400</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.2854</td>\n",
       "      <td>0.9323</td>\n",
       "      <td>95.9</td>\n",
       "      <td>(95.5, 96.3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wrn_28_10_cm_1_1</td>\n",
       "      <td>400</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>0.5985</td>\n",
       "      <td>0.856</td>\n",
       "      <td>89.7</td>\n",
       "      <td>(88.3, 91.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wrn_28_10_cm_1_1_refined400</td>\n",
       "      <td>50</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>0.6063</td>\n",
       "      <td>0.883</td>\n",
       "      <td>89.7</td>\n",
       "      <td>(88.3, 91.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wrn_28_10_cm_1_1_refined400</td>\n",
       "      <td>50</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>0.2518</td>\n",
       "      <td>0.952</td>\n",
       "      <td>95.9</td>\n",
       "      <td>(95.5, 96.3)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Model Epoch    Testset    Loss Accuracy  \\\n",
       "0             wrn_28_10_cm_1_1   400    cifar10  0.2854   0.9323   \n",
       "1             wrn_28_10_cm_1_1   400  cifar10.1  0.5985    0.856   \n",
       "2  wrn_28_10_cm_1_1_refined400    50  cifar10.1  0.6063    0.883   \n",
       "3  wrn_28_10_cm_1_1_refined400    50    cifar10  0.2518    0.952   \n",
       "\n",
       "   Original_Accuracy   Original_CI  \n",
       "0               95.9  (95.5, 96.3)  \n",
       "1               89.7  (88.3, 91.0)  \n",
       "2               89.7  (88.3, 91.0)  \n",
       "3               95.9  (95.5, 96.3)  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "model = 'wrn_28_10_cm_1_1'\n",
    "model_refined = model + '_refined400'\n",
    "\n",
    "a = pd.Series([model, 400, 'cifar10', 0.2854, 0.9323])\n",
    "c = pd.Series([model, 400, 'cifar10.1', 0.5985, 0.8560])\n",
    "\n",
    "e = pd.Series([model_refined, 50, 'cifar10.1', 0.6063, 0.8830])\n",
    "f = pd.Series([model_refined, 50, 'cifar10', 0.2518, 0.9520])\n",
    "               \n",
    "df_results = pd.concat([a,c,e,f], axis=1).T\n",
    "df_results.columns = ['Model', 'Epoch', 'Testset', 'Loss', 'Accuracy']\n",
    "\n",
    "df_results['Original_Accuracy'] = df_results.apply((lambda row: 95.9 if row[2] == 'cifar10' else 89.7), axis=1)\n",
    "df_results['Original_CI'] = df_results.apply((lambda row: (95.5, 96.3) if row[2] == 'cifar10' else (88.3, 91.0)), axis=1)\n",
    "\n",
    "df_results.to_csv('/home/ec2-user/SageMaker/experiments/' + model + '/results.csv')\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the model checkpoints, configs, and results to S3 \n",
    "bucket='sagemaker-june29'\n",
    "prefix = 'sagemaker/results/original-models/wrn_28_10_cm_1_1'\n",
    "path = '/home/ec2-user/SageMaker/experiments/wrn_28_10_cm_1_1'\n",
    "\n",
    "s3_resource = boto3.resource(\"s3\", region_name=\"us-east-2\")\n",
    "\n",
    "def uploadDirectory(local_path,bucket_name,s3_prefix):\n",
    "\n",
    "    my_bucket = s3_resource.Bucket(bucket_name)\n",
    "    \n",
    "    for path, subdirs, files in os.walk(local_path):\n",
    "        path = path.replace(\"\\\\\",\"/\")\n",
    "        directory_name = path.replace(local_path,\"\")\n",
    "        for file in files:\n",
    "            #print(\"Local File:\", os.path.join(path, file))\n",
    "            #print(\"      Dest:\", s3_prefix+directory_name+'/'+file)\n",
    "            my_bucket.upload_file(os.path.join(path, file), s3_prefix+directory_name+'/'+file)\n",
    "    \n",
    "uploadDirectory(path,bucket,prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
