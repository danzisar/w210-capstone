{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESNET 110\n",
    "- Training Dataset: Unaugmented CIFAR10 Data\n",
    "- Sagemaker Notebook must be of type, conda_pytorch_p36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import numpy \n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 1)) (1.18.1)\n",
      "Requirement already satisfied: torch>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 2)) (1.4.0)\n",
      "Requirement already satisfied: torchvision in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (0.5.0)\n",
      "Requirement already satisfied: fvcore in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (0.1.1.post20200611)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 5)) (4.44.1)\n",
      "Requirement already satisfied: yacs in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 6)) (0.1.7)\n",
      "Requirement already satisfied: apex from git+https://github.com/NVIDIA/apex.git#egg=apex in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 7)) (0.1)\n",
      "Requirement already satisfied: termcolor in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 8)) (1.1.0)\n",
      "Requirement already satisfied: thop<0.0.31.post2004070130 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 9)) (0.0.31.post2001170342)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (1.14.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 3)) (7.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (5.3.1)\n",
      "Requirement already satisfied: portalocker in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (1.7.0)\n",
      "Requirement already satisfied: tabulate in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fvcore->-r /home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt (line 4)) (0.8.7)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Sagemaker Notebook must be of type, conda_pytorch_p36\n",
    "\n",
    "!pip install -r '/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/requirements.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorboard in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (2.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (3.2.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (2.23.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.6.0.post3)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (0.4.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (0.9.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.14.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (46.1.3.post20200330)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.18.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.0.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.17.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (1.29.0)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (3.12.0)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard) (0.34.2)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard) (1.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (2020.4.5.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (1.25.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (3.0.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (4.1.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (0.2.8)\n",
      "Requirement already satisfied: rsa<5.0,>=3.1.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (3.4.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard) (2.2.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.1.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard) (0.4.8)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Need to add this to requirements.txt\n",
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-14 03:50:22] __main__ INFO: \u001b[0mdevice: cuda\n",
      "cudnn:\n",
      "  benchmark: True\n",
      "  deterministic: False\n",
      "dataset:\n",
      "  name: CIFAR10\n",
      "  dataset_dir: ~/.torch/datasets/CIFAR10\n",
      "  image_size: 32\n",
      "  n_channels: 3\n",
      "  n_classes: 10\n",
      "model:\n",
      "  type: cifar\n",
      "  name: resnet\n",
      "  init_mode: kaiming_fan_out\n",
      "  vgg:\n",
      "    n_channels: [64, 128, 256, 512, 512]\n",
      "    n_layers: [2, 2, 3, 3, 3]\n",
      "    use_bn: True\n",
      "  resnet:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "  resnet_preact:\n",
      "    depth: 110\n",
      "    n_blocks: [2, 2, 2, 2]\n",
      "    block_type: basic\n",
      "    initial_channels: 16\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "  wrn:\n",
      "    depth: 28\n",
      "    initial_channels: 16\n",
      "    widening_factor: 10\n",
      "    drop_rate: 0.0\n",
      "  densenet:\n",
      "    depth: 100\n",
      "    n_blocks: [6, 12, 24, 16]\n",
      "    block_type: bottleneck\n",
      "    growth_rate: 12\n",
      "    drop_rate: 0.0\n",
      "    compression_rate: 0.5\n",
      "  pyramidnet:\n",
      "    depth: 272\n",
      "    n_blocks: [3, 24, 36, 3]\n",
      "    initial_channels: 16\n",
      "    block_type: bottleneck\n",
      "    alpha: 200\n",
      "  resnext:\n",
      "    depth: 29\n",
      "    n_blocks: [3, 4, 6, 3]\n",
      "    initial_channels: 64\n",
      "    cardinality: 8\n",
      "    base_channels: 4\n",
      "  shake_shake:\n",
      "    depth: 26\n",
      "    initial_channels: 96\n",
      "    shake_forward: True\n",
      "    shake_backward: True\n",
      "    shake_image: True\n",
      "  se_resnet_preact:\n",
      "    depth: 110\n",
      "    initial_channels: 16\n",
      "    se_reduction: 16\n",
      "    block_type: basic\n",
      "    remove_first_relu: False\n",
      "    add_last_bn: False\n",
      "    preact_stage: [True, True, True]\n",
      "train:\n",
      "  checkpoint: ''\n",
      "  resume: False\n",
      "  precision: O0\n",
      "  batch_size: 128\n",
      "  subdivision: 1\n",
      "  optimizer: sgd\n",
      "  base_lr: 0.1\n",
      "  momentum: 0.9\n",
      "  nesterov: True\n",
      "  weight_decay: 0.0001\n",
      "  no_weight_decay_on_bn: False\n",
      "  gradient_clip: 0\n",
      "  start_epoch: 0\n",
      "  seed: 0\n",
      "  val_first: True\n",
      "  val_period: 1\n",
      "  val_ratio: 0.1\n",
      "  use_test_as_val: False\n",
      "  output_dir: /home/ec2-user/SageMaker/experiments/resnet_basic_110/exp00\n",
      "  log_period: 100\n",
      "  checkpoint_period: 100\n",
      "  use_tensorboard: True\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: True\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "  distributed: False\n",
      "  dist:\n",
      "    backend: nccl\n",
      "    init_method: env://\n",
      "    world_size: -1\n",
      "    node_rank: -1\n",
      "    local_rank: 0\n",
      "    use_sync_bn: False\n",
      "tensorboard:\n",
      "  train_images: False\n",
      "  val_images: False\n",
      "  model_params: False\n",
      "optim:\n",
      "  adam:\n",
      "    betas: (0.9, 0.999)\n",
      "  lars:\n",
      "    eps: 1e-09\n",
      "    threshold: 0.01\n",
      "  adabound:\n",
      "    betas: (0.9, 0.999)\n",
      "    final_lr: 0.1\n",
      "    gamma: 0.001\n",
      "scheduler:\n",
      "  epochs: 160\n",
      "  warmup:\n",
      "    type: none\n",
      "    epochs: 0\n",
      "    start_factor: 0.001\n",
      "    exponent: 4\n",
      "  type: multistep\n",
      "  milestones: [80, 120]\n",
      "  lr_decay: 0.1\n",
      "  lr_min_factor: 0.001\n",
      "  T0: 10\n",
      "  T_mul: 1.0\n",
      "validation:\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    drop_last: False\n",
      "    pin_memory: False\n",
      "    non_blocking: False\n",
      "augmentation:\n",
      "  use_random_crop: True\n",
      "  use_random_horizontal_flip: True\n",
      "  use_cutout: False\n",
      "  use_random_erasing: False\n",
      "  use_dual_cutout: False\n",
      "  use_mixup: False\n",
      "  use_ricap: False\n",
      "  use_cutmix: False\n",
      "  use_label_smoothing: False\n",
      "  random_crop:\n",
      "    padding: 4\n",
      "    fill: 0\n",
      "    padding_mode: constant\n",
      "  random_horizontal_flip:\n",
      "    prob: 0.5\n",
      "  cutout:\n",
      "    prob: 1.0\n",
      "    mask_size: 16\n",
      "    cut_inside: False\n",
      "    mask_color: 0\n",
      "    dual_cutout_alpha: 0.1\n",
      "  random_erasing:\n",
      "    prob: 0.5\n",
      "    area_ratio_range: [0.02, 0.4]\n",
      "    min_aspect_ratio: 0.3\n",
      "    max_attempt: 20\n",
      "  mixup:\n",
      "    alpha: 1.0\n",
      "  ricap:\n",
      "    beta: 0.3\n",
      "  cutmix:\n",
      "    alpha: 1.0\n",
      "  label_smoothing:\n",
      "    epsilon: 0.1\n",
      "tta:\n",
      "  use_resize: False\n",
      "  use_center_crop: False\n",
      "  resize: 256\n",
      "test:\n",
      "  checkpoint: ''\n",
      "  output_dir: ''\n",
      "  batch_size: 256\n",
      "  dataloader:\n",
      "    num_workers: 2\n",
      "    pin_memory: False\n",
      "\u001b[32m[2020-06-14 03:50:22] __main__ INFO: \u001b[0menv_info:\n",
      "  pytorch_version: 1.4.0\n",
      "  cuda_version: 10.1\n",
      "  cudnn_version: 7603\n",
      "  num_gpus: 1\n",
      "  gpu_name: Tesla K80\n",
      "  gpu_capability: 3.7\n",
      "Files already downloaded and verified\n",
      "\u001b[32m[2020-06-14 03:50:25] __main__ INFO: \u001b[0mMACs  : 255.27M\n",
      "\u001b[32m[2020-06-14 03:50:25] __main__ INFO: \u001b[0m#params: 1.73M\n",
      "Selected optimization level O0:  Pure FP32 training.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O0\n",
      "cast_model_type        : torch.float32\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\",)\n",
      "\u001b[32m[2020-06-14 03:50:25] __main__ INFO: \u001b[0mVal 0\n",
      "\u001b[32m[2020-06-14 03:50:29] __main__ INFO: \u001b[0mEpoch 0 loss 18369714543.0016 acc@1 0.0988 acc@5 0.5008\n",
      "\u001b[32m[2020-06-14 03:50:29] __main__ INFO: \u001b[0mElapsed 3.57\n",
      "\u001b[32m[2020-06-14 03:50:29] __main__ INFO: \u001b[0mTrain 1 0\n",
      "\u001b[32m[2020-06-14 03:51:00] __main__ INFO: \u001b[0mEpoch 1 Step 100/351 lr 0.100000 loss 2.3054 (4.1157) acc@1 0.0859 (0.1009) acc@5 0.4766 (0.4989)\n",
      "\u001b[32m[2020-06-14 03:51:30] __main__ INFO: \u001b[0mEpoch 1 Step 200/351 lr 0.100000 loss 2.3047 (3.2117) acc@1 0.1094 (0.1020) acc@5 0.4766 (0.5007)\n",
      "\u001b[32m[2020-06-14 03:52:00] __main__ INFO: \u001b[0mEpoch 1 Step 300/351 lr 0.100000 loss 2.2938 (2.9098) acc@1 0.0938 (0.1030) acc@5 0.5000 (0.5008)\n",
      "\u001b[32m[2020-06-14 03:52:16] __main__ INFO: \u001b[0mEpoch 1 Step 351/351 lr 0.100000 loss 2.3049 (2.8216) acc@1 0.0781 (0.1036) acc@5 0.5078 (0.5004)\n",
      "\u001b[32m[2020-06-14 03:52:16] __main__ INFO: \u001b[0mElapsed 106.95\n",
      "\u001b[32m[2020-06-14 03:52:16] __main__ INFO: \u001b[0mVal 1\n",
      "\u001b[32m[2020-06-14 03:52:19] __main__ INFO: \u001b[0mEpoch 1 loss 2.3060 acc@1 0.1062 acc@5 0.5376\n",
      "\u001b[32m[2020-06-14 03:52:19] __main__ INFO: \u001b[0mElapsed 3.23\n",
      "\u001b[32m[2020-06-14 03:52:19] __main__ INFO: \u001b[0mTrain 2 351\n",
      "\u001b[32m[2020-06-14 03:52:50] __main__ INFO: \u001b[0mEpoch 2 Step 100/351 lr 0.100000 loss 2.3207 (2.3004) acc@1 0.1016 (0.0993) acc@5 0.5234 (0.5045)\n",
      "\u001b[32m[2020-06-14 03:53:20] __main__ INFO: \u001b[0mEpoch 2 Step 200/351 lr 0.100000 loss 2.2812 (2.2977) acc@1 0.1406 (0.1036) acc@5 0.5234 (0.5119)\n",
      "\u001b[32m[2020-06-14 03:53:50] __main__ INFO: \u001b[0mEpoch 2 Step 300/351 lr 0.100000 loss 2.2635 (2.2951) acc@1 0.1562 (0.1075) acc@5 0.5391 (0.5180)\n",
      "\u001b[32m[2020-06-14 03:54:06] __main__ INFO: \u001b[0mEpoch 2 Step 351/351 lr 0.100000 loss 2.2847 (2.2939) acc@1 0.1016 (0.1085) acc@5 0.5625 (0.5232)\n",
      "\u001b[32m[2020-06-14 03:54:06] __main__ INFO: \u001b[0mElapsed 106.94\n",
      "\u001b[32m[2020-06-14 03:54:06] __main__ INFO: \u001b[0mVal 2\n",
      "\u001b[32m[2020-06-14 03:54:09] __main__ INFO: \u001b[0mEpoch 2 loss 2.2680 acc@1 0.1214 acc@5 0.6134\n",
      "\u001b[32m[2020-06-14 03:54:09] __main__ INFO: \u001b[0mElapsed 3.20\n",
      "\u001b[32m[2020-06-14 03:54:09] __main__ INFO: \u001b[0mTrain 3 702\n",
      "\u001b[32m[2020-06-14 03:54:40] __main__ INFO: \u001b[0mEpoch 3 Step 100/351 lr 0.100000 loss 2.1093 (2.2319) acc@1 0.1797 (0.1617) acc@5 0.7578 (0.6438)\n",
      "\u001b[32m[2020-06-14 03:55:10] __main__ INFO: \u001b[0mEpoch 3 Step 200/351 lr 0.100000 loss 1.9671 (2.1607) acc@1 0.1875 (0.1893) acc@5 0.8125 (0.6968)\n",
      "\u001b[32m[2020-06-14 03:55:41] __main__ INFO: \u001b[0mEpoch 3 Step 300/351 lr 0.100000 loss 2.0255 (2.1035) acc@1 0.2109 (0.2092) acc@5 0.8984 (0.7294)\n",
      "\u001b[32m[2020-06-14 03:55:56] __main__ INFO: \u001b[0mEpoch 3 Step 351/351 lr 0.100000 loss 2.0167 (2.0780) acc@1 0.3125 (0.2177) acc@5 0.7969 (0.7421)\n",
      "\u001b[32m[2020-06-14 03:55:56] __main__ INFO: \u001b[0mElapsed 106.95\n",
      "\u001b[32m[2020-06-14 03:55:56] __main__ INFO: \u001b[0mVal 3\n",
      "\u001b[32m[2020-06-14 03:55:59] __main__ INFO: \u001b[0mEpoch 3 loss 1.9073 acc@1 0.2606 acc@5 0.8314\n",
      "\u001b[32m[2020-06-14 03:55:59] __main__ INFO: \u001b[0mElapsed 3.25\n",
      "\u001b[32m[2020-06-14 03:55:59] __main__ INFO: \u001b[0mTrain 4 1053\n",
      "\u001b[32m[2020-06-14 03:56:30] __main__ INFO: \u001b[0mEpoch 4 Step 100/351 lr 0.100000 loss 1.7835 (1.8865) acc@1 0.3125 (0.2861) acc@5 0.8750 (0.8291)\n",
      "\u001b[32m[2020-06-14 03:57:00] __main__ INFO: \u001b[0mEpoch 4 Step 200/351 lr 0.100000 loss 1.6765 (1.8483) acc@1 0.3672 (0.3029) acc@5 0.8828 (0.8396)\n",
      "\u001b[32m[2020-06-14 03:57:31] __main__ INFO: \u001b[0mEpoch 4 Step 300/351 lr 0.100000 loss 1.6900 (1.8152) acc@1 0.3359 (0.3188) acc@5 0.8750 (0.8473)\n",
      "\u001b[32m[2020-06-14 03:57:46] __main__ INFO: \u001b[0mEpoch 4 Step 351/351 lr 0.100000 loss 1.5857 (1.7945) acc@1 0.4062 (0.3272) acc@5 0.8750 (0.8530)\n",
      "\u001b[32m[2020-06-14 03:57:46] __main__ INFO: \u001b[0mElapsed 107.10\n",
      "\u001b[32m[2020-06-14 03:57:46] __main__ INFO: \u001b[0mVal 4\n",
      "\u001b[32m[2020-06-14 03:57:50] __main__ INFO: \u001b[0mEpoch 4 loss 1.8422 acc@1 0.3386 acc@5 0.8680\n",
      "\u001b[32m[2020-06-14 03:57:50] __main__ INFO: \u001b[0mElapsed 3.20\n",
      "\u001b[32m[2020-06-14 03:57:50] __main__ INFO: \u001b[0mTrain 5 1404\n",
      "\u001b[32m[2020-06-14 03:58:20] __main__ INFO: \u001b[0mEpoch 5 Step 100/351 lr 0.100000 loss 1.7391 (1.6524) acc@1 0.3516 (0.3920) acc@5 0.8594 (0.8873)\n",
      "\u001b[32m[2020-06-14 03:58:51] __main__ INFO: \u001b[0mEpoch 5 Step 200/351 lr 0.100000 loss 1.6404 (1.6185) acc@1 0.4297 (0.4055) acc@5 0.8672 (0.8933)\n",
      "\u001b[32m[2020-06-14 03:59:21] __main__ INFO: \u001b[0mEpoch 5 Step 300/351 lr 0.100000 loss 1.4329 (1.5864) acc@1 0.4688 (0.4180) acc@5 0.9297 (0.8996)\n",
      "\u001b[32m[2020-06-14 03:59:37] __main__ INFO: \u001b[0mEpoch 5 Step 351/351 lr 0.100000 loss 1.4204 (1.5713) acc@1 0.5234 (0.4234) acc@5 0.9219 (0.9018)\n",
      "\u001b[32m[2020-06-14 03:59:37] __main__ INFO: \u001b[0mElapsed 107.02\n",
      "\u001b[32m[2020-06-14 03:59:37] __main__ INFO: \u001b[0mVal 5\n",
      "\u001b[32m[2020-06-14 03:59:40] __main__ INFO: \u001b[0mEpoch 5 loss 1.6824 acc@1 0.4242 acc@5 0.8920\n",
      "\u001b[32m[2020-06-14 03:59:40] __main__ INFO: \u001b[0mElapsed 3.20\n",
      "\u001b[32m[2020-06-14 03:59:40] __main__ INFO: \u001b[0mTrain 6 1755\n",
      "\u001b[32m[2020-06-14 04:00:10] __main__ INFO: \u001b[0mEpoch 6 Step 100/351 lr 0.100000 loss 1.4795 (1.4335) acc@1 0.4844 (0.4783) acc@5 0.9062 (0.9209)\n",
      "\u001b[32m[2020-06-14 04:00:41] __main__ INFO: \u001b[0mEpoch 6 Step 200/351 lr 0.100000 loss 1.3322 (1.4157) acc@1 0.5469 (0.4870) acc@5 0.8984 (0.9221)\n",
      "\u001b[32m[2020-06-14 04:01:11] __main__ INFO: \u001b[0mEpoch 6 Step 300/351 lr 0.100000 loss 1.4598 (1.3957) acc@1 0.4922 (0.4949) acc@5 0.9219 (0.9253)\n",
      "\u001b[32m[2020-06-14 04:01:27] __main__ INFO: \u001b[0mEpoch 6 Step 351/351 lr 0.100000 loss 1.2432 (1.3841) acc@1 0.5234 (0.5000) acc@5 0.9375 (0.9270)\n",
      "\u001b[32m[2020-06-14 04:01:27] __main__ INFO: \u001b[0mElapsed 106.91\n",
      "\u001b[32m[2020-06-14 04:01:27] __main__ INFO: \u001b[0mVal 6\n",
      "\u001b[32m[2020-06-14 04:01:30] __main__ INFO: \u001b[0mEpoch 6 loss 1.4114 acc@1 0.5046 acc@5 0.9246\n",
      "\u001b[32m[2020-06-14 04:01:30] __main__ INFO: \u001b[0mElapsed 3.21\n",
      "\u001b[32m[2020-06-14 04:01:30] __main__ INFO: \u001b[0mTrain 7 2106\n",
      "\u001b[32m[2020-06-14 04:02:01] __main__ INFO: \u001b[0mEpoch 7 Step 100/351 lr 0.100000 loss 1.2053 (1.2829) acc@1 0.5469 (0.5373) acc@5 0.9375 (0.9396)\n",
      "\u001b[32m[2020-06-14 04:02:31] __main__ INFO: \u001b[0mEpoch 7 Step 200/351 lr 0.100000 loss 1.2632 (1.2736) acc@1 0.5859 (0.5434) acc@5 0.9141 (0.9407)\n",
      "\u001b[32m[2020-06-14 04:03:01] __main__ INFO: \u001b[0mEpoch 7 Step 300/351 lr 0.100000 loss 1.3133 (1.2541) acc@1 0.5703 (0.5512) acc@5 0.9219 (0.9427)\n"
     ]
    }
   ],
   "source": [
    "# Train the model per the settings specified in the original paper\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "!python train.py --config configs/cifar/resnet.yaml \\\n",
    "    model.resnet.depth 110 \\\n",
    "    train.batch_size 128 \\\n",
    "    train.base_lr 0.1 \\\n",
    "    train.output_dir /home/ec2-user/SageMaker/experiments/resnet_basic_110/exp00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-15 02:01:21] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/resnet_basic_110/exp00/checkpoint_00160.pth\n",
      "Files already downloaded and verified\n",
      "100%|███████████████████████████████████████████| 40/40 [00:08<00:00,  4.97it/s]\n",
      "\u001b[32m[2020-06-15 02:01:30] __main__ INFO: \u001b[0mElapsed 8.05\n",
      "\u001b[32m[2020-06-15 02:01:30] __main__ INFO: \u001b[0mLoss 0.4658 Accuracy 0.9079\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "!python evaluate.py --config configs/cifar/resnet.yaml \\\n",
    "   model.resnet.depth 110 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/resnet_basic_110/exp00/checkpoint_00160.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/resnet_basic_110/exp00/test_results_0160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-15 02:01:40] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/resnet_basic_110/exp00/checkpoint_00100.pth\n",
      "Files already downloaded and verified\n",
      "100%|███████████████████████████████████████████| 40/40 [00:08<00:00,  4.96it/s]\n",
      "\u001b[32m[2020-06-15 02:01:49] __main__ INFO: \u001b[0mElapsed 8.07\n",
      "\u001b[32m[2020-06-15 02:01:49] __main__ INFO: \u001b[0mLoss 0.4113 Accuracy 0.9045\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "!python evaluate.py --config configs/cifar/resnet.yaml \\\n",
    "   model.resnet.depth 110 \\\n",
    "   test.checkpoint /home/ec2-user/SageMaker/experiments/resnet_basic_110/exp00/checkpoint_00100.pth \\\n",
    "   test.output_dir /home/ec2-user/SageMaker/experiments/resnet_basic_110/exp00/test_results_0100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-14 17:18:33] fvcore.common.checkpoint INFO: \u001b[0mLoading checkpoint from /home/ec2-user/SageMaker/experiments/resnet_basic_110/exp00/checkpoint_00160.pth\n",
      "CIFAR 10.1\n",
      "100%|███████████████████████████████████████████| 16/16 [00:02<00:00,  7.30it/s]\n",
      "\u001b[32m[2020-06-14 17:18:35] __main__ INFO: \u001b[0mElapsed 2.19\n",
      "\u001b[32m[2020-06-14 17:18:35] __main__ INFO: \u001b[0mLoss 0.8455 Accuracy 0.8140\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the trained, saved model using the CIFAR 10.1 test dataset \n",
    "# Right the results to the test output directory specified.\n",
    "os.chdir('/home/ec2-user/SageMaker/w210-capstone/models/pytorch_imageclass/')\n",
    "!python evaluate.py --config configs/cifar/resnet.yaml \\\n",
    "    model.resnet.depth 110 \\\n",
    "    test.batch_size 128 \\\n",
    "    dataset.name CIFAR101 \\\n",
    "    test.checkpoint /home/ec2-user/SageMaker/experiments/resnet_basic_110/exp00/checkpoint_00160.pth \\\n",
    "    test.output_dir /home/ec2-user/SageMaker/experiments/resnet_basic_110/exp00/test_results_0160_CIFAR101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Testset</th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Original_Accuracy</th>\n",
       "      <th>Original_CI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>resnet_basic_110</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>100</td>\n",
       "      <td>0.8455</td>\n",
       "      <td>0.9045</td>\n",
       "      <td>93.5</td>\n",
       "      <td>(93.0, 93.9)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>resnet_basic_110</td>\n",
       "      <td>cifar10</td>\n",
       "      <td>160</td>\n",
       "      <td>0.4658</td>\n",
       "      <td>0.9079</td>\n",
       "      <td>93.5</td>\n",
       "      <td>(93.0, 93.9)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>resnet_basic_110</td>\n",
       "      <td>cifar10.1</td>\n",
       "      <td>160</td>\n",
       "      <td>0.8455</td>\n",
       "      <td>0.8140</td>\n",
       "      <td>85.2</td>\n",
       "      <td>(83.5, 86.7)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Model    Testset  Epoch    Loss  Accuracy  Original_Accuracy  \\\n",
       "0  resnet_basic_110    cifar10    100  0.8455    0.9045               93.5   \n",
       "1  resnet_basic_110    cifar10    160  0.4658    0.9079               93.5   \n",
       "2  resnet_basic_110  cifar10.1    160  0.8455    0.8140               85.2   \n",
       "\n",
       "    Original_CI  \n",
       "0  (93.0, 93.9)  \n",
       "1  (93.0, 93.9)  \n",
       "2  (83.5, 86.7)  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write the results to a CSV file so that we can analyze later.\n",
    "import pandas as pd\n",
    "\n",
    "results = {'Model': ['resnet_basic_110', 'resnet_basic_110', 'resnet_basic_110'],\n",
    "           'Testset': ['cifar10', 'cifar10', 'cifar10.1'],\n",
    "           'Epoch': [100, 160, 160],\n",
    "           'Loss': [0.8455, 0.4658, 0.8455],\n",
    "           'Accuracy': [0.9045, 0.9079, 0.8140],\n",
    "           'Original_Accuracy': [93.5, 93.5, 85.2],\n",
    "           'Original_CI': [(93.0, 93.9), (93.0, 93.9), (83.5, 86.7)]\n",
    "           }\n",
    "\n",
    "df = pd.DataFrame(results, columns = ['Model', 'Testset', 'Epoch', 'Loss', 'Accuracy', \n",
    "                                      'Original_Accuracy', 'Original_CI'])\n",
    "\n",
    "\n",
    "df.to_csv('/home/ec2-user/SageMaker/experiments/resnet_basic_110/exp00/results.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['preds', 'probs', 'labels', 'loss', 'acc']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ -7.153804  ,  -0.1832159 ,  -0.69570637, ...,  -0.50926757,\n",
       "         -5.526208  , -12.987257  ],\n",
       "       [  2.862379  ,   7.963458  ,  -6.603018  , ...,  -4.740323  ,\n",
       "         25.90399   ,  -0.52988565],\n",
       "       [  4.25749   ,   8.408992  ,  -4.3299227 , ...,  -2.3715498 ,\n",
       "         13.468082  ,   4.5792727 ],\n",
       "       ...,\n",
       "       [ -4.7270765 ,  -1.2400844 ,   1.3852903 , ...,  -0.51062894,\n",
       "         -3.399443  ,  -2.4969094 ],\n",
       "       [ -2.7640457 ,  14.635863  ,   6.7449965 , ...,  -1.3011913 ,\n",
       "         -3.036379  ,  -7.061736  ],\n",
       "       [ -2.6933427 ,   1.8961854 ,  -3.6396854 , ...,  18.63456   ,\n",
       "         -2.7524152 ,  -3.2204888 ]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Peak inside the output file for predictions\n",
    "import numpy as np\n",
    "output = '/home/ec2-user/SageMaker/experiments/resnet_basic_110/exp00/test_results_0160/predictions.npz'\n",
    "npzfile = np.load(output)\n",
    "print(npzfile.files)\n",
    "npzfile['preds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the model checkpoints, configs, and results to S3 \n",
    "bucket='sagemaker-may29'\n",
    "prefix = 'sagemaker/results/original-models/resnet_basic_110'\n",
    "path = '/home/ec2-user/SageMaker/experiments/resnet_basic_110'\n",
    "\n",
    "s3_resource = boto3.resource(\"s3\", region_name=\"us-east-2\")\n",
    "\n",
    "def uploadDirectory(local_path,bucket_name,s3_prefix):\n",
    "\n",
    "    my_bucket = s3_resource.Bucket(bucket_name)\n",
    "    \n",
    "    for path, subdirs, files in os.walk(local_path):\n",
    "        path = path.replace(\"\\\\\",\"/\")\n",
    "        directory_name = path.replace(local_path,\"\")\n",
    "        for file in files:\n",
    "            #print(\"Local File:\", os.path.join(path, file))\n",
    "            #print(\"      Dest:\", s3_prefix+directory_name+'/'+file)\n",
    "            my_bucket.upload_file(os.path.join(path, file), s3_prefix+directory_name+'/'+file)\n",
    "    \n",
    "uploadDirectory(path,bucket,prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
